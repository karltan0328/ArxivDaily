{"2024-10-31T00:00:00Z":{"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.24220v1","updated":"2024-10-31T17:59:53Z","published":"2024-10-31T17:59:53Z","title":"Bridging Geometric States via Geometric Diffusion Bridge","summary":"  The accurate prediction of geometric state evolution in complex systems is\ncritical for advancing scientific domains such as quantum chemistry and\nmaterial modeling. Traditional experimental and computational methods face\nchallenges in terms of environmental constraints and computational demands,\nwhile current deep learning approaches still fall short in terms of precision\nand generality. In this work, we introduce the Geometric Diffusion Bridge\n(GDB), a novel generative modeling framework that accurately bridges initial\nand target geometric states. GDB leverages a probabilistic approach to evolve\ngeometric state distributions, employing an equivariant diffusion bridge\nderived by a modified version of Doob's $h$-transform for connecting geometric\nstates. This tailored diffusion process is anchored by initial and target\ngeometric states as fixed endpoints and governed by equivariant transition\nkernels. Moreover, trajectory data can be seamlessly leveraged in our GDB\nframework by using a chain of equivariant diffusion bridges, providing a more\ndetailed and accurate characterization of evolution dynamics. Theoretically, we\nconduct a thorough examination to confirm our framework's ability to preserve\njoint distributions of geometric states and capability to completely model the\nunderlying dynamics inducing trajectory distributions with negligible error.\nExperimental evaluations across various real-world scenarios show that GDB\nsurpasses existing state-of-the-art approaches, opening up a new pathway for\naccurately bridging geometric states and tackling crucial scientific challenges\nwith improved accuracy and applicability.\n","authors":["Shengjie Luo","Yixian Xu","Di He","Shuxin Zheng","Tie-Yan Liu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24220v1.pdf","comment":"33 pages, 5 tables; NeurIPS 2024 Camera Ready version"},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24206v1","updated":"2024-10-31T17:58:13Z","published":"2024-10-31T17:58:13Z","title":"Understanding Optimization in Deep Learning with Central Flows","summary":"  Optimization in deep learning remains poorly understood, even in the simple\nsetting of deterministic (i.e. full-batch) training. A key difficulty is that\nmuch of an optimizer's behavior is implicitly determined by complex oscillatory\ndynamics, referred to as the \"edge of stability.\" The main contribution of this\npaper is to show that an optimizer's implicit behavior can be explicitly\ncaptured by a \"central flow:\" a differential equation which models the\ntime-averaged optimization trajectory. We show that these flows can empirically\npredict long-term optimization trajectories of generic neural networks with a\nhigh degree of numerical accuracy. By interpreting these flows, we reveal for\nthe first time 1) the precise sense in which RMSProp adapts to the local loss\nlandscape, and 2) an \"acceleration via regularization\" mechanism, wherein\nadaptive optimizers implicitly navigate towards low-curvature regions in which\nthey can take larger steps. This mechanism is key to the efficacy of these\nadaptive optimizers. Overall, we believe that central flows constitute a\npromising tool for reasoning about optimization in deep learning.\n","authors":["Jeremy M. Cohen","Alex Damian","Ameet Talwalkar","Zico Kolter","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.24206v1.pdf","comment":"first two authors contributed equally; author order determined by\n  coin flip"},{"id":"http://arxiv.org/abs/2410.24205v1","updated":"2024-10-31T17:57:51Z","published":"2024-10-31T17:57:51Z","title":"Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision Probability\n  and Zone Connectivity","summary":"  Path planning in high-dimensional spaces poses significant challenges,\nparticularly in achieving both time efficiency and a fair success rate. To\naddress these issues, we introduce a novel path-planning algorithm, Zonal\nRL-RRT, that leverages kd-tree partitioning to segment the map into zones while\naddressing zone connectivity, ensuring seamless transitions between zones. By\nbreaking down the complex environment into multiple zones and using Q-learning\nas the high-level decision-maker, our algorithm achieves a 3x improvement in\ntime efficiency compared to basic sampling methods such as RRT and RRT* in\nforest-like maps. Our approach outperforms heuristic-guided methods like BIT*\nand Informed RRT* by 1.5x in terms of runtime while maintaining robust and\nreliable success rates across 2D to 6D environments. Compared to learning-based\nmethods like NeuralRRT* and MPNetSMP, as well as the heuristic RRT*J, our\nalgorithm demonstrates, on average, 1.5x better performance in the same\nenvironments. We also evaluate the effectiveness of our approach through\nsimulations of the UR10e arm manipulator in the MuJoCo environment. A key\nobservation of our approach lies in its use of zone partitioning and\nReinforcement Learning (RL) for adaptive high-level planning allowing the\nalgorithm to accommodate flexible policies across diverse environments, making\nit a versatile tool for advanced path planning.\n","authors":["AmirMohammad Tahmasbi","MohammadSaleh Faghfoorian","Saeed Khodaygan","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2410.24205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24203v1","updated":"2024-10-31T17:57:02Z","published":"2024-10-31T17:57:02Z","title":"DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion","summary":"  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n","authors":["Weicai Ye","Chenhao Ji","Zheng Chen","Junyao Gao","Xiaoshui Huang","Song-Hai Zhang","Wanli Ouyang","Tong He","Cairong Zhao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24203v1.pdf","comment":"NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:\n  https://github.com/zju3dv/DiffPano"},{"id":"http://arxiv.org/abs/2410.24200v1","updated":"2024-10-31T17:55:36Z","published":"2024-10-31T17:55:36Z","title":"Length-Induced Embedding Collapse in Transformer-based Models","summary":"  Text embeddings enable various applications, but their performance\ndeteriorates on longer texts. In this paper, we find that the performance\ndegradation is due to a phenomenon called Length Collapse, where longer text\nembeddings collapse into a narrow space. This collapse results in a\ndistributional inconsistency between embeddings of different text lengths,\nultimately hurting the performance of downstream tasks. Theoretically, by\nconsidering the self-attention mechanism inherently functions as a low-pass\nfilter, we prove that long sequences increase the attenuation rate of the\nlow-pass filter effect of the self-attention mechanism. With layers going\ndeeper, excessive low-pass filtering causes the token signals to retain only\ntheir Direct-Current (DC) component, which means the input token feature maps\nwill collapse into a narrow space, especially in long texts. Based on the above\nanalysis, we propose to mitigate the undesirable length collapse limitation by\nintroducing a temperature in softmax(), which achieves a higher low-filter\nattenuation rate. The tuning-free method, called TempScale, can be plugged into\nmultiple transformer-based embedding models. Empirically, we demonstrate that\nTempScale can improve existing embedding models, especially on long text\ninputs, bringing up to 0.53% performance gains on 40 datasets from Massive Text\nEmbedding Benchmark (MTEB) and 0.82% performance gains on 4 datasets from\nLongEmbed, which specifically focuses on long context retrieval.\n","authors":["Yuqi Zhou","Sunhao Dai","Zhanshuo Cao","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24187v1","updated":"2024-10-31T17:49:44Z","published":"2024-10-31T17:49:44Z","title":"Chasing Better Deep Image Priors between Over- and\n  Under-parameterization","summary":"  Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep\nimage priors (DIP) that regularize various image inverse problems. Meanwhile,\nresearchers also proposed extremely compact, under-parameterized image priors\n(e.g., deep decoder) that are strikingly competent for image restoration too,\ndespite a loss of accuracy. These two extremes push us to think whether there\nexists a better solution in the middle: between over- and under-parameterized\nimage priors, can one identify \"intermediate\" parameterized image priors that\nachieve better trade-offs between performance, efficiency, and even preserving\nstrong transferability? Drawing inspirations from the lottery ticket hypothesis\n(LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by\nexploiting DNN inherent sparsity, stated as: given an over-parameterized\nDNN-based image prior, it will contain a sparse subnetwork that can be trained\nin isolation, to match the original DNN's performance when being applied as a\nprior to various image inverse problems. Our results validate the superiority\nof LIPs: we can successfully locate the LIP subnetworks from over-parameterized\nDIPs at substantial sparsity ranges. Those LIP subnetworks significantly\noutperform deep decoders under comparably compact model sizes (by often fully\npreserving the effectiveness of their over-parameterized counterparts), and\nthey also possess high transferability across different images as well as\nrestoration task types. Besides, we also extend LIP to compressive sensing\nimage reconstruction, where a pre-trained GAN generator is used as the prior\n(in contrast to untrained DIP or deep decoder), and confirm its validity in\nthis setting too. To our best knowledge, this is the first time that LTH is\ndemonstrated to be relevant in the context of inverse problems or image priors.\n","authors":["Qiming Wu","Xiaohan Chen","Yifan Jiang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24187v1.pdf","comment":"Codes are available at\n  https://github.com/VITA-Group/Chasing-Better-DIPs"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2306.01953v3","updated":"2024-10-31T17:47:21Z","published":"2023-06-02T23:29:28Z","title":"Invisible Image Watermarks Are Provably Removable Using Generative AI","summary":"  Invisible watermarks safeguard images' copyrights by embedding hidden\nmessages only detectable by owners. They also prevent people from misusing\nimages, especially those generated by AI models. We propose a family of\nregeneration attacks to remove these invisible watermarks. The proposed attack\nmethod first adds random noise to an image to destroy the watermark and then\nreconstructs the image. This approach is flexible and can be instantiated with\nmany existing image-denoising algorithms and pre-trained generative models such\nas diffusion models. Through formal proofs and extensive empirical evaluations,\nwe demonstrate that pixel-level invisible watermarks are vulnerable to this\nregeneration attack. Our results reveal that, across four different pixel-level\nwatermarking schemes, the proposed method consistently achieves superior\nperformance compared to existing attack techniques, with lower detection rates\nand higher image quality. However, watermarks that keep the image semantically\nsimilar can be an alternative defense against our attacks. Our finding\nunderscores the need for a shift in research/industry emphasis from invisible\nwatermarks to semantic-preserving watermarks. Code is available at\nhttps://github.com/XuandongZhao/WatermarkAttacker\n","authors":["Xuandong Zhao","Kexun Zhang","Zihao Su","Saastha Vasan","Ilya Grishchenko","Christopher Kruegel","Giovanni Vigna","Yu-Xiang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2306.01953v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24175v1","updated":"2024-10-31T17:42:26Z","published":"2024-10-31T17:42:26Z","title":"Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models","summary":"  Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.\n","authors":["Yunjia Qi","Hao Peng","Xiaozhi Wang","Bin Xu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2410.24175v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.04690v3","updated":"2024-10-31T17:32:26Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v3.pdf","comment":"To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.10796v2","updated":"2024-10-31T17:29:37Z","published":"2024-06-16T03:45:03Z","title":"Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction\n  Data","summary":"  A major challenge in materials science is the determination of the structure\nof nanometer sized objects. Here we present a novel approach that uses a\ngenerative machine learning model based on diffusion processes that is trained\non 45,229 known structures. The model factors both the measured diffraction\npattern as well as relevant statistical priors on the unit cell of atomic\ncluster structures. Conditioned only on the chemical formula and the\ninformation-scarce finite-size broadened powder diffraction pattern, we find\nthat our model, PXRDnet, can successfully solve simulated nanocrystals as small\nas 10 angstroms across 200 materials of varying symmetry and complexity,\nincluding structures from all seven crystal systems. We show that our model can\nsuccessfully and verifiably determine structural candidates four out of five\ntimes, with average error among these candidates being only 7% (as measured by\npost-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solving\nstructures from noisy diffraction patterns gathered in real-world experiments.\nWe suggest that data driven approaches, bootstrapped from theoretical\nsimulation, will ultimately provide a path towards determining the structure of\npreviously unsolved nano-materials.\n","authors":["Gabe Guo","Tristan Saidi","Maxwell Terban","Michele Valsecchi","Simon JL Billinge","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2406.10796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17005v2","updated":"2024-10-31T17:21:13Z","published":"2024-09-25T15:08:08Z","title":"Models Can and Should Embrace the Communicative Nature of\n  Human-Generated Math","summary":"  Math is constructed by people for people: just as natural language corpora\nreflect not just propositions but the communicative goals of language users,\nthe math data that models are trained on reflects not just idealized\nmathematical entities but rich communicative intentions. While there are\nimportant advantages to treating math in a purely symbolic manner, we here\nhypothesize that there are benefits to treating math as situated linguistic\ncommunication and that language models are well suited for this goal, in ways\nthat are not fully appreciated. We illustrate these points with two case\nstudies. First, we ran an experiment in which we found that language models\ninterpret the equals sign in a humanlike way -- generating systematically\ndifferent word problems for the same underlying equation arranged in different\nways. Second, we found that language models prefer proofs to be ordered in\nnaturalistic ways, even though other orders would be logically equivalent. We\nadvocate for AI systems that learn from and represent the communicative\nintentions latent in human-generated math.\n","authors":["Sasha Boguraev","Ben Lipkin","Leonie Weissweiler","Kyle Mahowald"],"pdf_url":"https://arxiv.org/pdf/2409.17005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12747v2","updated":"2024-10-31T17:18:16Z","published":"2024-06-18T16:07:33Z","title":"TSI-Bench: Benchmarking Time Series Imputation","summary":"  Effective imputation is a crucial preprocessing step for time series\nanalysis. Despite the development of numerous deep learning algorithms for time\nseries imputation, the community lacks standardized and comprehensive benchmark\nplatforms to effectively evaluate imputation performance across different\nsettings. Moreover, although many deep learning forecasting algorithms have\ndemonstrated excellent performance, whether their modelling achievements can be\ntransferred to time series imputation tasks remains unexplored. To bridge these\ngaps, we develop TSI-Bench, the first (to our knowledge) comprehensive\nbenchmark suite for time series imputation utilizing deep learning techniques.\nThe TSI-Bench pipeline standardizes experimental settings to enable fair\nevaluation of imputation algorithms and identification of meaningful insights\ninto the influence of domain-appropriate missing rates and patterns on model\nperformance. Furthermore, TSI-Bench innovatively provides a systematic paradigm\nto tailor time series forecasting algorithms for imputation purposes. Our\nextensive study across 34,804 experiments, 28 algorithms, and 8 datasets with\ndiverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse\ndownstream tasks and potential to unlock future directions in time series\nimputation research and analysis. All source code and experiment logs are\nreleased at https://github.com/WenjieDu/AwesomeImputation.\n","authors":["Wenjie Du","Jun Wang","Linglong Qian","Yiyuan Yang","Zina Ibrahim","Fanxing Liu","Zepu Wang","Haoxin Liu","Zhiyuan Zhao","Yingjie Zhou","Wenjia Wang","Kaize Ding","Yuxuan Liang","B. Aditya Prakash","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.12747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22382v2","updated":"2024-10-31T17:12:27Z","published":"2024-10-29T12:54:55Z","title":"Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference","summary":"  Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.\n","authors":["Chris Lam"],"pdf_url":"https://arxiv.org/pdf/2410.22382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07963v3","updated":"2024-10-31T17:05:49Z","published":"2024-02-12T10:32:47Z","title":"SPO: Sequential Monte Carlo Policy Optimisation","summary":"  Leveraging planning during learning and decision-making is central to the\nlong-term development of intelligent agents. Recent works have successfully\ncombined tree-based search methods and self-play learning mechanisms to this\nend. However, these methods typically face scaling challenges due to the\nsequential nature of their search. While practical engineering solutions can\npartly overcome this, they often result in a negative impact on performance. In\nthis paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a\nmodel-based reinforcement learning algorithm grounded within the Expectation\nMaximisation (EM) framework. We show that SPO provides robust policy\nimprovement and efficient scaling properties. The sample-based search makes it\ndirectly applicable to both discrete and continuous action spaces without\nmodifications. We demonstrate statistically significant improvements in\nperformance relative to model-free and model-based baselines across both\ncontinuous and discrete environments. Furthermore, the parallel nature of SPO's\nsearch enables effective utilisation of hardware accelerators, yielding\nfavourable scaling laws.\n","authors":["Matthew V Macfarlane","Edan Toledo","Donal Byrne","Paul Duckworth","Alexandre Laterre"],"pdf_url":"https://arxiv.org/pdf/2402.07963v3.pdf","comment":"Accepted to NeurIPS 2024. 34 pages, 3 main figures"},{"id":"http://arxiv.org/abs/2407.01079v3","updated":"2024-10-31T16:59:13Z","published":"2024-07-01T08:34:40Z","title":"On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)","summary":"  We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data.\n","authors":["Jerry Yao-Chieh Hu","Weimin Wu","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01079v3.pdf","comment":"Accepted at NeurIPS 2024. v3 updated to camera-ready version with\n  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications"},{"id":"http://arxiv.org/abs/2407.01903v2","updated":"2024-10-31T16:49:26Z","published":"2024-07-02T03:08:20Z","title":"Text-Aware Diffusion for Policy Learning","summary":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment, without having access to any in-domain\ndemonstrations.\n","authors":["Calvin Luo","Mandy He","Zilai Zeng","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04823v2","updated":"2024-10-31T16:48:51Z","published":"2024-06-07T10:48:45Z","title":"BERTs are Generative In-Context Learners","summary":"  While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives.\n","authors":["David Samuel"],"pdf_url":"https://arxiv.org/pdf/2406.04823v2.pdf","comment":"26 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24119v1","updated":"2024-10-31T16:48:41Z","published":"2024-10-31T16:48:41Z","title":"Leveraging Large Language Models for Code Translation and Software\n  Development in Scientific Computing","summary":"  The emergence of foundational models and generative artificial intelligence\n(GenAI) is poised to transform productivity in scientific computing, especially\nin code development, refactoring, and translating from one programming language\nto another. However, because the output of GenAI cannot be guaranteed to be\ncorrect, manual intervention remains necessary. Some of this intervention can\nbe automated through task-specific tools, alongside additional methodologies\nfor correctness verification and effective prompt development. We explored the\napplication of GenAI in assisting with code translation, language\ninteroperability, and codebase inspection within a legacy Fortran codebase used\nto simulate particle interactions at the Large Hadron Collider (LHC). In the\nprocess, we developed a tool, CodeScribe, which combines prompt engineering\nwith user supervision to establish an efficient process for code conversion. In\nthis paper, we demonstrate how CodeScribe assists in converting Fortran code to\nC++, generating Fortran-C APIs for integrating legacy systems with modern C++\nlibraries, and providing developer support for code organization and algorithm\nimplementation. We also address the challenges of AI-driven code translation\nand highlight its benefits for enhancing productivity in scientific computing\nworkflows.\n","authors":["Akash Dhruv","Anshu Dubey"],"pdf_url":"https://arxiv.org/pdf/2410.24119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24116v1","updated":"2024-10-31T16:46:23Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization","summary":"  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"https://arxiv.org/pdf/2410.24116v1.pdf","comment":"19 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.24114v1","updated":"2024-10-31T16:44:10Z","published":"2024-10-31T16:44:10Z","title":"Nearest Neighbor Normalization Improves Multimodal Retrieval","summary":"  Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.\n","authors":["Neil Chowdhury","Franklin Wang","Sumedh Shenoy","Douwe Kiela","Sarah Schwettmann","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2410.24114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24108v1","updated":"2024-10-31T16:38:51Z","published":"2024-10-31T16:38:51Z","title":"Reinforcement Learning Gradients as Vitamin for Online Finetuning\n  Decision Transformers","summary":"  Decision Transformers have recently emerged as a new and compelling paradigm\nfor offline Reinforcement Learning (RL), completing a trajectory in an\nautoregressive way. While improvements have been made to overcome initial\nshortcomings, online finetuning of decision transformers has been surprisingly\nunder-explored. The widely adopted state-of-the-art Online Decision Transformer\n(ODT) still struggles when pretrained with low-reward offline data. In this\npaper, we theoretically analyze the online-finetuning of the decision\ntransformer, showing that the commonly used Return-To-Go (RTG) that's far from\nthe expected return hampers the online fine-tuning process. This problem,\nhowever, is well-addressed by the value function and advantage of standard RL\nalgorithms. As suggested by our analysis, in our experiments, we hence find\nthat simply adding TD3 gradients to the finetuning process of ODT effectively\nimproves the online finetuning performance of ODT, especially if ODT is\npretrained with low-reward offline data. These findings provide new directions\nto further improve decision transformers.\n","authors":["Kai Yan","Alexander G. Schwing","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24108v1.pdf","comment":"Accepted as NeurIPS 2024 spotlight. 33 pages, 26 figures"},{"id":"http://arxiv.org/abs/2410.24091v1","updated":"2024-10-31T16:22:53Z","published":"2024-10-31T16:22:53Z","title":"3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing","summary":"  Tactile and visual perception are both crucial for humans to perform\nfine-grained interactions with their environment. Developing similar\nmulti-modal sensing capabilities for robots can significantly enhance and\nexpand their manipulation skills. This paper introduces \\textbf{3D-ViTac}, a\nmulti-modal sensing and learning system designed for dexterous bimanual\nmanipulation. Our system features tactile sensors equipped with dense sensing\nunits, each covering an area of 3$mm^2$. These sensors are low-cost and\nflexible, providing detailed and extensive coverage of physical contacts,\neffectively complementing visual information. To integrate tactile and visual\ndata, we fuse them into a unified 3D representation space that preserves their\n3D structures and spatial relationships. The multi-modal representation can\nthen be coupled with diffusion policies for imitation learning. Through\nconcrete hardware experiments, we demonstrate that even low-cost robots can\nperform precise manipulations and significantly outperform vision-only\npolicies, particularly in safe interactions with fragile items and executing\nlong-horizon tasks involving in-hand manipulation. Our project page is\navailable at \\url{https://binghao-huang.github.io/3D-ViTac/}.\n","authors":["Binghao Huang","Yixuan Wang","Xinyi Yang","Yiyue Luo","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.24091v1.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.24087v1","updated":"2024-10-31T16:20:04Z","published":"2024-10-31T16:20:04Z","title":"In-Context Fine-Tuning for Time-Series Foundation Models","summary":"  Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.\n","authors":["Abhimanyu Das","Matthew Faw","Rajat Sen","Yichen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.24087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24080v1","updated":"2024-10-31T16:16:51Z","published":"2024-10-31T16:16:51Z","title":"Graph Learning for Numeric Planning","summary":"  Graph learning is naturally well suited for use in symbolic, object-centric\nplanning due to its ability to exploit relational structures exhibited in\nplanning domains and to take as input planning instances with arbitrary numbers\nof objects. Numeric planning is an extension of symbolic planning in which\nstates may now also exhibit numeric variables. In this work, we propose\ndata-efficient and interpretable machine learning models for learning to solve\nnumeric planning tasks. This involves constructing a new graph kernel for\ngraphs with both continuous and categorical attributes, as well as new\noptimisation methods for learning heuristic functions for numeric planning.\nExperiments show that our graph kernels are vastly more efficient and\ngeneralise better than graph neural networks for numeric planning, and also\nyield competitive coverage performance compared to domain-independent numeric\nplanners. Code is available at https://github.com/DillonZChen/goose\n","authors":["Dillon Z. Chen","Sylvie Thi√©baux"],"pdf_url":"https://arxiv.org/pdf/2410.24080v1.pdf","comment":"Extended version of NeurIPS 2024 paper"},{"id":"http://arxiv.org/abs/2311.18760v3","updated":"2024-10-31T16:12:16Z","published":"2023-11-30T18:02:44Z","title":"TaskBench: Benchmarking Large Language Models for Task Automation","summary":"  In recent years, the remarkable progress of large language models (LLMs) has\nsparked interest in task automation, which involves decomposing complex tasks\ndescribed by user instructions into sub-tasks and invoking external tools to\nexecute them, playing a central role in autonomous agents. However, there is a\nlack of systematic and standardized benchmarks to promote the development of\nLLMs in task automation. To address this, we introduce TaskBench, a\ncomprehensive framework to evaluate the capability of LLMs in task automation.\nSpecifically, task automation can be divided into three critical stages: task\ndecomposition, tool selection, and parameter prediction. To tackle the\ncomplexities inherent in these stages, we introduce the concept of Tool Graph\nto represent decomposed tasks and adopt a back-instruct method to generate\nhigh-quality user instructions. We propose TaskEval, a multi-faceted evaluation\nmethodology that assesses LLM performance across these three stages. Our\napproach combines automated construction with rigorous human verification,\nensuring high consistency with human evaluation. Experimental results\ndemonstrate that TaskBench effectively reflects the capabilities of various\nLLMs in task automation. It provides insights into model performance across\ndifferent task complexities and domains, pushing the boundaries of what current\nmodels can achieve. TaskBench offers a scalable, adaptable, and reliable\nbenchmark for advancing LLM-based autonomous agents.\n","authors":["Yongliang Shen","Kaitao Song","Xu Tan","Wenqi Zhang","Kan Ren","Siyu Yuan","Weiming Lu","Dongsheng Li","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.18760v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24070v1","updated":"2024-10-31T16:07:21Z","published":"2024-10-31T16:07:21Z","title":"Dynamical similarity analysis uniquely captures how computations develop\n  in RNNs","summary":"  Methods for analyzing representations in neural systems are increasingly\npopular tools in neuroscience and mechanistic interpretability. Measures\ncomparing neural activations across conditions, architectures, and species give\nscalable ways to understand information transformation within different neural\nnetworks. However, recent findings show that some metrics respond to spurious\nsignals, leading to misleading results. Establishing benchmark test cases is\nthus essential for identifying the most reliable metric and potential\nimprovements. We propose that compositional learning in recurrent neural\nnetworks (RNNs) can provide a test case for dynamical representation alignment\nmetrics. Implementing this case allows us to evaluate if metrics can identify\nrepresentations that develop throughout learning and determine if\nrepresentations identified by metrics reflect the network's actual\ncomputations. Building both attractor and RNN based test cases, we show that\nthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robust\nand reliably identifies behaviorally relevant representations compared to prior\nmetrics (Procrustes, CKA). We also demonstrate how such test cases can extend\nbeyond metric evaluation to study new architectures. Specifically, testing DSA\nin modern (Mamba) state space models suggests that these models, unlike RNNs,\nmay not require changes in recurrent dynamics due to their expressive hidden\nstates. Overall, we develop test cases that showcase how DSA's enhanced ability\nto detect dynamical motifs makes it highly effective for identifying ongoing\ncomputations in RNNs and revealing how networks learn tasks.\n","authors":["Quentin Guilhot","Jascha Achterberg","Micha≈Ç W√≥jcik","Rui Ponte Costa"],"pdf_url":"https://arxiv.org/pdf/2410.24070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02119v3","updated":"2024-10-31T15:57:42Z","published":"2023-12-04T18:49:23Z","title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","summary":"  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.\n","authors":["Anay Mehrotra","Manolis Zampetakis","Paul Kassianik","Blaine Nelson","Hyrum Anderson","Yaron Singer","Amin Karbasi"],"pdf_url":"https://arxiv.org/pdf/2312.02119v3.pdf","comment":"Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP"},{"id":"http://arxiv.org/abs/2410.24059v1","updated":"2024-10-31T15:56:50Z","published":"2024-10-31T15:56:50Z","title":"Identifying General Mechanism Shifts in Linear Causal Representations","summary":"  We consider the linear causal representation learning setting where we\nobserve a linear mixing of $d$ unknown latent factors, which follow a linear\nstructural causal model. Recent work has shown that it is possible to recover\nthe latent factors as well as the underlying structural causal model over them,\nup to permutation and scaling, provided that we have at least $d$ environments,\neach of which corresponds to perfect interventions on a single latent node\n(factor). After this powerful result, a key open problem faced by the community\nhas been to relax these conditions: allow for coarser than perfect single-node\ninterventions, and allow for fewer than $d$ of them, since the number of latent\nfactors $d$ could be very large. In this work, we consider precisely such a\nsetting, where we allow a smaller than $d$ number of environments, and also\nallow for very coarse interventions that can very coarsely \\textit{change the\nentire causal graph over the latent factors}. On the flip side, we relax what\nwe wish to extract to simply the \\textit{list of nodes that have shifted\nbetween one or more environments}. We provide a surprising identifiability\nresult that it is indeed possible, under some very mild standard assumptions,\nto identify the set of shifted nodes. Our identifiability proof moreover is a\nconstructive one: we explicitly provide necessary and sufficient conditions for\na node to be a shifted node, and show that we can check these conditions given\nobserved data. Our algorithm lends itself very naturally to the sample setting\nwhere instead of just interventional distributions, we are provided datasets of\nsamples from each of these distributions. We corroborate our results on both\nsynthetic experiments as well as an interesting psychometric dataset. The code\ncan be found at https://github.com/TianyuCodings/iLCS.\n","authors":["Tianyu Chen","Kevin Bello","Francesco Locatello","Bryon Aragam","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.24059v1.pdf","comment":"NeuIPS 2024"},{"id":"http://arxiv.org/abs/2409.18946v2","updated":"2024-10-31T15:53:15Z","published":"2024-09-27T17:46:05Z","title":"Unconditional stability of a recurrent neural circuit implementing\n  divisive normalization","summary":"  Stability in recurrent neural models poses a significant challenge,\nparticularly in developing biologically plausible neurodynamical models that\ncan be seamlessly trained. Traditional cortical circuit models are notoriously\ndifficult to train due to expansive nonlinearities in the dynamical system,\nleading to an optimization problem with nonlinear stability constraints that\nare difficult to impose. Conversely, recurrent neural networks (RNNs) excel in\ntasks involving sequential data but lack biological plausibility and\ninterpretability. In this work, we address these challenges by linking dynamic\ndivisive normalization (DN) to the stability of ORGaNICs, a biologically\nplausible recurrent cortical circuit model that dynamically achieves DN and\nthat has been shown to simulate a wide range of neurophysiological phenomena.\nBy using the indirect method of Lyapunov, we prove the remarkable property of\nunconditional local stability for an arbitrary-dimensional ORGaNICs circuit\nwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to a\nsystem of coupled damped harmonic oscillators, which enables us to derive the\ncircuit's energy function, providing a normative principle of what the circuit,\nand individual neurons, aim to accomplish. Further, for a generic recurrent\nweight matrix, we prove the stability of the 2D model and demonstrate\nempirically that stability holds in higher dimensions. Finally, we show that\nORGaNICs can be trained by backpropagation through time without gradient\nclipping/scaling, thanks to its intrinsic stability property and adaptive time\nconstants, which address the problems of exploding, vanishing, and oscillating\ngradients. By evaluating the model's performance on RNN benchmarks, we find\nthat ORGaNICs outperform alternative neurodynamical models on static image\nclassification tasks and perform comparably to LSTMs on sequential tasks.\n","authors":["Shivang Rawat","David J. Heeger","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2409.18946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08557v2","updated":"2024-10-31T15:52:52Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study reviews recent developments in low-light pedestrian\ndetection approaches. It systematically categorizes and analyses various\nalgorithms from region-based to non-region-based and graph-based learning\nmethodologies by highlighting their methodologies, implementation issues, and\nchallenges. It also outlines the key benchmark datasets that can be used for\nresearch and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations.\n","authors":["Thangarajah Akilan","Hrishikesh Vachhani"],"pdf_url":"https://arxiv.org/pdf/2311.08557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22114v2","updated":"2024-10-31T15:34:35Z","published":"2024-10-29T15:16:02Z","title":"Policy Gradient for Robust Markov Decision Processes","summary":"  We develop a generic policy gradient method with the global optimality\nguarantee for robust Markov Decision Processes (MDPs). While policy gradient\nmethods are widely used for solving dynamic decision problems due to their\nscalable and efficient nature, adapting these methods to account for model\nambiguity has been challenging, often making it impractical to learn robust\npolicies. This paper introduces a novel policy gradient method, Double-Loop\nRobust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a\ngeneral mirror descent update rule for the policy optimization with adaptive\ntolerance per iteration, guaranteeing convergence to a globally optimal policy.\nWe provide a comprehensive analysis of DRPMD, including new convergence results\nunder both direct and softmax parameterizations, and provide novel insights\ninto the inner problem solution through Transition Mirror Ascent (TMA).\nAdditionally, we propose innovative parametric transition kernels for both\ndiscrete and continuous state-action spaces, broadening the applicability of\nour approach. Empirical results validate the robustness and global convergence\nof DRPMD across various challenging robust MDP settings.\n","authors":["Qiuhao Wang","Shaohang Xu","Chin Pang Ho","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2410.22114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24035v1","updated":"2024-10-31T15:32:32Z","published":"2024-10-31T15:32:32Z","title":"State- and context-dependent robotic manipulation and grasping via\n  uncertainty-aware imitation learning","summary":"  Generating context-adaptive manipulation and grasping actions is a\nchallenging problem in robotics. Classical planning and control algorithms tend\nto be inflexible with regard to parameterization by external variables such as\nobject shapes. In contrast, Learning from Demonstration (LfD) approaches, due\nto their nature as function approximators, allow for introducing external\nvariables to modulate policies in response to the environment. In this paper,\nwe utilize this property by introducing an LfD approach to acquire\ncontext-dependent grasping and manipulation strategies. We treat the problem as\na kernel-based function approximation, where the kernel inputs include generic\ncontext variables describing task-dependent parameters such as the object\nshape. We build on existing work on policy fusion with uncertainty\nquantification to propose a state-dependent approach that automatically returns\nto demonstrations, avoiding unpredictable behavior while smoothly adapting to\ncontext changes. The approach is evaluated against the LASA handwriting dataset\nand on a real 7-DoF robot in two scenarios: adaptation to slippage while\ngrasping and manipulating a deformable food item.\n","authors":["Tim R. Winter","Ashok M. Sundaram","Werner Friedl","Maximo A. Roa","Freek Stulp","Jo√£o Silv√©rio"],"pdf_url":"https://arxiv.org/pdf/2410.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24032v1","updated":"2024-10-31T15:30:55Z","published":"2024-10-31T15:30:55Z","title":"Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks","summary":"  The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.\n","authors":["Yingzhe Peng","Xiaoting Qin","Zhiyang Zhang","Jue Zhang","Qingwei Lin","Xu Yang","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24031v1","updated":"2024-10-31T15:29:51Z","published":"2024-10-31T15:29:51Z","title":"A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps","summary":"  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n","authors":["Ariel Larey","Eyal Rond","Omer Achrack"],"pdf_url":"https://arxiv.org/pdf/2410.24031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24024v1","updated":"2024-10-31T15:25:20Z","published":"2024-10-31T15:25:20Z","title":"AndroidLab: Training and Systematic Benchmarking of Android Autonomous\n  Agents","summary":"  Autonomous agents have become increasingly important for interacting with the\nreal world. Android agents, in particular, have been recently a\nfrequently-mentioned interaction method. However, existing studies for training\nand evaluating Android agents lack systematic research on both open-source and\nclosed-source models. In this work, we propose AndroidLab as a systematic\nAndroid agent framework. It includes an operation environment with different\nmodalities, action space, and a reproducible benchmark. It supports both large\nlanguage models (LLMs) and multimodal models (LMMs) in the same action space.\nAndroidLab benchmark includes predefined Android virtual devices and 138 tasks\nacross nine apps built on these devices. By using the AndroidLab environment,\nwe develop an Android Instruction dataset and train six open-source LLMs and\nLMMs, lifting the average success rates from 4.59\\% to 21.50\\% for LLMs and\nfrom 1.93\\% to 13.28\\% for LMMs. AndroidLab is open-sourced and publicly\navailable at \\url{https://github.com/THUDM/Android-Lab}.\n","authors":["Yifan Xu","Xiao Liu","Xueqiao Sun","Siyi Cheng","Hao Yu","Hanyu Lai","Shudan Zhang","Dan Zhang","Jie Tang","Yuxiao Dong"],"pdf_url":"https://arxiv.org/pdf/2410.24024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24017v1","updated":"2024-10-31T15:19:33Z","published":"2024-10-31T15:19:33Z","title":"Assessing the Impact of Packing on Machine Learning-Based Malware\n  Detection and Classification Systems","summary":"  The proliferation of malware, particularly through the use of packing,\npresents a significant challenge to static analysis and signature-based malware\ndetection techniques. The application of packing to the original executable\ncode renders extracting meaningful features and signatures challenging. To deal\nwith the increasing amount of malware in the wild, researchers and anti-malware\ncompanies started harnessing machine learning capabilities with very promising\nresults. However, little is known about the effects of packing on static\nmachine learning-based malware detection and classification systems. This work\naddresses this gap by investigating the impact of packing on the performance of\nstatic machine learning-based models used for malware detection and\nclassification, with a particular focus on those using visualisation\ntechniques. To this end, we present a comprehensive analysis of various packing\ntechniques and their effects on the performance of machine learning-based\ndetectors and classifiers. Our findings highlight the limitations of current\nstatic detection and classification systems and underscore the need to be\nproactive to effectively counteract the evolving tactics of malware authors.\n","authors":["Daniel Gibert","Nikolaos Totosis","Constantinos Patsakis","Giulio Zizzo","Quan Le"],"pdf_url":"https://arxiv.org/pdf/2410.24017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23996v1","updated":"2024-10-31T14:57:31Z","published":"2024-10-31T14:57:31Z","title":"An Information Criterion for Controlled Disentanglement of Multimodal\n  Data","summary":"  Multimodal representation learning seeks to relate and decompose information\ninherent in multiple modalities. By disentangling modality-specific information\nfrom information that is shared across modalities, we can improve\ninterpretability and robustness and enable downstream tasks such as the\ngeneration of counterfactual outcomes. Separating the two types of information\nis challenging since they are often deeply entangled in many real-world\napplications. We propose Disentangled Self-Supervised Learning\n(DisentangledSSL), a novel self-supervised approach for learning disentangled\nrepresentations. We present a comprehensive analysis of the optimality of each\ndisentangled representation, particularly focusing on the scenario not covered\nin prior work where the so-called Minimum Necessary Information (MNI) point is\nnot attainable. We demonstrate that DisentangledSSL successfully learns shared\nand modality-specific features on multiple synthetic and real-world datasets\nand consistently outperforms baselines on various downstream tasks, including\nprediction tasks for vision-language data, as well as molecule-phenotype\nretrieval tasks for biological data.\n","authors":["Chenyu Wang","Sharut Gupta","Xinyi Zhang","Sana Tonekaboni","Stefanie Jegelka","Tommi Jaakkola","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2410.23996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01158v3","updated":"2024-10-31T14:53:43Z","published":"2023-06-01T21:31:59Z","title":"Heterogeneous Knowledge for Augmented Modular Reinforcement Learning","summary":"  Existing modular Reinforcement Learning (RL) architectures are generally\nbased on reusable components, also allowing for \"plug-and-play\" integration.\nHowever, these modules are homogeneous in nature - in fact, they essentially\nprovide policies obtained via RL through the maximization of individual reward\nfunctions. Consequently, such solutions still lack the ability to integrate and\nprocess multiple types of information (i.e., heterogeneous knowledge\nrepresentations), such as rules, sub-goals, and skills from various sources. In\nthis paper, we discuss several practical examples of heterogeneous knowledge\nand propose Augmented Modular Reinforcement Learning (AMRL) to address these\nlimitations. Our framework uses a selector to combine heterogeneous modules and\nseamlessly incorporate different types of knowledge representations and\nprocessing mechanisms. Our results demonstrate the performance and efficiency\nimprovements, also in terms of generalization, that can be achieved by\naugmenting traditional modular RL with heterogeneous knowledge sources and\nprocessing mechanisms. Finally, we examine the safety, robustness, and\ninterpretability issues stemming from the introduction of knowledge\nheterogeneity.\n","authors":["Lorenz Wolf","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2306.01158v3.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.23991v1","updated":"2024-10-31T14:50:48Z","published":"2024-10-31T14:50:48Z","title":"Localization, balance and affinity: a stronger multifaceted\n  collaborative salient object detector in remote sensing images","summary":"  Despite significant advancements in salient object detection(SOD) in optical\nremote sensing images(ORSI), challenges persist due to the intricate edge\nstructures of ORSIs and the complexity of their contextual relationships.\nCurrent deep learning approaches encounter difficulties in accurately\nidentifying boundary features and lack efficiency in collaboratively modeling\nthe foreground and background by leveraging contextual features. To address\nthese challenges, we propose a stronger multifaceted collaborative salient\nobject detector in ORSIs, termed LBA-MCNet, which incorporates aspects of\nlocalization, balance, and affinity. The network focuses on accurately locating\ntargets, balancing detailed features, and modeling image-level global context\ninformation. Specifically, we design the Edge Feature Adaptive Balancing and\nAdjusting(EFABA) module for precise edge localization, using edge features to\nguide attention to boundaries and preserve spatial details. Moreover, we design\nthe Global Distributed Affinity Learning(GDAL) module to model global context.\nIt captures global context by generating an affinity map from the encoders\nfinal layer, ensuring effective modeling of global patterns. Additionally, deep\nsupervision during deconvolution further enhances feature representation.\nFinally, we compared with 28 state of the art approaches on three publicly\navailable datasets. The results clearly demonstrate the superiority of our\nmethod.\n","authors":["Yakun Xie","Suning Liu","Hongyu Chen","Shaohan Cao","Huixin Zhang","Dejun Feng","Qian Wan","Jun Zhu","Qing Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.23991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10363v2","updated":"2024-10-31T14:44:15Z","published":"2023-11-17T07:33:42Z","title":"Quantum-Assisted Simulation: A Framework for Developing Machine Learning\n  Models in Quantum Computing","summary":"  Machine Learning (ML) models are trained using historical data to classify\nnew, unseen data. However, traditional computing resources often struggle to\nhandle the immense amount of data, commonly known as Big Data, within a\nreasonable time frame. Quantum Computing (QC) provides a novel approach to\ninformation processing, offering the potential to process classical data\nexponentially faster than classical computing through quantum algorithms. By\nmapping Quantum Machine Learning (QML) algorithms into the quantum mechanical\ndomain, we can potentially achieve exponential improvements in data processing\nspeed, reduced resource requirements, and enhanced accuracy and efficiency. In\nthis article, we delve into both the QC and ML fields, exploring the interplay\nof ideas between them, as well as the current capabilities and limitations of\nhardware. We investigate the history of quantum computing, examine existing QML\nalgorithms, and present a simplified procedure for setting up simulations of\nQML algorithms, making it accessible and understandable for readers.\nFurthermore, we conduct simulations on a dataset using both traditional machine\nlearning and quantum machine learning approaches. We then compare their\nrespective performances by utilizing a quantum simulator.\n","authors":["Minati Rath","Hema Date"],"pdf_url":"https://arxiv.org/pdf/2311.10363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15852v2","updated":"2024-10-31T14:43:58Z","published":"2024-03-23T14:04:48Z","title":"SOEN-101: Code Generation by Emulating Software Process Models Using\n  Large Language Model Agents","summary":"  Software process models are essential to facilitate collaboration and\ncommunication among software teams to solve complex development tasks. Inspired\nby these software engineering practices, we present FlowGen - a code generation\nframework that emulates software process models based on multiple Large\nLanguage Model (LLM) agents. We emulate three process models, FlowGenWaterfall,\nFlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e.,\nrequirement engineer, architect, developer, tester, and scrum master) that\ncorrespond to everyday development activities and organize their communication\npatterns. The agents work collaboratively using chain-of-thought and prompt\ncomposition with continuous self-refinement to improve the code quality. We use\nGPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion)\nto evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP,\nand MBPP-ET. Our findings show that FlowGenScrum excels compared to other\nprocess models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval,\nHumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement\nover RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum\nachieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming\nReflexion. Notably, integrating CodeT into FlowGenScrum resulted in\nstatistically significant improvements, achieving the highest Pass@1 scores.\nOur analysis also reveals that the development activities impacted code smell\nand exception handling differently, with design and code review adding more\nexception handling and reducing code smells. Finally, FlowGen models maintain\nstable Pass@1 scores across GPT3.5 versions and temperature values,\nhighlighting the effectiveness of software process models in enhancing the\nquality and stability of LLM-generated code.\n","authors":["Feng Lin","Dong Jae Kim"," Tse-Husn"," Chen"],"pdf_url":"https://arxiv.org/pdf/2403.15852v2.pdf","comment":"ICSE 2025"},{"id":"http://arxiv.org/abs/2409.17692v2","updated":"2024-10-31T14:38:27Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v2.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2407.12582v2","updated":"2024-10-31T14:37:42Z","published":"2024-07-17T14:09:46Z","title":"Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection","summary":"  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n","authors":["Hu Cao","Zehua Zhang","Yan Xia","Xinyi Li","Jiahao Xia","Guang Chen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.12582v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.19534v4","updated":"2024-10-31T14:32:28Z","published":"2024-05-29T21:29:44Z","title":"Preference Learning Algorithms Do Not Learn Preference Rankings","summary":"  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.\n","authors":["Angelica Chen","Sadhika Malladi","Lily H. Zhang","Xinyi Chen","Qiuyi Zhang","Rajesh Ranganath","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2405.19534v4.pdf","comment":"NeurIPS 2024 camera-ready"},{"id":"http://arxiv.org/abs/2410.23975v1","updated":"2024-10-31T14:30:33Z","published":"2024-10-31T14:30:33Z","title":"Average Controlled and Average Natural Micro Direct Effects in Summary\n  Causal Graphs","summary":"  In this paper, we investigate the identifiability of average controlled\ndirect effects and average natural direct effects in causal systems represented\nby summary causal graphs, which are abstractions of full causal graphs, often\nused in dynamic systems where cycles and omitted temporal information\ncomplicate causal inference. Unlike in the traditional linear setting, where\ndirect effects are typically easier to identify and estimate, non-parametric\ndirect effects, which are crucial for handling real-world complexities,\nparticularly in epidemiological contexts where relationships between variables\n(e.g, genetic, environmental, and behavioral factors) are often non-linear, are\nmuch harder to define and identify. In particular, we give sufficient\nconditions for identifying average controlled micro direct effect and average\nnatural micro direct effect from summary causal graphs in the presence of\nhidden confounding. Furthermore, we show that the conditions given for the\naverage controlled micro direct effect become also necessary in the setting\nwhere there is no hidden confounding and where we are only interested in\nidentifiability by adjustment.\n","authors":["Simon Ferreira","Charles K. Assaad"],"pdf_url":"https://arxiv.org/pdf/2410.23975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00114v2","updated":"2024-10-31T14:27:50Z","published":"2024-06-27T13:46:11Z","title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents","summary":"  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.\n","authors":["Zihao Wang","Shaofei Cai","Zhancun Mu","Haowei Lin","Ceyao Zhang","Xuejie Liu","Qing Li","Anji Liu","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2407.00114v2.pdf","comment":"accepted on NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.14808v2","updated":"2024-10-31T14:19:49Z","published":"2024-05-23T17:18:46Z","title":"Implicit Personalization in Language Models: A Systematic Study","summary":"  Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.\n","authors":["Zhijing Jin","Nils Heil","Jiarui Liu","Shehzaad Dhuliawala","Yahang Qi","Bernhard Sch√∂lkopf","Rada Mihalcea","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.14808v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.10476v2","updated":"2024-10-31T14:15:49Z","published":"2024-10-14T13:10:45Z","title":"Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?","summary":"  The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.\n","authors":["Gabriel Roccabruna","Massimo Rizzoli","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2410.10476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23962v1","updated":"2024-10-31T14:14:30Z","published":"2024-10-31T14:14:30Z","title":"Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical\n  Scene Segmentation","summary":"  Surgical scene segmentation is essential for enhancing surgical precision,\nyet it is frequently compromised by the scarcity and imbalance of available\ndata. To address these challenges, semantic image synthesis methods based on\ngenerative adversarial networks and diffusion models have been developed.\nHowever, these models often yield non-diverse images and fail to capture small,\ncritical tissue classes, limiting their effectiveness. In response, we propose\nthe Class-Aware Semantic Diffusion Model (CASDM), a novel approach which\nutilizes segmentation maps as conditions for image synthesis to tackle data\nscarcity and imbalance. Novel class-aware mean squared error and class-aware\nself-perceptual loss functions have been defined to prioritize critical, less\nvisible classes, thereby enhancing image quality and relevance. Furthermore, to\nour knowledge, we are the first to generate multi-class segmentation maps using\ntext prompts in a novel fashion to specify their contents. These maps are then\nused by CASDM to generate surgical scene images, enhancing datasets for\ntraining and validating segmentation models. Our evaluation, which assesses\nboth image quality and downstream segmentation performance, demonstrates the\nstrong effectiveness and generalisability of CASDM in producing realistic\nimage-map pairs, significantly advancing surgical scene segmentation across\ndiverse and challenging datasets.\n","authors":["Yihang Zhou","Rebecca Towning","Zaid Awad","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2410.23962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23953v1","updated":"2024-10-31T14:07:26Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v1.pdf","comment":"Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2410.23934v1","updated":"2024-10-31T13:48:46Z","published":"2024-10-31T13:48:46Z","title":"Towards Fast Algorithms for the Preference Consistency Problem Based on\n  Hierarchical Models","summary":"  In this paper, we construct and compare algorithmic approaches to solve the\nPreference Consistency Problem for preference statements based on hierarchical\nmodels. Instances of this problem contain a set of preference statements that\nare direct comparisons (strict and non-strict) between some alternatives, and a\nset of evaluation functions by which all alternatives can be rated. An instance\nis consistent based on hierarchical preference models, if there exists an\nhierarchical model on the evaluation functions that induces an order relation\non the alternatives by which all relations given by the preference statements\nare satisfied. Deciding if an instance is consistent is known to be NP-complete\nfor hierarchical models. We develop three approaches to solve this decision\nproblem. The first involves a Mixed Integer Linear Programming (MILP)\nformulation, the other two are recursive algorithms that are based on\nproperties of the problem by which the search space can be pruned. Our\nexperiments on synthetic data show that the recursive algorithms are faster\nthan solving the MILP formulation and that the ratio between the running times\nincreases extremely quickly.\n","authors":["Anne-Marie George","Nic Wilson","Barry O'Sullivan"],"pdf_url":"https://arxiv.org/pdf/2410.23934v1.pdf","comment":"Longer Version of IJCAI'16 publication\n  https://www.ijcai.org/Proceedings/16/Papers/157.pdf"},{"id":"http://arxiv.org/abs/2410.20578v2","updated":"2024-10-31T13:41:39Z","published":"2024-10-27T20:14:32Z","title":"Meta-Learning Approaches for Improving Detection of Unseen Speech\n  Deepfakes","summary":"  Current speech deepfake detection approaches perform satisfactorily against\nknown adversaries; however, generalization to unseen attacks remains an open\nchallenge. The proliferation of speech deepfakes on social media underscores\nthe need for systems that can generalize to unseen attacks not observed during\ntraining. We address this problem from the perspective of meta-learning, aiming\nto learn attack-invariant features to adapt to unseen attacks with very few\nsamples available. This approach is promising since generating of a high-scale\ntraining dataset is often expensive or infeasible. Our experiments demonstrated\nan improvement in the Equal Error Rate (EER) from 21.67% to 10.42% on the\nInTheWild dataset, using just 96 samples from the unseen dataset. Continuous\nfew-shot adaptation ensures that the system remains up-to-date.\n","authors":["Ivan Kukanov","Janne Laakkonen","Tomi Kinnunen","Ville Hautam√§ki"],"pdf_url":"https://arxiv.org/pdf/2410.20578v2.pdf","comment":"6 pages, accepted to the IEEE Spoken Language Technology Workshop\n  (SLT) 2024"},{"id":"http://arxiv.org/abs/2410.22938v2","updated":"2024-10-31T13:39:20Z","published":"2024-10-30T11:47:40Z","title":"DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic\n  Signal Control with Missing Data","summary":"  The application of reinforcement learning in traffic signal control (TSC) has\nbeen extensively researched and yielded notable achievements. However, most\nexisting works for TSC assume that traffic data from all surrounding\nintersections is fully and continuously available through sensors. In\nreal-world applications, this assumption often fails due to sensor malfunctions\nor data loss, making TSC with missing data a critical challenge. To meet the\nneeds of practical applications, we introduce DiffLight, a novel conditional\ndiffusion model for TSC under data-missing scenarios in the offline setting.\nSpecifically, we integrate two essential sub-tasks, i.e., traffic data\nimputation and decision-making, by leveraging a Partial Rewards Conditioned\nDiffusion (PRCD) model to prevent missing rewards from interfering with the\nlearning process. Meanwhile, to effectively capture the spatial-temporal\ndependencies among intersections, we design a Spatial-Temporal transFormer\n(STFormer) architecture. In addition, we propose a Diffusion Communication\nMechanism (DCM) to promote better communication and control performance under\ndata-missing scenarios. Extensive experiments on five datasets with various\ndata-missing scenarios demonstrate that DiffLight is an effective controller to\naddress TSC with missing data. The code of DiffLight is released at\nhttps://github.com/lokol5579/DiffLight-release.\n","authors":["Hanyang Chen","Yang Jiang","Shengnan Guo","Xiaowei Mao","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.22938v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23916v1","updated":"2024-10-31T13:23:10Z","published":"2024-10-31T13:23:10Z","title":"Transformer-based Model Predictive Control: Trajectory Optimization via\n  Sequence Modeling","summary":"  Model predictive control (MPC) has established itself as the primary\nmethodology for constrained control, enabling general-purpose robot autonomy in\ndiverse real-world scenarios. However, for most problems of interest, MPC\nrelies on the recursive solution of highly non-convex trajectory optimization\nproblems, leading to high computational complexity and strong dependency on\ninitialization. In this work, we present a unified framework to combine the\nmain strengths of optimization-based and learning-based methods for MPC. Our\napproach entails embedding high-capacity, transformer-based neural network\nmodels within the optimization process for trajectory generation, whereby the\ntransformer provides a near-optimal initial guess, or target plan, to a\nnon-convex optimization problem. Our experiments, performed in simulation and\nthe real world onboard a free flyer platform, demonstrate the capabilities of\nour framework to improve MPC convergence and runtime. Compared to purely\noptimization-based approaches, results show that our approach can improve\ntrajectory generation performance by up to 75%, reduce the number of solver\niterations by up to 45%, and improve overall MPC runtime by 7x without loss in\nperformance.\n","authors":["Davide Celestini","Daniele Gammelli","Tommaso Guffanti","Simone D'Amico","Elisa Capello","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2410.23916v1.pdf","comment":"8 pages, 7 figures. Datasets, videos and code available at:\n  https://transformermpc.github.io"},{"id":"http://arxiv.org/abs/2302.09160v3","updated":"2024-10-31T13:20:42Z","published":"2023-02-17T22:15:20Z","title":"Identifying Equivalent Training Dynamics","summary":"  Study of the nonlinear evolution deep neural network (DNN) parameters undergo\nduring training has uncovered regimes of distinct dynamical behavior. While a\ndetailed understanding of these phenomena has the potential to advance\nimprovements in training efficiency and robustness, the lack of methods for\nidentifying when DNN models have equivalent dynamics limits the insight that\ncan be gained from prior work. Topological conjugacy, a notion from dynamical\nsystems theory, provides a precise definition of dynamical equivalence,\noffering a possible route to address this need. However, topological\nconjugacies have historically been challenging to compute. By leveraging\nadvances in Koopman operator theory, we develop a framework for identifying\nconjugate and non-conjugate training dynamics. To validate our approach, we\ndemonstrate that comparing Koopman eigenvalues can correctly identify a known\nequivalence between online mirror descent and online gradient descent. We then\nutilize our approach to: (a) identify non-conjugate training dynamics between\nshallow and wide fully connected neural networks; (b) characterize the early\nphase of training dynamics in convolutional neural networks; (c) uncover\nnon-conjugate training dynamics in Transformers that do and do not undergo\ngrokking. Our results, across a range of DNN architectures, illustrate the\nflexibility of our framework and highlight its potential for shedding new light\non training dynamics.\n","authors":["William T. Redman","Juan M. Bello-Rivas","Maria Fonoberova","Ryan Mohr","Ioannis G. Kevrekidis","Igor Meziƒá"],"pdf_url":"https://arxiv.org/pdf/2302.09160v3.pdf","comment":"23 pages, 5 figures, 6 supplemental figures"},{"id":"http://arxiv.org/abs/2410.23913v1","updated":"2024-10-31T13:19:31Z","published":"2024-10-31T13:19:31Z","title":"Efficient Inference and Computation of Optimal Alternatives for\n  Preference Languages Based On Lexicographic Models","summary":"  We analyse preference inference, through consistency, for general preference\nlanguages based on lexicographic models. We identify a property, which we call\nstrong compositionality, that applies for many natural kinds of preference\nstatement, and that allows a greedy algorithm for determining consistency of a\nset of preference statements. We also consider different natural definitions of\noptimality, and their relations to each other, for general preference languages\nbased on lexicographic models. Based on our framework, we show that testing\nconsistency, and thus inference, is polynomial for a specific preference\nlanguage LpqT, which allows strict and non-strict statements, comparisons\nbetween outcomes and between partial tuples, both ceteris paribus and strong\nstatements, and their combination. Computing different kinds of optimal sets is\nalso shown to be polynomial; this is backed up by our experimental results.\n","authors":["Nic Wilson","Anne-Marie George"],"pdf_url":"https://arxiv.org/pdf/2410.23913v1.pdf","comment":"Longer Version of IJCAI'17 publication\n  https://www.ijcai.org/proceedings/2017/0182.pdf"},{"id":"http://arxiv.org/abs/2406.04230v2","updated":"2024-10-31T13:18:14Z","published":"2024-06-06T16:30:41Z","title":"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating\n  Interferometric SAR and Multispectral Data","summary":"  Satellite-based remote sensing has revolutionised the way we address global\nchallenges. Huge quantities of Earth Observation (EO) data are generated by\nsatellite sensors daily, but processing these large datasets for use in ML\npipelines is technically and computationally challenging. While some\npreprocessed Earth observation datasets exist, their content is often limited\nto optical or near-optical wavelength data, which is ineffective at night or in\nadverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing\ntechnique based on microwave length radiation, offers a viable alternative.\nHowever, the application of machine learning to SAR has been limited due to a\nlack of ML-ready data and pipelines, particularly for the full diversity of SAR\ndata, including polarimetry, coherence and interferometry. In this work, we\nintroduce M3LEO, a multi-modal, multi-label Earth observation dataset that\nincludes polarimetric, interferometric, and coherence SAR data derived from\nSentinel-1, alongside multispectral Sentinel-2 imagery and auxiliary data\ndescribing terrain properties such as land use. M3LEO spans approximately 17M\n4x4 km data chips from six diverse geographic regions. The dataset is\ncomplemented by a flexible PyTorch Lightning framework configured using Hydra\nto accommodate its use across diverse ML applications in Earth observation. We\nprovide tools to process any dataset available on popular platforms such as\nGoogle Earth Engine for seamless integration with our framework. We show that\nthe distribution shift in self-supervised embeddings is substantial across\ngeographic regions, even when controlling for terrain properties. Data:\nhuggingface.co/M3LEO, Code: github.com/spaceml-org/M3LEO.\n","authors":["Matthew J Allen","Francisco Dorr","Joseph Alejandro Gallego Mejia","Laura Mart√≠nez-Ferrer","Anna Jungbluth","Freddie Kalaitzis","Ra√∫l Ramos-Poll√°n"],"pdf_url":"https://arxiv.org/pdf/2406.04230v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23912v1","updated":"2024-10-31T13:17:53Z","published":"2024-10-31T13:17:53Z","title":"RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner","summary":"  The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs.\n","authors":["Fu-Chieh Chang","Yu-Ting Lee","Hui-Ying Shih","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.23912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20087v2","updated":"2024-10-31T13:10:12Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2410.23903v1","updated":"2024-10-31T13:05:46Z","published":"2024-10-31T13:05:46Z","title":"Neural Network Verification with PyRAT","summary":"  As AI systems are becoming more and more popular and used in various critical\ndomains (health, transport, energy, ...), the need to provide guarantees and\ntrust of their safety is undeniable. To this end, we present PyRAT, a tool\nbased on abstract interpretation to verify the safety and the robustness of\nneural networks. In this paper, we describe the different abstractions used by\nPyRAT to find the reachable states of a neural network starting from its input\nas well as the main features of the tool to provide fast and accurate analysis\nof neural networks. PyRAT has already been used in several collaborations to\nensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing\nits performance.\n","authors":["Augustin Lemesle","Julien Lehmann","Tristan Le Gall"],"pdf_url":"https://arxiv.org/pdf/2410.23903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04559v3","updated":"2024-10-31T12:59:46Z","published":"2024-02-07T03:37:19Z","title":"Can Large Language Model Agents Simulate Human Trust Behavior?","summary":"  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.\n","authors":["Chengxing Xie","Canyu Chen","Feiran Jia","Ziyu Ye","Shiyang Lai","Kai Shu","Jindong Gu","Adel Bibi","Ziniu Hu","David Jurgens","James Evans","Philip Torr","Bernard Ghanem","Guohao Li"],"pdf_url":"https://arxiv.org/pdf/2402.04559v3.pdf","comment":"Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org"},{"id":"http://arxiv.org/abs/2410.20745v2","updated":"2024-10-31T12:54:46Z","published":"2024-10-28T05:25:47Z","title":"Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models","summary":"  Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.\n","authors":["Yilun Jin","Zheng Li","Chenwei Zhang","Tianyu Cao","Yifan Gao","Pratik Jayarao","Mao Li","Xin Liu","Ritesh Sarkhel","Xianfeng Tang","Haodong Wang","Zhengyang Wang","Wenju Xu","Jingfeng Yang","Qingyu Yin","Xian Li","Priyanka Nigam","Yi Xu","Kai Chen","Qiang Yang","Meng Jiang","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2410.20745v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos\n  in Figure 9"},{"id":"http://arxiv.org/abs/2410.23891v1","updated":"2024-10-31T12:52:52Z","published":"2024-10-31T12:52:52Z","title":"AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in\n  Satellite Imagery","summary":"  Clouds in satellite imagery pose a significant challenge for downstream\napplications. A major challenge in current cloud removal research is the\nabsence of a comprehensive benchmark and a sufficiently large and diverse\ntraining dataset. To address this problem, we introduce the largest public\ndataset -- $\\textit{AllClear}$ for cloud removal, featuring 23,742 globally\ndistributed regions of interest (ROIs) with diverse land-use patterns,\ncomprising 4 million images in total. Each ROI includes complete temporal\ncaptures from the year 2022, with (1) multi-spectral optical imagery from\nSentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from\nSentinel-1, and (3) auxiliary remote sensing products such as cloud masks and\nland cover maps. We validate the effectiveness of our dataset by benchmarking\nperformance, demonstrating the scaling law -- the PSNR rises from $28.47$ to\n$33.87$ with $30\\times$ more data, and conducting ablation studies on the\ntemporal length and the importance of individual modalities. This dataset aims\nto provide comprehensive coverage of the Earth's surface and promote better\ncloud removal results.\n","authors":["Hangyu Zhou","Chia-Hsiang Kao","Cheng Perng Phoo","Utkarsh Mall","Bharath Hariharan","Kavita Bala"],"pdf_url":"https://arxiv.org/pdf/2410.23891v1.pdf","comment":"Accepted at NeurIPS 2024 Datasets and Benchmarks Track. Code and data\n  available at https://allclear.cs.cornell.edu/"},{"id":"http://arxiv.org/abs/2410.23890v1","updated":"2024-10-31T12:52:26Z","published":"2024-10-31T12:52:26Z","title":"Leveraging LLMs for MT in Crisis Scenarios: a blueprint for low-resource\n  languages","summary":"  In an evolving landscape of crisis communication, the need for robust and\nadaptable Machine Translation (MT) systems is more pressing than ever,\nparticularly for low-resource languages. This study presents a comprehensive\nexploration of leveraging Large Language Models (LLMs) and Multilingual LLMs\n(MLLMs) to enhance MT capabilities in such scenarios. By focusing on the unique\nchallenges posed by crisis situations where speed, accuracy, and the ability to\nhandle a wide range of languages are paramount, this research outlines a novel\napproach that combines the cutting-edge capabilities of LLMs with fine-tuning\ntechniques and community-driven corpus development strategies. At the core of\nthis study is the development and empirical evaluation of MT systems tailored\nfor two low-resource language pairs, illustrating the process from initial\nmodel selection and fine-tuning through to deployment. Bespoke systems are\ndeveloped and modelled on the recent Covid-19 pandemic. The research highlights\nthe importance of community involvement in creating highly specialised,\ncrisis-specific datasets and compares custom GPTs with NLLB-adapted MLLM\nmodels. It identifies fine-tuned MLLM models as offering superior performance\ncompared with their LLM counterparts. A scalable and replicable model for rapid\nMT system development in crisis scenarios is outlined. Our approach enhances\nthe field of humanitarian technology by offering a blueprint for developing\nmultilingual communication systems during emergencies.\n","authors":["S√©amus Lankford","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2410.23890v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.02370,\n  arXiv:2403.01580"},{"id":"http://arxiv.org/abs/2410.23889v1","updated":"2024-10-31T12:51:40Z","published":"2024-10-31T12:51:40Z","title":"GEPS: Boosting Generalization in Parametric PDE Neural Solvers through\n  Adaptive Conditioning","summary":"  Solving parametric partial differential equations (PDEs) presents significant\nchallenges for data-driven methods due to the sensitivity of spatio-temporal\ndynamics to variations in PDE parameters. Machine learning approaches often\nstruggle to capture this variability. To address this, data-driven approaches\nlearn parametric PDEs by sampling a very large variety of trajectories with\nvarying PDE parameters. We first show that incorporating conditioning\nmechanisms for learning parametric PDEs is essential and that among them,\n$\\textit{adaptive conditioning}$, allows stronger generalization. As existing\nadaptive conditioning methods do not scale well with respect to the number of\nparameters to adapt in the neural solver, we propose GEPS, a simple adaptation\nmechanism to boost GEneralization in Pde Solvers via a first-order optimization\nand low-rank rapid adaptation of a small set of context parameters. We\ndemonstrate the versatility of our approach for both fully data-driven and for\nphysics-aware neural solvers. Validation performed on a whole range of\nspatio-temporal forecasting problems demonstrates excellent performance for\ngeneralizing to unseen conditions including initial conditions, PDE\ncoefficients, forcing terms and solution domain. $\\textit{Project page}$:\nhttps://geps-project.github.io\n","authors":["Armand Kassa√Ø Koupa√Ø","Jorge Misfut Benet","Yuan Yin","Jean-No√´l Vittaut","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2410.23889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23875v1","updated":"2024-10-31T12:37:24Z","published":"2024-10-31T12:37:24Z","title":"Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model\n  on Knowledge Graphs","summary":"  Large Language Models (LLMs) have shown remarkable reasoning capabilities on\ncomplex tasks, but they still suffer from out-of-date knowledge,\nhallucinations, and opaque decision-making. In contrast, Knowledge Graphs (KGs)\ncan provide explicit and editable knowledge for LLMs to alleviate these issues.\nExisting paradigm of KG-augmented LLM manually predefines the breadth of\nexploration space and requires flawless navigation in KGs. However, this\nparadigm cannot adaptively explore reasoning paths in KGs based on the question\nsemantics and self-correct erroneous reasoning paths, resulting in a bottleneck\nin efficiency and effect. To address these limitations, we propose a novel\nself-correcting adaptive planning paradigm for KG-augmented LLM named\nPlan-on-Graph (PoG), which first decomposes the question into several\nsub-objectives and then repeats the process of adaptively exploring reasoning\npaths, updating memory, and reflecting on the need to self-correct erroneous\nreasoning paths until arriving at the answer. Specifically, three important\nmechanisms of Guidance, Memory, and Reflection are designed to work together,\nto guarantee the adaptive breadth of self-correcting planning for graph\nreasoning. Finally, extensive experiments on three real-world datasets\ndemonstrate the effectiveness and efficiency of PoG.\n","authors":["Liyi Chen","Panrong Tong","Zhongming Jin","Ying Sun","Jieping Ye","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.23875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03941v2","updated":"2024-10-31T12:27:30Z","published":"2024-02-06T12:18:54Z","title":"Discovery of the Hidden World with Large Language Models","summary":"  Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.\n","authors":["Chenxi Liu","Yongqiang Chen","Tongliang Liu","Mingming Gong","James Cheng","Bo Han","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.03941v2.pdf","comment":"NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72\n  figures; Project page: https://causalcoat.github.io/"},{"id":"http://arxiv.org/abs/2404.12404v3","updated":"2024-10-31T12:27:14Z","published":"2024-04-15T17:49:16Z","title":"EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n","authors":["Jinhee Kim","Taesung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.12404v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.06255v4","updated":"2024-10-31T12:24:14Z","published":"2024-02-09T09:09:39Z","title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning","summary":"  While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.\n","authors":["Yichuan Mo","Yuji Wang","Zeming Wei","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.06255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14791v3","updated":"2024-10-31T12:23:42Z","published":"2024-08-27T05:53:02Z","title":"Optimizing Structured Data Processing through Robotic Process Automation","summary":"  Robotic Process Automation (RPA) has emerged as a game-changing technology in\ndata extraction, revolutionizing the way organizations process and analyze\nlarge volumes of documents such as invoices, purchase orders, and payment\nadvices. This study investigates the use of RPA for structured data extraction\nand evaluates its advantages over manual processes. By comparing\nhuman-performed tasks with those executed by RPA software bots, we assess\nefficiency and accuracy in data extraction from invoices, focusing on the\neffectiveness of the RPA system. Through four distinct scenarios involving\nvarying numbers of invoices, we measure efficiency in terms of time and effort\nrequired for task completion, as well as accuracy by comparing error rates\nbetween manual and RPA processes. Our findings highlight the significant\nefficiency gains achieved by RPA, with bots completing tasks in significantly\nless time compared to manual efforts across all cases. Moreover, the RPA system\nconsistently achieves perfect accuracy, mitigating the risk of errors and\nenhancing process reliability. These results underscore the transformative\npotential of RPA in optimizing operational efficiency, reducing human labor\ncosts, and improving overall business performance.\n","authors":["Vivek Bhardwaj","Ajit Noonia","Sandeep Chaurasia","Mukesh Kumar","Abdulnaser Rashid","Mohamed Tahar Ben Othman"],"pdf_url":"https://arxiv.org/pdf/2408.14791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09695v2","updated":"2024-10-31T12:22:50Z","published":"2024-10-13T02:10:26Z","title":"Can In-context Learning Really Generalize to Out-of-distribution Tasks?","summary":"  In this work, we explore the mechanism of in-context learning (ICL) on\nout-of-distribution (OOD) tasks that were not encountered during training. To\nachieve this, we conduct synthetic experiments where the objective is to learn\nOOD mathematical functions through ICL using a GPT-2 model. We reveal that\nTransformers may struggle to learn OOD task functions through ICL.\nSpecifically, ICL performance resembles implementing a function within the\npretraining hypothesis space and optimizing it with gradient descent based on\nthe in-context examples. Additionally, we investigate ICL's well-documented\nability to learn unseen abstract labels in context. We demonstrate that such\nability only manifests in the scenarios without distributional shifts and,\ntherefore, may not serve as evidence of new-task-learning ability. Furthermore,\nwe assess ICL's performance on OOD tasks when the model is pretrained on\nmultiple tasks. Both empirical and theoretical analyses demonstrate the\nexistence of the \\textbf{low-test-error preference} of ICL, where it tends to\nimplement the pretraining function that yields low test error in the testing\ncontext. We validate this through numerical experiments. This new theoretical\nresult, combined with our empirical findings, elucidates the mechanism of ICL\nin addressing OOD tasks.\n","authors":["Qixun Wang","Yifei Wang","Yisen Wang","Xianghua Ying"],"pdf_url":"https://arxiv.org/pdf/2410.09695v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2410.23855v1","updated":"2024-10-31T12:05:21Z","published":"2024-10-31T12:05:21Z","title":"RAGraph: A General Retrieval-Augmented Graph Learning Framework","summary":"  Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability.\n","authors":["Xinke Jiang","Rihong Qiu","Yongxin Xu","Wentao Zhang","Yichen Zhu","Ruizhe Zhang","Yuchen Fang","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23855v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2305.05368v3","updated":"2024-10-31T12:04:12Z","published":"2023-05-09T12:03:42Z","title":"Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive\n  Residual Module","summary":"  Graph Neural Networks (GNNs), a type of neural network that can learn from\ngraph-structured data through neighborhood information aggregation, have shown\nsuperior performance in various downstream tasks. However, as the number of\nlayers increases, node representations become indistinguishable, which is known\nas over-smoothing. To address this issue, many residual methods have emerged.\nIn this paper, we focus on the over-smoothing issue and related residual\nmethods. Firstly, we revisit over-smoothing from the perspective of overlapping\nneighborhood subgraphs, and based on this, we explain how residual methods can\nalleviate over-smoothing by integrating multiple orders neighborhood subgraphs\nto avoid the indistinguishability of the single high-order neighborhood\nsubgraphs. Additionally, we reveal the drawbacks of previous residual methods,\nsuch as the lack of node adaptability and severe loss of high-order\nneighborhood subgraph information, and propose a\n\\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We\ntheoretically demonstrate that PSNR can alleviate the drawbacks of previous\nresidual methods. Furthermore, extensive experiments verify the superiority of\nthe PSNR module in fully observed node classification and missing feature\nscenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.\n","authors":["Jingbo Zhou","Yixuan Du","Ruqiong Zhang","Jun Xia","Zhizhi Yu","Zelin Zang","Di Jin","Carl Yang","Rui Zhang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2305.05368v3.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23844v1","updated":"2024-10-31T11:50:24Z","published":"2024-10-31T11:50:24Z","title":"Commonsense Knowledge Editing Based on Free-Text in LLMs","summary":"  Knowledge editing technology is crucial for maintaining the accuracy and\ntimeliness of large language models (LLMs) . However, the setting of this task\noverlooks a significant portion of commonsense knowledge based on free-text in\nthe real world, characterized by broad knowledge scope, long content and non\ninstantiation. The editing objects of previous methods (e.g., MEMIT) were\nsingle token or entity, which were not suitable for commonsense knowledge in\nfree-text form. To address the aforementioned challenges, we conducted\nexperiments from two perspectives: knowledge localization and knowledge\nediting. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)\nmethod, revealing the challenges associated with the distribution of\ncommonsense knowledge in MLP and Attention layers, as well as in decentralized\ndistribution. Next, we propose a Dynamics-aware Editing Method(DEM), which\nutilizes a Dynamics-aware Module to locate the parameter positions\ncorresponding to commonsense knowledge, and uses Knowledge Editing Module to\nupdate knowledge. The DEM method fully explores the potential of the MLP and\nAttention layers, and successfully edits commonsense knowledge based on\nfree-text. The experimental results indicate that the DEM can achieve excellent\nediting performance.\n","authors":["Xiusheng Huang","Yequan Wang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23844v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23843v1","updated":"2024-10-31T11:49:44Z","published":"2024-10-31T11:49:44Z","title":"Reasons and Solutions for the Decline in Model Performance after Editing","summary":"  Knowledge editing technology has received widespread attention for low-cost\nupdates of incorrect or outdated knowledge in large-scale language models.\nHowever, recent research has found that edited models often exhibit varying\ndegrees of performance degradation. The reasons behind this phenomenon and\npotential solutions have not yet been provided. In order to investigate the\nreasons for the performance decline of the edited model and optimize the\nediting method, this work explores the underlying reasons from both data and\nmodel perspectives. Specifically, 1) from a data perspective, to clarify the\nimpact of data on the performance of editing models, this paper first\nconstructs a Multi-Question Dataset (MQD) to evaluate the impact of different\ntypes of editing data on model performance. The performance of the editing\nmodel is mainly affected by the diversity of editing targets and sequence\nlength, as determined through experiments. 2) From a model perspective, this\narticle explores the factors that affect the performance of editing models. The\nresults indicate a strong correlation between the L1-norm of the editing model\nlayer and the editing accuracy, and clarify that this is an important factor\nleading to the bottleneck of editing performance. Finally, in order to improve\nthe performance of the editing model, this paper further proposes a Dump for\nSequence (D4S) method, which successfully overcomes the previous editing\nbottleneck by reducing the L1-norm of the editing layer, allowing users to\nperform multiple effective edits and minimizing model damage. Our code is\navailable at https://github.com/nlpkeg/D4S.\n","authors":["Xiusheng Huang","Jiaxiang Liu","Yequan Wang","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23843v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23842v1","updated":"2024-10-31T11:49:16Z","published":"2024-10-31T11:49:16Z","title":"Auditing Google's Search Algorithm: Measuring News Diversity Across\n  Brazil, the UK, and the US","summary":"  This study examines the influence of Google's search algorithm on news\ndiversity by analyzing search results in Brazil, the UK, and the US. It\nexplores how Google's system preferentially favors a limited number of news\noutlets. Utilizing algorithm auditing techniques, the research measures source\nconcentration with the Herfindahl-Hirschman Index (HHI) and Gini coefficient,\nrevealing significant concentration trends. The study underscores the\nimportance of conducting horizontal analyses across multiple search queries, as\nfocusing solely on individual results pages may obscure these patterns. Factors\nsuch as popularity, political bias, and recency were evaluated for their impact\non news rankings. Findings indicate a slight leftward bias in search outcomes\nand a preference for popular, often national outlets. This bias, combined with\na tendency to prioritize recent content, suggests that Google's algorithm may\nreinforce existing media inequalities. By analyzing the largest dataset to date\n-- 221,863 search results -- this research provides comprehensive, longitudinal\ninsights into how algorithms shape public access to diverse news sources.\n","authors":["Raphael Hernandes","Giulio Corsi"],"pdf_url":"https://arxiv.org/pdf/2410.23842v1.pdf","comment":"21 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.21488v2","updated":"2024-10-31T11:45:00Z","published":"2024-07-31T09:52:53Z","title":"Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing\n  the Upper Bound of Generative Retrieval","summary":"  Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.\n","authors":["Zhirui Kuai","Zuxu Chen","Huimu Wang","Mingming Li","Dadong Miao","Binbin Wang","Xusong Chen","Li Kuang","Yuxing Han","Jiaxing Wang","Guoyu Tang","Lin Liu","Songlin Wang","Jingwei Zhuo"],"pdf_url":"https://arxiv.org/pdf/2407.21488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12490v2","updated":"2024-10-31T11:42:07Z","published":"2024-10-16T12:13:17Z","title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective","summary":"  Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling by applying K-Means on the latent features of\nself-supervised learning models. Experimental results show that image\nautoregressive modeling with our tokenizer (DiGIT) benefits both image\nunderstanding and image generation with the next token prediction principle,\nwhich is inherently straightforward for GPT models but challenging for other\ngenerative models. Remarkably, for the first time, a GPT-style autoregressive\nmodel for images outperforms LDMs, which also exhibits substantial improvement\nakin to GPT when scaling up model size. Our findings underscore the potential\nof an optimized latent space and the integration of discrete tokenization in\nadvancing the capabilities of image generative models. The code is available at\n\\url{https://github.com/DAMO-NLP-SG/DiGIT}.\n","authors":["Yongxin Zhu","Bocheng Li","Hang Zhang","Xin Li","Linli Xu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12490v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.00626v2","updated":"2024-10-31T11:39:25Z","published":"2024-06-30T08:52:17Z","title":"Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with\n  Energy-Based Models","summary":"  We present a maximum entropy inverse reinforcement learning (IRL) approach\nfor improving the sample quality of diffusion generative models, especially\nwhen the number of generation time steps is small. Similar to how IRL trains a\npolicy based on the reward function learned from expert demonstrations, we\ntrain (or fine-tune) a diffusion model using the log probability density\nestimated from training data. Since we employ an energy-based model (EBM) to\nrepresent the log density, our approach boils down to the joint training of a\ndiffusion model and an EBM. Our IRL formulation, named Diffusion by Maximum\nEntropy IRL (DxMI), is a minimax problem that reaches equilibrium when both\nmodels converge to the data distribution. The entropy maximization plays a key\nrole in DxMI, facilitating the exploration of the diffusion model and ensuring\nthe convergence of the EBM. We also propose Diffusion by Dynamic Programming\n(DxDP), a novel reinforcement learning algorithm for diffusion models, as a\nsubroutine in DxMI. DxDP makes the diffusion model update in DxMI efficient by\ntransforming the original problem into an optimal control formulation where\nvalue functions replace back-propagation in time. Our empirical studies show\nthat diffusion models fine-tuned using DxMI can generate high-quality samples\nin as few as 4 and 10 steps. Additionally, DxMI enables the training of an EBM\nwithout MCMC, stabilizing EBM training dynamics and enhancing anomaly detection\nperformance.\n","authors":["Sangwoong Yoon","Himchan Hwang","Dohyun Kwon","Yung-Kyun Noh","Frank C. Park"],"pdf_url":"https://arxiv.org/pdf/2407.00626v2.pdf","comment":"NeurIPS 2024 Oral Presentation. Code is released at\n  https://github.com/swyoon/Diffusion-by-MaxEntIRL"},{"id":"http://arxiv.org/abs/2401.11849v3","updated":"2024-10-31T11:33:24Z","published":"2024-01-22T11:08:36Z","title":"Self-Labeling the Job Shop Scheduling Problem","summary":"  This work proposes a self-supervised training strategy designed for\ncombinatorial problems. An obstacle in applying supervised paradigms to such\nproblems is the need for costly target solutions often produced with exact\nsolvers. Inspired by semi- and self-supervised learning, we show that\ngenerative models can be trained by sampling multiple solutions and using the\nbest one according to the problem objective as a pseudo-label. In this way, we\niteratively improve the model generation capability by relying only on its\nself-supervision, eliminating the need for optimality information. We validate\nthis Self-Labeling Improvement Method (SLIM) on the Job Shop Scheduling (JSP),\na complex combinatorial problem that is receiving much attention from the\nneural combinatorial community. We propose a generative model based on the\nwell-known Pointer Network and train it with SLIM. Experiments on popular\nbenchmarks demonstrate the potential of this approach as the resulting models\noutperform constructive heuristics and state-of-the-art learning proposals for\nthe JSP. Lastly, we prove the robustness of SLIM to various parameters and its\ngenerality by applying it to the Traveling Salesman Problem.\n","authors":["Andrea Corsini","Angelo Porrello","Simone Calderara","Mauro Dell'Amico"],"pdf_url":"https://arxiv.org/pdf/2401.11849v3.pdf","comment":"Accepted at the 38th Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.23835v1","updated":"2024-10-31T11:29:41Z","published":"2024-10-31T11:29:41Z","title":"Counterfactual MRI Data Augmentation using Conditional Denoising\n  Diffusion Generative Models","summary":"  Deep learning (DL) models in medical imaging face challenges in\ngeneralizability and robustness due to variations in image acquisition\nparameters (IAP). In this work, we introduce a novel method using conditional\ndenoising diffusion generative models (cDDGMs) to generate counterfactual\nmagnetic resonance (MR) images that simulate different IAP without altering\npatient anatomy. We demonstrate that using these counterfactual images for data\naugmentation can improve segmentation accuracy, particularly in\nout-of-distribution settings, enhancing the overall generalizability and\nrobustness of DL models across diverse imaging conditions. Our approach shows\npromise in addressing domain and covariate shifts in medical imaging. The code\nis publicly available at https:\n//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation\n","authors":["Pedro Mor√£o","Joao Santinha","Yasna Forghani","Nuno Lou√ß√£o","Pedro Gouveia","Mario A. T. Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2410.23835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19400v3","updated":"2024-10-31T11:27:03Z","published":"2024-10-25T09:01:37Z","title":"Offline Reinforcement Learning with OOD State Correction and OOD Action\n  Suppression","summary":"  In offline reinforcement learning (RL), addressing the out-of-distribution\n(OOD) action issue has been a focus, but we argue that there exists an OOD\nstate issue that also impairs performance yet has been underexplored. Such an\nissue describes the scenario when the agent encounters states out of the\noffline dataset during the test phase, leading to uncontrolled behavior and\nperformance degradation. To this end, we propose SCAS, a simple yet effective\napproach that unifies OOD state correction and OOD action suppression in\noffline RL. Technically, SCAS achieves value-aware OOD state correction,\ncapable of correcting the agent from OOD states to high-value in-distribution\nstates. Theoretical and empirical results show that SCAS also exhibits the\neffect of suppressing OOD actions. On standard offline RL benchmarks, SCAS\nachieves excellent performance without additional hyperparameter tuning.\nMoreover, benefiting from its OOD state correction feature, SCAS demonstrates\nenhanced robustness against environmental perturbations.\n","authors":["Yixiu Mao","Qi Wang","Chen Chen","Yun Qu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2410.19400v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.07966v3","updated":"2024-10-31T11:25:40Z","published":"2024-09-12T11:53:05Z","title":"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE","summary":"  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n","authors":["Sichun Wu","Kazi Injamamul Haque","Zerrin Yumak"],"pdf_url":"https://arxiv.org/pdf/2409.07966v3.pdf","comment":"14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024"},{"id":"http://arxiv.org/abs/2410.23111v2","updated":"2024-10-31T11:16:46Z","published":"2024-10-30T15:23:44Z","title":"Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in\n  Federated Fine-Tuning of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, particularly in task generalization for both text and vision\ndata. While fine-tuning these models can significantly enhance their\nperformance on specific downstream tasks, it often requires high-quality data\nthat cannot be shared due to privacy concerns. Federated Learning (FL) offers a\npromising solution for collaborative training without direct data sharing.\nHowever, many parameter-efficient fine-tuning strategies for LLMs in FL,\nparticularly those based on Low-Rank Adaptation (LoRA), face limitations. In\nthis paper, we critically analyze the convergence and performance guarantees of\npopular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to\nconstrained subspace learning of low-rank matrices. This limitation hinders\neffective fine-tuning of LLMs in federated settings. Through rigorous\nanalytical and empirical evaluations, we demonstrate that direct weight\naveraging outperforms LoRA-based strategies, leading to superior performance\nfor fine-tuned models. Our comprehensive comparison exposes inefficiencies in\nLoRA approaches and underscores the advantages of direct weight aggregation. We\nextend our analysis to low-rank gradient-based optimizers, such as GaLore, used\nduring local training steps. Our findings show that GaLore is a more effective\nalternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA\nacross both text and image modalities. While privacy remains paramount in FL\ndiscourse, our focus is on assessing performance outcomes of federated\nfine-tuned models and evaluating various FL frameworks from both theoretical\nand empirical perspectives. Our findings advocate reassessing the reliance on\nLoRA within FL contexts, paving the way for more efficient training\nmethodologies.\n","authors":["Navyansh Mahla","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.23111v2.pdf","comment":"24 pages, 10 figures, pre-print"},{"id":"http://arxiv.org/abs/2408.09881v2","updated":"2024-10-31T11:14:58Z","published":"2024-08-19T10:46:19Z","title":"Uncertainty Quantification of Surrogate Models using Conformal\n  Prediction","summary":"  Data-driven surrogate models have shown immense potential as quick,\ninexpensive approximations to complex numerical and experimental modelling\ntasks. However, most surrogate models of physical systems do not quantify their\nuncertainty, rendering their predictions unreliable, requiring further\nvalidation. Though Bayesian approximations offer some solace in estimating the\nerror associated with these models, they cannot provide guarantees, and the\nquality of their inferences depends on the availability of prior information\nand good approximations to posteriors for complex problems. This is\nparticularly pertinent to multi-variable or spatio-temporal problems. Our work\nconstructs and formalises a conformal prediction framework that satisfies\nmarginal coverage for spatio-temporal predictions in a model-agnostic manner,\nrequiring near-zero computational costs. We provide an extensive empirical\nstudy of the application of the framework to ascertain valid error bars that\nprovide guaranteed coverage across the surrogate model's domain of operation.\nThe application scope of our work extends across a large range of\nspatio-temporal models, from solving partial differential equations to weather\nforecasting. Through the applications, the paper looks at providing\nstatistically valid error bars for deterministic models, as well as crafting\nguarantees to the error bars of probabilistic models. Our conformal prediction\nformalisation provides guaranteed coverage of the surrogate model, regardless\nof model architecture, and its training regime and is unbothered by the curse\nof dimensionality.\n","authors":["Vignesh Gopakumar","Ander Gray","Joel Oskarsson","Lorenzo Zanisi","Stanislas Pamela","Daniel Giles","Matt Kusner","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2408.09881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23825v1","updated":"2024-10-31T11:14:12Z","published":"2024-10-31T11:14:12Z","title":"GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for\n  Minority Languages","summary":"  The need for large text corpora has increased with the advent of pretrained\nlanguage models and, in particular, the discovery of scaling laws for these\nmodels. Most available corpora have sufficient data only for languages with\nlarge dominant communities. However, there is no corpus available that (i)\ncovers a wide range of minority languages; (ii) is generated by an open-source\nreproducible pipeline; and (iii) is rigorously cleaned from noise, making it\ntrustworthy to use. We present GlotCC, a clean, document-level, 2TB general\ndomain corpus derived from CommonCrawl, covering more than 1000 languages. We\nmake GlotCC and the system used to generate it - including the pipeline,\nlanguage identification model, and filters - available to the research\ncommunity. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1,\nPipeline v. 3.0 https://github.com/cisnlp/GlotCC.\n","authors":["Amir Hossein Kargaran","Fran√ßois Yvon","Hinrich Sch√ºtze"],"pdf_url":"https://arxiv.org/pdf/2410.23825v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2306.01762v4","updated":"2024-10-31T11:11:24Z","published":"2023-05-27T06:00:51Z","title":"Rapid Plug-in Defenders","summary":"  In the realm of daily services, the deployment of deep neural networks\nunderscores the paramount importance of their reliability. However, the\nvulnerability of these networks to adversarial attacks, primarily\nevasion-based, poses a concerning threat to their functionality. Common methods\nfor enhancing robustness involve heavy adversarial training or leveraging\nlearned knowledge from clean data, both necessitating substantial computational\nresources. This inherent time-intensive nature severely limits the agility of\nlarge foundational models to swiftly counter adversarial perturbations. To\naddress this challenge, this paper focuses on the Rapid Plug-in Defender\n(RaPiD) problem, aiming to rapidly counter adversarial perturbations without\naltering the deployed model. Drawing inspiration from the generalization and\nthe universal computation ability of pre-trained transformer models, we propose\na novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)\nfor RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes\nthe normalization layer parameters within the defender using a limited set of\nclean and adversarial examples. Our evaluation centers on assessing CeTaD's\neffectiveness, transferability, and the impact of different components in\nscenarios involving one-shot adversarial examples. The proposed method is\ncapable of rapidly adapting to various attacks and different application\nscenarios without altering the target model and clean training data. We also\nexplore the influence of varying training data conditions on CeTaD's\nperformance. Notably, CeTaD exhibits adaptability across differentiable service\nmodels and proves the potential of continuous learning.\n","authors":["Kai Wu","Yujian Betterest Li","Jian Lou","Xiaoyu Zhang","Handing Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2306.01762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08126v2","updated":"2024-10-31T11:11:18Z","published":"2024-10-10T17:10:34Z","title":"Mars: Situated Inductive Reasoning in an Open-World Environment","summary":"  Large Language Models (LLMs) trained on massive corpora have shown remarkable\nsuccess in knowledge-intensive tasks. Yet, most of them rely on pre-stored\nknowledge. Inducing new general knowledge from a specific environment and\nperforming reasoning with the acquired knowledge -- \\textit{situated inductive\nreasoning}, is crucial and challenging for machine intelligence. In this paper,\nwe design Mars, an interactive environment devised for situated inductive\nreasoning. It introduces counter-commonsense game mechanisms by modifying\nterrain, survival setting and task dependency while adhering to certain\nprinciples. In Mars, agents need to actively interact with their surroundings,\nderive useful rules and perform decision-making tasks in specific contexts. We\nconduct experiments on various RL-based and LLM-based methods, finding that\nthey all struggle on this challenging situated inductive reasoning benchmark.\nFurthermore, we explore \\textit{Induction from Reflection}, where we instruct\nagents to perform inductive reasoning from history trajectory. The superior\nperformance underscores the importance of inductive reasoning in Mars. Through\nMars, we aim to galvanize advancements in situated inductive reasoning and set\nthe stage for developing the next generation of AI systems that can reason in\nan adaptive and context-sensitive way.\n","authors":["Xiaojuan Tang","Jiaqi Li","Yitao Liang","Song-chun Zhu","Muhan Zhang","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08126v2.pdf","comment":"Accepted by NeurIPS 2024 Track Datasets and Benchmarks. Project page:\n  https://marscrafter.github.io/"},{"id":"http://arxiv.org/abs/2410.23822v1","updated":"2024-10-31T11:07:26Z","published":"2024-10-31T11:07:26Z","title":"Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding","summary":"  Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11393v2","updated":"2024-10-31T11:07:11Z","published":"2024-09-17T17:54:17Z","title":"LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents","summary":"  In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents.\n","authors":["Amine Ben Hassouna","Hana Chaari","Ines Belhaj"],"pdf_url":"https://arxiv.org/pdf/2409.11393v2.pdf","comment":"36 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.23820v1","updated":"2024-10-31T11:05:09Z","published":"2024-10-31T11:05:09Z","title":"Disentangling Disentangled Representations: Towards Improved Latent\n  Units via Diffusion Models","summary":"  Disentangled representation learning (DRL) aims to break down observed data\ninto core intrinsic factors for a profound understanding of the data. In\nreal-world scenarios, manually defining and labeling these factors are\nnon-trivial, making unsupervised methods attractive. Recently, there have been\nlimited explorations of utilizing diffusion models (DMs), which are already\nmainstream in generative modeling, for unsupervised DRL. They implement their\nown inductive bias to ensure that each latent unit input to the DM expresses\nonly one distinct factor. In this context, we design Dynamic Gaussian Anchoring\nto enforce attribute-separated latent units for more interpretable DRL. This\nunconventional inductive bias explicitly delineates the decision boundaries\nbetween attributes while also promoting the independence among latent units.\nAdditionally, we also propose Skip Dropout technique, which easily modifies the\ndenoising U-Net to be more DRL-friendly, addressing its uncooperative nature\nwith the disentangling feature extractor. Our methods, which carefully consider\nthe latent unit semantics and the distinct DM structure, enhance the\npracticality of DM-based disentangled representations, demonstrating\nstate-of-the-art disentanglement performance on both synthetic and real data,\nas well as advantages in downstream tasks.\n","authors":["Youngjun Jun","Jiwoo Park","Kyobin Choo","Tae Eun Choi","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.23820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23815v1","updated":"2024-10-31T10:58:59Z","published":"2024-10-31T10:58:59Z","title":"The NPU-HWC System for the ISCSLP 2024 Inspirational and Convincing\n  Audio Generation Challenge","summary":"  This paper presents the NPU-HWC system submitted to the ISCSLP 2024\nInspirational and Convincing Audio Generation Challenge 2024 (ICAGC). Our\nsystem consists of two modules: a speech generator for Track 1 and a background\naudio generator for Track 2. In Track 1, we employ Single-Codec to tokenize the\nspeech into discrete tokens and use a language-model-based approach to achieve\nzero-shot speaking style cloning. The Single-Codec effectively decouples timbre\nand speaking style at the token level, reducing the acoustic modeling burden on\nthe autoregressive language model. Additionally, we use DSPGAN to upsample 16\nkHz mel-spectrograms to high-fidelity 48 kHz waveforms. In Track 2, we propose\na background audio generator based on large language models (LLMs). This system\nproduces scene-appropriate accompaniment descriptions, synthesizes background\naudio with Tango 2, and integrates it with the speech generated by our Track 1\nsystem. Our submission achieves the second place and the first place in Track 1\nand Track 2 respectively.\n","authors":["Dake Guo","Jixun Yao","Xinfa Zhu","Kangxiang Xia","Zhao Guo","Ziyu Zhang","Yao Wang","Jie Liu","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2410.23815v1.pdf","comment":"accepted by ISCSLP 2024"},{"id":"http://arxiv.org/abs/2410.23810v1","updated":"2024-10-31T10:52:42Z","published":"2024-10-31T10:52:42Z","title":"CALE: Continuous Arcade Learning Environment","summary":"  We introduce the Continuous Arcade Learning Environment (CALE), an extension\nof the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013].\nThe CALE uses the same underlying emulator of the Atari 2600 gaming system\n(Stella), but adds support for continuous actions. This enables the\nbenchmarking and evaluation of continuous-control agents (such as PPO [Schulman\net al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as\nDQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same\nenvironment suite. We provide a series of open questions and research\ndirections that CALE enables, as well as initial baseline results using Soft\nActor-Critic. CALE is available as part of the ALE\nathttps://github.com/Farama-Foundation/Arcade-Learning-Environment.\n","authors":["Jesse Farebrother","Pablo Samuel Castro"],"pdf_url":"https://arxiv.org/pdf/2410.23810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08245v2","updated":"2024-10-31T10:44:50Z","published":"2024-10-10T09:37:21Z","title":"Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible\n  Mixture-of-Experts","summary":"  Multimodal learning has gained increasing importance across various fields,\noffering the ability to integrate data from diverse sources such as images,\ntext, and personalized records, which are frequently observed in medical\ndomains. However, in scenarios where some modalities are missing, many existing\nframeworks struggle to accommodate arbitrary modality combinations, often\nrelying heavily on a single modality or complete data. This oversight of\npotential modality combinations limits their applicability in real-world\nsituations. To address this challenge, we propose Flex-MoE (Flexible\nMixture-of-Experts), a new framework designed to flexibly incorporate arbitrary\nmodality combinations while maintaining robustness to missing data. The core\nidea of Flex-MoE is to first address missing modalities using a new missing\nmodality bank that integrates observed modality combinations with the\ncorresponding missing ones. This is followed by a uniquely designed Sparse MoE\nframework. Specifically, Flex-MoE first trains experts using samples with all\nmodalities to inject generalized knowledge through the generalized router\n($\\mathcal{G}$-Router). The $\\mathcal{S}$-Router then specializes in handling\nfewer modality combinations by assigning the top-1 gate to the expert\ncorresponding to the observed modality combination. We evaluate Flex-MoE on the\nADNI dataset, which encompasses four modalities in the Alzheimer's Disease\ndomain, as well as on the MIMIC-IV dataset. The results demonstrate the\neffectiveness of Flex-MoE highlighting its ability to model arbitrary modality\ncombinations in diverse missing modality scenarios. Code is available at\nhttps://github.com/UNITES-Lab/flex-moe.\n","authors":["Sukwon Yun","Inyoung Choi","Jie Peng","Yangfan Wu","Jingxuan Bao","Qiyiwen Zhang","Jiayi Xin","Qi Long","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.08245v2.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2410.23803v1","updated":"2024-10-31T10:43:43Z","published":"2024-10-31T10:43:43Z","title":"Generative AI for Accessible and Inclusive Extended Reality","summary":"  Artificial Intelligence-Generated Content (AIGC) has the potential to\ntransform how people build and interact with virtual environments. Within this\npaper, we discuss potential benefits but also challenges that AIGC has for the\ncreation of inclusive and accessible virtual environments. Specifically, we\ntouch upon the decreased need for 3D modeling expertise, benefits of\nsymbolic-only as well as multimodal input, 3D content editing, and 3D model\naccessibility as well as foundation model-specific challenges.\n","authors":["Jens Grubert","Junlong Chen","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2410.23803v1.pdf","comment":"Presented at the CHI 2024 Workshop \"Building a Metaverse for All:\n  Opportunities and Challenges for Future Inclusive and Accessible Virtual\n  Environments\", May 11, 2024, Honolulu, Hawaii"},{"id":"http://arxiv.org/abs/2410.14067v2","updated":"2024-10-31T10:38:47Z","published":"2024-10-17T22:35:50Z","title":"Provable Benefits of Complex Parameterizations for Structured State\n  Space Models","summary":"  Structured state space models (SSMs), the core engine behind prominent neural\nnetworks such as S4 and Mamba, are linear dynamical systems adhering to a\nspecified structure, most notably diagonal. In contrast to typical neural\nnetwork modules, whose parameterizations are real, SSMs often use complex\nparameterizations. Theoretically explaining the benefits of complex\nparameterizations for SSMs is an open problem. The current paper takes a step\ntowards its resolution, by establishing formal gaps between real and complex\ndiagonal SSMs. Firstly, we prove that while a moderate dimension suffices in\norder for a complex SSM to express all mappings of a real SSM, a much higher\ndimension is needed for a real SSM to express mappings of a complex SSM.\nSecondly, we prove that even if the dimension of a real SSM is high enough to\nexpress a given mapping, typically, doing so requires the parameters of the\nreal SSM to hold exponentially large values, which cannot be learned in\npractice. In contrast, a complex SSM can express any given mapping with\nmoderate parameter values. Experiments corroborate our theory, and suggest a\npotential extension of the theory that accounts for selectivity, a new\narchitectural feature yielding state of the art performance.\n","authors":["Yuval Ran-Milo","Eden Lumbroso","Edo Cohen-Karlik","Raja Giryes","Amir Globerson","Nadav Cohen"],"pdf_url":"https://arxiv.org/pdf/2410.14067v2.pdf","comment":"12 pages. Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23796v1","updated":"2024-10-31T10:27:48Z","published":"2024-10-31T10:27:48Z","title":"Improving snore detection under limited dataset through\n  harmonic/percussive source separation and convolutional neural networks","summary":"  Snoring, an acoustic biomarker commonly observed in individuals with\nObstructive Sleep Apnoea Syndrome (OSAS), holds significant potential for\ndiagnosing and monitoring this recognized clinical disorder. Irrespective of\nsnoring types, most snoring instances exhibit identifiable harmonic patterns\nmanifested through distinctive energy distributions over time. In this work, we\npropose a novel method to differentiate monaural snoring from non-snoring\nsounds by analyzing the harmonic content of the input sound using\nharmonic/percussive sound source separation (HPSS). The resulting feature,\nbased on the harmonic spectrogram from HPSS, is employed as input data for\nconventional neural network architectures, aiming to enhance snoring detection\nperformance even under a limited data learning framework. To evaluate the\nperformance of our proposal, we studied two different scenarios: 1) using a\nlarge dataset of snoring and interfering sounds, and 2) using a reduced\ntraining set composed of around 1% of the data material. In the former\nscenario, the proposed HPSS-based feature provides competitive results compared\nto other input features from the literature. However, the key advantage of the\nproposed method lies in the superior performance of the harmonic spectrogram\nderived from HPSS in a limited data learning context. In this particular\nscenario, using the proposed harmonic feature significantly enhances the\nperformance of all the studied architectures in comparison to the classical\ninput features documented in the existing literature. This finding clearly\ndemonstrates that incorporating harmonic content enables more reliable learning\nof the essential time-frequency characteristics that are prevalent in most\nsnoring sounds, even in scenarios where the amount of training data is limited.\n","authors":["F. D. Gonzalez-Martinez","J. J. Carabias-Orti","F. J. Canadas-Quesada","N. Ruiz-Reyes","D. Martinez-Munoz","S. Garcia-Galan"],"pdf_url":"https://arxiv.org/pdf/2410.23796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15081v3","updated":"2024-10-31T10:19:17Z","published":"2024-10-19T12:00:13Z","title":"A Distribution Semantics for Probabilistic Term Rewriting","summary":"  Probabilistic programming is becoming increasingly popular thanks to its\nability to specify problems with a certain degree of uncertainty. In this work,\nwe focus on term rewriting, a well-known computational formalism. In\nparticular, we consider systems that combine traditional rewriting rules with\nprobabilities. Then, we define a distribution semantics for such systems that\ncan be used to model the probability of reducing a term to some value. We also\nshow how to compute a set of \"explanations\" for a given reduction, which can be\nused to compute its probability. Finally, we illustrate our approach with\nseveral examples and outline a couple of extensions that may prove useful to\nimprove the expressive power of probabilistic rewrite systems.\n","authors":["Germ√°n Vidal"],"pdf_url":"https://arxiv.org/pdf/2410.15081v3.pdf","comment":"Submitted for publication"},{"id":"http://arxiv.org/abs/2410.23788v1","updated":"2024-10-31T10:13:05Z","published":"2024-10-31T10:13:05Z","title":"EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like\n  Sketching","summary":"  Transformer-based Diffusion Probabilistic Models (DPMs) have shown more\npotential than CNN-based DPMs, yet their extensive computational requirements\nhinder widespread practical applications. To reduce the computation budget of\ntransformer-based DPMs, this work proposes the Efficient Diffusion Transformer\n(EDT) framework. The framework includes a lightweight-design diffusion model\narchitecture, and a training-free Attention Modulation Matrix and its\nalternation arrangement in EDT inspired by human-like sketching. Additionally,\nwe propose a token relation-enhanced masking training strategy tailored\nexplicitly for EDT to augment its token relation learning capability. Our\nextensive experiments demonstrate the efficacy of EDT. The EDT framework\nreduces training and inference costs and surpasses existing transformer-based\ndiffusion models in image synthesis performance, thereby achieving a\nsignificant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL\nattained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training\nphase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the\ncorresponding sizes of MDTv2. The source code is released at\nhttps://github.com/xinwangChen/EDT.\n","authors":["Xinwang Chen","Ning Liu","Yichen Zhu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2410.23788v1.pdf","comment":"Xinwang Chen and Ning Liu are with equal contributions. This paper\n  has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.15187v2","updated":"2024-10-31T10:10:28Z","published":"2024-06-21T14:29:39Z","title":"UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world\n  Document Analysis","summary":"  The use of Retrieval-Augmented Generation (RAG) has improved Large Language\nModels (LLMs) in collaborating with external data, yet significant challenges\nexist in real-world scenarios. In areas such as academic literature and finance\nquestion answering, data are often found in raw text and tables in HTML or PDF\nformats, which can be lengthy and highly unstructured. In this paper, we\nintroduce a benchmark suite, namely Unstructured Document Analysis (UDA), that\ninvolves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We\nrevisit popular LLM- and RAG-based solutions for document analysis and evaluate\nthe design choices and answer qualities across multiple document domains and\ndiverse query types. Our evaluation yields interesting findings and highlights\nthe importance of data parsing and retrieval. We hope our benchmark can shed\nlight and better serve real-world document analysis applications. The benchmark\nsuite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.\n","authors":["Yulong Hui","Yao Lu","Huanchen Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.15187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05266v2","updated":"2024-10-31T10:07:54Z","published":"2024-03-08T12:42:36Z","title":"ERBench: An Entity-Relationship based Automatically Verifiable\n  Hallucination Benchmark for Large Language Models","summary":"  Large language models (LLMs) have achieved unprecedented performances in\nvarious applications, yet evaluating them is still challenging. Existing\nbenchmarks are either manually constructed or are automatic, but lack the\nability to evaluate the thought process of LLMs with arbitrary complexity. We\ncontend that utilizing existing relational databases based on the\nentity-relationship (ER) model is a promising approach for constructing\nbenchmarks as they contain structured knowledge that can be used to question\nLLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational\ndatabases have integrity constraints that can be used to better construct\ncomplex in-depth questions and verify answers: (1) functional dependencies can\nbe used to pinpoint critical keywords that an LLM must know to properly answer\na given question containing certain attribute values; and (2) foreign key\nconstraints can be used to join relations and construct multi-hop questions,\nwhich can be arbitrarily long and used to debug intermediate answers. We thus\npropose ERBench, which uses these integrity constraints to convert any database\ninto an LLM benchmark. ERBench supports continuous evaluation as databases\nchange, multimodal questions, and various prompt engineering techniques. In our\nexperiments, we construct LLM benchmarks using databases of multiple domains\nand make an extensive comparison of contemporary LLMs. We show how ERBench can\nproperly evaluate any LLM by not only checking for answer correctness, but also\neffectively verifying the rationales by looking for the right keywords.\n","authors":["Jio Oh","Soyeon Kim","Junseok Seo","Jindong Wang","Ruochen Xu","Xing Xie","Steven Euijong Whang"],"pdf_url":"https://arxiv.org/pdf/2403.05266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13711v2","updated":"2024-10-31T09:58:47Z","published":"2024-07-18T17:08:58Z","title":"FSP-Laplace: Function-Space Priors for the Laplace Approximation in\n  Bayesian Deep Learning","summary":"  Laplace approximations are popular techniques for endowing deep networks with\nepistemic uncertainty estimates as they can be applied without altering the\npredictions of the trained network, and they scale to large models and\ndatasets. While the choice of prior strongly affects the resulting posterior\ndistribution, computational tractability and lack of interpretability of the\nweight space typically limit the Laplace approximation to isotropic Gaussian\npriors, which are known to cause pathological behavior as depth increases. As a\nremedy, we directly place a prior on function space. More precisely, since\nLebesgue densities do not exist on infinite-dimensional function spaces, we\nrecast training as finding the so-called weak mode of the posterior measure\nunder a Gaussian process (GP) prior restricted to the space of functions\nrepresentable by the neural network. Through the GP prior, one can express\nstructured and interpretable inductive biases, such as regularity or\nperiodicity, directly in function space, while still exploiting the implicit\ninductive biases that allow deep networks to generalize. After model\nlinearization, the training objective induces a negative log-posterior density\nto which we apply a Laplace approximation, leveraging highly scalable methods\nfrom matrix-free linear algebra. Our method provides improved results where\nprior knowledge is abundant (as is the case in many scientific inference\ntasks). At the same time, it stays competitive for black-box supervised\nlearning problems, where neural networks typically excel.\n","authors":["Tristan Cinquin","Marvin Pf√∂rtner","Vincent Fortuin","Philipp Hennig","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2407.13711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23780v1","updated":"2024-10-31T09:53:21Z","published":"2024-10-31T09:53:21Z","title":"Driving by the Rules: A Benchmark for Integrating Traffic Sign\n  Regulations into Vectorized HD Map","summary":"  Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current benchmark datasets concentrate\non lane perception or basic traffic sign recognition, they often overlook the\nintricate task of integrating these regulations into lane operations.\nAddressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over 10,000 annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from\nTraffic Sign, which accurately deciphers regulatory instructions, and 2)\nRule-Lane Correspondence Reasoning, which aligns these rules with their\nrespective lanes. Built upon this benchmark, we provide a multimodal solution\nthat offers a strong baseline for advancing autonomous driving technologies. It\nfills a critical gap in the integration of traffic sign rules, contributing to\nthe development of reliable autonomous navigation systems.\n","authors":["Xinyuan Chang","Maixuan Xue","Xinran Liu","Zheng Pan","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2410.23780v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2402.15393v4","updated":"2024-10-31T09:46:13Z","published":"2024-02-23T15:51:45Z","title":"NeuralSolver: Learning Algorithms For Consistent and Efficient\n  Extrapolation Across General Tasks","summary":"  We contribute NeuralSolver, a novel recurrent solver that can efficiently and\nconsistently extrapolate, i.e., learn algorithms from smaller problems (in\nterms of observation size) and execute those algorithms in large problems.\nContrary to previous recurrent solvers, NeuralSolver can be naturally applied\nin both same-size problems, where the input and output sizes are the same, and\nin different-size problems, where the size of the input and output differ. To\nallow for this versatility, we design NeuralSolver with three main components:\na recurrent module, that iteratively processes input information at different\nscales, a processing module, responsible for aggregating the previously\nprocessed information, and a curriculum-based training scheme, that improves\nthe extrapolation performance of the method. To evaluate our method we\nintroduce a set of novel different-size tasks and we show that NeuralSolver\nconsistently outperforms the prior state-of-the-art recurrent solvers in\nextrapolating to larger problems, considering smaller training problems and\nrequiring less parameters than other approaches.\n","authors":["Bernardo Esteves","Miguel Vasco","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2402.15393v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17619v2","updated":"2024-10-31T09:45:51Z","published":"2024-10-23T07:17:31Z","title":"From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management","summary":"  This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows.\n","authors":["Juhani Merilehto"],"pdf_url":"https://arxiv.org/pdf/2410.17619v2.pdf","comment":"11 pages, 1 figure; corrected the corresponding authors e-mail"},{"id":"http://arxiv.org/abs/2410.14790v2","updated":"2024-10-31T09:34:45Z","published":"2024-10-02T13:42:38Z","title":"SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for\n  Efficient 3D Plant Reconstruction by a Robot","summary":"  The 3D reconstruction of plants is challenging due to their complex shape\ncausing many occlusions. Next-Best-View (NBV) methods address this by\niteratively selecting new viewpoints to maximize information gain (IG).\nDeep-learning-based NBV (DL-NBV) methods demonstrate higher computational\nefficiency over classic voxel-based NBV approaches but current methods require\nextensive training using ground-truth plant models, making them impractical for\nreal-world plants. These methods, moreover, rely on offline training with\npre-collected data, limiting adaptability in changing agricultural\nenvironments. This paper proposes a self-supervised learning-based NBV method\n(SSL-NBV) that uses a deep neural network to predict the IG for candidate\nviewpoints. The method allows the robot to gather its own training data during\ntask execution by comparing new 3D sensor data to the earlier gathered data and\nby employing weakly-supervised learning and experience replay for efficient\nonline learning. Comprehensive evaluations were conducted in simulation and\nreal-world environments using cross-validation. The results showed that SSL-NBV\nrequired fewer views for plant reconstruction than non-NBV methods and was over\n800 times faster than a voxel-based method. SSL-NBV reduced training\nannotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV\ncould adapt to novel scenarios through online fine-tuning. Also using real\nplants, the results showed that the proposed method can learn to effectively\nplan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV\nautomated the entire network training and uses continuous online learning,\nallowing it to operate in changing agricultural environments.\n","authors":["Jianchao Ci","Eldert J. van Henten","Xin Wang","Akshay K. Burusa","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2410.14790v2.pdf","comment":"22 pages, 11 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.23769v1","updated":"2024-10-31T09:33:37Z","published":"2024-10-31T09:33:37Z","title":"The Potential of LLMs in Medical Education: Generating Questions and\n  Answers for Qualification Exams","summary":"  Recent research on large language models (LLMs) has primarily focused on\ntheir adaptation and application in specialized domains. The application of\nLLMs in the medical field is mainly concentrated on tasks such as the\nautomation of medical report generation, summarization, diagnostic reasoning,\nand question-and-answer interactions between doctors and patients. The\nchallenge of becoming a good teacher is more formidable than that of becoming a\ngood student, and this study pioneers the application of LLMs in the field of\nmedical education. In this work, we investigate the extent to which LLMs can\ngenerate medical qualification exam questions and corresponding answers based\non few-shot prompts. Utilizing a real-world Chinese dataset of elderly chronic\ndiseases, we tasked the LLMs with generating open-ended questions and answers\nbased on a subset of sampled admission reports across eight widely used LLMs,\nincluding ERNIE 4, ChatGLM 4, Doubao, Hunyuan, Spark 4, Qwen, Llama 3, and\nMistral. Furthermore, we engaged medical experts to manually evaluate these\nopen-ended questions and answers across multiple dimensions. The study found\nthat LLMs, after using few-shot prompts, can effectively mimic real-world\nmedical qualification exam questions, whereas there is room for improvement in\nthe correctness, evidence-based statements, and professionalism of the\ngenerated answers. Moreover, LLMs also demonstrate a decent level of ability to\ncorrect and rectify reference answers. Given the immense potential of\nartificial intelligence in the medical field, the task of generating questions\nand answers for medical qualification exams aimed at medical students, interns\nand residents can be a significant focus of future research.\n","authors":["Yunqi Zhu","Wen Tang","Ying Sun","Xuebing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.23769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23753v1","updated":"2024-10-31T09:18:47Z","published":"2024-10-31T09:18:47Z","title":"Enhancing Chess Reinforcement Learning with Graph Representation","summary":"  Mastering games is a hard task, as games can be extremely complex, and still\nfundamentally different in structure from one another. While the AlphaZero\nalgorithm has demonstrated an impressive ability to learn the rules and\nstrategy of a large variety of games, ranging from Go and Chess, to Atari\ngames, its reliance on extensive computational resources and rigid\nConvolutional Neural Network (CNN) architecture limits its adaptability and\nscalability. A model trained to play on a $19\\times 19$ Go board cannot be used\nto play on a smaller $13\\times 13$ board, despite the similarity between the\ntwo Go variants. In this paper, we focus on Chess, and explore using a more\ngeneric Graph-based Representation of a game state, rather than a grid-based\none, to introduce a more general architecture based on Graph Neural Networks\n(GNN). We also expand the classical Graph Attention Network (GAT) layer to\nincorporate edge-features, to naturally provide a generic policy output format.\nOur experiments, performed on smaller networks than the initial AlphaZero\npaper, show that this new architecture outperforms previous architectures with\na similar number of parameters, being able to increase playing strength an\norder of magnitude faster. We also show that the model, when trained on a\nsmaller $5\\times 5$ variant of chess, is able to be quickly fine-tuned to play\non regular $8\\times 8$ chess, suggesting that this approach yields promising\ngeneralization abilities. Our code is available at\nhttps://github.com/akulen/AlphaGateau.\n","authors":["Tomas Rigaux","Hisashi Kashima"],"pdf_url":"https://arxiv.org/pdf/2410.23753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05975v4","updated":"2024-10-31T09:14:56Z","published":"2024-01-11T15:22:55Z","title":"End-to-end Learnable Clustering for Intent Learning in Recommendation","summary":"  Intent learning, which aims to learn users' intents for user understanding\nand item recommendation, has become a hot research spot in recent years.\nHowever, existing methods suffer from complex and cumbersome alternating\noptimization, limiting performance and scalability. To this end, we propose a\nnovel intent learning method termed \\underline{ELCRec}, by unifying behavior\nrepresentation learning into an \\underline{E}nd-to-end \\underline{L}earnable\n\\underline{C}lustering framework, for effective and efficient\n\\underline{Rec}ommendation. Concretely, we encode user behavior sequences and\ninitialize the cluster centers (latent intents) as learnable neurons. Then, we\ndesign a novel learnable clustering module to separate different cluster\ncenters, thus decoupling users' complex intents. Meanwhile, it guides the\nnetwork to learn intents from behaviors by forcing behavior embeddings close to\ncluster centers. This allows simultaneous optimization of recommendation and\nclustering via mini-batch data. Moreover, we propose intent-assisted\ncontrastive learning by using cluster centers as self-supervision signals,\nfurther enhancing mutual promotion. Both experimental results and theoretical\nanalyses demonstrate the superiority of ELCRec from six perspectives. Compared\nto the runner-up, ELCRec improves NDCG@5 by 8.9\\% and reduces computational\ncosts by 22.5\\% on the Beauty dataset. Furthermore, due to the scalability and\nuniversal applicability, we deploy this method on the industrial recommendation\nsystem with 130 million page views and achieve promising results. The codes are\navailable on GitHub (https://github.com/yueliu1999/ELCRec). A collection\n(papers, codes, datasets) of deep group recommendation/intent learning methods\nis available on GitHub\n(https://github.com/yueliu1999/Awesome-Deep-Group-Recommendation).\n","authors":["Yue Liu","Shihao Zhu","Jun Xia","Yingwei Ma","Jian Ma","Xinwang Liu","Shengju Yu","Kejun Zhang","Wenliang Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.05975v4.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2410.23749v1","updated":"2024-10-31T09:09:39Z","published":"2024-10-31T09:09:39Z","title":"LSEAttention is All You Need for Time Series Forecasting","summary":"  Transformer-based architectures have achieved remarkable success in natural\nlanguage processing and computer vision. However, their performance in\nmultivariate long-term forecasting often lags behind simpler linear baselines.\nPrevious studies have identified the traditional attention mechanism as a\nsignificant factor contributing to this limitation. To unlock the full\npotential of transformers for multivariate time series forecasting, I introduce\n\\textbf{LSEAttention}, an approach designed to address entropy collapse and\ntraining instability commonly observed in transformer models. I validate the\neffectiveness of LSEAttention across various real-world multivariate time\nseries datasets, demonstrating that it not only outperforms existing time\nseries transformer models but also exceeds the performance of some\nstate-of-the-art models on specific datasets.\n","authors":["Dizhen Liang"],"pdf_url":"https://arxiv.org/pdf/2410.23749v1.pdf","comment":"7 pages with referencing, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2405.14702v2","updated":"2024-10-31T09:08:48Z","published":"2024-05-23T15:37:06Z","title":"G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n  Using Large Multi-Modality Models","summary":"  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods. Our code and data are available online for\nreproduction.\n","authors":["Pengyue Jia","Yiding Liu","Xiaopeng Li","Yuhao Wang","Yantong Du","Xiao Han","Xuetao Wei","Shuaiqiang Wang","Dawei Yin","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.14702v2.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23748v1","updated":"2024-10-31T09:07:08Z","published":"2024-10-31T09:07:08Z","title":"Exploring Consistency in Graph Representations:from Graph Kernels to\n  Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have emerged as a dominant approach in graph\nrepresentation learning, yet they often struggle to capture consistent\nsimilarity relationships among graphs. While graph kernel methods such as the\nWeisfeiler-Lehman subtree (WL-subtree) and Weisfeiler-Lehman optimal assignment\n(WLOA) kernels are effective in capturing similarity relationships, they rely\nheavily on predefined kernels and lack sufficient non-linearity for more\ncomplex data patterns. Our work aims to bridge the gap between neural network\nmethods and kernel approaches by enabling GNNs to consistently capture\nrelational structures in their learned representations. Given the analogy\nbetween the message-passing process of GNNs and WL algorithms, we thoroughly\ncompare and analyze the properties of WL-subtree and WLOA kernels. We find that\nthe similarities captured by WLOA at different iterations are asymptotically\nconsistent, ensuring that similar graphs remain similar in subsequent\niterations, thereby leading to superior performance over the WL-subtree kernel.\nInspired by these findings, we conjecture that the consistency in the\nsimilarities of graph representations across GNN layers is crucial in capturing\nrelational structures and enhancing graph classification performance. Thus, we\npropose a loss to enforce the similarity of graph representations to be\nconsistent across different layers. Our empirical analysis verifies our\nconjecture and shows that our proposed consistency loss can significantly\nenhance graph classification performance across several GNN backbones on\nvarious datasets.\n","authors":["Xuyuan Liu","Yinghao Cai","Qihui Yang","Yujun Yan"],"pdf_url":"https://arxiv.org/pdf/2410.23748v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23746v1","updated":"2024-10-31T09:01:25Z","published":"2024-10-31T09:01:25Z","title":"DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios","summary":"  Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating advanced prompt usages, human\nrevisions like word substitutions, and writing errors. Our development of\nDetectRL reveals the strengths and limitations of current SOTA detectors. More\nimportantly, we analyzed the potential impact of writing styles, model types,\nattack methods, the text lengths, and real-world human writing factors on\ndifferent types of detectors. We believe DetectRL could serve as an effective\nbenchmark for assessing detectors in real-world scenarios, evolving with\nadvanced attack methods, thus providing more stressful evaluation to drive the\ndevelopment of more efficient detectors. Data and code are publicly available\nat: https://github.com/NLP2CT/DetectRL.\n","authors":["Junchao Wu","Runzhe Zhan","Derek F. Wong","Shu Yang","Xinyi Yang","Yulin Yuan","Lidia S. Chao"],"pdf_url":"https://arxiv.org/pdf/2410.23746v1.pdf","comment":"Accepted to NeurIPS 2024 Dataset & Benchmarking Track"},{"id":"http://arxiv.org/abs/2410.23745v1","updated":"2024-10-31T09:00:24Z","published":"2024-10-31T09:00:24Z","title":"Syno: Structured Synthesis for Neural Operators","summary":"  The desires for better prediction accuracy and higher execution performance\nin neural networks never end. Neural architecture search (NAS) and tensor\ncompilers are two popular techniques to optimize these two goals, but they are\nboth limited to composing or optimizing existing manually designed operators\nrather than coming up with completely new designs. In this work, we explore the\nless studied direction of neural operator synthesis, which aims to\nautomatically and efficiently discover novel neural operators with better\naccuracy and/or speed. We develop an end-to-end framework Syno, to realize\npractical neural operator synthesis. Syno makes use of a novel set of\nfine-grained primitives defined on tensor dimensions, which ensure various\ndesired properties to ease model training, and also enable expression\ncanonicalization techniques to avoid redundant candidates during search. Syno\nfurther adopts a novel guided synthesis flow to obtain valid operators matched\nwith the specified input/output dimension sizes, and leverages efficient\nstochastic tree search algorithms to quickly explore the design space. We\ndemonstrate that Syno discovers better operators with an average of\n$2.06\\times$ speedup and less than $1\\%$ accuracy loss, even on NAS-optimized\nmodels.\n","authors":["Yongqi Zhuo","Zhengyuan Su","Chenggang Zhao","Mingyu Gao"],"pdf_url":"https://arxiv.org/pdf/2410.23745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23743v1","updated":"2024-10-31T08:58:06Z","published":"2024-10-31T08:58:06Z","title":"What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A\n  Gradient Perspective","summary":"  What makes a difference in the post-training of LLMs? We investigate the\ntraining patterns of different layers in large language models (LLMs), through\nthe lens of gradient, when training with different responses and initial\nmodels. We are specifically interested in how fast vs. slow thinking affects\nthe layer-wise gradients, given the recent popularity of training LLMs on\nreasoning paths such as chain-of-thoughts (CoT) and process rewards. In our\nstudy, fast thinking without CoT leads to larger gradients and larger\ndifferences of gradients across layers than slow thinking (Detailed CoT),\nindicating the learning stability brought by the latter. Moreover, pre-trained\nLLMs are less affected by the instability of fast thinking than\ninstruction-tuned LLMs. Additionally, we study whether the gradient patterns\ncan reflect the correctness of responses when training different LLMs using\nslow vs. fast thinking paths. The results show that the gradients of slow\nthinking can distinguish correct and irrelevant reasoning paths. As a\ncomparison, we conduct similar gradient analyses on non-reasoning knowledge\nlearning tasks, on which, however, trivially increasing the response length\ndoes not lead to similar behaviors of slow thinking. Our study strengthens\nfundamental understandings of LLM training and sheds novel insights on its\nefficiency and stability, which pave the way towards building a generalizable\nSystem-2 agent. Our code, data, and gradient statistics can be found in:\nhttps://github.com/MingLiiii/Layer_Gradient.\n","authors":["Ming Li","Yanhong Li","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16045v3","updated":"2024-10-31T08:55:49Z","published":"2023-12-26T13:17:25Z","title":"Algebraic Positional Encodings","summary":"  We introduce a novel positional encoding strategy for Transformer-style\nmodels, addressing the shortcomings of existing, often ad hoc, approaches. Our\nframework provides a flexible mapping from the algebraic specification of a\ndomain to an interpretation as orthogonal operators. This design preserves the\nalgebraic characteristics of the source domain, ensuring that the model upholds\nits desired structural properties. Our scheme can accommodate various\nstructures, ncluding sequences, grids and trees, as well as their compositions.\nWe conduct a series of experiments to demonstrate the practical applicability\nof our approach. Results suggest performance on par with or surpassing the\ncurrent state-of-the-art, without hyper-parameter optimizations or \"task\nsearch\" of any kind. Code is available at\nhttps://github.com/konstantinosKokos/ape.\n","authors":["Konstantinos Kogkalidis","Jean-Philippe Bernardy","Vikas Garg"],"pdf_url":"https://arxiv.org/pdf/2312.16045v3.pdf","comment":"NeurIPS 2024 (spotlight)"},{"id":"http://arxiv.org/abs/2406.11906v2","updated":"2024-10-31T08:54:52Z","published":"2024-06-16T08:23:21Z","title":"NovoBench: Benchmarking Deep Learning-based De Novo Peptide Sequencing\n  Methods in Proteomics","summary":"  Tandem mass spectrometry has played a pivotal role in advancing proteomics,\nenabling the high-throughput analysis of protein composition in biological\ntissues. Many deep learning methods have been developed for \\emph{de novo}\npeptide sequencing task, i.e., predicting the peptide sequence for the observed\nmass spectrum. However, two key challenges seriously hinder the further\nadvancement of this important task. Firstly, since there is no consensus for\nthe evaluation datasets, the empirical results in different research papers are\noften not comparable, leading to unfair comparison. Secondly, the current\nmethods are usually limited to amino acid-level or peptide-level precision and\nrecall metrics. In this work, we present the first unified benchmark NovoBench\nfor \\emph{de novo} peptide sequencing, which comprises diverse mass spectrum\ndata, integrated models, and comprehensive evaluation metrics. Recent\nimpressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo\nand $\\pi$-HelixNovo are integrated into our framework. In addition to amino\nacid-level and peptide-level precision and recall, we evaluate the models'\nperformance in terms of identifying post-tranlational modifications (PTMs),\nefficiency and robustness to peptide length, noise peaks and missing fragment\nratio, which are important influencing factors while seldom be considered.\nLeveraging this benchmark, we conduct a large-scale study of current methods,\nreport many insightful findings that open up new possibilities for future\ndevelopment.\n","authors":["Jingbo Zhou","Shaorong Chen","Jun Xia","Sizhe Liu","Tianze Ling","Wenjie Du","Yue Liu","Jianwei Yin","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2406.11906v2.pdf","comment":"NeurIPS 2024 D&B track"},{"id":"http://arxiv.org/abs/2410.05920v3","updated":"2024-10-31T08:47:01Z","published":"2024-10-08T11:16:03Z","title":"FINALLY: fast and universal speech enhancement with studio-like quality","summary":"  In this paper, we address the challenge of speech enhancement in real-world\nrecordings, which often contain various forms of distortion, such as background\nnoise, reverberation, and microphone artifacts. We revisit the use of\nGenerative Adversarial Networks (GANs) for speech enhancement and theoretically\nshow that GANs are naturally inclined to seek the point of maximum density\nwithin the conditional clean speech distribution, which, as we argue, is\nessential for the speech enhancement task. We study various feature extractors\nfor perceptual loss to facilitate the stability of adversarial training,\ndeveloping a methodology for probing the structure of the feature space. This\nleads us to integrate WavLM-based perceptual loss into MS-STFT adversarial\ntraining pipeline, creating an effective and stable training procedure for the\nspeech enhancement model. The resulting speech enhancement model, which we\nrefer to as FINALLY, builds upon the HiFi++ architecture, augmented with a\nWavLM encoder and a novel training pipeline. Empirical results on various\ndatasets confirm our model's ability to produce clear, high-quality speech at\n48 kHz, achieving state-of-the-art performance in the field of speech\nenhancement. Demo page: https://samsunglabs.github.io/FINALLY-page\n","authors":["Nicholas Babaev","Kirill Tamogashev","Azat Saginbaev","Ivan Shchekotov","Hanbin Bae","Hosang Sung","WonJun Lee","Hoon-Young Cho","Pavel Andreev"],"pdf_url":"https://arxiv.org/pdf/2410.05920v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2204.07481v3","updated":"2024-10-31T08:41:45Z","published":"2022-04-15T14:31:17Z","title":"Knowledge Equivalence in Digital Twins of Intelligent Systems","summary":"  A digital twin contains up-to-date data-driven models of the physical world\nbeing studied and can use simulation to optimise the physical world. However,\nthe analysis made by the digital twin is valid and reliable only when the model\nis equivalent to the physical world. Maintaining such an equivalent model is\nchallenging, especially when the physical systems being modelled are\nintelligent and autonomous. The paper focuses in particular on digital twin\nmodels of intelligent systems where the systems are knowledge-aware but with\nlimited capability. The digital twin improves the acting of the physical system\nat a meta-level by accumulating more knowledge in the simulated environment.\nThe modelling of such an intelligent physical system requires replicating the\nknowledge-awareness capability in the virtual space. Novel equivalence\nmaintaining techniques are needed, especially in synchronising the knowledge\nbetween the model and the physical system. This paper proposes the notion of\nknowledge equivalence and an equivalence maintaining approach by knowledge\ncomparison and updates. A quantitative analysis of the proposed approach\nconfirms that compared to state equivalence, knowledge equivalence maintenance\ncan tolerate deviation thus reducing unnecessary updates and achieve more\nPareto efficient solutions for the trade-off between update overhead and\nsimulation reliability.\n","authors":["Nan Zhang","Rami Bahsoon","Nikos Tziritas","Georgios Theodoropoulos"],"pdf_url":"https://arxiv.org/pdf/2204.07481v3.pdf","comment":"35 pages, 16 figures. In ACM Transactions on Modeling and Computer\n  Simulation (TOMACS)"},{"id":"http://arxiv.org/abs/2410.23726v1","updated":"2024-10-31T08:26:51Z","published":"2024-10-31T08:26:51Z","title":"Towards Reliable Alignment: Uncertainty-aware RLHF","summary":"  Recent advances in aligning Large Language Models with human preferences have\nbenefited from larger reward models and better preference data. However, most\nof these methodologies rely on the accuracy of the reward model. The reward\nmodels used in Reinforcement Learning with Human Feedback (RLHF) are typically\nlearned from small datasets using stochastic optimization algorithms, making\nthem prone to high variability. We illustrate the inconsistencies between\nreward models empirically on numerous open-source datasets.\n  We theoretically show that the fluctuation of the reward models can be\ndetrimental to the alignment problem because the derived policies are more\noverfitted to the reward model and, hence, are riskier if the reward model\nitself is uncertain. We use concentration of measure to motivate an\nuncertainty-aware, conservative algorithm for policy optimization. We show that\nsuch policies are more risk-averse in the sense that they are more cautious of\nuncertain rewards. We theoretically prove that our proposed methodology has\nless risk than the vanilla method.\n  We corroborate our theoretical results with experiments based on designing an\nensemble of reward models. We use this ensemble of reward models to align a\nlanguage model using our methodology and observe that our empirical findings\nmatch our theoretical predictions.\n","authors":["Debangshu Banerjee","Aditya Gopalan"],"pdf_url":"https://arxiv.org/pdf/2410.23726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23725v1","updated":"2024-10-31T08:24:37Z","published":"2024-10-31T08:24:37Z","title":"Artificial intelligence to improve clinical coding practice in\n  Scandinavia: a crossover randomized controlled trial","summary":"  \\textbf{Trial design} Crossover randomized controlled trial. \\textbf{Methods}\nAn AI tool, Easy-ICD, was developed to assist clinical coders and was tested\nfor improving both accuracy and time in a user study in Norway and Sweden.\nParticipants were randomly assigned to two groups, and crossed over between\ncoding complex (longer) texts versus simple (shorter) texts, while using our\ntool versus not using our tool. \\textbf{Results} Based on Mann-Whitney U test,\nthe median coding time difference for complex clinical text sequences was 123\nseconds (\\emph{P}\\textless.001, 95\\% CI: 81 to 164), representing a 46\\%\nreduction in median coding time when our tool is used. There was no significant\ntime difference for simpler text sequences. For coding accuracy, the\nimprovement we noted for both complex and simple texts was not significant.\n\\textbf{Conclusions} This study demonstrates the potential of AI to transform\ncommon tasks in clinical workflows, with ostensible positive impacts on work\nefficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly\nunderstood.\n","authors":["Taridzo Chomutare","Therese Olsen Svenning","Miguel √Ångel Tejedor Hern√°ndez","Phuong Dinh Ngo","Andrius Budrionis","Kaisa Markljung","Lill Irene Hind","Torbj√∏rn Torsvik","Karl √òyvind Mikalsen","Aleksandar Babic","Hercules Dalianis"],"pdf_url":"https://arxiv.org/pdf/2410.23725v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.23724v1","updated":"2024-10-31T08:19:58Z","published":"2024-10-31T08:19:58Z","title":"Argumentation and Machine Learning","summary":"  This chapter provides an overview of research works that present approaches\nwith some degree of cross-fertilisation between Computational Argumentation and\nMachine Learning. Our review of the literature identified two broad themes\nrepresenting the purpose of the interaction between these two areas:\nargumentation for machine learning and machine learning for argumentation.\nAcross these two themes, we systematically evaluate the spectrum of works\nacross various dimensions, including the type of learning and the form of\nargumentation framework used. Further, we identify three types of interaction\nbetween these two areas: synergistic approaches, where the Argumentation and\nMachine Learning components are tightly integrated; segmented approaches, where\nthe two are interleaved such that the outputs of one are the inputs of the\nother; and approximated approaches, where one component shadows the other at a\nchosen level of detail. We draw conclusions about the suitability of certain\nforms of Argumentation for supporting certain types of Machine Learning, and\nvice versa, with clear patterns emerging from the review. Whilst the reviewed\nworks provide inspiration for successfully combining the two fields of\nresearch, we also identify and discuss limitations and challenges that ought to\nbe addressed in order to ensure that they remain a fruitful pairing as AI\nadvances.\n","authors":["Antonio Rago","Kristijonas ƒåyras","Jack Mumford","Oana Cocarascu"],"pdf_url":"https://arxiv.org/pdf/2410.23724v1.pdf","comment":"44 pages, to appear in the Handbook of Formal Argumentation and the\n  Journal of Applied Logics"},{"id":"http://arxiv.org/abs/2407.03978v3","updated":"2024-10-31T08:11:04Z","published":"2024-07-04T14:50:45Z","title":"Benchmarking Complex Instruction-Following with Multiple Constraints\n  Composition","summary":"  Instruction following is one of the fundamental capabilities of large\nlanguage models (LLMs). As the ability of LLMs is constantly improving, they\nhave been increasingly applied to deal with complex human instructions in\nreal-world scenarios. Therefore, how to evaluate the ability of complex\ninstruction-following of LLMs has become a critical research problem. Existing\nbenchmarks mainly focus on modeling different types of constraints in human\ninstructions while neglecting the composition of different constraints, which\nis an indispensable constituent in complex instructions. To this end, we\npropose ComplexBench, a benchmark for comprehensively evaluating the ability of\nLLMs to follow complex instructions composed of multiple constraints. We\npropose a hierarchical taxonomy for complex instructions, including 4\nconstraint types, 19 constraint dimensions, and 4 composition types, and\nmanually collect a high-quality dataset accordingly. To make the evaluation\nreliable, we augment LLM-based evaluators with rules to effectively verify\nwhether generated texts can satisfy each constraint and composition.\nFurthermore, we obtain the final evaluation score based on the dependency\nstructure determined by different composition types. ComplexBench identifies\nsignificant deficiencies in existing LLMs when dealing with complex\ninstructions with multiple constraints composition.\n","authors":["Bosi Wen","Pei Ke","Xiaotao Gu","Lindong Wu","Hao Huang","Jinfeng Zhou","Wenchuang Li","Binxin Hu","Wendy Gao","Jiaxin Xu","Yiming Liu","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03978v3.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.14754v2","updated":"2024-10-31T07:50:22Z","published":"2024-10-18T06:57:37Z","title":"On the Sparsity of the Strong Lottery Ticket Hypothesis","summary":"  Considerable research efforts have recently been made to show that a random\nneural network $N$ contains subnetworks capable of accurately approximating any\ngiven neural network that is sufficiently smaller than $N$, without any\ntraining. This line of research, known as the Strong Lottery Ticket Hypothesis\n(SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which\nstates that a sufficiently large random neural network $N$ contains\n\\emph{sparse} subnetworks that can be trained efficiently to achieve\nperformance comparable to that of training the entire network $N$. Despite its\noriginal motivation, results on the SLTH have so far not provided any guarantee\non the size of subnetworks. Such limitation is due to the nature of the main\ntechnical tool leveraged by these results, the Random Subset Sum (RSS) Problem.\nInformally, the RSS Problem asks how large a random i.i.d. sample $\\Omega$\nshould be so that we are able to approximate any number in $[-1,1]$, up to an\nerror of $ \\epsilon$, as the sum of a suitable subset of $\\Omega$. We provide\nthe first proof of the SLTH in classical settings, such as dense and\nequivariant networks, with guarantees on the sparsity of the subnetworks.\nCentral to our results, is the proof of an essentially tight bound on the\nRandom Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in\nwhich we only ask for subsets of a given size, which is of independent\ninterest.\n","authors":["Emanuele Natale","Davide Ferre'","Giordano Giambartolomei","Fr√©d√©ric Giroire","Frederik Mallmann-Trenn"],"pdf_url":"https://arxiv.org/pdf/2410.14754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07933v2","updated":"2024-10-31T07:36:39Z","published":"2024-06-12T06:56:20Z","title":"Large Language Model Unlearning via Embedding-Corrupted Prompts","summary":"  Large language models (LLMs) have advanced to encompass extensive knowledge\nacross diverse domains. Yet controlling what a large language model should not\nknow is important for ensuring alignment and thus safe use. However, accurately\nand efficiently unlearning knowledge from an LLM remains challenging due to the\npotential collateral damage caused by the fuzzy boundary between retention and\nforgetting, and the large computational requirements for optimization across\nstate-of-the-art models with hundreds of billions of parameters. In this work,\nwe present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning\nframework for large language models to address both the challenges of knowledge\nentanglement and unlearning efficiency. Instead of relying on the LLM itself to\nunlearn, we enforce an unlearned state during inference by employing a prompt\nclassifier to identify and safeguard prompts to forget. We learn corruptions\nadded to prompt embeddings via zeroth order optimization toward the unlearning\nobjective offline and corrupt prompts flagged by the classifier during\ninference. We find that these embedding-corrupted prompts not only lead to\ndesirable outputs that satisfy the unlearning objective but also closely\napproximate the output from a model that has never been trained on the data\nintended for forgetting. Through extensive experiments on unlearning, we\ndemonstrate the superiority of our method in achieving promising unlearning at\n\\textit{nearly zero side effects} in general domains and domains closely\nrelated to the unlearned ones. Additionally, we highlight the scalability of\nour method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no\nadditional cost as the number of parameters increases. We have made our code\npublicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.\n","authors":["Chris Yuhao Liu","Yaxuan Wang","Jeffrey Flanigan","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07933v2.pdf","comment":"NeurIPS 2024 Poster"},{"id":"http://arxiv.org/abs/2410.21991v2","updated":"2024-10-31T07:24:06Z","published":"2024-10-29T12:22:07Z","title":"From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System","summary":"  Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual-branch architecture, RuleVM\nachieves interpretable coarse-grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises.\n","authors":["Wen-Dong Jiang","Chih-Yung Chang","Hsiang-Chuan Chang","Diptendu Sinha Roy"],"pdf_url":"https://arxiv.org/pdf/2410.21991v2.pdf","comment":"12 pages,7 figures"},{"id":"http://arxiv.org/abs/2308.14329v2","updated":"2024-10-31T07:16:46Z","published":"2023-08-28T06:17:15Z","title":"End-to-End Driving via Self-Supervised Imitation Learning Using Camera\n  and LiDAR Data","summary":"  In autonomous driving, the end-to-end (E2E) driving approach that predicts\nvehicle control signals directly from sensor data is rapidly gaining attention.\nTo learn a safe E2E driving system, one needs an extensive amount of driving\ndata and human intervention. Vehicle control data is constructed by many hours\nof human driving, and it is challenging to construct large vehicle control\ndatasets. Often, publicly available driving datasets are collected with limited\ndriving scenes, and collecting vehicle control data is only available by\nvehicle manufacturers. To address these challenges, this letter proposes the\nfirst fully self-supervised learning framework, self-supervised imitation\nlearning (SSIL), for E2E driving, based on the self-supervised regression\nlearning framework. The proposed SSIL framework can learn E2E driving networks\nwithout using driving command data. To construct pseudo steering angle data,\nproposed SSIL predicts a pseudo target from the vehicle's poses at the current\nand previous time points that are estimated with light detection and ranging\nsensors. In addition, we propose two modified E2E driving networks that predict\ndriving commands depending on high-level instruction. Our numerical experiments\nwith three different benchmark datasets demonstrate that the proposed SSIL\nframework achieves very comparable E2E driving accuracy with the supervised\nlearning counterpart.\n","authors":["Jin Bok Park","Jinkyu Lee","Muhyun Back","Hyunmin Han","David T. Ma","Sang Min Won","Sung Soo Hwang","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2308.14329v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23680v1","updated":"2024-10-31T07:08:14Z","published":"2024-10-31T07:08:14Z","title":"Rethinking Inverse Reinforcement Learning: from Data Alignment to Task\n  Alignment","summary":"  Many imitation learning (IL) algorithms use inverse reinforcement learning\n(IRL) to infer a reward function that aligns with the demonstration. However,\nthe inferred reward functions often fail to capture the underlying task\nobjectives. In this paper, we propose a novel framework for IRL-based IL that\nprioritizes task alignment over conventional data alignment. Our framework is a\nsemi-supervised approach that leverages expert demonstrations as weak\nsupervision to derive a set of candidate reward functions that align with the\ntask rather than only with the data. It then adopts an adversarial mechanism to\ntrain a policy with this set of reward functions to gain a collective\nvalidation of the policy's ability to accomplish the task. We provide\ntheoretical insights into this framework's ability to mitigate task-reward\nmisalignment and present a practical implementation. Our experimental results\nshow that our framework outperforms conventional IL baselines in complex and\ntransfer learning scenarios.\n","authors":["Weichao Zhou","Wenchao Li"],"pdf_url":"https://arxiv.org/pdf/2410.23680v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2306.01731"},{"id":"http://arxiv.org/abs/2410.23672v1","updated":"2024-10-31T06:41:10Z","published":"2024-10-31T06:41:10Z","title":"Provable Benefit of Cutout and CutMix for Feature Learning","summary":"  Patch-level data augmentation techniques such as Cutout and CutMix have\ndemonstrated significant efficacy in enhancing the performance of vision tasks.\nHowever, a comprehensive theoretical understanding of these methods remains\nelusive. In this paper, we study two-layer neural networks trained using three\ndistinct methods: vanilla training without augmentation, Cutout training, and\nCutMix training. Our analysis focuses on a feature-noise data model, which\nconsists of several label-dependent features of varying rarity and\nlabel-independent noises of differing strengths. Our theorems demonstrate that\nCutout training can learn low-frequency features that vanilla training cannot,\nwhile CutMix training can learn even rarer features that Cutout cannot capture.\nFrom this, we establish that CutMix yields the highest test accuracy among the\nthree. Our novel analysis reveals that CutMix training makes the network learn\nall features and noise vectors \"evenly\" regardless of the rarity and strength,\nwhich provides an interesting insight into understanding patch-level\naugmentation.\n","authors":["Junsoo Oh","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2410.23672v1.pdf","comment":"NeurIPS 2024 camera-ready version, 81 pages"},{"id":"http://arxiv.org/abs/2410.23668v1","updated":"2024-10-31T06:32:47Z","published":"2024-10-31T06:32:47Z","title":"Kernel Looping: Eliminating Synchronization Boundaries for Peak\n  Inference Performance","summary":"  Token generation speed is critical to power the next wave of AI inference\napplications. GPUs significantly underperform during token generation due to\nsynchronization overheads at kernel boundaries, utilizing only 21% of their\npeak memory bandwidth. While recent dataflow architectures mitigate these\noverheads by enabling aggressive fusion of decoder layers into a single kernel,\nthey too leave performance on the table due to synchronization penalties at\nlayer boundaries.\n  This paper presents kernel looping, a specialized global optimization\ntechnique which exploits an optimization opportunity brought by combining the\nunique layer-level fusion possible in modern dataflow architectures with the\nrepeated layer structure found in language models. Kernel looping eliminates\nsynchronization costs between consecutive calls to the same kernel by\ntransforming these calls into a single call to a modified kernel containing a\npipelined outer loop. We evaluate kernel looping on the SambaNova SN40L\nReconfigurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI.\nExperiments demonstrate that kernel looping speeds up the decode phase of a\nwide array of powerful open-source models by up to 2.2$\\times$ on SN40L. Kernel\nlooping allows scaling of decode performance over multiple SN40L sockets,\nachieving speedups of up to 2.5$\\times$. Finally, kernel looping enables SN40L\nto achieve over 90% of peak performance on 8 and 16 sockets and achieve a\nspeedup of up to 3.7$\\times$ over DGX H100. Kernel looping, as well as the\nmodels evaluated in this paper, are deployed in production in a commercial AI\ninference cloud.\n","authors":["David Koeplinger","Darshan Gandhi","Pushkar Nandkar","Nathan Sheeley","Matheen Musaddiq","Leon Zhang","Reid Goodbar","Matthew Shaffer","Han Wang","Angela Wang","Mingran Wang","Raghu Prabhakar"],"pdf_url":"https://arxiv.org/pdf/2410.23668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02769v3","updated":"2024-10-31T06:17:08Z","published":"2024-02-05T07:05:17Z","title":"Learning from Teaching Regularization: Generalizable Correlations Should\n  be Easy to Imitate","summary":"  Generalization remains a central challenge in machine learning. In this work,\nwe propose Learning from Teaching (LoT), a novel regularization technique for\ndeep neural networks to enhance generalization. Inspired by the human ability\nto capture concise and abstract patterns, we hypothesize that generalizable\ncorrelations are expected to be easier to imitate. LoT operationalizes this\nconcept to improve the generalization of the main model with auxiliary student\nlearners. The student learners are trained by the main model and, in turn,\nprovide feedback to help the main model capture more generalizable and imitable\ncorrelations. Our experimental results across several domains, including\nComputer Vision, Natural Language Processing, and methodologies like\nReinforcement Learning, demonstrate that the introduction of LoT brings\nsignificant benefits compared to training models on the original dataset. The\nresults suggest the effectiveness and efficiency of LoT in identifying\ngeneralizable information at the right scales while discarding spurious data\ncorrelations, thus making LoT a valuable addition to current machine learning.\nCode is available at https://github.com/jincan333/LoT.\n","authors":["Can Jin","Tong Che","Hongwu Peng","Yiyuan Li","Dimitris N. Metaxas","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2402.02769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20109v2","updated":"2024-10-31T06:16:24Z","published":"2024-07-29T15:36:42Z","title":"Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement\n  Learning","summary":"  One important property of DIstribution Correction Estimation (DICE) methods\nis that the solution is the optimal stationary distribution ratio between the\noptimized and data collection policy. In this work, we show that DICE-based\nmethods can be viewed as a transformation from the behavior distribution to the\noptimal policy distribution. Based on this, we propose a novel approach,\nDiffusion-DICE, that directly performs this transformation using diffusion\nmodels. We find that the optimal policy's score function can be decomposed into\ntwo terms: the behavior policy's score function and the gradient of a guidance\nterm which depends on the optimal distribution ratio. The first term can be\nobtained from a diffusion model trained on the dataset and we propose an\nin-sample learning objective to learn the second term. Due to the\nmulti-modality contained in the optimal policy distribution, the transformation\nin Diffusion-DICE may guide towards those local-optimal modes. We thus generate\na few candidate actions and carefully select from them to approach\nglobal-optimum. Different from all other diffusion-based offline RL methods,\nthe guide-then-select paradigm in Diffusion-DICE only uses in-sample actions\nfor training and brings minimal error exploitation in the value function. We\nuse a didatic toycase example to show how previous diffusion-based methods fail\nto generate optimal actions due to leveraging these errors and how\nDiffusion-DICE successfully avoids that. We then conduct extensive experiments\non benchmark datasets to show the strong performance of Diffusion-DICE. Project\npage at https://ryanxhr.github.io/Diffusion-DICE/.\n","authors":["Liyuan Mao","Haoran Xu","Xianyuan Zhan","Weinan Zhang","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.20109v2.pdf","comment":"NeurIPS 2024, first two authors contribute equally"},{"id":"http://arxiv.org/abs/2410.21352v2","updated":"2024-10-31T06:01:26Z","published":"2024-10-28T14:45:01Z","title":"LLMCBench: Benchmarking Large Language Model Compression for Efficient\n  Deployment","summary":"  Although large language models (LLMs) have demonstrated their strong\nintelligence ability, the high demand for computation and storage hinders their\npractical application. To this end, many model compression techniques are\nproposed to increase the efficiency of LLMs. However, current researches only\nvalidate their methods on limited models, datasets, metrics, etc, and still\nlack a comprehensive evaluation under more general scenarios. So it is still a\nquestion of which model compression approach we should use under a specific\ncase. To mitigate this gap, we present the Large Language Model Compression\nBenchmark (LLMCBench), a rigorously designed benchmark with an in-depth\nanalysis for LLM compression algorithms. We first analyze the actual model\nproduction requirements and carefully design evaluation tracks and metrics.\nThen, we conduct extensive experiments and comparison using multiple mainstream\nLLM compression approaches. Finally, we perform an in-depth analysis based on\nthe evaluation and provide useful insight for LLM compression design. We hope\nour LLMCBench can contribute insightful suggestions for LLM compression\nalgorithm design and serve as a foundation for future research. Our code is\navailable at https://github.com/AboveParadise/LLMCBench.\n","authors":["Ge Yang","Changyi He","Jinyang Guo","Jianyu Wu","Yifu Ding","Aishan Liu","Haotong Qin","Pengliang Ji","Xianglong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21352v2.pdf","comment":"Accepted by NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.18475v2","updated":"2024-10-31T05:56:03Z","published":"2024-10-24T06:54:27Z","title":"Gene-Metabolite Association Prediction with Interactive Knowledge\n  Transfer Enhanced Graph for Metabolite Production","summary":"  In the rapidly evolving field of metabolic engineering, the quest for\nefficient and precise gene target identification for metabolite production\nenhancement presents significant challenges. Traditional approaches, whether\nknowledge-based or model-based, are notably time-consuming and labor-intensive,\ndue to the vast scale of research literature and the approximation nature of\ngenome-scale metabolic model (GEM) simulations. Therefore, we propose a new\ntask, Gene-Metabolite Association Prediction based on metabolic graphs, to\nautomate the process of candidate gene discovery for a given pair of metabolite\nand candidate-associated genes, as well as presenting the first benchmark\ncontaining 2474 metabolites and 1947 genes of two commonly used microorganisms\nSaccharomyces cerevisiae (SC) and Issatchenkia orientalis (IO). This task is\nchallenging due to the incompleteness of the metabolic graphs and the\nheterogeneity among distinct metabolisms. To overcome these limitations, we\npropose an Interactive Knowledge Transfer mechanism based on Metabolism Graph\n(IKT4Meta), which improves the association prediction accuracy by integrating\nthe knowledge from different metabolism graphs. First, to build a bridge\nbetween two graphs for knowledge transfer, we utilize Pretrained Language\nModels (PLMs) with external knowledge of genes and metabolites to help generate\ninter-graph links, significantly alleviating the impact of heterogeneity.\nSecond, we propagate intra-graph links from different metabolic graphs using\ninter-graph links as anchors. Finally, we conduct the gene-metabolite\nassociation prediction based on the enriched metabolism graphs, which integrate\nthe knowledge from multiple microorganisms. Experiments on both types of\norganisms demonstrate that our proposed methodology outperforms baselines by up\nto 12.3% across various link prediction frameworks.\n","authors":["Kexuan Xin","Qingyun Wang","Junyu Chen","Pengfei Yu","Huimin Zhao","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2410.18475v2.pdf","comment":"10 PAGES, 4 FIGURES; bibm 2024"},{"id":"http://arxiv.org/abs/2410.23649v1","updated":"2024-10-31T05:40:08Z","published":"2024-10-31T05:40:08Z","title":"Deep Convolutional Neural Networks on Multiclass Classification of\n  Three-Dimensional Brain Images for Parkinson's Disease Stage Prediction","summary":"  Parkinson's disease (PD), a degenerative disorder of the central nervous\nsystem, is commonly diagnosed using functional medical imaging techniques such\nas single-photon emission computed tomography (SPECT). In this study, we\nutilized two SPECT data sets (n = 634 and n = 202) from different hospitals to\ndevelop a model capable of accurately predicting PD stages, a multiclass\nclassification task. We used the entire three-dimensional (3D) brain images as\ninput and experimented with various model architectures. Initially, we treated\nthe 3D images as sequences of two-dimensional (2D) slices and fed them\nsequentially into 2D convolutional neural network (CNN) models pretrained on\nImageNet, averaging the outputs to obtain the final predicted stage. We also\napplied 3D CNN models pretrained on Kinetics-400. Additionally, we incorporated\nan attention mechanism to account for the varying importance of different\nslices in the prediction process. To further enhance model efficacy and\nrobustness, we simultaneously trained the two data sets using weight sharing, a\ntechnique known as cotraining. Our results demonstrated that 2D models\npretrained on ImageNet outperformed 3D models pretrained on Kinetics-400, and\nmodels utilizing the attention mechanism outperformed both 2D and 3D models.\nThe cotraining technique proved effective in improving model performance when\nthe cotraining data sets were sufficiently large.\n","authors":["Guan-Hua Huang","Wan-Chen Lai","Tai-Been Chen","Chien-Chin Hsu","Huei-Yung Chen","Yi-Chen Wu","Li-Ren Yeh"],"pdf_url":"https://arxiv.org/pdf/2410.23649v1.pdf","comment":"34 pages, 7 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2406.14596v3","updated":"2024-10-31T05:38:39Z","published":"2024-06-20T17:45:02Z","title":"VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs","summary":"  Large-scale generative language and vision-language models excel in\nin-context learning for decision making. However, they require high-quality\nexemplar demonstrations to be included in their context window. In this work,\nwe ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal\ndemonstrations? We propose In-Context Abstraction Learning (ICAL), a method\nthat builds a memory of multimodal experience from sub-optimal demonstrations\nand human feedback. Given a task demonstration that may contain inefficiencies\nor mistakes, a VLM abstracts the trajectory into a generalized program by\ncorrecting inefficient actions and annotating cognitive abstractions: causal\nrelationships, object state changes, temporal subgoals, and task-relevant\nvisual elements. These abstractions are iteratively improved through human\nfeedback while the agent attempts to execute the trajectory. The resulting\nexamples, when used as exemplars in the prompt, significantly improve\ndecision-making in retrieval-augmented LLM and VLM agents. Moreover, as the\nagent's library of examples grows, it becomes more efficient, relying less on\nhuman feedback and requiring fewer environment interactions per demonstration.\nOur ICAL agent surpasses the state-of-the-art in dialogue-based instruction\nfollowing in TEACh, multimodal web agents in VisualWebArena, and action\nanticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in\ngoal-condition success. In VisualWebArena, our task success rate improves over\nthe SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we\nimprove over few-shot GPT-4V and remain competitive with supervised models. We\nshow finetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such abstractions.\n","authors":["Gabriel Sarch","Lawrence Jang","Michael J. Tarr","William W. Cohen","Kenneth Marino","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2406.14596v3.pdf","comment":"Project website: http://ical-learning.github.io/"},{"id":"http://arxiv.org/abs/2405.10624v3","updated":"2024-10-31T05:24:19Z","published":"2024-05-17T08:39:05Z","title":"Sample-Efficient Constrained Reinforcement Learning with General\n  Parameterization","summary":"  We consider a constrained Markov Decision Problem (CMDP) where the goal of an\nagent is to maximize the expected discounted sum of rewards over an infinite\nhorizon while ensuring that the expected discounted sum of costs exceeds a\ncertain threshold. Building on the idea of momentum-based acceleration, we\ndevelop the Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG) algorithm\nthat ensures an $\\epsilon$ global optimality gap and $\\epsilon$ constraint\nviolation with $\\tilde{\\mathcal{O}}((1-\\gamma)^{-7}\\epsilon^{-2})$ sample\ncomplexity for general parameterized policies where $\\gamma$ denotes the\ndiscount factor. This improves the state-of-the-art sample complexity in\ngeneral parameterized CMDPs by a factor of\n$\\mathcal{O}((1-\\gamma)^{-1}\\epsilon^{-2})$ and achieves the theoretical lower\nbound in $\\epsilon^{-1}$.\n","authors":["Washim Uddin Mondal","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2405.10624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14430v3","updated":"2024-10-31T05:14:31Z","published":"2024-05-23T11:00:07Z","title":"PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers\n  Inference","summary":"  This paper presents PipeFusion, an innovative parallel methodology to tackle\nthe high latency issues associated with generating high-resolution images using\ndiffusion transformers (DiTs) models. PipeFusion partitions images into patches\nand the model layers across multiple GPUs. It employs a patch-level pipeline\nparallel strategy to orchestrate communication and computation efficiently. By\ncapitalizing on the high similarity between inputs from successive diffusion\nsteps, PipeFusion reuses one-step stale feature maps to provide context for the\ncurrent pipeline step. This approach notably reduces communication costs\ncompared to existing DiTs inference parallelism, including tensor parallel,\nsequence parallel and DistriFusion. PipeFusion also exhibits superior memory\nefficiency, because it can distribute model parameters across multiple devices,\nmaking it more suitable for DiTs with large parameter sizes, such as Flux.1.\nExperimental results demonstrate that PipeFusion achieves state-of-the-art\nperformance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1\nmodels.Our Source code is available at https://github.com/xdit-project/xDiT.\n","authors":["Jiarui Fang","Jinzhe Pan","Jiannan Wang","Aoyu Li","Xibo Sun"],"pdf_url":"https://arxiv.org/pdf/2405.14430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23637v1","updated":"2024-10-31T05:07:01Z","published":"2024-10-31T05:07:01Z","title":"Anytime-Constrained Multi-Agent Reinforcement Learning","summary":"  We introduce anytime constraints to the multi-agent setting with the\ncorresponding solution concept being anytime-constrained equilibrium (ACE).\nThen, we present a comprehensive theory of anytime-constrained Markov games,\nwhich includes (1) a computational characterization of feasible policies, (2) a\nfixed-parameter tractable algorithm for computing ACE, and (3) a\npolynomial-time algorithm for approximately computing feasible ACE. Since\ncomputing a feasible policy is NP-hard even for two-player zero-sum games, our\napproximation guarantees are the best possible under worst-case analysis. We\nalso develop the first theory of efficient computation for action-constrained\nMarkov games, which may be of independent interest.\n","authors":["Jeremy McMahan","Xiaojin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.23637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04601v2","updated":"2024-10-31T05:03:22Z","published":"2023-12-07T07:15:11Z","title":"Weak Supervision Performance Evaluation via Partial Identification","summary":"  Programmatic Weak Supervision (PWS) enables supervised model training without\ndirect access to ground truth labels, utilizing weak labels from heuristics,\ncrowdsourcing, or pre-trained models. However, the absence of ground truth\ncomplicates model evaluation, as traditional metrics such as accuracy,\nprecision, and recall cannot be directly calculated. In this work, we present a\nnovel method to address this challenge by framing model evaluation as a partial\nidentification problem and estimating performance bounds using Fr\\'echet\nbounds. Our approach derives reliable bounds on key metrics without requiring\nlabeled data, overcoming core limitations in current weak supervision\nevaluation techniques. Through scalable convex optimization, we obtain accurate\nand computationally efficient bounds for metrics including accuracy, precision,\nrecall, and F1-score, even in high-dimensional settings. This framework offers\na robust approach to assessing model quality without ground truth labels,\nenhancing the practicality of weakly supervised learning for real-world\napplications.\n","authors":["Felipe Maia Polo","Subha Maity","Mikhail Yurochkin","Moulinath Banerjee","Yuekai Sun"],"pdf_url":"https://arxiv.org/pdf/2312.04601v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.05421v3","updated":"2024-10-31T04:53:19Z","published":"2024-02-08T05:26:40Z","title":"DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement\n  and Imitation Learning","summary":"  This paper introduces DiffTORI, which utilizes Differentiable Trajectory\nOptimization as the policy representation to generate actions for deep\nReinforcement and Imitation learning. Trajectory optimization is a powerful and\nwidely used algorithm in control, parameterized by a cost and a dynamics\nfunction. The key to our approach is to leverage the recent progress in\ndifferentiable trajectory optimization, which enables computing the gradients\nof the loss with respect to the parameters of trajectory optimization. As a\nresult, the cost and dynamics functions of trajectory optimization can be\nlearned end-to-end. DiffTORI addresses the ``objective mismatch'' issue of\nprior model-based RL algorithms, as the dynamics model in DiffTORI is learned\nto directly maximize task performance by differentiating the policy gradient\nloss through the trajectory optimization process. We further benchmark DiffTORI\nfor imitation learning on standard robotic manipulation task suites with\nhigh-dimensional sensory observations and compare our method to feed-forward\npolicy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15\nmodel-based RL tasks and 35 imitation learning tasks with high-dimensional\nimage and point cloud inputs, DiffTORI outperforms prior state-of-the-art\nmethods in both domains.\n","authors":["Weikang Wan","Ziyu Wang","Yufei Wang","Zackory Erickson","David Held"],"pdf_url":"https://arxiv.org/pdf/2402.05421v3.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2402.02420v3","updated":"2024-10-31T04:50:59Z","published":"2024-02-04T09:36:31Z","title":"Factuality of Large Language Models: A Survey","summary":"  Large language models (LLMs), especially when instruction-tuned for chat,\nhave become part of our daily lives, freeing people from the process of\nsearching, extracting, and integrating information from multiple sources by\noffering a straightforward answer to a variety of questions in a single place.\nUnfortunately, in many cases, LLM responses are factually incorrect, which\nlimits their applicability in real-world scenarios. As a result, research on\nevaluating and improving the factuality of LLMs has attracted a lot of\nattention recently. In this survey, we critically analyze existing work with\nthe aim to identify the major challenges and their associated causes, pointing\nout to potential solutions for improving the factuality of LLMs, and analyzing\nthe obstacles to automated factuality evaluation for open-ended text\ngeneration. We further offer an outlook on where future research should go.\n","authors":["Yuxia Wang","Minghan Wang","Muhammad Arslan Manzoor","Fei Liu","Georgi Georgiev","Rocktim Jyoti Das","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2402.02420v3.pdf","comment":"11 pages, 1 figure and 2 tables"},{"id":"http://arxiv.org/abs/2410.23630v1","updated":"2024-10-31T04:46:52Z","published":"2024-10-31T04:46:52Z","title":"Adaptive Alignment: Dynamic Preference Adjustments via Multi-Objective\n  Reinforcement Learning for Pluralistic AI","summary":"  Emerging research in Pluralistic Artificial Intelligence (AI) alignment seeks\nto address how intelligent systems can be designed and deployed in accordance\nwith diverse human needs and values. We contribute to this pursuit with a\ndynamic approach for aligning AI with diverse and shifting user preferences\nthrough Multi Objective Reinforcement Learning (MORL), via post-learning policy\nselection adjustment. In this paper, we introduce the proposed framework for\nthis approach, outline its anticipated advantages and assumptions, and discuss\ntechnical details about the implementation. We also examine the broader\nimplications of adopting a retroactive alignment approach through the\nsociotechnical systems perspective.\n","authors":["Hadassah Harland","Richard Dazeley","Peter Vamplew","Hashini Senaratne","Bahareh Nakisa","Francisco Cruz"],"pdf_url":"https://arxiv.org/pdf/2410.23630v1.pdf","comment":"Accepted for the Pluralistic Alignment workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23629v1","updated":"2024-10-31T04:42:43Z","published":"2024-10-31T04:42:43Z","title":"Posture-Informed Muscular Force Learning for Robust Hand Pressure\n  Estimation","summary":"  We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface\nelectromyography (sEMG) signals. Our approach utilizes detailed spatial\ninformation from 3D hand poses in conjunction with dynamic muscle activity from\nsEMG to enable accurate and robust whole-hand pressure measurements under\ndiverse hand-object interactions. We also developed a multimodal data\ncollection system that combines a pressure glove, an sEMG armband, and a\nmarkerless finger-tracking module. We created a comprehensive dataset from 21\nparticipants, capturing synchronized data of hand posture, sEMG signals, and\nexerted hand pressure across various hand postures and hand-object interaction\nscenarios using our collection system. Our framework enables precise hand\npressure estimation in complex and natural interaction scenarios. Our approach\nsubstantially mitigates the limitations of traditional sEMG-based or\nvision-based methods by integrating 3D hand posture information with sEMG\nsignals. Video demos, data, and code are available online.\n","authors":["Kyungjin Seo","Junghoon Seo","Hanseok Jeong","Sangpil Kim","Sang Ho Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.23629v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.15154v2","updated":"2024-10-31T04:32:26Z","published":"2024-04-22T08:28:13Z","title":"Do not think about pink elephant!","summary":"  Large Models (LMs) have heightened expectations for the potential of general\nAI as they are akin to human intelligence. This paper shows that recent large\nmodels such as Stable Diffusion and DALL-E3 also share the vulnerability of\nhuman intelligence, namely the \"white bear phenomenon\". We investigate the\ncauses of the white bear phenomenon by analyzing their representation space.\nBased on this analysis, we propose a simple prompt-based attack method, which\ngenerates figures prohibited by the LM provider's policy. To counter these\nattacks, we introduce prompt-based defense strategies inspired by cognitive\ntherapy techniques, successfully mitigating attacks by up to 48.22\\%.\n","authors":["Kyomin Hwang","Suyoung Kim","JunHoo Lee","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2404.15154v2.pdf","comment":"This paper is accepted in CVPR 2024 Responsible Generative AI\n  Workshop (ReGenAI)"}],"Computational Geometry":[{"id":"http://arxiv.org/abs/2401.12881v2","updated":"2024-10-31T14:29:31Z","published":"2024-01-23T16:20:20Z","title":"Computing Diameter +1 in Truly Subquadratic Time for Unit-Disk Graphs","summary":"  Finding the diameter of a graph in general cannot be done in truly\nsubquadratic assuming the Strong Exponential Time Hypothesis (SETH), even when\nthe underlying graph is unweighted and sparse. When restricting to concrete\nclasses of graphs and assuming SETH, planar graphs and minor-free graphs admit\ntruly subquadratic algorithms, while geometric intersection graphs of unit\nballs, congruent equilateral triangles, and unit segments do not. Unit-disk\ngraphs are one of the major open cases where the complexity of diameter\ncomputation remains unknown. More generally, it is conjectured that a\ntruly-subquadratic time algorithm exists for pseudo-disk graphs where each pair\nof objects has at most two intersections on the boundary.\n  In this paper, we show a truly-subquadratic algorithm of running time\n$\\tilde{O}(n^{2-1/18})$, for finding the diameter in a unit-disk graph, whose\noutput differs from the optimal solution by at most 1. This is the first\nalgorithm that provides an additive guarantee in distortion, independent of the\nsize or the diameter of the graph. Our algorithm requires two important\ntechnical elements. First, we show that for the intersection graph of\npseudo-disks, the graph VC-dimension, either of $k$-hop balls or the distance\nencoding vectors, is 4. Second, we introduce a clique-based $r$-clustering for\ngeometric intersection graphs, which is an analog of the $r$-division\nconstruction for planar graphs. We also showcase the new techniques by\nestablishing new results for distance oracles for unit-disk graphs with\nsubquadratic storage and $O(1)$ query time. The results naturally extend to\nunit $L_1$- or $L_\\infty$-disks and fat pseudo-disks of similar size. Last, if\nthe pseudo-disks additionally have bounded ply, we have a truly-subquadratic\nalgorithm to find the exact diameter.\n","authors":["Hsien-Chih Chang","Jie Gao","Hung Le"],"pdf_url":"https://arxiv.org/pdf/2401.12881v2.pdf","comment":"Improving the approximation to +1 from +2 in the previous conference\n  version. Incorporating reviewers' comments. 30 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.23809v1","updated":"2024-10-31T10:48:28Z","published":"2024-10-31T10:48:28Z","title":"Flipping Non-Crossing Spanning Trees","summary":"  For a set $P$ of $n$ points in general position in the plane, the flip graph\n$F(P)$ has a vertex for each non-crossing spanning tree on $P$ and an edge\nbetween any two spanning trees that can be transformed into each other by one\nedge flip. The diameter ${\\rm diam}(F(P))$ of this graph is subject of\nintensive study. For points in general position, it is between $3n/2-5$ and\n$2n-4$, with no improvement for 25 years. For points in convex position, it\nlies between $3n/2 - 5$ and $\\approx1.95n$, where the lower bound was\nconjectured to be tight up to an additive constant and the upper bound is a\nrecent breakthrough improvement over several bounds of the form $2n-o(n)$.\n  In this work, we provide new upper and lower bounds on ${\\rm diam}(F(P))$,\nmainly focusing on points in convex position. We show $14n/9 - O(1) \\le {\\rm\ndiam}(F(P)) \\le 5n/3 - 3$, by this disproving the conjectured upper bound of\n$3n/2$ for convex position, and relevantly improving both the long-standing\nlower bound for general position and the recent new upper bound for convex\nposition. We complement these by showing that if one of $T,T'$ has at most two\nboundary edges, then ${\\rm dist}(T,T') \\le 2d/2 < 3n/2$, where $d = |T-T'|$ is\nthe number of edges in one tree that are not in the other.\n  To prove both the upper and the lower bound, we introduce a new powerful\ntool. Specifically, we convert the flip distance problem for given $T,T'$ to\nthe problem of a largest acyclic subset in an associated conflict graph\n$H(T,T')$. In fact, this method is powerful enough to give an equivalent\nformulation of the diameter of $F(P)$ for points $P$ in convex position up to\nlower-order terms. As such, conflict graphs are likely the key to a complete\nresolution of this and possibly also other reconfiguration problems.\n","authors":["H√•vard Bakke Bjerkevik","Linda Kleist","Torsten Ueckerdt","Birgit Vogtenhuber"],"pdf_url":"https://arxiv.org/pdf/2410.23809v1.pdf","comment":"an extended abstract appears at SODA 2025"},{"id":"http://arxiv.org/abs/2410.16833v2","updated":"2024-10-31T04:22:43Z","published":"2024-10-22T09:09:15Z","title":"Toroidal density-equalizing map for genus-one surfaces","summary":"  Density-equalizing map is a shape deformation technique originally developed\nfor cartogram creation and sociological data visualization on planar\ngeographical maps. In recent years, there has been an increasing interest in\ndeveloping density-equalizing mapping methods for surface and volumetric\ndomains and applying them to various problems in geometry processing and\nimaging science. However, the existing surface density-equalizing mapping\nmethods are only applicable to surfaces with relatively simple topologies but\nnot surfaces with topological holes. In this work, we develop a novel algorithm\nfor computing density-equalizing maps for toroidal surfaces. In particular,\ndifferent shape deformation effects can be easily achieved by prescribing\ndifferent population functions on the torus and performing diffusion-based\ndeformations on a planar domain with periodic boundary conditions. Furthermore,\nthe proposed toroidal density-equalizing mapping method naturally leads to an\neffective method for computing toroidal parameterizations of genus-one surfaces\nwith controllable shape changes, with the toroidal area-preserving\nparameterization being a prime example. Experimental results are presented to\ndemonstrate the effectiveness of our proposed methods.\n","authors":["Shunyu Yao","Gary P. T. Choi"],"pdf_url":"https://arxiv.org/pdf/2410.16833v2.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.24191v1","updated":"2024-10-31T17:51:10Z","published":"2024-10-31T17:51:10Z","title":"Flexible Demand Manipulation","summary":"  We develop a simple framework to analyze how targeted advertising interacts\nwith market power. A designer chooses an advertising plan which allows it to\nflexibly manipulate the demand curve at some cost. A monopolist prices against\nthis manipulated demand curve. We fully characterize the form and value of\nproducer-optimal and consumer-optimal advertising plans under both ex-ante and\nex-post measures of welfare. Flexibility is double-edged: producer-optimal\nplans substantially reduce consumer surplus vis-a-vis uniform advertising, but\nconsumer-optimal plans can substantially improve consumer surplus. We discuss\nimplications for the regulation of targeted advertising.\n","authors":["Yifan Dai","Andrew Koh"],"pdf_url":"https://arxiv.org/pdf/2410.24191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02147v2","updated":"2024-10-31T16:29:26Z","published":"2024-02-03T13:32:12Z","title":"Team-Fictitious Play for Reaching Team-Nash Equilibrium in Multi-team\n  Games","summary":"  Multi-team games, prevalent in robotics and resource management, involve team\nmembers striving for a joint best response against other teams. Team-Nash\nequilibrium (TNE) predicts the outcomes of such coordinated interactions.\nHowever, can teams of self-interested agents reach TNE? We introduce\nTeam-Fictitious Play (Team-FP), a new variant of fictitious play where agents\nrespond to the last actions of team members and the beliefs formed about other\nteams with some inertia in action updates. This design is essential in team\ncoordination beyond the classical fictitious play dynamics. We focus on\nzero-sum potential team games (ZSPTGs) where teams can interact pairwise while\nthe team members do not necessarily have identical payoffs. We show that\nTeam-FP reaches near TNE in ZSPTGs with a quantifiable error bound. We extend\nTeam-FP dynamics to multi-team Markov games for model-based and model-free\ncases. The convergence analysis tackles the challenge of non-stationarity\ninduced by evolving opponent strategies based on the optimal coupling lemma and\nstochastic differential inclusion approximation methods. Our work strengthens\nthe foundation for using TNE to predict the behavior of decentralized teams and\noffers a practical rule for team learning in multi-team environments. We\nprovide extensive simulations of Team-FP dynamics and compare its performance\nwith other widely studied dynamics such as smooth fictitious play and\nmultiplicative weights update. We further explore how different parameters\nimpact the speed of convergence.\n","authors":["Ahmed Said Donmez","Yuksel Arslantas","Muhammed O. Sayin"],"pdf_url":"https://arxiv.org/pdf/2402.02147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24095v1","updated":"2024-10-31T16:28:19Z","published":"2024-10-31T16:28:19Z","title":"Network Games Induced Prior for Graph Topology Learning","summary":"  Learning the graph topology of a complex network is challenging due to\nlimited data availability and imprecise data models. A common remedy in\nexisting works is to incorporate priors such as sparsity or modularity which\nhighlight on the structural property of graph topology. We depart from these\napproaches to develop priors that are directly inspired by complex network\ndynamics. Focusing on social networks with actions modeled by equilibriums of\nlinear quadratic games, we postulate that the social network topologies are\noptimized with respect to a social welfare function. Utilizing this prior\nknowledge, we propose a network games induced regularizer to assist graph\nlearning. We then formulate the graph topology learning problem as a bilevel\nprogram. We develop a two-timescale gradient algorithm to tackle the latter. We\ndraw theoretical insights on the optimal graph structure of the bilevel program\nand show that they agree with the topology in several man-made networks.\nEmpirically, we demonstrate the proposed formulation gives rise to reliable\nestimate of graph topology.\n","authors":["Chenyue Zhang","Shangyuan Liu","Hoi-To Wai","Anthony Man-Cho So"],"pdf_url":"https://arxiv.org/pdf/2410.24095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24062v1","updated":"2024-10-31T15:58:35Z","published":"2024-10-31T15:58:35Z","title":"HarvestTech agriculture cooperatives: Beneficiaries and compensations","summary":"  Agricultural industries face increasing pressure to optimize efficiency and\nreduce costs in a competitive and resource-constrained global market. As firms\nseek innovative ways to enhance productivity, cooperative strategies have\nemerged as a promising solution to address these challenges. In this context,\ngame theory provides a powerful framework for analyzing and structuring such\ncooperative efforts, ensuring that each firm's contribution is fairly rewarded.\nThis paper presents an innovative approach to address challenges in\nagricultural crop processing through inter-firm cooperation. A new class of\ntotally balanced games is introduced, which models the strategic interactions\namong companies processing agricultural products. The objective is to identify\nprofit allocations that fairly compensate firms contributing to cost reduction\nand surplus processing for others. To achieve this, the allocations resulting\nfrom each type of compensation will be thoroughly examined, and a coalitionally\nstable compensation procedure will be established. The study demonstrates the\nfeasibility and effectiveness of cooperative strategies for optimizing\nagricultural processes. Lastly, the findings will be applied to a case study.\n","authors":["Anjeza Bekolli","Luis A. Guardiola","Ana Meca"],"pdf_url":"https://arxiv.org/pdf/2410.24062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23989v1","updated":"2024-10-31T14:45:32Z","published":"2024-10-31T14:45:32Z","title":"Persuading a Credible Agent","summary":"  How to optimally persuade an agent who has a private type? When elicitation\nis feasible, this amounts to a fairly standard principal-agent-style mechanism\ndesign problem, where the persuader employs a mechanism to first elicit the\nagent's type and then plays the corresponding persuasion strategy based on the\nagent's report. The optimal mechanism design problem in this setting is\nrelatively well-understood in the literature, with incentive compatible (IC)\nmechanisms known to be optimal and computationally tractable. In this paper, we\nstudy this problem given a credible agent, i.e., if the agent claims they are\nof a certain type in response to the mechanism's elicitation, then they will\nact optimally with respect to the claimed type, even if they are actually not\nof that type.\n  We present several interesting findings in this new setting that differ\nsignificantly from results in the non-credible setting. In terms of the\nstructure of optimal mechanisms, we show that not only may IC mechanisms fail\nto be optimal, but all mechanisms following the standard\n`eliciting-then-persuading' mechanism design structure may be suboptimal. To\nachieve optimality requires two additional instruments -- pre-signaling and\nnon-binding elicitation -- which naturally result in multi-stage mechanisms. We\ncharacterize optimal mechanisms under these design choices. Based on our\ncharacterization, we provide a polynomial-time algorithm for computing optimal\nmulti-stage mechanisms. We also discover that in scenarios that allow for it,\npartial information elicitation can be employed to improve the principal's\npayoff even further. Though, surprisingly, an unbounded number of rounds of\ninformation exchange between the principal and the agent may be necessary to\nachieve optimality.\n","authors":["Jiarui Gan","Abheek Ghosh","Nicholas Teh"],"pdf_url":"https://arxiv.org/pdf/2410.23989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23979v1","updated":"2024-10-31T14:32:25Z","published":"2024-10-31T14:32:25Z","title":"Fair Division of Chores with Budget Constraints","summary":"  We study fair allocation of indivisible chores to agents under budget\nconstraints, where each chore has an objective size and disutility. This model\ncaptures scenarios where a set of chores need to be divided among agents with\nlimited time, and each chore has a specific time needed for completion. We\npropose a budget-constrained model for allocating indivisible chores, and\nsystematically explore the differences between goods and chores in this\nsetting. We establish the existence of an EFX allocation. We then show that EF2\nallocations are polynomial-time computable in general; for many restricted\nsettings, we strengthen this result to EF1. For divisible chores, we develop an\nefficient algorithm for computing an EF allocation.\n","authors":["Edith Elkind","Ayumi Igarashi","Nicholas Teh"],"pdf_url":"https://arxiv.org/pdf/2410.23979v1.pdf","comment":"Appears in the 17th International Symposium on Algorithmic Game\n  Theory (SAGT), 2024"},{"id":"http://arxiv.org/abs/2410.23953v1","updated":"2024-10-31T14:07:26Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v1.pdf","comment":"Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2410.23923v1","updated":"2024-10-31T13:33:19Z","published":"2024-10-31T13:33:19Z","title":"The museum pass problem with consortia","summary":"  In this paper, we extend the museum pass problem to incorporate the market\nstructure. To be more precise, we consider that museums are organized into\nseveral pass programs or consortia. Within this framework, we propose four\nallocation mechanisms based on the market structure and the principles of\nproportionality and egalitarianism. All these mechanisms satisfy different\nreasonable properties related to fairness and stability which serve to\naxiomatically characterize them.\n","authors":["Juan Carlos Gon√ßalves-Dosantos","Ricardo Mart√≠nez","Joaqu√≠n S√°nchez-Soriano"],"pdf_url":"https://arxiv.org/pdf/2410.23923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08541v2","updated":"2024-10-31T13:30:20Z","published":"2024-02-13T15:45:37Z","title":"Continuous-Time Best-Response and Related Dynamics in Tullock Contests\n  with Convex Costs","summary":"  Tullock contests model real-life scenarios that range from competition among\nproof-of-work blockchain miners to rent-seeking and lobbying activities. We\nshow that continuous-time best-response dynamics in Tullock contests with\nconvex costs converges to the unique equilibrium using Lyapunov-style\narguments. We then use this result to provide an algorithm for computing an\napproximate equilibrium. We also establish convergence of related discrete-time\ndynamics, e.g., when the agents best-respond to the empirical average action of\nother agents. These results indicate that the equilibrium is a reliable\npredictor of the agents' behavior in these games.\n","authors":["Edith Elkind","Abheek Ghosh","Paul W. Goldberg"],"pdf_url":"https://arxiv.org/pdf/2402.08541v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2309.10609v4","updated":"2024-10-31T12:50:28Z","published":"2023-09-19T13:32:34Z","title":"Game Connectivity and Adaptive Dynamics","summary":"  We analyse the typical structure of games in terms of the connectivity\nproperties of their best-response graphs. Our central result shows that almost\nevery game that is 'generic' (without indifferences) and has a pure Nash\nequilibrium and a 'large' number of players is connected, meaning that every\naction profile that is not a pure Nash equilibrium can reach every pure Nash\nequilibrium via best-response paths. This has important implications for\ndynamics in games. In particular, we show that there are simple, uncoupled,\nadaptive dynamics for which period-by-period play converges almost surely to a\npure Nash equilibrium in almost every large generic game that has one (which\ncontrasts with the known fact that there is no such dynamic that leads almost\nsurely to a pure Nash equilibrium in every generic game that has one). We build\non recent results in probabilistic combinatorics for our characterisation of\ngame connectivity.\n","authors":["Tom Johnston","Michael Savery","Alex Scott","Bassel Tarbush"],"pdf_url":"https://arxiv.org/pdf/2309.10609v4.pdf","comment":"42 pages; v4: modified exposition in the main text + minor changes\n  and additions"},{"id":"http://arxiv.org/abs/2410.23869v1","updated":"2024-10-31T12:21:26Z","published":"2024-10-31T12:21:26Z","title":"New Combinatorial Insights for Monotone Apportionment","summary":"  The apportionment problem constitutes a fundamental problem in democratic\nsocieties: How to distribute a fixed number of seats among a set of states in\nproportion to the states' populations? This--seemingly simple--task has led to\na rich literature and has become well known in the context of the US House of\nRepresentatives. In this paper, we connect the design of monotone apportionment\nmethods to classic problems from discrete geometry and combinatorial\noptimization and explore the extent to which randomization can enhance\nproportionality.\n  We first focus on the well-studied family of stationary divisor methods,\nwhich satisfy the strong population monotonicity property, and show that this\nfamily produces only a slightly superlinear number of different outputs as a\nfunction of the number of states. While our upper and lower bounds leave a\nsmall gap, we show that--surprisingly--closing this gap would solve a\nlong-standing open problem from discrete geometry, known as the complexity of\n$k$-levels in line arrangements. The main downside of divisor methods is their\nviolation of the quota axiom, i.e., every state should receive $\\lfloor\nq_i\\rfloor$ or $\\lceil q_i\\rceil$ seats, where $q_i$ is the proportional share\nof the state. As we show that randomizing over divisor methods can only\npartially overcome this issue, we propose a relaxed version of divisor methods\nin which the total number of seats may slightly deviate from the house size. By\nrandomizing over them, we can simultaneously satisfy population monotonicity,\nquota, and ex-ante proportionality.\n  Finally, we turn our attention to quota-compliant methods that are\nhouse-monotone, i.e., no state may lose a seat when the house size is\nincreased. We provide a polyhedral characterization based on network flows,\nwhich implies a simple description of all ex-ante proportional randomized\nmethods that are house-monotone and quota-compliant.\n","authors":["Javier Cembrano","Jos√© Correa","Ulrike Schmidt-Kraepelin","Alexandros Tsigonias-Dimitriadis","Victor Verdugo"],"pdf_url":"https://arxiv.org/pdf/2410.23869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13788v2","updated":"2024-10-31T11:37:46Z","published":"2024-05-22T16:12:45Z","title":"Quantum algorithm for large-scale market equilibrium computation","summary":"  Classical algorithms for market equilibrium computation such as proportional\nresponse dynamics face scalability issues with Internet-based applications such\nas auctions, recommender systems, and fair division, despite having an almost\nlinear runtime in terms of the product of buyers and goods. In this work, we\nprovide the first quantum algorithm for market equilibrium computation with\nsub-linear performance. Our algorithm provides a polynomial runtime speedup in\nterms of the product of the number of buyers and goods while reaching the same\noptimization objective value as the classical algorithm. Numerical simulations\nof a system with 16384 buyers and goods support our theoretical results that\nour quantum algorithm provides a significant speedup.\n","authors":["Po-Wei Huang","Patrick Rebentrost"],"pdf_url":"https://arxiv.org/pdf/2405.13788v2.pdf","comment":"Accepted to NeurIPS 2024, 15+8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.18958v2","updated":"2024-10-31T10:36:04Z","published":"2024-05-29T10:15:36Z","title":"Pessimism of the Will, Optimism of the Intellect: Fair Protocols with\n  Malicious but Rational Agents","summary":"  Fairness is a desirable and crucial property of many protocols that handle,\nfor instance, exchanges of message.\n  It states that if at least one agent engaging in the protocol is honest, then\neither the protocol will unfold correctly and fulfill its intended goal for all\nparticipants, or it will fail for everyone.\n  In this work, we present a game-based framework for the study of fairness\nprotocols, that does not define a priori an attacker model.\n  It is based on the notion of strong secure equilibria, and leverages the\nconceptual and algorithmic toolbox of game theory.\n  In the case of finite games, we provide decision procedures with tight\ncomplexity bounds for determining whether a protocol is immune to nefarious\nattacks from a coalition of participants, and whether such a protocol could\nexist based on the underlying graph structure and objectives.\n","authors":["L√©onard Brice","Jean-Fran√ßois Raskin","Mathieu Sassolas","Guillaume Scerri","Marie van den Bogaard"],"pdf_url":"https://arxiv.org/pdf/2405.18958v2.pdf","comment":"53 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.19030v2","updated":"2024-10-31T10:22:20Z","published":"2024-10-24T15:44:28Z","title":"State-Dependent Linear Utility Functions for Monetary Returns","summary":"  We present a theory of expected utility with state-dependent linear utility\nfunction for monetary returns, that includes results on first order stochastic\ndominance, mean-preserving spread, increasing-concave linear utility profiles\nand risk aversion. As an application of the expected utility theory developed\nhere, we analyze the contract that a monopolist would offer in an insurance\nmarket that allowed for partial coverage of loss. We also define a utility\nfunction for monetary returns that in a certain sense reconciles\nstate-dependent constant average utility of money with loss aversion and the\nFriedman-Savage hypothesis. As an immediate consequence of such a utility\nfunction, we obtain a profile of state-dependent linear utility functions for\nmonetary returns, where states of nature correspond to mutually disjoint\nintervals in which monetary gains and losses may occur.\n","authors":["Somdeb Lahiri"],"pdf_url":"https://arxiv.org/pdf/2410.19030v2.pdf","comment":"16 pages. Along with an earlier note on page 5, a new note on page 6\n  suggests a way of reconciling ambiguity aversion with expected utility and\n  loss aversion"},{"id":"http://arxiv.org/abs/2410.23683v1","updated":"2024-10-31T07:19:22Z","published":"2024-10-31T07:19:22Z","title":"Unveiling User Satisfaction and Creator Productivity Trade-Offs in\n  Recommendation Platforms","summary":"  On User-Generated Content (UGC) platforms, recommendation algorithms\nsignificantly impact creators' motivation to produce content as they compete\nfor algorithmically allocated user traffic. This phenomenon subtly shapes the\nvolume and diversity of the content pool, which is crucial for the platform's\nsustainability. In this work, we demonstrate, both theoretically and\nempirically, that a purely relevance-driven policy with low exploration\nstrength boosts short-term user satisfaction but undermines the long-term\nrichness of the content pool. In contrast, a more aggressive exploration policy\nmay slightly compromise user satisfaction but promote higher content creation\nvolume. Our findings reveal a fundamental trade-off between immediate user\nsatisfaction and overall content production on UGC platforms. Building on this\nfinding, we propose an efficient optimization method to identify the optimal\nexploration strength, balancing user and creator engagement. Our model can\nserve as a pre-deployment audit tool for recommendation algorithms on UGC\nplatforms, helping to align their immediate objectives with sustainable,\nlong-term goals.\n","authors":["Fan Yao","Yiming Liao","Jingzhou Liu","Shaoliang Nie","Qifan Wang","Haifeng Xu","Hongning Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23637v1","updated":"2024-10-31T05:07:01Z","published":"2024-10-31T05:07:01Z","title":"Anytime-Constrained Multi-Agent Reinforcement Learning","summary":"  We introduce anytime constraints to the multi-agent setting with the\ncorresponding solution concept being anytime-constrained equilibrium (ACE).\nThen, we present a comprehensive theory of anytime-constrained Markov games,\nwhich includes (1) a computational characterization of feasible policies, (2) a\nfixed-parameter tractable algorithm for computing ACE, and (3) a\npolynomial-time algorithm for approximately computing feasible ACE. Since\ncomputing a feasible policy is NP-hard even for two-player zero-sum games, our\napproximation guarantees are the best possible under worst-case analysis. We\nalso develop the first theory of efficient computation for action-constrained\nMarkov games, which may be of independent interest.\n","authors":["Jeremy McMahan","Xiaojin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.23637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00943v2","updated":"2024-10-31T02:22:59Z","published":"2024-03-01T19:41:54Z","title":"On the Hardness of Fair Allocation under Ternary Valuations","summary":"  We study the problem of fair allocation of indivisible items when agents have\nternary additive valuations -- each agent values each item at some fixed\ninteger values $a$, $b$, or $c$ that are common to all agents. The notions of\nfairness we consider are max Nash welfare (MNW), when $a$, $b$, and $c$ are\nnon-negative, and max egalitarian welfare (MEW). We show that for any distinct\nnon-negative $a$, $b$, and $c$, maximizing Nash welfare is APX-hard -- i.e.,\nthe problem does not admit a PTAS unless P = NP. We also show that for any\ndistinct $a$, $b$, and $c$, maximizing egalitarian welfare is APX-hard except\nfor a few cases when $b = 0$ that admit efficient algorithms. These results\nmake significant progress towards completely characterizing the complexity of\ncomputing exact MNW allocations and MEW allocations. En route, we resolve open\nquestions left by prior work regarding the complexity of computing MNW\nallocations under bivalued valuations, and MEW allocations under ternary mixed\nmanna.\n","authors":["Zack Fitzsimmons","Vignesh Viswanathan","Yair Zick"],"pdf_url":"https://arxiv.org/pdf/2403.00943v2.pdf","comment":"Fixed minor typos"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.24223v1","updated":"2024-10-31T17:59:56Z","published":"2024-10-31T17:59:56Z","title":"URAvatar: Universal Relightable Gaussian Codec Avatars","summary":"  We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.\n","authors":["Junxuan Li","Chen Cao","Gabriel Schwartz","Rawal Khirodkar","Christian Richardt","Tomas Simon","Yaser Sheikh","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2410.24223v1.pdf","comment":"SIGGRAPH Asia 2024. Website:\n  https://junxuan-li.github.io/urgca-website/"},{"id":"http://arxiv.org/abs/2410.24221v1","updated":"2024-10-31T17:59:55Z","published":"2024-10-31T17:59:55Z","title":"EgoMimic: Scaling Imitation Learning via Egocentric Video","summary":"  The scale and diversity of demonstration data required for imitation learning\nis a significant challenge. We present EgoMimic, a full-stack framework which\nscales manipulation via human embodiment data, specifically egocentric human\nvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) a\nsystem to capture human embodiment data using the ergonomic Project Aria\nglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap\nto human data, (3) cross-domain data alignment techniques, and (4) an imitation\nlearning architecture that co-trains on human and robot data. Compared to prior\nworks that only extract high-level intent from human videos, our approach\ntreats human and robot data equally as embodied demonstration data and learns a\nunified policy from both data sources. EgoMimic achieves significant\nimprovement on a diverse set of long-horizon, single-arm and bimanual\nmanipulation tasks over state-of-the-art imitation learning methods and enables\ngeneralization to entirely new scenes. Finally, we show a favorable scaling\ntrend for EgoMimic, where adding 1 hour of additional hand data is\nsignificantly more valuable than 1 hour of additional robot data. Videos and\nadditional information can be found at https://egomimic.github.io/\n","authors":["Simar Kareer","Dhruv Patel","Ryan Punamiya","Pranay Mathur","Shuo Cheng","Chen Wang","Judy Hoffman","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24219v1","updated":"2024-10-31T17:59:53Z","published":"2024-10-31T17:59:53Z","title":"Enhancing Motion in Text-to-Video Generation with Decomposed Encoding\n  and Conditioning","summary":"  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n","authors":["Penghui Ruan","Pichao Wang","Divya Saxena","Jiannong Cao","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2410.24219v1.pdf","comment":"Accepted at NeurIPS 2024, code available at\n  https://github.com/PR-Ryan/DEMO"},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2410.24214v1","updated":"2024-10-31T17:59:37Z","published":"2024-10-31T17:59:37Z","title":"ARQ: A Mixed-Precision Quantization Framework for Accurate and\n  Certifiably Robust DNNs","summary":"  Mixed precision quantization has become an important technique for enabling\nthe execution of deep neural networks (DNNs) on limited resource computing\nplatforms. Traditional quantization methods have primarily concentrated on\nmaintaining neural network accuracy, either ignoring the impact of quantization\non the robustness of the network, or using only empirical techniques for\nimproving robustness. In contrast, techniques for robustness certification,\nwhich can provide strong guarantees about the robustness of DNNs have not been\nused during quantization due to their high computation cost.\n  This paper introduces ARQ, an innovative mixed-precision quantization method\nthat not only preserves the clean accuracy of the smoothed classifiers but also\nmaintains their certified robustness. ARQ uses reinforcement learning to find\naccurate and robust DNN quantization, while efficiently leveraging randomized\nsmoothing, a popular class of statistical DNN verification algorithms, to guide\nthe search process.\n  We compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate\nthat ARQ consistently performs better than these baselines across all the\nbenchmarks and the input perturbation levels. In many cases, the performance of\nARQ quantized networks can reach that of the original DNN with floating-point\nweights, but with only 1.5% instructions.\n","authors":["Yuchen Yang","Shubham Ugare","Yifan Zhao","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2410.24214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24213v1","updated":"2024-10-31T17:59:30Z","published":"2024-10-31T17:59:30Z","title":"Learning Video Representations without Natural Videos","summary":"  In this paper, we show that useful video representations can be learned from\nsynthetic videos and natural images, without incorporating natural videos in\nthe training. We propose a progression of video datasets synthesized by simple\ngenerative processes, that model a growing set of natural video properties\n(e.g. motion, acceleration, and shape transformations). The downstream\nperformance of video models pre-trained on these generated datasets gradually\nincreases with the dataset progression. A VideoMAE model pre-trained on our\nsynthetic videos closes 97.2% of the performance gap on UCF101 action\nclassification between training from scratch and self-supervised pre-training\nfrom natural videos, and outperforms the pre-trained model on HMDB51.\nIntroducing crops of static images to the pre-training stage results in similar\nperformance to UCF101 pre-training and outperforms the UCF101 pre-trained model\non 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the\nlow-level properties of the datasets, we identify correlations between frame\ndiversity, frame similarity to natural data, and downstream performance. Our\napproach provides a more controllable and transparent alternative to video data\ncuration processes for pre-training.\n","authors":["Xueyang Yu","Xinlei Chen","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2410.24213v1.pdf","comment":"Project page: https://unicorn53547.github.io/video_syn_rep/"},{"id":"http://arxiv.org/abs/2410.24211v1","updated":"2024-10-31T17:59:01Z","published":"2024-10-31T17:59:01Z","title":"DELTA: Dense Efficient Long-range 3D Tracking for any video","summary":"  Tracking dense 3D motion from monocular videos remains challenging,\nparticularly when aiming for pixel-level precision over long sequences. We\nintroduce \\Approach, a novel method that efficiently tracks every pixel in 3D\nspace, enabling accurate motion estimation across entire videos. Our approach\nleverages a joint global-local attention mechanism for reduced-resolution\ntracking, followed by a transformer-based upsampler to achieve high-resolution\npredictions. Unlike existing methods, which are limited by computational\ninefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale,\nrunning over 8x faster than previous methods while achieving state-of-the-art\naccuracy. Furthermore, we explore the impact of depth representation on\ntracking performance and identify log-depth as the optimal choice. Extensive\nexperiments demonstrate the superiority of \\Approach on multiple benchmarks,\nachieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\nOur method provides a robust solution for applications requiring fine-grained,\nlong-term motion tracking in 3D space.\n","authors":["Tuan Duc Ngo","Peiye Zhuang","Chuang Gan","Evangelos Kalogerakis","Sergey Tulyakov","Hsin-Ying Lee","Chaoyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24211v1.pdf","comment":"Project Page: https://snap-research.github.io/DELTA/"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24207v1","updated":"2024-10-31T17:58:22Z","published":"2024-10-31T17:58:22Z","title":"No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images","summary":"  We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/.\n","authors":["Botao Ye","Sifei Liu","Haofei Xu","Xueting Li","Marc Pollefeys","Ming-Hsuan Yang","Songyou Peng"],"pdf_url":"https://arxiv.org/pdf/2410.24207v1.pdf","comment":"Project page: https://noposplat.github.io/"},{"id":"http://arxiv.org/abs/2410.24204v1","updated":"2024-10-31T17:57:07Z","published":"2024-10-31T17:57:07Z","title":"GeoSplatting: Towards Geometry Guided Gaussian Splatting for\n  Physically-based Inverse Rendering","summary":"  We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively.\n","authors":["Kai Ye","Chong Gao","Guanbin Li","Wenzheng Chen","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.24204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24203v1","updated":"2024-10-31T17:57:02Z","published":"2024-10-31T17:57:02Z","title":"DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion","summary":"  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n","authors":["Weicai Ye","Chenhao Ji","Zheng Chen","Junyao Gao","Xiaoshui Huang","Song-Hai Zhang","Wanli Ouyang","Tong He","Cairong Zhao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24203v1.pdf","comment":"NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:\n  https://github.com/zju3dv/DiffPano"},{"id":"http://arxiv.org/abs/2410.24187v1","updated":"2024-10-31T17:49:44Z","published":"2024-10-31T17:49:44Z","title":"Chasing Better Deep Image Priors between Over- and\n  Under-parameterization","summary":"  Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep\nimage priors (DIP) that regularize various image inverse problems. Meanwhile,\nresearchers also proposed extremely compact, under-parameterized image priors\n(e.g., deep decoder) that are strikingly competent for image restoration too,\ndespite a loss of accuracy. These two extremes push us to think whether there\nexists a better solution in the middle: between over- and under-parameterized\nimage priors, can one identify \"intermediate\" parameterized image priors that\nachieve better trade-offs between performance, efficiency, and even preserving\nstrong transferability? Drawing inspirations from the lottery ticket hypothesis\n(LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by\nexploiting DNN inherent sparsity, stated as: given an over-parameterized\nDNN-based image prior, it will contain a sparse subnetwork that can be trained\nin isolation, to match the original DNN's performance when being applied as a\nprior to various image inverse problems. Our results validate the superiority\nof LIPs: we can successfully locate the LIP subnetworks from over-parameterized\nDIPs at substantial sparsity ranges. Those LIP subnetworks significantly\noutperform deep decoders under comparably compact model sizes (by often fully\npreserving the effectiveness of their over-parameterized counterparts), and\nthey also possess high transferability across different images as well as\nrestoration task types. Besides, we also extend LIP to compressive sensing\nimage reconstruction, where a pre-trained GAN generator is used as the prior\n(in contrast to untrained DIP or deep decoder), and confirm its validity in\nthis setting too. To our best knowledge, this is the first time that LTH is\ndemonstrated to be relevant in the context of inverse problems or image priors.\n","authors":["Qiming Wu","Xiaohan Chen","Yifan Jiang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24187v1.pdf","comment":"Codes are available at\n  https://github.com/VITA-Group/Chasing-Better-DIPs"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2409.06711v2","updated":"2024-10-31T17:48:06Z","published":"2024-08-25T13:14:59Z","title":"Quantized neural network for complex hologram generation","summary":"  Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency.\n","authors":["Yutaka Endo","Minoru Oikawa","Timothy D. Wilkinson","Tomoyoshi Shimobaba","Tomoyoshi Ito"],"pdf_url":"https://arxiv.org/pdf/2409.06711v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.04312v2","updated":"2024-10-31T17:47:54Z","published":"2024-06-06T17:56:40Z","title":"ReNO: Enhancing One-step Text-to-Image Models through Reward-based Noise\n  Optimization","summary":"  Text-to-Image (T2I) models have made significant advancements in recent\nyears, but they still struggle to accurately capture intricate details\nspecified in complex compositional prompts. While fine-tuning T2I models with\nreward objectives has shown promise, it suffers from \"reward hacking\" and may\nnot generalize well to unseen prompt distributions. In this work, we propose\nReward-based Noise Optimization (ReNO), a novel approach that enhances T2I\nmodels at inference by optimizing the initial noise based on the signal from\none or multiple human preference reward models. Remarkably, solving this\noptimization problem with gradient ascent for 50 iterations yields impressive\nresults on four different one-step models across two competitive benchmarks,\nT2I-CompBench and GenEval. Within a computational budget of 20-50 seconds,\nReNO-enhanced one-step models consistently surpass the performance of all\ncurrent open-source Text-to-Image models. Extensive user studies demonstrate\nthat our model is preferred nearly twice as often compared to the popular SDXL\nmodel and is on par with the proprietary Stable Diffusion 3 with 8B parameters.\nMoreover, given the same computational resources, a ReNO-optimized one-step\nmodel outperforms widely-used open-source models such as SDXL and\nPixArt-$\\alpha$, highlighting the efficiency and effectiveness of ReNO in\nenhancing T2I model performance at inference time. Code is available at\nhttps://github.com/ExplainableML/ReNO.\n","authors":["Luca Eyring","Shyamgopal Karthik","Karsten Roth","Alexey Dosovitskiy","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2406.04312v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2306.01953v3","updated":"2024-10-31T17:47:21Z","published":"2023-06-02T23:29:28Z","title":"Invisible Image Watermarks Are Provably Removable Using Generative AI","summary":"  Invisible watermarks safeguard images' copyrights by embedding hidden\nmessages only detectable by owners. They also prevent people from misusing\nimages, especially those generated by AI models. We propose a family of\nregeneration attacks to remove these invisible watermarks. The proposed attack\nmethod first adds random noise to an image to destroy the watermark and then\nreconstructs the image. This approach is flexible and can be instantiated with\nmany existing image-denoising algorithms and pre-trained generative models such\nas diffusion models. Through formal proofs and extensive empirical evaluations,\nwe demonstrate that pixel-level invisible watermarks are vulnerable to this\nregeneration attack. Our results reveal that, across four different pixel-level\nwatermarking schemes, the proposed method consistently achieves superior\nperformance compared to existing attack techniques, with lower detection rates\nand higher image quality. However, watermarks that keep the image semantically\nsimilar can be an alternative defense against our attacks. Our finding\nunderscores the need for a shift in research/industry emphasis from invisible\nwatermarks to semantic-preserving watermarks. Code is available at\nhttps://github.com/XuandongZhao/WatermarkAttacker\n","authors":["Xuandong Zhao","Kexun Zhang","Zihao Su","Saastha Vasan","Ilya Grishchenko","Christopher Kruegel","Giovanni Vigna","Yu-Xiang Wang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2306.01953v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24183v1","updated":"2024-10-31T17:46:54Z","published":"2024-10-31T17:46:54Z","title":"Extended Object Tracking and Classification based on Linear Splines","summary":"  This paper introduces a framework based on linear splines for 2-dimensional\nextended object tracking and classification. Unlike state of the art models,\nlinear splines allow to represent extended objects whose contour is an\narbitrarily complex curve. An exact likelihood is derived for the case in which\nnoisy measurements can be scattered from any point on the contour of the\nextended object, while an approximate Monte Carlo likelihood is provided for\nthe case wherein scattering points can be anywhere, i.e. inside or on the\ncontour, on the object surface. Exploiting such likelihood to measure how well\nthe observed data fit a given shape, a suitable estimator is developed. The\nproposed estimator models the extended object in terms of a kinematic state,\nproviding object position and orientation, along with a shape vector,\ncharacterizing object contour and surface. The kinematic state is estimated via\na nonlinear Kalman filter, while the shape vector is estimated via a Bayesian\nclassifier so that classification is implicitly solved during shape estimation.\nNumerical experiments are provided to assess, compared to state of the art\nextended object estimators, the effectiveness of the proposed one.\n","authors":["Matteo Tesori","Giorgio Battistelli","Luigi Chisci"],"pdf_url":"https://arxiv.org/pdf/2410.24183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24181v1","updated":"2024-10-31T17:45:09Z","published":"2024-10-31T17:45:09Z","title":"Federated Black-Box Adaptation for Semantic Segmentation","summary":"  Federated Learning (FL) is a form of distributed learning that allows\nmultiple institutions or clients to collaboratively learn a global model to\nsolve a task. This allows the model to utilize the information from every\ninstitute while preserving data privacy. However, recent studies show that the\npromise of protecting the privacy of data is not upheld by existing methods and\nthat it is possible to recreate the training data from the different\ninstitutions. This is done by utilizing gradients transferred between the\nclients and the global server during training or by knowing the model\narchitecture at the client end. In this paper, we propose a federated learning\nframework for semantic segmentation without knowing the model architecture nor\ntransferring gradients between the client and the server, thus enabling better\nprivacy preservation. We propose BlackFed - a black-box adaptation of neural\nnetworks that utilizes zero order optimization (ZOO) to update the client model\nweights and first order optimization (FOO) to update the server weights. We\nevaluate our approach on several computer vision and medical imaging datasets\nto demonstrate its effectiveness. To the best of our knowledge, this work is\none of the first works in employing federated learning for segmentation, devoid\nof gradients or model information exchange. Code:\nhttps://github.com/JayParanjape/blackfed/tree/master\n","authors":["Jay N. Paranjape","Shameema Sikder","S. Swaroop Vedula","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2410.24181v1.pdf","comment":"Accepted at NEURIPS 2024"},{"id":"http://arxiv.org/abs/2404.13046v2","updated":"2024-10-31T17:39:34Z","published":"2024-04-19T17:59:48Z","title":"MoVA: Adapting Mixture of Vision Experts to Multimodal Context","summary":"  As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.\n","authors":["Zhuofan Zong","Bingqi Ma","Dazhong Shen","Guanglu Song","Hao Shao","Dongzhi Jiang","Hongsheng Li","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.13046v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.04690v3","updated":"2024-10-31T17:32:26Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v3.pdf","comment":"To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.24160v1","updated":"2024-10-31T17:19:03Z","published":"2024-10-31T17:19:03Z","title":"Redefining <Creative> in Dictionary: Towards a Enhanced Semantic\n  Understanding of Creative Generation","summary":"  Creativity, both in human and diffusion models, remains an inherently\nabstract concept; thus, simply adding \"creative\" to a prompt does not yield\nreliable semantic recognition by the model. In this work, we concretize the\nabstract notion of \"creative\" through the TP2O task, which aims to merge two\nunrelated concepts, and introduce CreTok, redefining \"creative\" as the token\n$\\texttt{<CreTok>}$. This redefinition offers a more concrete and universally\nadaptable representation for concept blending. This redefinition occurs\ncontinuously, involving the repeated random sampling of text pairs with\ndifferent concepts and optimizing cosine similarity between target and constant\nprompts. This approach enables $\\texttt{<CreTok>}$ to learn a method for\ncreative concept fusion. Extensive experiments demonstrate that the creative\ncapability enabled by $\\texttt{<CreTok>}$ substantially surpasses recent SOTA\ndiffusion models and achieves superior creative generation. CreTok exhibits\ngreater flexibility and reduced time overhead, as $\\texttt{<CreTok>}$ can\nfunction as a universal token for any concept, facilitating creative generation\nwithout retraining.\n","authors":["Fu Feng","Yucheng Xie","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.24160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00885v2","updated":"2024-10-31T17:12:57Z","published":"2024-06-02T22:40:05Z","title":"Visual place recognition for aerial imagery: A survey","summary":"  Aerial imagery and its direct application to visual localization is an\nessential problem for many Robotics and Computer Vision tasks. While Global\nNavigation Satellite Systems (GNSS) are the standard default solution for\nsolving the aerial localization problem, it is subject to a number of\nlimitations, such as, signal instability or solution unreliability that make\nthis option not so desirable. Consequently, visual geolocalization is emerging\nas a viable alternative. However, adapting Visual Place Recognition (VPR) task\nto aerial imagery presents significant challenges, including weather variations\nand repetitive patterns. Current VPR reviews largely neglect the specific\ncontext of aerial data. This paper introduces a methodology tailored for\nevaluating VPR techniques specifically in the domain of aerial imagery,\nproviding a comprehensive assessment of various methods and their performance.\nHowever, we not only compare various VPR methods, but also demonstrate the\nimportance of selecting appropriate zoom and overlap levels when constructing\nmap tiles to achieve maximum efficiency of VPR algorithms in the case of aerial\nimagery. The code is available on our GitHub repository --\nhttps://github.com/prime-slam/aero-vloc.\n","authors":["Ivan Moskalenko","Anastasiia Kornilova","Gonzalo Ferrer"],"pdf_url":"https://arxiv.org/pdf/2406.00885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24151v1","updated":"2024-10-31T17:09:55Z","published":"2024-10-31T17:09:55Z","title":"Scaling Concept With Text-Guided Diffusion Models","summary":"  Text-guided diffusion models have revolutionized generative tasks by\nproducing high-fidelity content from text descriptions. They have also enabled\nan editing paradigm where concepts can be replaced through text conditioning\n(e.g., a dog to a tiger). In this work, we explore a novel approach: instead of\nreplacing a concept, can we enhance or suppress the concept itself? Through an\nempirical study, we identify a trend where concepts can be decomposed in\ntext-guided diffusion models. Leveraging this insight, we introduce\nScalingConcept, a simple yet effective method to scale decomposed concepts up\nor down in real input without introducing new elements. To systematically\nevaluate our approach, we present the WeakConcept-10 dataset, where concepts\nare imperfect and need to be enhanced. More importantly, ScalingConcept enables\na variety of novel zero-shot applications across image and audio domains,\nincluding tasks such as canonical pose generation and generative sound\nhighlighting or removal.\n","authors":["Chao Huang","Susan Liang","Yunlong Tang","Yapeng Tian","Anurag Kumar","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24151v1.pdf","comment":"Project page: https://wikichao.github.io/ScalingConcept/"},{"id":"http://arxiv.org/abs/2410.24148v1","updated":"2024-10-31T17:09:19Z","published":"2024-10-31T17:09:19Z","title":"Exploring Vision Language Models for Facial Attribute Recognition:\n  Emotion, Race, Gender, and Age","summary":"  Technologies for recognizing facial attributes like race, gender, age, and\nemotion have several applications, such as surveillance, advertising content,\nsentiment analysis, and the study of demographic trends and social behaviors.\nAnalyzing demographic characteristics based on images and analyzing facial\nexpressions have several challenges due to the complexity of humans' facial\nattributes. Traditional approaches have employed CNNs and various other deep\nlearning techniques, trained on extensive collections of labeled images. While\nthese methods demonstrated effective performance, there remains potential for\nfurther enhancements. In this paper, we propose to utilize vision language\nmodels (VLMs) such as generative pre-trained transformer (GPT), GEMINI, large\nlanguage and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 to\nrecognize facial attributes such as race, gender, age, and emotion from images\nwith human faces. Various datasets like FairFace, AffectNet, and UTKFace have\nbeen utilized to evaluate the solutions. The results show that VLMs are\ncompetitive if not superior to traditional techniques. Additionally, we propose\n\"FaceScanPaliGemma\"--a fine-tuned PaliGemma model--for race, gender, age, and\nemotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and\n59.4% for race, gender, age group, and emotion classification, respectively,\noutperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.\nFinally, we propose \"FaceScanGPT\", which is a GPT-4o model to recognize the\nabove attributes when several individuals are present in the image using a\nprompt engineered for a person with specific facial and/or physical attributes.\nThe results underscore the superior multitasking capability of FaceScanGPT to\ndetect the individual's attributes like hair cut, clothing color, postures,\netc., using only a prompt to drive the detection and recognition tasks.\n","authors":["Nouar AlDahoul","Myles Joshua Toledo Tan","Harishwar Reddy Kasireddy","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.24148v1.pdf","comment":"52 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.24144v1","updated":"2024-10-31T17:05:44Z","published":"2024-10-31T17:05:44Z","title":"HoloChrome: Polychromatic Illumination for Speckle Reduction in\n  Holographic Near-Eye Displays","summary":"  Holographic displays hold the promise of providing authentic depth cues,\nresulting in enhanced immersive visual experiences for near-eye applications.\nHowever, current holographic displays are hindered by speckle noise, which\nlimits accurate reproduction of color and texture in displayed images. We\npresent HoloChrome, a polychromatic holographic display framework designed to\nmitigate these limitations. HoloChrome utilizes an ultrafast,\nwavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)\narchitecture, enabling the multiplexing of a large set of discrete wavelengths\nacross the visible spectrum. By leveraging spatial separation in our dual-SLM\nsetup, we independently manipulate speckle patterns across multiple\nwavelengths. This novel approach effectively reduces speckle noise through\nincoherent averaging achieved by wavelength multiplexing. Our method is\ncomplementary to existing speckle reduction techniques, offering a new pathway\nto address this challenge. Furthermore, the use of polychromatic illumination\nbroadens the achievable color gamut compared to traditional three-color primary\nholographic displays.\n  Our simulations and tabletop experiments validate that HoloChrome\nsignificantly reduces speckle noise and expands the color gamut. These\nadvancements enhance the performance of holographic near-eye displays, moving\nus closer to practical, immersive next-generation visual experiences.\n","authors":["Florian Schiffers","Grace Kuo","Nathan Matsuda","Douglas Lanman","Oliver Cossairt"],"pdf_url":"https://arxiv.org/pdf/2410.24144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24139v1","updated":"2024-10-31T17:03:38Z","published":"2024-10-31T17:03:38Z","title":"COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries\n  in Cluttered Scenes","summary":"  Automated waste recycling aims to efficiently separate the recyclable objects\nfrom the waste by employing vision-based systems. However, the presence of\nvarying shaped objects having different material types makes it a challenging\nproblem, especially in cluttered environments. Existing segmentation methods\nperform reasonably on many semantic segmentation datasets by employing\nmulti-contextual representations, however, their performance is degraded when\nutilized for waste object segmentation in cluttered scenarios. In addition,\nplastic objects further increase the complexity of the problem due to their\ntranslucent nature. To address these limitations, we introduce an efficacious\nsegmentation network, named COSNet, that uses boundary cues along with\nmulti-contextual information to accurately segment the objects in cluttered\nscenes. COSNet introduces novel components including feature sharpening block\n(FSB) and boundary enhancement module (BEM) for enhancing the features and\nhighlighting the boundary information of irregular waste objects in cluttered\nenvironment. Extensive experiments on three challenging datasets including\nZeroWaste-f, SpectralWaste, and ADE20K demonstrate the effectiveness of the\nproposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-f\nand 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.\n","authors":["Muhammad Ali","Mamoona Javaid","Mubashir Noman","Mustansar Fiaz","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2410.24139v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2311.00371v2","updated":"2024-10-31T17:01:50Z","published":"2023-11-01T08:53:05Z","title":"Learning Cooperative Trajectory Representations for Motion Forecasting","summary":"  Motion forecasting is an essential task for autonomous driving, and utilizing\ninformation from infrastructure and other vehicles can enhance forecasting\ncapabilities. Existing research mainly focuses on leveraging single-frame\ncooperative information to enhance the limited perception capability of the ego\nvehicle, while underutilizing the motion and interaction context of traffic\nparticipants observed from cooperative devices. In this paper, we propose a\nforecasting-oriented representation paradigm to utilize motion and interaction\nfeatures from cooperative information. Specifically, we present V2X-Graph, a\nrepresentative framework to achieve interpretable and end-to-end trajectory\nfeature fusion for cooperative motion forecasting. V2X-Graph is evaluated on\nV2X-Seq in vehicle-to-infrastructure (V2I) scenarios. To further evaluate on\nvehicle-to-everything (V2X) scenario, we construct the first real-world V2X\nmotion forecasting dataset V2X-Traj, which contains multiple autonomous\nvehicles and infrastructure in every scenario. Experimental results on both\nV2X-Seq and V2X-Traj show the advantage of our method. We hope both V2X-Graph\nand V2X-Traj will benefit the further development of cooperative motion\nforecasting. Find the project at https://github.com/AIR-THU/V2X-Graph.\n","authors":["Hongzhi Ruan","Haibao Yu","Wenxian Yang","Siqi Fan","Zaiqing Nie"],"pdf_url":"https://arxiv.org/pdf/2311.00371v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.01650v3","updated":"2024-10-31T16:53:49Z","published":"2024-01-03T10:07:11Z","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","summary":"  Source-free domain adaptation aims to adapt a source-trained model to an\nunlabeled target domain without access to the source data. It has attracted\ngrowing attention in recent years, where existing approaches focus on\nself-training that usually includes pseudo-labeling techniques. In this paper,\nwe introduce a novel noise-learning approach tailored to address noise\ndistribution in domain adaptation settings and learn to de-confuse the\npseudo-labels. More specifically, we learn a noise transition matrix of the\npseudo-labels to capture the label corruption of each class and learn the\nunderlying true label distribution. Estimating the noise transition matrix\nenables a better true class-posterior estimation, resulting in better\nprediction accuracy. We demonstrate the effectiveness of our approach when\ncombined with several source-free domain adaptation methods: SHOT, SHOT++, and\nAaD. We obtain state-of-the-art results on three domain adaptation datasets:\nVisDA, DomainNet, and OfficeHome.\n","authors":["Idit Diamant","Amir Rosenfeld","Idan Achituve","Jacob Goldberger","Arnon Netzer"],"pdf_url":"https://arxiv.org/pdf/2401.01650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01903v2","updated":"2024-10-31T16:49:26Z","published":"2024-07-02T03:08:20Z","title":"Text-Aware Diffusion for Policy Learning","summary":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment, without having access to any in-domain\ndemonstrations.\n","authors":["Calvin Luo","Mandy He","Zilai Zeng","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24116v1","updated":"2024-10-31T16:46:23Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization","summary":"  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"https://arxiv.org/pdf/2410.24116v1.pdf","comment":"19 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.24114v1","updated":"2024-10-31T16:44:10Z","published":"2024-10-31T16:44:10Z","title":"Nearest Neighbor Normalization Improves Multimodal Retrieval","summary":"  Multimodal models leverage large-scale pre-training to achieve strong but\nstill imperfect performance on tasks such as image captioning, visual question\nanswering, and cross-modal retrieval. In this paper, we present a simple and\nefficient method for correcting errors in trained contrastive image-text\nretrieval models with no additional training, called Nearest Neighbor\nNormalization (NNN). We show an improvement on retrieval metrics in both text\nretrieval and image retrieval for all of the contrastive models that we tested\n(CLIP, BLIP, ALBEF, SigLIP, BEiT) and for both of the datasets that we used\n(MS-COCO and Flickr30k). NNN requires a reference database, but does not\nrequire any training on this database, and can even increase the retrieval\naccuracy of a model after finetuning.\n","authors":["Neil Chowdhury","Franklin Wang","Sumedh Shenoy","Douwe Kiela","Sarah Schwettmann","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2410.24114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14919v2","updated":"2024-10-31T16:36:14Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nstate-of-the-art performance in image generation by leveraging only a\npretrained diffusion model, without requiring any training data. However, the\nultimate performance of SiD is constrained by the accuracy with which the\npretrained model captures the true data scores at different stages of the\ndiffusion process. In this paper, we introduce SiDA (SiD with Adversarial\nLoss), which not only enhances generation quality but also improves\ndistillation efficiency by incorporating real images and adversarial loss. SiDA\nutilizes the encoder from the generator's score network as a discriminator,\nboosting its ability to distinguish between real images and those generated by\nSiD. The adversarial loss is batch-normalized within each GPU and then combined\nwith the original SiD loss. This integration effectively incorporates the\naverage \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA\nto distill a single-step generator either from scratch or by fine-tuning an\nexisting one. SiDA converges significantly faster than its predecessor when\ntrained from scratch, and swiftly improves upon the original model's\nperformance after an initial warmup period during fine-tuning from a\npre-distilled SiD generator. This one-step adversarial distillation method\nestablishes new benchmarks in generation performance when distilling EDM\ndiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving\nFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when\ndistilling EDM2 models trained on ImageNet (512x512), surpassing even the\nlargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156\nfor EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,\ndemonstrating significant improvements across all model sizes. Our open-source\ncode will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24098v1","updated":"2024-10-31T16:28:49Z","published":"2024-10-31T16:28:49Z","title":"Parameter choices in HaarPSI for IQA with medical images","summary":"  When developing machine learning models, image quality assessment (IQA)\nmeasures are a crucial component for evaluation. However, commonly used IQA\nmeasures have been primarily developed and optimized for natural images. In\nmany specialized settings, such as medical images, this poses an\noften-overlooked problem regarding suitability. In previous studies, the IQA\nmeasure HaarPSI showed promising behavior for natural and medical images.\nHaarPSI is based on Haar wavelet representations and the framework allows\noptimization of two parameters. So far, these parameters have been aligned for\nnatural images. Here, we optimize these parameters for two annotated medical\ndata sets, a photoacoustic and a chest X-Ray data set. We observe that they are\nmore sensitive to the parameter choices than the employed natural images, and\non the other hand both medical data sets lead to similar parameter values when\noptimized. We denote the optimized setting, which improves the performance for\nthe medical images notably, by HaarPSI$_{MED}$. The results suggest that\nadapting common IQA measures within their frameworks for medical images can\nprovide a valuable, generalizable addition to the employment of more specific\ntask-based measures.\n","authors":["Clemens Karner","Janek Gr√∂hl","Ian Selby","Judith Babar","Jake Beckford","Thomas R Else","Timothy J Sadler","Shahab Shahipasand","Arthikkaa Thavakumar","Michael Roberts","James H. F. Rudd","Carola-Bibiane Sch√∂nlieb","Jonathan R Weir-McCall","Anna Breger"],"pdf_url":"https://arxiv.org/pdf/2410.24098v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.08570v2","updated":"2024-10-31T16:20:26Z","published":"2024-08-16T07:12:47Z","title":"EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver\n  Attention Estimation","summary":"  Associating driver attention with driving scene across two fields of views\n(FOVs) is a hard cross-domain perception problem, which requires comprehensive\nconsideration of cross-view mapping, dynamic driving scene analysis, and driver\nstatus tracking. Previous methods typically focus on a single view or map\nattention to the scene via estimated gaze, failing to exploit the implicit\nconnection between them. Moreover, simple fusion modules are insufficient for\nmodeling the complex relationships between the two views, making information\nintegration challenging. To address these issues, we propose a novel method for\nend-to-end scene-associated driver attention estimation, called EraW-Net. This\nmethod enhances the most discriminative dynamic cues, refines feature\nrepresentations, and facilitates semantically aligned cross-domain integration\nthrough a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive\nFilter Module (DAF-Module) is proposed to address the challenges of frequently\nchanging driving environments by extracting vital regions. It suppresses the\nindiscriminately recorded dynamics and highlights crucial ones by innovative\njoint frequency-spatial analysis, enhancing the model's ability to parse\ncomplex dynamics. Additionally, to track driver states during non-fixed facial\nposes, we propose a Global Context Sharing Module (GCS-Module) to construct\nrefined feature representations by capturing hierarchical features that adapt\nto various scales of head and eye movements. Finally, W-Net achieves systematic\ncross-view information integration through its \"Encoding-Independent Partial\nDecoding-Fusion Decoding\" structure, addressing semantic misalignment in\nheterogeneous data integration. Experiments demonstrate that the proposed\nmethod robustly and accurately estimates the mapping of driver attention in\nscene on large public datasets.\n","authors":["Jun Zhou","Chunsheng Liu","Faliang Chang","Wenqian Wang","Penghui Hao","Yiming Huang","Zhiqiang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08570v2.pdf","comment":"13pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.24075v1","updated":"2024-10-31T16:13:55Z","published":"2024-10-31T16:13:55Z","title":"Identifying Spatio-Temporal Drivers of Extreme Events","summary":"  The spatio-temporal relations of impacts of extreme events and their drivers\nin climate data are not fully understood and there is a need of machine\nlearning approaches to identify such spatio-temporal relations from data. The\ntask, however, is very challenging since there are time delays between extremes\nand their drivers, and the spatial response of such drivers is inhomogeneous.\nIn this work, we propose a first approach and benchmarks to tackle this\nchallenge. Our approach is trained end-to-end to predict spatio-temporally\nextremes and spatio-temporally drivers in the physical input variables jointly.\nBy enforcing the network to predict extremes from spatio-temporal binary masks\nof identified drivers, the network successfully identifies drivers that are\ncorrelated with extremes. We evaluate our approach on three newly created\nsynthetic benchmarks, where two of them are based on remote sensing or\nreanalysis climate data, and on two real-world reanalysis datasets. The source\ncode and datasets are publicly available at the project page\nhttps://hakamshams.github.io/IDE.\n","authors":["Mohamad Hakam Shams Eddin","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2410.24075v1.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.22551v2","updated":"2024-10-31T16:04:48Z","published":"2024-10-29T21:37:03Z","title":"FairSkin: Fair Diffusion for Skin Disease Image Generation","summary":"  Image generation is a prevailing technique for clinical data augmentation for\nadvancing diagnostic accuracy and reducing healthcare disparities. Diffusion\nModel (DM) has become a leading method in generating synthetic medical images,\nbut it suffers from a critical twofold bias: (1) The quality of images\ngenerated for Caucasian individuals is significantly higher, as measured by the\nFrechet Inception Distance (FID). (2) The ability of the downstream-task\nlearner to learn critical features from disease images varies across different\nskin tones. These biases pose significant risks, particularly in skin disease\ndetection, where underrepresentation of certain skin tones can lead to\nmisdiagnosis or neglect of specific conditions. To address these challenges, we\npropose FairSkin, a novel DM framework that mitigates these biases through a\nthree-level resampling mechanism, ensuring fairer representation across racial\nand disease categories. Our approach significantly improves the diversity and\nquality of generated images, contributing to more equitable skin disease\ndetection in clinical settings.\n","authors":["Ruichen Zhang","Yuguang Yao","Zhen Tan","Zhiming Li","Pan Wang","Huan Liu","Jingtong Hu","Sijia Liu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.22551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24060v1","updated":"2024-10-31T15:57:04Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08557v2","updated":"2024-10-31T15:52:52Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study reviews recent developments in low-light pedestrian\ndetection approaches. It systematically categorizes and analyses various\nalgorithms from region-based to non-region-based and graph-based learning\nmethodologies by highlighting their methodologies, implementation issues, and\nchallenges. It also outlines the key benchmark datasets that can be used for\nresearch and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations.\n","authors":["Thangarajah Akilan","Hrishikesh Vachhani"],"pdf_url":"https://arxiv.org/pdf/2311.08557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24055v1","updated":"2024-10-31T15:48:36Z","published":"2024-10-31T15:48:36Z","title":"Advanced Predictive Quality Assessment for Ultrasonic Additive\n  Manufacturing with Deep Learning Model","summary":"  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond\nsimilar or dissimilar metal foils to a substrate, resulting in solid,\nconsolidated metal components. However, certain processing conditions can lead\nto inter-layer defects, affecting the final product's quality. This study\ndevelops a method to monitor in-process quality using deep learning-based\nconvolutional neural networks (CNNs). The CNN models were evaluated on their\nability to classify samples with and without embedded thermocouples across five\npower levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with\nsupervised labeling. Four distinct CNN classification models were created for\ndifferent scenarios including without (baseline) and with thermocouples, only\nwithout thermocouples across power levels, only with thermocouples across power\nlevels, and combined without and with thermocouples across power levels. The\nmodels achieved 98.29% accuracy on combined baseline and thermocouple images,\n97.10% for baseline images across power levels, 97.43% for thermocouple images,\nand 97.27% for both types across power levels. The high accuracy, above 97%,\ndemonstrates the system's effectiveness in identifying and classifying\nconditions within the UAM process, providing a reliable tool for quality\nassurance and process control in manufacturing environments.\n","authors":["Lokendra Poudel","Sushant Jha","Ryan Meeker","Duy-Nhat Phan","Rahul Bhowmik"],"pdf_url":"https://arxiv.org/pdf/2410.24055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16129v2","updated":"2024-10-31T15:46:45Z","published":"2024-06-23T15:03:35Z","title":"UDHF2-Net: Uncertainty-diffusion-model-based High-Frequency TransFormer\n  Network for Remotely Sensed Imagery Interpretation","summary":"  Remotely sensed imagery interpretation (RSII) faces the three major problems:\n(1) objective representation of spatial distribution patterns; (2) edge\nuncertainty problem caused by downsampling encoder and intrinsic edge noises\n(e.g., mixed pixel and edge occlusion etc.); and (3) false detection problem\ncaused by geometric registration error in change detection. To solve the\naforementioned problems, uncertainty-diffusion-model-based high-Frequency\nTransFormer network (UDHF2-Net) is the first to be proposed, whose\nsuperiorities are as follows: (1) a spatially-stationary-and-non-stationary\nhigh-frequency connection paradigm (SHCP) is proposed to enhance the\ninteraction of spatially frequency-wise stationary and non-stationary features\nto yield high-fidelity edge extraction result. Inspired by HRFormer, SHCP\nproposes high-frequency-wise stream to replace high-resolution-wise stream in\nHRFormer through the whole encoder-decoder process with parallel frequency-wise\nhigh-to-low streams, so it improves the edge extraction accuracy by\ncontinuously remaining high-frequency information; (2) a\nmask-and-geo-knowledge-based uncertainty diffusion module (MUDM), which is a\nself-supervised learning strategy, is proposed to improve the edge accuracy of\nextraction and change detection by gradually removing the simulated spectrum\nnoises based on geo-knowledge and the generated diffused spectrum noises; (3) a\nfrequency-wise semi-pseudo-Siamese UDHF2-Net is the first to be proposed to\nbalance accuracy and complexity for change detection. Besides the\naforementioned spectrum noises in semantic segmentation, MUDM is also a\nself-supervised learning strategy to effectively reduce the edge false change\ndetection from the generated imagery with geometric registration error.\n","authors":["Pengfei Zhang","Chang Li","Yongjun Zhang","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2406.16129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v2","updated":"2024-10-31T15:44:36Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes at the cellular\nand subcellular levels. Scanning confocal microscopy allows the capture of\nhigh-quality images from thick three-dimensional (3D) samples, yet suffers from\nwell-known limitations such as photobleaching and phototoxicity of specimens\ncaused by intense light exposure, limiting its applications. Cellular damage\ncan be alleviated by changing imaging parameters to reduce light exposure,\noften at the expense of image quality. Machine/deep learning methods for\nsingle-image super-resolution (SISR) can be applied to restore image quality by\nupscaling lower-resolution (LR) images to yield high-resolution images (HR).\nThese SISR methods have been successfully applied to photo-realistic images due\npartly to the abundance of publicly available data. In contrast, the lack of\npublicly available data partly limits their application and success in scanning\nconfocal microscopy. In this paper, we introduce a large scanning confocal\nmicroscopy dataset named SR-CACO-2 that is comprised of low- and\nhigh-resolution image pairs marked for three different fluorescent markers. It\nallows the evaluation of performance of SISR methods on three different\nupscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell\nline Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured\nwith four resolutions and three markers, forming 9,937 image patches for SISR\nmethods. We provide benchmarking results for 16 state-of-the-art methods of the\nmain SISR families. Results show that these methods have limited success in\nproducing high-resolution textures. The dataset is freely accessible under a\nCreative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained\nweights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v2.pdf","comment":"27 pages, 15 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24046v1","updated":"2024-10-31T15:42:24Z","published":"2024-10-31T15:42:24Z","title":"Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis","summary":"  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a\ncutting-edge deep learning approach for the early diagnosis of glaucoma. The\nHM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,\nenabling the extraction of key features that are vital for identifying early\nsigns of glaucoma. Despite the common reliance on large annotated datasets, the\nHM-VGG model excels in scenarios with limited data, achieving remarkable\nresults with small sample sizes. The model's performance is underscored by its\nhigh metrics in Precision, Accuracy, and F1-Score, indicating its potential for\nreal-world application in glaucoma detection. The paper also discusses the\nchallenges associated with ophthalmic image analysis, particularly the\ndifficulty of obtaining large volumes of annotated data. It highlights the\nimportance of moving beyond single-modality data, such as VF or Optical\nCoherence Tomography (OCT) images alone, to a multimodal approach that can\nprovide a richer, more comprehensive dataset. This integration of different\ndata types is shown to significantly enhance diagnostic accuracy. The HM- VGG\nmodel offers a promising tool for doctors, streamlining the diagnostic process\nand improving patient outcomes. Furthermore, its applicability extends to\ntelemedicine and mobile healthcare, making diagnostic services more accessible.\nThe research presented in this paper is a significant step forward in the field\nof medical image processing and has profound implications for clinical\nophthalmology.\n","authors":["Junliang Du","Yiru Cang","Tong Zhou","Jiacheng Hu","Weijie He"],"pdf_url":"https://arxiv.org/pdf/2410.24046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24037v1","updated":"2024-10-31T15:34:49Z","published":"2024-10-31T15:34:49Z","title":"TPC: Test-time Procrustes Calibration for Diffusion-based Human Image\n  Animation","summary":"  Human image animation aims to generate a human motion video from the inputs\nof a reference human image and a target motion video. Current diffusion-based\nimage animation systems exhibit high precision in transferring human identity\ninto targeted motion, yet they still exhibit irregular quality in their\noutputs. Their optimal precision is achieved only when the physical\ncompositions (i.e., scale and rotation) of the human shapes in the reference\nimage and target pose frame are aligned. In the absence of such alignment,\nthere is a noticeable decline in fidelity and consistency. Especially, in\nreal-world environments, this compositional misalignment commonly occurs,\nposing significant challenges to the practical usage of current systems. To\nthis end, we propose Test-time Procrustes Calibration (TPC), which enhances the\nrobustness of diffusion-based image animation systems by maintaining optimal\nperformance even when faced with compositional misalignment, effectively\naddressing real-world scenarios. The TPC provides a calibrated reference image\nfor the diffusion model, enhancing its capability to understand the\ncorrespondence between human shapes in the reference and target images. Our\nmethod is simple and can be applied to any diffusion-based image animation\nsystem in a model-agnostic manner, improving the effectiveness at test time\nwithout additional training.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Younghwan Lee","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.24037v1.pdf","comment":"24 pages, 16 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24034v1","updated":"2024-10-31T15:32:14Z","published":"2024-10-31T15:32:14Z","title":"Handwriting Recognition in Historical Documents with Multimodal LLM","summary":"  There is an immense quantity of historical and cultural documentation that\nexists only as handwritten manuscripts. At the same time, performing OCR across\nscripts and different handwriting styles has proven to be an enormously\ndifficult problem relative to the process of digitizing print. While recent\nTransformer based models have achieved relatively strong performance, they rely\nheavily on manually transcribed training data and have difficulty generalizing\nacross writers. Multimodal LLM, such as GPT-4v and Gemini, have demonstrated\neffectiveness in performing OCR and computer vision tasks with few shot\nprompting. In this paper, I evaluate the accuracy of handwritten document\ntranscriptions generated by Gemini against the current state of the art\nTransformer based methods.\n  Keywords: Optical Character Recognition, Multimodal Language Models, Cultural\nPreservation, Mass digitization, Handwriting Recognitio\n","authors":["Lucian Li"],"pdf_url":"https://arxiv.org/pdf/2410.24034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24031v1","updated":"2024-10-31T15:29:51Z","published":"2024-10-31T15:29:51Z","title":"A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps","summary":"  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n","authors":["Ariel Larey","Eyal Rond","Omer Achrack"],"pdf_url":"https://arxiv.org/pdf/2410.24031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24018v1","updated":"2024-10-31T15:20:43Z","published":"2024-10-31T15:20:43Z","title":"Bayesian-guided Label Mapping for Visual Reprogramming","summary":"  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained\nvision models by adapting their input or output interfaces to solve downstream\ntasks whose labels (i.e., downstream labels) might be totally different from\nthe labels associated with the pretrained models (i.e., pretrained labels).\nWhen adapting the output interface, label mapping methods transform the\npretrained labels to downstream labels by establishing a gradient-free\none-to-one correspondence between the two sets of labels. However, in this\npaper, we reveal that one-to-one mappings may overlook the complex relationship\nbetween pretrained and downstream labels. Motivated by this observation, we\npropose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an\niteratively-updated probabilistic label mapping matrix, with each element\nquantifying a pairwise relationship between pretrained and downstream labels.\nThe assignment of values to the constructed matrix is guided by Bayesian\nconditional probability, considering the joint distribution of the downstream\nlabels and the labels predicted by the pretrained model on downstream samples.\nExperiments conducted on both pretrained vision models (e.g., ResNeXt) and\nvision-language models (e.g., CLIP) demonstrate the superior performance of BLM\nover existing label mapping methods. The success of BLM also offers a\nprobabilistic lens through which to understand and analyze the effectiveness of\nVR. Our code is available at https://github.com/tmlr-group/BayesianLM.\n","authors":["Chengyi Cai","Zesheng Ye","Lei Feng","Jianzhong Qi","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24015v1","updated":"2024-10-31T15:17:14Z","published":"2024-10-31T15:17:14Z","title":"Unveiling Synthetic Faces: How Synthetic Datasets Can Expose Real\n  Identities","summary":"  Synthetic data generation is gaining increasing popularity in different\ncomputer vision applications. Existing state-of-the-art face recognition models\nare trained using large-scale face datasets, which are crawled from the\nInternet and raise privacy and ethical concerns. To address such concerns,\nseveral works have proposed generating synthetic face datasets to train face\nrecognition models. However, these methods depend on generative models, which\nare trained on real face images. In this work, we design a simple yet effective\nmembership inference attack to systematically study if any of the existing\nsynthetic face recognition datasets leak any information from the real data\nused to train the generator model. We provide an extensive study on 6\nstate-of-the-art synthetic face recognition datasets, and show that in all\nthese synthetic datasets, several samples from the original real dataset are\nleaked. To our knowledge, this paper is the first work which shows the leakage\nfrom training data of generator models into the generated synthetic face\nrecognition datasets. Our study demonstrates privacy pitfalls in synthetic face\nrecognition datasets and paves the way for future studies on generating\nresponsible synthetic face datasets.\n","authors":["Hatef Otroshi Shahreza","S√©bastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2410.24015v1.pdf","comment":"Accepted in NeurIPS 2024 Workshop on New Frontiers in Adversarial\n  Machine Learning"},{"id":"http://arxiv.org/abs/2410.24010v1","updated":"2024-10-31T15:10:38Z","published":"2024-10-31T15:10:38Z","title":"Re-assembling the past: The RePAIR dataset and benchmark for real world\n  2D and 3D puzzle solving","summary":"  This paper proposes the RePAIR dataset that represents a challenging\nbenchmark to test modern computational and data driven methods for\npuzzle-solving and reassembly tasks. Our dataset has unique properties that are\nuncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and\nfractures are realistic, caused by a collapse of a fresco during a World War II\nbombing at the Pompeii archaeological park. The fragments are also eroded and\nhave missing pieces with irregular shapes and different dimensions, challenging\nfurther the reassembly algorithms. The dataset is multi-modal providing high\nresolution images with characteristic pictorial elements, detailed 3D scans of\nthe fragments and meta-data annotated by the archaeologists. Ground truth has\nbeen generated through several years of unceasing fieldwork, including the\nexcavation and cleaning of each fragment, followed by manual puzzle solving by\narchaeologists of a subset of approx. 1000 pieces among the 16000 available.\nAfter digitizing all the fragments in 3D, a benchmark was prepared to challenge\ncurrent reassembly and puzzle-solving methods that often solve more simplistic\nsynthetic scenarios. The tested baselines show that there clearly exists a gap\nto fill in solving this computationally complex problem.\n","authors":["Theodore Tsesmelis","Luca Palmieri","Marina Khoroshiltseva","Adeela Islam","Gur Elkin","Ofir Itzhak Shahar","Gianluca Scarpellini","Stefano Fiorini","Yaniv Ohayon","Nadav Alali","Sinem Aslan","Pietro Morerio","Sebastiano Vascon","Elena Gravina","Maria Cristina Napolitano","Giuseppe Scarpati","Gabriel Zuchtriegel","Alexandra Sp√ºhler","Michel E. Fuchs","Stuart James","Ohad Ben-Shahar","Marcello Pelillo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2410.24010v1.pdf","comment":"NeurIPS 2024, Track Datasets and Benchmarks, 10 pages"},{"id":"http://arxiv.org/abs/2410.24006v1","updated":"2024-10-31T15:09:36Z","published":"2024-10-31T15:09:36Z","title":"DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination","summary":"  In the ever-evolving adversarial machine learning landscape, developing\neffective defenses against patch attacks has become a critical challenge,\nnecessitating reliable solutions to safeguard real-world AI systems. Although\ndiffusion models have shown remarkable capacity in image synthesis and have\nbeen recently utilized to counter $\\ell_p$-norm bounded attacks, their\npotential in mitigating localized patch attacks remains largely underexplored.\nIn this work, we propose DiffPAD, a novel framework that harnesses the power of\ndiffusion models for adversarial patch decontamination. DiffPAD first performs\nsuper-resolution restoration on downsampled input images, then adopts\nbinarization, dynamic thresholding scheme and sliding window for effective\nlocalization of adversarial patches. Such a design is inspired by the\ntheoretically derived correlation between patch size and diffusion restoration\nerror that is generalized across diverse patch attack scenarios. Finally,\nDiffPAD applies inpainting techniques to the original input images with the\nestimated patch region being masked. By integrating closed-form solutions for\nsuper-resolution restoration and image inpainting into the conditional reverse\nsampling process of a pre-trained diffusion model, DiffPAD obviates the need\nfor text guidance or fine-tuning. Through comprehensive experiments, we\ndemonstrate that DiffPAD not only achieves state-of-the-art adversarial\nrobustness against patch attacks but also excels in recovering naturalistic\nimages without patch remnants.\n","authors":["Jia Fu","Xiao Zhang","Sepideh Pashami","Fatemeh Rahimian","Anders Holst"],"pdf_url":"https://arxiv.org/pdf/2410.24006v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2405.10723v2","updated":"2024-10-31T15:05:50Z","published":"2024-05-17T12:11:58Z","title":"Eddeep: Fast eddy-current distortion correction for diffusion MRI with\n  deep learning","summary":"  Modern diffusion MRI sequences commonly acquire a large number of volumes\nwith diffusion sensitization gradients of differing strengths or directions.\nSuch sequences rely on echo-planar imaging (EPI) to achieve reasonable scan\nduration. However, EPI is vulnerable to off-resonance effects, leading to\ntissue susceptibility and eddy-current induced distortions. The latter is\nparticularly problematic because it causes misalignment between volumes,\ndisrupting downstream modelling and analysis. The essential correction of eddy\ndistortions is typically done post-acquisition, with image registration.\nHowever, this is non-trivial because correspondence between volumes can be\nseverely disrupted due to volume-specific signal attenuations induced by\nvarying directions and strengths of the applied gradients. This challenge has\nbeen successfully addressed by the popular FSL~Eddy tool but at considerable\ncomputational cost. We propose an alternative approach, leveraging recent\nadvances in image processing enabled by deep learning (DL). It consists of two\nconvolutional neural networks: 1) An image translator to restore correspondence\nbetween images; 2) A registration model to align the translated images. Results\ndemonstrate comparable distortion estimates to FSL~Eddy, while requiring only\nmodest training sample sizes. This work, to the best of our knowledge, is the\nfirst to tackle this problem with deep learning. Together with recently\ndeveloped DL-based susceptibility correction techniques, they pave the way for\nreal-time preprocessing of diffusion MRI, facilitating its wider uptake in the\nclinic.\n","authors":["Antoine Legouhy","Ross Callaghan","Whitney Stee","Philippe Peigneux","Hojjat Azadbakht","Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.10723v2.pdf","comment":"accepted in MICCAI 2024 conference"},{"id":"http://arxiv.org/abs/2410.24002v1","updated":"2024-10-31T15:02:16Z","published":"2024-10-31T15:02:16Z","title":"Assessing the Efficacy of Classical and Deep Neuroimaging Biomarkers in\n  Early Alzheimer's Disease Diagnosis","summary":"  Alzheimer's disease (AD) is the leading cause of dementia, and its early\ndetection is crucial for effective intervention, yet current diagnostic methods\noften fall short in sensitivity and specificity. This study aims to detect\nsignificant indicators of early AD by extracting and integrating various\nimaging biomarkers, including radiomics, hippocampal texture descriptors,\ncortical thickness measurements, and deep learning features. We analyze\nstructural magnetic resonance imaging (MRI) scans from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) cohorts, utilizing comprehensive image analysis\nand machine learning techniques. Our results show that combining multiple\nbiomarkers significantly improves detection accuracy. Radiomics and texture\nfeatures emerged as the most effective predictors for early AD, achieving AUCs\nof 0.88 and 0.72 for AD and MCI detection, respectively. Although deep learning\nfeatures proved to be less effective than traditional approaches, incorporating\nage with other biomarkers notably enhanced MCI detection performance.\nAdditionally, our findings emphasize the continued importance of classical\nimaging biomarkers in the face of modern deep-learning approaches, providing a\nrobust framework for early AD diagnosis.\n","authors":["Milla E. Nielsen","Mads Nielsen","Mostafa Mehdipour Ghazi"],"pdf_url":"https://arxiv.org/pdf/2410.24002v1.pdf","comment":"SPIE Medical Imaging (MI25)"},{"id":"http://arxiv.org/abs/2410.24001v1","updated":"2024-10-31T15:02:05Z","published":"2024-10-31T15:02:05Z","title":"ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from\n  Only 2D Images","summary":"  Open-vocabulary 3D object detection (OV-3Det) aims to generalize beyond the\nlimited number of base categories labeled during the training phase. The\nbiggest bottleneck is the scarcity of annotated 3D data, whereas 2D image\ndatasets are abundant and richly annotated. Consequently, it is intuitive to\nleverage the wealth of annotations in 2D images to alleviate the inherent data\nscarcity in OV-3Det. In this paper, we push the task setup to its limits by\nexploring the potential of using solely 2D images to learn OV-3Det. The major\nchallenges for this setup is the modality gap between training images and\ntesting point clouds, which prevents effective integration of 2D knowledge into\nOV-3Det. To address this challenge, we propose a novel framework ImOV3D to\nleverage pseudo multimodal representation containing both images and point\nclouds (PC) to close the modality gap. The key of ImOV3D lies in flexible\nmodality conversion where 2D images can be lifted into 3D using monocular depth\nestimation and can also be derived from 3D scenes through rendering. This\nallows unifying both training images and testing point clouds into a common\nimage-PC representation, encompassing a wealth of 2D semantic information and\nalso incorporating the depth and structural characteristics of 3D spatial data.\nWe carefully conduct such conversion to minimize the domain gap between\ntraining and test cases. Extensive experiments on two benchmark datasets,\nSUNRGBD and ScanNet, show that ImOV3D significantly outperforms existing\nmethods, even in the absence of ground truth 3D training data. With the\ninclusion of a minimal amount of real 3D data for fine-tuning, the performance\nalso significantly surpasses previous state-of-the-art. Codes and pre-trained\nmodels are released on the https://github.com/yangtiming/ImOV3D.\n","authors":["Timing Yang","Yuanliang Ju","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2410.24001v1.pdf","comment":"Accepted by NeurIPS 2024. Code link\n  https://github.com/yangtiming/ImOV3D"},{"id":"http://arxiv.org/abs/2211.15656v3","updated":"2024-10-31T15:01:41Z","published":"2022-11-28T18:59:02Z","title":"SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation","summary":"  High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.\n","authors":["Hao Dong","Weihao Gu","Xianjing Zhang","Jintao Xu","Rui Ai","Huimin Lu","Juho Kannala","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15656v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2407.03550v2","updated":"2024-10-31T14:51:32Z","published":"2024-07-04T00:07:50Z","title":"CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding","summary":"  The comic domain is rapidly advancing with the development of single-page\nanalysis and synthesis models. However, evaluation metrics and datasets lag\nbehind, often limited to small-scale or single-style test sets. We introduce a\nnovel benchmark, CoMix, designed to evaluate the multi-task capabilities of\nmodels in comic analysis. Unlike existing benchmarks that focus on isolated\ntasks such as object detection or text recognition, CoMix addresses a broader\nrange of tasks including object detection, speaker identification, character\nre-identification, reading order, and multi-modal reasoning tasks like\ncharacter naming and dialogue generation. Our benchmark comprises three\nexisting datasets with expanded annotations to support multi-task evaluation.\nTo mitigate the over-representation of manga-style data, we have incorporated a\nnew dataset of carefully selected American comic-style books, thereby enriching\nthe diversity of comic styles. CoMix is designed to assess pre-trained models\nin zero-shot and limited fine-tuning settings, probing their transfer\ncapabilities across different comic styles and tasks. The validation split of\nthe benchmark is publicly available for research purposes, and an evaluation\nserver for the held-out test split is also provided. Comparative results\nbetween human performance and state-of-the-art models reveal a significant\nperformance gap, highlighting substantial opportunities for advancements in\ncomic understanding. The dataset, baseline models, and code are accessible at\nhttps://github.com/emanuelevivoli/CoMix-dataset. This initiative sets a new\nstandard for comprehensive comic analysis, providing the community with a\ncommon benchmark for evaluation on a large and varied set.\n","authors":["Emanuele Vivoli","Marco Bertini","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2407.03550v2.pdf","comment":"Accepted at NeurIPS 2024 (D&B)"},{"id":"http://arxiv.org/abs/2410.23991v1","updated":"2024-10-31T14:50:48Z","published":"2024-10-31T14:50:48Z","title":"Localization, balance and affinity: a stronger multifaceted\n  collaborative salient object detector in remote sensing images","summary":"  Despite significant advancements in salient object detection(SOD) in optical\nremote sensing images(ORSI), challenges persist due to the intricate edge\nstructures of ORSIs and the complexity of their contextual relationships.\nCurrent deep learning approaches encounter difficulties in accurately\nidentifying boundary features and lack efficiency in collaboratively modeling\nthe foreground and background by leveraging contextual features. To address\nthese challenges, we propose a stronger multifaceted collaborative salient\nobject detector in ORSIs, termed LBA-MCNet, which incorporates aspects of\nlocalization, balance, and affinity. The network focuses on accurately locating\ntargets, balancing detailed features, and modeling image-level global context\ninformation. Specifically, we design the Edge Feature Adaptive Balancing and\nAdjusting(EFABA) module for precise edge localization, using edge features to\nguide attention to boundaries and preserve spatial details. Moreover, we design\nthe Global Distributed Affinity Learning(GDAL) module to model global context.\nIt captures global context by generating an affinity map from the encoders\nfinal layer, ensuring effective modeling of global patterns. Additionally, deep\nsupervision during deconvolution further enhances feature representation.\nFinally, we compared with 28 state of the art approaches on three publicly\navailable datasets. The results clearly demonstrate the superiority of our\nmethod.\n","authors":["Yakun Xie","Suning Liu","Hongyu Chen","Shaohan Cao","Huixin Zhang","Dejun Feng","Qian Wan","Jun Zhu","Qing Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.23991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15856v2","updated":"2024-10-31T14:48:23Z","published":"2023-12-26T02:50:42Z","title":"SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields","summary":"  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n","authors":["Kaichen Zhou","Lanqing Hong","Enze Xie","Yongxin Yang","Zhenguo Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22629v2","updated":"2024-10-31T14:44:44Z","published":"2024-10-30T01:22:37Z","title":"CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable\n  Remote Sensing Semantic Segmentation","summary":"  The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.\n","authors":["Ziyang Gong","Zhixiang Wei","Di Wang","Xianzheng Ma","Hongruixuan Chen","Yuru Jia","Yupeng Deng","Zhenming Ji","Xiangwei Zhu","Naoto Yokoya","Jing Zhang","Bo Du","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.22629v2.pdf","comment":"The codes and models will be available at\n  https://github.com/Cuzyoung/CrossEarth"},{"id":"http://arxiv.org/abs/2410.23988v1","updated":"2024-10-31T14:42:26Z","published":"2024-10-31T14:42:26Z","title":"JEMA: A Joint Embedding Framework for Scalable Co-Learning with\n  Multimodal Alignment","summary":"  This work introduces JEMA (Joint Embedding with Multimodal Alignment), a\nnovel co-learning framework tailored for laser metal deposition (LMD), a\npivotal process in metal additive manufacturing. As Industry 5.0 gains traction\nin industrial applications, efficient process monitoring becomes increasingly\ncrucial. However, limited data and the opaque nature of AI present challenges\nfor its application in an industrial setting. JEMA addresses this challenges by\nleveraging multimodal data, including multi-view images and metadata such as\nprocess parameters, to learn transferable semantic representations. By applying\na supervised contrastive loss function, JEMA enables robust learning and\nsubsequent process monitoring using only the primary modality, simplifying\nhardware requirements and computational overhead. We investigate the\neffectiveness of JEMA in LMD process monitoring, focusing specifically on its\ngeneralization to downstream tasks such as melt pool geometry prediction,\nachieved without extensive fine-tuning. Our empirical evaluation demonstrates\nthe high scalability and performance of JEMA, particularly when combined with\nVision Transformer models. We report an 8% increase in performance in\nmultimodal settings and a 1% improvement in unimodal settings compared to\nsupervised contrastive learning. Additionally, the learned embedding\nrepresentation enables the prediction of metadata, enhancing interpretability\nand making possible the assessment of the added metadata's contributions. Our\nframework lays the foundation for integrating multisensor data with metadata,\nenabling diverse downstream tasks within the LMD domain and beyond.\n","authors":["Joao Sousa","Roya Darabi","Armando Sousa","Frank Brueckner","Lu√≠s Paulo Reis","Ana Reis"],"pdf_url":"https://arxiv.org/pdf/2410.23988v1.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2312.14556v2","updated":"2024-10-31T14:37:59Z","published":"2023-12-22T09:29:45Z","title":"CaptainCook4D: A Dataset for Understanding Errors in Procedural\n  Activities","summary":"  Following step-by-step procedures is an essential component of various\nactivities carried out by individuals in their daily lives. These procedures\nserve as a guiding framework that helps to achieve goals efficiently, whether\nit is assembling furniture or preparing a recipe. However, the complexity and\nduration of procedural activities inherently increase the likelihood of making\nerrors. Understanding such procedural activities from a sequence of frames is a\nchallenging task that demands an accurate interpretation of visual information\nand the ability to reason about the structure of the activity. To this end, we\ncollect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings\n(94.5 hours) of people performing recipes in real kitchen environments. This\ndataset consists of two distinct types of activity: one in which participants\nadhere to the provided recipe instructions and another in which they deviate\nand induce errors. We provide 5.3K step annotations and 10K fine-grained action\nannotations and benchmark the dataset for the following tasks: supervised error\nrecognition, multistep localization, and procedure learning\n","authors":["Rohith Peddi","Shivvrat Arya","Bharath Challa","Likhitha Pallapothula","Akshay Vyas","Bhavya Gouripeddi","Jikai Wang","Qifan Zhang","Vasundhara Komaragiri","Eric Ragan","Nicholas Ruozzi","Yu Xiang","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2312.14556v2.pdf","comment":"Accepted to the 2024 Neural Information Processing Systems Datasets\n  and Benchmarks Track, Project Page:\n  https://captaincook4d.github.io/captain-cook/"},{"id":"http://arxiv.org/abs/2407.12582v2","updated":"2024-10-31T14:37:42Z","published":"2024-07-17T14:09:46Z","title":"Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection","summary":"  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n","authors":["Hu Cao","Zehua Zhang","Yan Xia","Xinyi Li","Jiahao Xia","Guang Chen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.12582v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2410.22637v2","updated":"2024-10-31T14:35:31Z","published":"2024-10-30T02:04:23Z","title":"Consistency Diffusion Bridge Models","summary":"  Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.\n","authors":["Guande He","Kaiwen Zheng","Jianfei Chen","Fan Bao","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22637v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23970v1","updated":"2024-10-31T14:25:55Z","published":"2024-10-31T14:25:55Z","title":"TrAct: Making First-layer Pre-Activations Trainable","summary":"  We consider the training of the first layer of vision models and notice the\nclear relationship between pixel values and gradient update magnitudes: the\ngradients arriving at the weights of a first layer are by definition directly\nproportional to (normalized) input pixel values. Thus, an image with low\ncontrast has a smaller impact on learning than an image with higher contrast,\nand a very bright or very dark image has a stronger impact on the weights than\nan image with moderate brightness. In this work, we propose performing gradient\ndescent on the embeddings produced by the first layer of the model. However,\nswitching to discrete inputs with an embedding layer is not a reasonable option\nfor vision models. Thus, we propose the conceptual procedure of (i) a gradient\ndescent step on first layer activations to construct an activation proposal,\nand (ii) finding the optimal weights of the first layer, i.e., those weights\nwhich minimize the squared distance to the activation proposal. We provide a\nclosed form solution of the procedure and adjust it for robust stochastic\ntraining while computing everything efficiently. Empirically, we find that\nTrAct (Training Activations) speeds up training by factors between 1.25x and 4x\nwhile requiring only a small computational overhead. We demonstrate the utility\nof TrAct with different optimizers for a range of different vision models\nincluding convolutional and transformer architectures.\n","authors":["Felix Petersen","Christian Borgelt","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.23970v1.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23962v1","updated":"2024-10-31T14:14:30Z","published":"2024-10-31T14:14:30Z","title":"Image Synthesis with Class-Aware Semantic Diffusion Models for Surgical\n  Scene Segmentation","summary":"  Surgical scene segmentation is essential for enhancing surgical precision,\nyet it is frequently compromised by the scarcity and imbalance of available\ndata. To address these challenges, semantic image synthesis methods based on\ngenerative adversarial networks and diffusion models have been developed.\nHowever, these models often yield non-diverse images and fail to capture small,\ncritical tissue classes, limiting their effectiveness. In response, we propose\nthe Class-Aware Semantic Diffusion Model (CASDM), a novel approach which\nutilizes segmentation maps as conditions for image synthesis to tackle data\nscarcity and imbalance. Novel class-aware mean squared error and class-aware\nself-perceptual loss functions have been defined to prioritize critical, less\nvisible classes, thereby enhancing image quality and relevance. Furthermore, to\nour knowledge, we are the first to generate multi-class segmentation maps using\ntext prompts in a novel fashion to specify their contents. These maps are then\nused by CASDM to generate surgical scene images, enhancing datasets for\ntraining and validating segmentation models. Our evaluation, which assesses\nboth image quality and downstream segmentation performance, demonstrates the\nstrong effectiveness and generalisability of CASDM in producing realistic\nimage-map pairs, significantly advancing surgical scene segmentation across\ndiverse and challenging datasets.\n","authors":["Yihang Zhou","Rebecca Towning","Zaid Awad","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2410.23962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23946v1","updated":"2024-10-31T14:02:40Z","published":"2024-10-31T14:02:40Z","title":"MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption","summary":"  Remote sensing image change caption (RSICC) aims to provide natural language\ndescriptions for bi-temporal remote sensing images. Since Change Caption (CC)\ntask requires both spatial and temporal features, previous works follow an\nencoder-fusion-decoder architecture. They use an image encoder to extract\nspatial features and the fusion module to integrate spatial features and\nextract temporal features, which leads to increasingly complex manual design of\nthe fusion module. In this paper, we introduce a novel video model-based\nparadigm without design of the fusion module and propose a Mask-enhanced Video\nmodel for Change Caption (MV-CC). Specifically, we use the off-the-shelf video\nencoder to simultaneously extract the temporal and spatial features of\nbi-temporal images. Furthermore, the types of changes in the CC are set based\non specific task requirements, and to enable the model to better focus on the\nregions of interest, we employ masks obtained from the Change Detection (CD)\nmethod to explicitly guide the CC model. Experimental results demonstrate that\nour proposed method can obtain better performance compared with other\nstate-of-the-art RSICC methods. The code is available at\nhttps://github.com/liuruixun/MV-CC.\n","authors":["Ruixun Liu","Kaiyu Li","Jiayi Song","Dongwei Sun","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2410.23946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03438v2","updated":"2024-10-31T13:41:19Z","published":"2024-10-04T13:52:22Z","title":"Dessie: Disentanglement for Articulated 3D Horse Shape and Pose\n  Estimation from Images","summary":"  In recent years, 3D parametric animal models have been developed to aid in\nestimating 3D shape and pose from images and video. While progress has been\nmade for humans, it's more challenging for animals due to limited annotated\ndata. To address this, we introduce the first method using synthetic data\ngeneration and disentanglement to learn to regress 3D shape and pose. Focusing\non horses, we use text-based texture generation and a synthetic data pipeline\nto create varied shapes, poses, and appearances, learning disentangled spaces.\nOur method, Dessie, surpasses existing 3D horse reconstruction methods and\ngeneralizes to other large animals like zebras, cows, and deer. See the project\nwebsite at: \\url{https://celiali.github.io/Dessie/}.\n","authors":["Ci Li","Yi Yang","Zehang Weng","Elin Hernlund","Silvia Zuffi","Hedvig Kjellstr√∂m"],"pdf_url":"https://arxiv.org/pdf/2410.03438v2.pdf","comment":"ACCV2024"},{"id":"http://arxiv.org/abs/2410.23931v1","updated":"2024-10-31T13:41:16Z","published":"2024-10-31T13:41:16Z","title":"Manipulating Vehicle 3D Shapes through Latent Space Editing","summary":"  Although 3D object editing has the potential to significantly influence\nvarious industries, recent research in 3D generation and editing has primarily\nfocused on converting text and images into 3D models, often overlooking the\nneed for fine-grained control over the editing of existing 3D objects. This\npaper introduces a framework that employs a pre-trained regressor, enabling\ncontinuous, precise, attribute-specific modifications to both the stylistic and\ngeometric attributes of vehicle 3D models. Our method not only preserves the\ninherent identity of vehicle 3D objects, but also supports multi-attribute\nediting, allowing for extensive customization without compromising the model's\nstructural integrity. Experimental results demonstrate the efficacy of our\napproach in achieving detailed edits on various vehicle 3D models.\n","authors":["JiangDong Miao","Tatsuya Ikeda","Bisser Raytchev","Ryota Mizoguchi","Takenori Hiraoka","Takuji Nakashima","Keigo Shimizu","Toru Higaki","Kazufumi Kaneda"],"pdf_url":"https://arxiv.org/pdf/2410.23931v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04230v2","updated":"2024-10-31T13:18:14Z","published":"2024-06-06T16:30:41Z","title":"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating\n  Interferometric SAR and Multispectral Data","summary":"  Satellite-based remote sensing has revolutionised the way we address global\nchallenges. Huge quantities of Earth Observation (EO) data are generated by\nsatellite sensors daily, but processing these large datasets for use in ML\npipelines is technically and computationally challenging. While some\npreprocessed Earth observation datasets exist, their content is often limited\nto optical or near-optical wavelength data, which is ineffective at night or in\nadverse weather conditions. Synthetic Aperture Radar (SAR), an active sensing\ntechnique based on microwave length radiation, offers a viable alternative.\nHowever, the application of machine learning to SAR has been limited due to a\nlack of ML-ready data and pipelines, particularly for the full diversity of SAR\ndata, including polarimetry, coherence and interferometry. In this work, we\nintroduce M3LEO, a multi-modal, multi-label Earth observation dataset that\nincludes polarimetric, interferometric, and coherence SAR data derived from\nSentinel-1, alongside multispectral Sentinel-2 imagery and auxiliary data\ndescribing terrain properties such as land use. M3LEO spans approximately 17M\n4x4 km data chips from six diverse geographic regions. The dataset is\ncomplemented by a flexible PyTorch Lightning framework configured using Hydra\nto accommodate its use across diverse ML applications in Earth observation. We\nprovide tools to process any dataset available on popular platforms such as\nGoogle Earth Engine for seamless integration with our framework. We show that\nthe distribution shift in self-supervised embeddings is substantial across\ngeographic regions, even when controlling for terrain properties. Data:\nhuggingface.co/M3LEO, Code: github.com/spaceml-org/M3LEO.\n","authors":["Matthew J Allen","Francisco Dorr","Joseph Alejandro Gallego Mejia","Laura Mart√≠nez-Ferrer","Anna Jungbluth","Freddie Kalaitzis","Ra√∫l Ramos-Poll√°n"],"pdf_url":"https://arxiv.org/pdf/2406.04230v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23910v1","updated":"2024-10-31T13:13:32Z","published":"2024-10-31T13:13:32Z","title":"Uncertainty Estimation for 3D Object Detection via Evidential Learning","summary":"  3D object detection is an essential task for computer vision applications in\nautonomous vehicles and robotics. However, models often struggle to quantify\ndetection reliability, leading to poor performance on unfamiliar scenes. We\nintroduce a framework for quantifying uncertainty in 3D object detection by\nleveraging an evidential learning loss on Bird's Eye View representations in\nthe 3D detector. These uncertainty estimates require minimal computational\noverhead and are generalizable across different architectures. We demonstrate\nboth the efficacy and importance of these uncertainty estimates on identifying\nout-of-distribution scenes, poorly localized objects, and missing (false\nnegative) detections; our framework consistently improves over baselines by\n10-20% on average. Finally, we integrate this suite of tasks into a system\nwhere a 3D object detector auto-labels driving scenes and our uncertainty\nestimates verify label correctness before the labels are used to train a second\nmodel. Here, our uncertainty-driven verification results in a 1% improvement in\nmAP and a 1-2% improvement in NDS.\n","authors":["Nikita Durasov","Rafid Mahmood","Jiwoong Choi","Marc T. Law","James Lucas","Pascal Fua","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2410.23910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23906v1","updated":"2024-10-31T13:11:09Z","published":"2024-10-31T13:11:09Z","title":"From Web Data to Real Fields: Low-Cost Unsupervised Domain Adaptation\n  for Agricultural Robots","summary":"  In precision agriculture, vision models often struggle with new, unseen\nfields where crops and weeds have been influenced by external factors,\nresulting in compositions and appearances that differ from the learned\ndistribution. This paper aims to adapt to specific fields at low cost using\nUnsupervised Domain Adaptation (UDA). We explore a novel domain shift from a\ndiverse, large pool of internet-sourced data to a small set of data collected\nby a robot at specific locations, minimizing the need for extensive on-field\ndata collection. Additionally, we introduce a novel module -- the Multi-level\nAttention-based Adversarial Discriminator (MAAD) -- which can be integrated at\nthe feature extractor level of any detection model. In this study, we\nincorporate MAAD with CenterNet to simultaneously detect leaf, stem, and vein\ninstances. Our results show significant performance improvements in the\nunlabeled target domain compared to baseline models, with a 7.5% increase in\nobject detection accuracy and a 5.1% improvement in keypoint detection.\n","authors":["Vasileios Tzouras","Lazaros Nalpantidis","Ronja G√ºldenring"],"pdf_url":"https://arxiv.org/pdf/2410.23906v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.23905v1","updated":"2024-10-31T13:10:50Z","published":"2024-10-31T13:10:50Z","title":"Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on\n  Text-modulated Diffusion Model","summary":"  Existing multi-modal image fusion methods fail to address the compound\ndegradations presented in source images, resulting in fusion images plagued by\nnoise, color bias, improper exposure, \\textit{etc}. Additionally, these methods\noften overlook the specificity of foreground objects, weakening the salience of\nthe objects of interest within the fused images. To address these challenges,\nthis study proposes a novel interactive multi-modal image fusion framework\nbased on the text-modulated diffusion model, called Text-DiFuse. First, this\nframework integrates feature-level information integration into the diffusion\nprocess, allowing adaptive degradation removal and multi-modal information\nfusion. This is the first attempt to deeply and explicitly embed information\nfusion within the diffusion process, effectively addressing compound\ndegradation in image fusion. Second, by embedding the combination of the text\nand zero-shot location model into the diffusion fusion process, a\ntext-controlled fusion re-modulation strategy is developed. This enables\nuser-customized text control to improve fusion performance and highlight\nforeground objects in the fused images. Extensive experiments on diverse public\ndatasets show that our Text-DiFuse achieves state-of-the-art fusion performance\nacross various scenarios with complex degradation. Moreover, the semantic\nsegmentation experiment validates the significant enhancement in semantic\nperformance achieved by our text-controlled fusion re-modulation strategy. The\ncode is publicly available at https://github.com/Leiii-Cao/Text-DiFuse.\n","authors":["Hao Zhang","Lei Cao","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2410.23905v1.pdf","comment":"Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2303.15124v2","updated":"2024-10-31T13:10:11Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often incorporate doctor-added markers that can hinder\nAI-based diagnosis. This issue highlights the need of inpainting techniques to\nrestore the corrupted visual contents. However, existing methods require manual\nmask annotation as input, limiting the application scenarios. In this paper, we\npropose a novel blind inpainting method that automatically reconstructs visual\ncontents within the corrupted regions without mask input as guidance. Our model\nincludes a blind reconstruction network and an object-aware discriminator for\nadversarial training. The reconstruction network contains two branches that\npredict corrupted regions in images and simultaneously restore the missing\nvisual contents. Leveraging the potent recognition capability of a dense object\ndetector, the object-aware discriminator ensures markers undetectable after\ninpainting. Thus, the restored images closely resemble the clean ones. We\nevaluate our method on three datasets of various medical imaging modalities,\nconfirming better performance over other state-of-the-art methods.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23904v1","updated":"2024-10-31T13:06:29Z","published":"2024-10-31T13:06:29Z","title":"EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection","summary":"  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.\n","authors":["Qinqian Lei","Bo Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23904v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.04857v4","updated":"2024-10-31T13:01:13Z","published":"2024-02-07T13:54:56Z","title":"Advancing Video Anomaly Detection: A Concise Review and a New Dataset","summary":"  Video Anomaly Detection (VAD) finds widespread applications in security\nsurveillance, traffic monitoring, industrial monitoring, and healthcare.\nDespite extensive research efforts, there remains a lack of concise reviews\nthat provide insightful guidance for researchers. Such reviews would serve as\nquick references to grasp current challenges, research trends, and future\ndirections. In this paper, we present such a review, examining models and\ndatasets from various perspectives. We emphasize the critical relationship\nbetween model and dataset, where the quality and diversity of datasets\nprofoundly influence model performance, and dataset development adapts to the\nevolving needs of emerging approaches. Our review identifies practical issues,\nincluding the absence of comprehensive datasets with diverse scenarios. To\naddress this, we introduce a new dataset, Multi-Scenario Anomaly Detection\n(MSAD), comprising 14 distinct scenarios captured from various camera views.\nOur dataset has diverse motion patterns and challenging variations, such as\ndifferent lighting and weather conditions, providing a robust foundation for\ntraining superior models. We conduct an in-depth analysis of recent\nrepresentative models using MSAD and highlight its potential in addressing the\nchallenges of detecting anomalies across diverse and evolving surveillance\nscenarios. [Project website: https://msad-dataset.github.io/]\n","authors":["Liyun Zhu","Lei Wang","Arjun Raj","Tom Gedeon","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2402.04857v4.pdf","comment":"Accepted at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2407.08447v2","updated":"2024-10-31T12:58:08Z","published":"2024-07-11T12:41:32Z","title":"WildGaussians: 3D Gaussian Splatting in the Wild","summary":"  While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.\n","authors":["Jonas Kulhanek","Songyou Peng","Zuzana Kukelova","Marc Pollefeys","Torsten Sattler"],"pdf_url":"https://arxiv.org/pdf/2407.08447v2.pdf","comment":"NeurIPS 2024; Project page: https://wild-gaussians.github.io/"},{"id":"http://arxiv.org/abs/2410.23891v1","updated":"2024-10-31T12:52:52Z","published":"2024-10-31T12:52:52Z","title":"AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in\n  Satellite Imagery","summary":"  Clouds in satellite imagery pose a significant challenge for downstream\napplications. A major challenge in current cloud removal research is the\nabsence of a comprehensive benchmark and a sufficiently large and diverse\ntraining dataset. To address this problem, we introduce the largest public\ndataset -- $\\textit{AllClear}$ for cloud removal, featuring 23,742 globally\ndistributed regions of interest (ROIs) with diverse land-use patterns,\ncomprising 4 million images in total. Each ROI includes complete temporal\ncaptures from the year 2022, with (1) multi-spectral optical imagery from\nSentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from\nSentinel-1, and (3) auxiliary remote sensing products such as cloud masks and\nland cover maps. We validate the effectiveness of our dataset by benchmarking\nperformance, demonstrating the scaling law -- the PSNR rises from $28.47$ to\n$33.87$ with $30\\times$ more data, and conducting ablation studies on the\ntemporal length and the importance of individual modalities. This dataset aims\nto provide comprehensive coverage of the Earth's surface and promote better\ncloud removal results.\n","authors":["Hangyu Zhou","Chia-Hsiang Kao","Cheng Perng Phoo","Utkarsh Mall","Bharath Hariharan","Kavita Bala"],"pdf_url":"https://arxiv.org/pdf/2410.23891v1.pdf","comment":"Accepted at NeurIPS 2024 Datasets and Benchmarks Track. Code and data\n  available at https://allclear.cs.cornell.edu/"},{"id":"http://arxiv.org/abs/2410.10356v2","updated":"2024-10-31T12:49:09Z","published":"2024-10-14T10:17:24Z","title":"FasterDiT: Towards Faster Diffusion Transformers Training without\n  Architecture Modification","summary":"  Diffusion Transformers (DiT) have attracted significant attention in\nresearch. However, they suffer from a slow convergence rate. In this paper, we\naim to accelerate DiT training without any architectural modification. We\nidentify the following issues in the training process: firstly, certain\ntraining strategies do not consistently perform well across different data.\nSecondly, the effectiveness of supervision at specific timesteps is limited. In\nresponse, we propose the following contributions: (1) We introduce a new\nperspective for interpreting the failure of the strategies. Specifically, we\nslightly extend the definition of Signal-to-Noise Ratio (SNR) and suggest\nobserving the Probability Density Function (PDF) of SNR to understand the\nessence of the data robustness of the strategy. (2) We conduct numerous\nexperiments and report over one hundred experimental results to empirically\nsummarize a unified accelerating strategy from the perspective of PDF. (3) We\ndevelop a new supervision method that further accelerates the training process\nof DiT. Based on them, we propose FasterDiT, an exceedingly simple and\npracticable design strategy. With few lines of code modifications, it achieves\n2.30 FID on ImageNet 256 resolution at 1000k iterations, which is comparable to\nDiT (2.27 FID) but 7 times faster in training.\n","authors":["Jingfeng Yao","Wang Cheng","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10356v2.pdf","comment":"NeurIPS 2024 (poster); update to camera-ready version"},{"id":"http://arxiv.org/abs/2407.16430v2","updated":"2024-10-31T12:48:05Z","published":"2024-07-23T12:28:59Z","title":"Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution","summary":"  Detecting and rejecting unknown out-of-distribution (OOD) samples is critical\nfor deployed neural networks to void unreliable predictions. In real-world\nscenarios, however, the efficacy of existing OOD detection methods is often\nimpeded by the inherent imbalance of in-distribution (ID) data, which causes\nsignificant performance decline. Through statistical observations, we have\nidentified two common challenges faced by different OOD detectors:\nmisidentifying tail class ID samples as OOD, while erroneously predicting OOD\nsamples as head class from ID. To explain this phenomenon, we introduce a\ngeneralized statistical framework, termed ImOOD, to formulate the OOD detection\nproblem on imbalanced data distribution. Consequently, the theoretical analysis\nreveals that there exists a class-aware bias item between balanced and\nimbalanced OOD detection, which contributes to the performance gap. Building\nupon this finding, we present a unified training-time regularization technique\nto mitigate the bias and boost imbalanced OOD detectors across architecture\ndesigns. Our theoretically grounded method translates into consistent\nimprovements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT\nbenchmarks against several state-of-the-art OOD detection approaches. Code is\navailable at https://github.com/alibaba/imood.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Chao Chen","Ze Chen","Rongxin Jiang","Fan Zhou","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16430v2.pdf","comment":"This paper has been accepted by NeurIPS 2024. Code is available at\n  https://github.com/alibaba/imood"},{"id":"http://arxiv.org/abs/2408.12282v2","updated":"2024-10-31T12:30:46Z","published":"2024-08-22T10:34:01Z","title":"Subsurface Scattering for 3D Gaussian Splatting","summary":"  3D reconstruction and relighting of objects made from scattering materials\npresent a significant challenge due to the complex light transport beneath the\nsurface. 3D Gaussian Splatting introduced high-quality novel view synthesis at\nreal-time speeds. While 3D Gaussians efficiently approximate an object's\nsurface, they fail to capture the volumetric properties of subsurface\nscattering. We propose a framework for optimizing an object's shape together\nwith the radiance transfer field given multi-view OLAT (one light at a time)\ndata. Our method decomposes the scene into an explicit surface represented as\n3D Gaussians, with a spatially varying BRDF, and an implicit volumetric\nrepresentation of the scattering component. A learned incident light field\naccounts for shadowing. We optimize all parameters jointly via ray-traced\ndifferentiable rendering. Our approach enables material editing, relighting and\nnovel view synthesis at interactive rates. We show successful application on\nsynthetic data and introduce a newly acquired multi-view multi-light dataset of\nobjects in a light-stage setup. Compared to previous work we achieve comparable\nor better results at a fraction of optimization and rendering time while\nenabling detailed control over material attributes. Project page\nhttps://sss.jdihlmann.com/\n","authors":["Jan-Niklas Dihlmann","Arjun Majumdar","Andreas Engelhardt","Raphael Braun","Hendrik P. A. Lensch"],"pdf_url":"https://arxiv.org/pdf/2408.12282v2.pdf","comment":"Project page: https://sss.jdihlmann.com/"},{"id":"http://arxiv.org/abs/2405.15688v2","updated":"2024-10-31T12:24:34Z","published":"2024-05-24T16:27:05Z","title":"UNION: Unsupervised 3D Object Detection using Object Appearance-based\n  Pseudo-Classes","summary":"  Unsupervised 3D object detection methods have emerged to leverage vast\namounts of data without requiring manual labels for training. Recent approaches\nrely on dynamic objects for learning to detect mobile objects but penalize the\ndetections of static instances during training. Multiple rounds of (self)\ntraining are used to add detected static instances to the set of training\ntargets; this procedure to improve performance is computationally expensive. To\naddress this, we propose the method UNION. We use spatial clustering and\nself-supervised scene flow to obtain a set of static and dynamic object\nproposals from LiDAR. Subsequently, object proposals' visual appearances are\nencoded to distinguish static objects in the foreground and background by\nselecting static instances that are visually similar to dynamic objects. As a\nresult, static and dynamic mobile objects are obtained together, and existing\ndetectors can be trained with a single training. In addition, we extend 3D\nobject discovery to detection by using object appearance-based cluster labels\nas pseudo-class labels for training object classification. We conduct extensive\nexperiments on the nuScenes dataset and increase the state-of-the-art\nperformance for unsupervised 3D object discovery, i.e. UNION more than doubles\nthe average precision to 38.4. The code is available at\ngithub.com/TedLentsch/UNION.\n","authors":["Ted Lentsch","Holger Caesar","Dariu M. Gavrila"],"pdf_url":"https://arxiv.org/pdf/2405.15688v2.pdf","comment":"NeurIPS'24"},{"id":"http://arxiv.org/abs/2312.10112v3","updated":"2024-10-31T12:19:37Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid\n  Approach of Normalizing Flows and GAN","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as Generative Adversarial Networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We combine pixel-wise noise\nmodeling networks based on Normalizing Flows and spatial correlation modeling\nnetworks based on GAN. Specifically, the pixel-wise noise modeling network\nleverages the high training stability of Normalizing Flows to capture noise\ncharacteristics that are affected by a multitude of factors, and the spatial\ncorrelation networks efficiently model pixel-to-pixel relationships. In\nparticular, unlike recent methods that rely on paired noisy images, our method\nsynthesizes noise using clean images and factors that affect noise\ncharacteristics, such as easily obtainable parameters like camera type and ISO\nsettings, making it applicable to various fields where obtaining noisy-clean\nimage pairs is not feasible. In our experiments, our NM-FlowGAN outperforms\nother baselines in the sRGB noise synthesis task. Moreover, the denoising\nneural network trained with synthesized image pairs from our model shows\nsuperior performance compared to other baselines. Our code is available at:\n\\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v3.pdf","comment":"13 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2404.16022v2","updated":"2024-10-31T12:17:39Z","published":"2024-04-24T17:55:33Z","title":"PuLID: Pure and Lightning ID Customization via Contrastive Alignment","summary":"  We propose Pure and Lightning ID customization (PuLID), a novel tuning-free\nID customization method for text-to-image generation. By incorporating a\nLightning T2I branch with a standard diffusion one, PuLID introduces both\ncontrastive alignment loss and accurate ID loss, minimizing disruption to the\noriginal model and ensuring high ID fidelity. Experiments show that PuLID\nachieves superior performance in both ID fidelity and editability. Another\nattractive property of PuLID is that the image elements (e.g., background,\nlighting, composition, and style) before and after the ID insertion are kept as\nconsistent as possible. Codes and models are available at\nhttps://github.com/ToTheBeginning/PuLID\n","authors":["Zinan Guo","Yanze Wu","Zhuowei Chen","Lang Chen","Peng Zhang","Qian He"],"pdf_url":"https://arxiv.org/pdf/2404.16022v2.pdf","comment":"NeurIPS 2024. Codes and models are available at\n  https://github.com/ToTheBeginning/PuLID"},{"id":"http://arxiv.org/abs/2410.23854v1","updated":"2024-10-31T12:04:30Z","published":"2024-10-31T12:04:30Z","title":"Airway Labeling Meets Clinical Applications: Reflecting Topology\n  Consistency and Outliers via Learnable Attentions","summary":"  Accurate airway anatomical labeling is crucial for clinicians to identify and\nnavigate complex bronchial structures during bronchoscopy. Automatic airway\nanatomical labeling is challenging due to significant individual variability\nand anatomical variations. Previous methods are prone to generate inconsistent\npredictions, which is harmful for preoperative planning and intraoperative\nnavigation. This paper aims to address these challenges by proposing a novel\nmethod that enhances topological consistency and improves the detection of\nabnormal airway branches.\n  We propose a novel approach incorporating two modules: the Soft Subtree\nConsistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module\nconstructs a soft subtree to capture clinically relevant topological\nrelationships, allowing for flexible feature aggregation within and across\nsubtrees. The ABS module facilitates the interaction between node features and\nprototypes to distinguish abnormal branches, preventing the erroneous\naggregation of features between normal and abnormal nodes.\n  Evaluated on a challenging dataset characterized by severe airway distortion\nand atrophy, our method achieves superior performance compared to\nstate-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the\nsegmental level and an 83.7% accuracy at the subsegmental level, representing a\n1.4% increase in subsegmental accuracy and a 3.1% increase in topological\nconsistency. Notably, the method demonstrates reliable performance in cases\nwith disease-induced airway deformities, ensuring consistent and accurate\nlabeling.\n","authors":["Chenyu Li","Minghui Zhang","Chuyan Zhang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.23854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19370v2","updated":"2024-10-31T12:00:26Z","published":"2024-09-28T14:50:45Z","title":"MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN\n  Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation","summary":"  Segmenting anatomical structures and lesions from ultrasound images\ncontributes to disease assessment. Weakly supervised learning (WSL) based on\nsparse annotation has achieved encouraging performance and demonstrated the\npotential to reduce annotation costs. This study attempts to introduce\nscribble-based WSL into ultrasound image segmentation tasks. However,\nultrasound images often suffer from poor contrast and unclear edges, coupled\nwith insufficient supervison signals for edges, posing challenges to edge\nprediction. Uncertainty modeling has been proven to facilitate models in\ndealing with these issues. Nevertheless, existing uncertainty estimation\nparadigms are not robust enough and often filter out predictions near decision\nboundaries, resulting in unstable edge predictions. Therefore, we propose\nleveraging predictions near decision boundaries effectively. Specifically, we\nintroduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided\nConsistency strategy. This strategy utilizes high-evidence predictions, which\nare more likely to occur near high-density regions, to guide the optimization\nof low-evidence predictions that may appear near decision boundaries.\nFurthermore, the diverse sizes and locations of lesions in ultrasound images\npose a challenge for CNNs with local receptive fields, as they struggle to\nmodel global information. Therefore, we introduce Visual Mamba based on\nstructured state space sequence models, which achieves long-range dependency\nwith linear computational complexity, and we construct a novel hybrid CNN-Mamba\nframework. During training, the collaboration between the CNN branch and the\nMamba branch in the proposed framework draws inspiration from each other based\non the EGC strategy. Experiments demonstrate the competitiveness of the\nproposed method. Dataset and code will be available on\nhttps://github.com/GtLinyer/MambaEviScrib.\n","authors":["Xiaoxiang Han","Xinyu Li","Jiang Shang","Yiman Liu","Keyan Chen","Shugong Xu","Qiaohong Liu","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12490v2","updated":"2024-10-31T11:42:07Z","published":"2024-10-16T12:13:17Z","title":"Stabilize the Latent Space for Image Autoregressive Modeling: A Unified\n  Perspective","summary":"  Latent-based image generative models, such as Latent Diffusion Models (LDMs)\nand Mask Image Models (MIMs), have achieved notable success in image generation\ntasks. These models typically leverage reconstructive autoencoders like VQGAN\nor VAE to encode pixels into a more compact latent space and learn the data\ndistribution in the latent space instead of directly from pixels. However, this\npractice raises a pertinent question: Is it truly the optimal choice? In\nresponse, we begin with an intriguing observation: despite sharing the same\nlatent space, autoregressive models significantly lag behind LDMs and MIMs in\nimage generation. This finding contrasts sharply with the field of NLP, where\nthe autoregressive model GPT has established a commanding presence. To address\nthis discrepancy, we introduce a unified perspective on the relationship\nbetween latent space and generative models, emphasizing the stability of latent\nspace in image generative modeling. Furthermore, we propose a simple but\neffective discrete image tokenizer to stabilize the latent space for image\ngenerative modeling by applying K-Means on the latent features of\nself-supervised learning models. Experimental results show that image\nautoregressive modeling with our tokenizer (DiGIT) benefits both image\nunderstanding and image generation with the next token prediction principle,\nwhich is inherently straightforward for GPT models but challenging for other\ngenerative models. Remarkably, for the first time, a GPT-style autoregressive\nmodel for images outperforms LDMs, which also exhibits substantial improvement\nakin to GPT when scaling up model size. Our findings underscore the potential\nof an optimized latent space and the integration of discrete tokenization in\nadvancing the capabilities of image generative models. The code is available at\n\\url{https://github.com/DAMO-NLP-SG/DiGIT}.\n","authors":["Yongxin Zhu","Bocheng Li","Hang Zhang","Xin Li","Linli Xu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2410.12490v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23836v1","updated":"2024-10-31T11:32:33Z","published":"2024-10-31T11:32:33Z","title":"Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts","summary":"  This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes.\n","authors":["Xiang Deng","Youxin Pang","Xiaochen Zhao","Chao Xu","Lizhen Wang","Hongjiang Xiao","Shi Yan","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14927v3","updated":"2024-10-31T11:32:19Z","published":"2024-06-21T07:37:17Z","title":"GIC: Gaussian-Informed Continuum for Physical Property Identification\n  and Simulation","summary":"  This paper studies the problem of estimating physical properties (system\nidentification) through visual observations. To facilitate geometry-aware\nguidance in physical property estimation, we introduce a novel hybrid framework\nthat leverages 3D Gaussian representation to not only capture explicit shapes\nbut also enable the simulated continuum to render object masks as 2D shape\nsurrogates during training. We propose a new dynamic 3D Gaussian framework\nbased on motion factorization to recover the object as 3D Gaussian point sets\nacross different time states. Furthermore, we develop a coarse-to-fine filling\nstrategy to generate the density fields of the object from the Gaussian\nreconstruction, allowing for the extraction of object continuums along with\ntheir surfaces and the integration of Gaussian attributes into these continuum.\nIn addition to the extracted object surfaces, the Gaussian-informed continuum\nalso enables the rendering of object masks during simulations, serving as\n2D-shape guidance for physical property estimation. Extensive experimental\nevaluations demonstrate that our pipeline achieves state-of-the-art performance\nacross multiple benchmarks and metrics. Additionally, we illustrate the\neffectiveness of the proposed method through real-world demonstrations,\nshowcasing its practical utility. Our project page is at\nhttps://jukgei.github.io/project/gic.\n","authors":["Junhao Cai","Yuji Yang","Weihao Yuan","Yisheng He","Zilong Dong","Liefeng Bo","Hui Cheng","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14927v3.pdf","comment":"21 pages, 8 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23835v1","updated":"2024-10-31T11:29:41Z","published":"2024-10-31T11:29:41Z","title":"Counterfactual MRI Data Augmentation using Conditional Denoising\n  Diffusion Generative Models","summary":"  Deep learning (DL) models in medical imaging face challenges in\ngeneralizability and robustness due to variations in image acquisition\nparameters (IAP). In this work, we introduce a novel method using conditional\ndenoising diffusion generative models (cDDGMs) to generate counterfactual\nmagnetic resonance (MR) images that simulate different IAP without altering\npatient anatomy. We demonstrate that using these counterfactual images for data\naugmentation can improve segmentation accuracy, particularly in\nout-of-distribution settings, enhancing the overall generalizability and\nrobustness of DL models across diverse imaging conditions. Our approach shows\npromise in addressing domain and covariate shifts in medical imaging. The code\nis publicly available at https:\n//github.com/pedromorao/Counterfactual-MRI-Data-Augmentation\n","authors":["Pedro Mor√£o","Joao Santinha","Yasna Forghani","Nuno Lou√ß√£o","Pedro Gouveia","Mario A. T. Figueiredo"],"pdf_url":"https://arxiv.org/pdf/2410.23835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07966v3","updated":"2024-10-31T11:25:40Z","published":"2024-09-12T11:53:05Z","title":"ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE","summary":"  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n","authors":["Sichun Wu","Kazi Injamamul Haque","Zerrin Yumak"],"pdf_url":"https://arxiv.org/pdf/2409.07966v3.pdf","comment":"14 pages, 9 figures, 3 tables. Includes code. Accepted at ACM\n  SIGGRAPH MIG 2024"},{"id":"http://arxiv.org/abs/2410.23834v1","updated":"2024-10-31T11:23:19Z","published":"2024-10-31T11:23:19Z","title":"Denoising Diffusion Models for Anomaly Localization in Medical Images","summary":"  This chapter explores anomaly localization in medical images using denoising\ndiffusion models. After providing a brief methodological background of these\nmodels, including their application to image reconstruction and their\nconditioning using guidance mechanisms, we provide an overview of available\ndatasets and evaluation metrics suitable for their application to anomaly\nlocalization in medical images. In this context, we discuss supervision schemes\nranging from fully supervised segmentation to semi-supervised, weakly\nsupervised, self-supervised, and unsupervised methods, and provide insights\ninto the effectiveness and limitations of these approaches. Furthermore, we\nhighlight open challenges in anomaly localization, including detection bias,\ndomain shift, computational cost, and model interpretability. Our goal is to\nprovide an overview of the current state of the art in the field, outline\nresearch gaps, and highlight the potential of diffusion models for robust\nanomaly localization in medical images.\n","authors":["Cosmin I. Bercea","Philippe C. Cattin","Julia A. Schnabel","Julia Wolleb"],"pdf_url":"https://arxiv.org/pdf/2410.23834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23831v1","updated":"2024-10-31T11:21:21Z","published":"2024-10-31T11:21:21Z","title":"FRoundation: Are Foundation Models Ready for Face Recognition?","summary":"  Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models.\n","authors":["Tahar Chettaoui","Naser Damer","Fadi Boutros"],"pdf_url":"https://arxiv.org/pdf/2410.23831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23828v1","updated":"2024-10-31T11:20:13Z","published":"2024-10-31T11:20:13Z","title":"Show Me What and Where has Changed? Question Answering and Grounding for\n  Remote Sensing Change Detection","summary":"  Remote sensing change detection aims to perceive changes occurring on the\nEarth's surface from remote sensing data in different periods, and feed these\nchanges back to humans. However, most existing methods only focus on detecting\nchange regions, lacking the ability to interact with users to identify changes\nthat the users expect. In this paper, we introduce a new task named Change\nDetection Question Answering and Grounding (CDQAG), which extends the\ntraditional change detection task by providing interpretable textual answers\nand intuitive visual evidence. To this end, we construct the first CDQAG\nbenchmark dataset, termed QAG-360K, comprising over 360K triplets of questions,\ntextual answers, and corresponding high-quality visual masks. It encompasses 10\nessential land-cover categories and 8 comprehensive question types, which\nprovides a large-scale and diverse dataset for remote sensing applications.\nBased on this, we present VisTA, a simple yet effective baseline method that\nunifies the tasks of question answering and grounding by delivering both visual\nand textual answers. Our method achieves state-of-the-art results on both the\nclassic CDVQA and the proposed CDQAG datasets. Extensive qualitative and\nquantitative experimental results provide useful insights for the development\nof better CDQAG models, and we hope that our work can inspire further research\nin this important yet underexplored direction. The proposed benchmark dataset\nand method are available at https://github.com/like413/VisTA.\n","authors":["Ke Li","Fuyu Dong","Di Wang","Shaofeng Li","Quan Wang","Xinbo Gao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.23828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01762v4","updated":"2024-10-31T11:11:24Z","published":"2023-05-27T06:00:51Z","title":"Rapid Plug-in Defenders","summary":"  In the realm of daily services, the deployment of deep neural networks\nunderscores the paramount importance of their reliability. However, the\nvulnerability of these networks to adversarial attacks, primarily\nevasion-based, poses a concerning threat to their functionality. Common methods\nfor enhancing robustness involve heavy adversarial training or leveraging\nlearned knowledge from clean data, both necessitating substantial computational\nresources. This inherent time-intensive nature severely limits the agility of\nlarge foundational models to swiftly counter adversarial perturbations. To\naddress this challenge, this paper focuses on the Rapid Plug-in Defender\n(RaPiD) problem, aiming to rapidly counter adversarial perturbations without\naltering the deployed model. Drawing inspiration from the generalization and\nthe universal computation ability of pre-trained transformer models, we propose\na novel method termed CeTaD (Considering Pre-trained Transformers as Defenders)\nfor RaPiD, optimized for efficient computation. CeTaD strategically fine-tunes\nthe normalization layer parameters within the defender using a limited set of\nclean and adversarial examples. Our evaluation centers on assessing CeTaD's\neffectiveness, transferability, and the impact of different components in\nscenarios involving one-shot adversarial examples. The proposed method is\ncapable of rapidly adapting to various attacks and different application\nscenarios without altering the target model and clean training data. We also\nexplore the influence of varying training data conditions on CeTaD's\nperformance. Notably, CeTaD exhibits adaptability across differentiable service\nmodels and proves the potential of continuous learning.\n","authors":["Kai Wu","Yujian Betterest Li","Jian Lou","Xiaoyu Zhang","Handing Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2306.01762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23822v1","updated":"2024-10-31T11:07:26Z","published":"2024-10-31T11:07:26Z","title":"Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models\n  for Medical Visual Grounding","summary":"  Multimodal Large Language Models (MLLMs) inherit the superior text\nunderstanding capabilities of LLMs and extend these capabilities to multimodal\nscenarios. These models achieve excellent results in the general domain of\nmultimodal tasks. However, in the medical domain, the substantial training\ncosts and the requirement for extensive medical data pose challenges to the\ndevelopment of medical MLLMs. Furthermore, due to the free-text form of\nanswers, tasks such as visual grounding that need to produce output in a\nprescribed form become difficult for MLLMs. So far, there have been no medical\nMLLMs works in medical visual grounding area. For the medical vision grounding\ntask, which involves identifying locations in medical images based on short\ntext descriptions, we propose Parameter-efficient Fine-tuning medical\nmultimodal large language models for Medcial Visual Grounding (PFMVG). To\nvalidate the performance of the model, we evaluate it on a public benchmark\ndataset for medical visual grounding, where it achieves competitive results,\nand significantly outperforming GPT-4v. Our code will be open sourced after\npeer review.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23820v1","updated":"2024-10-31T11:05:09Z","published":"2024-10-31T11:05:09Z","title":"Disentangling Disentangled Representations: Towards Improved Latent\n  Units via Diffusion Models","summary":"  Disentangled representation learning (DRL) aims to break down observed data\ninto core intrinsic factors for a profound understanding of the data. In\nreal-world scenarios, manually defining and labeling these factors are\nnon-trivial, making unsupervised methods attractive. Recently, there have been\nlimited explorations of utilizing diffusion models (DMs), which are already\nmainstream in generative modeling, for unsupervised DRL. They implement their\nown inductive bias to ensure that each latent unit input to the DM expresses\nonly one distinct factor. In this context, we design Dynamic Gaussian Anchoring\nto enforce attribute-separated latent units for more interpretable DRL. This\nunconventional inductive bias explicitly delineates the decision boundaries\nbetween attributes while also promoting the independence among latent units.\nAdditionally, we also propose Skip Dropout technique, which easily modifies the\ndenoising U-Net to be more DRL-friendly, addressing its uncooperative nature\nwith the disentangling feature extractor. Our methods, which carefully consider\nthe latent unit semantics and the distinct DM structure, enhance the\npracticality of DM-based disentangled representations, demonstrating\nstate-of-the-art disentanglement performance on both synthetic and real data,\nas well as advantages in downstream tasks.\n","authors":["Youngjun Jun","Jiwoo Park","Kyobin Choo","Tae Eun Choi","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2410.23820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23806v1","updated":"2024-10-31T10:46:11Z","published":"2024-10-31T10:46:11Z","title":"Human Action Recognition (HAR) Using Skeleton-based Quantum Spatial\n  Temporal Relative Transformer Network: ST-RTR","summary":"  Quantum Human Action Recognition (HAR) is an interesting research area in\nhuman-computer interaction used to monitor the activities of elderly and\ndisabled individuals affected by physical and mental health. In the recent era,\nskeleton-based HAR has received much attention because skeleton data has shown\nthat it can handle changes in striking, body size, camera views, and complex\nbackgrounds. One key characteristic of ST-GCN is automatically learning spatial\nand temporal patterns from skeleton sequences. It has some limitations, as this\nmethod only works for short-range correlation due to its limited receptive\nfield. Consequently, understanding human action requires long-range\ninterconnection. To address this issue, we developed a quantum spatial-temporal\nrelative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,\nwhich allow efficient communication and data transmission within the network.\nThese nodes help to break the inherent spatial and temporal skeleton\ntopologies, which enables the model to understand long-range human action\nbetter. Furthermore, we combine quantum ST-RTR with a fusion model for further\nperformance improvements. To assess the performance of the quantum ST-RTR\nmethod, we conducted experiments on three skeleton-based HAR benchmarks: NTU\nRGB+D 60, NTU RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and\n1.45% on NTU RGB+D 60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets,\naccuracy improved by 2.54%. The experimental outcomes explain that the proposed\nST-RTR model significantly improves action recognition associated with the\nstandard ST-GCN method.\n","authors":["Faisal Mehmood","Enqing Chen","Touqeer Abbas","Samah M. Alzanin"],"pdf_url":"https://arxiv.org/pdf/2410.23806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23800v1","updated":"2024-10-31T10:35:59Z","published":"2024-10-31T10:35:59Z","title":"SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild","summary":"  Self-occlusion is common when capturing people in the wild, where the\nperformer do not follow predefined motion scripts. This challenges existing\nmonocular human reconstruction systems that assume full body visibility. We\nintroduce Self-Occluded Avatar Recovery (SOAR), a method for complete human\nreconstruction from partial observations where parts of the body are entirely\nunobserved. SOAR leverages structural normal prior and generative diffusion\nprior to address such an ill-posed reconstruction problem. For structural\nnormal prior, we model human with an reposable surfel model with well-defined\nand easily readable shapes. For generative diffusion prior, we perform an\ninitial reconstruction and refine it using score distillation. On various\nbenchmarks, we show that SOAR performs favorably than state-of-the-art\nreconstruction and generation methods, and on-par comparing to concurrent\nworks. Additional video results and code are available at\nhttps://soar-avatar.github.io/.\n","authors":["Zhuoyang Pan","Angjoo Kanazawa","Hang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.23800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23788v1","updated":"2024-10-31T10:13:05Z","published":"2024-10-31T10:13:05Z","title":"EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like\n  Sketching","summary":"  Transformer-based Diffusion Probabilistic Models (DPMs) have shown more\npotential than CNN-based DPMs, yet their extensive computational requirements\nhinder widespread practical applications. To reduce the computation budget of\ntransformer-based DPMs, this work proposes the Efficient Diffusion Transformer\n(EDT) framework. The framework includes a lightweight-design diffusion model\narchitecture, and a training-free Attention Modulation Matrix and its\nalternation arrangement in EDT inspired by human-like sketching. Additionally,\nwe propose a token relation-enhanced masking training strategy tailored\nexplicitly for EDT to augment its token relation learning capability. Our\nextensive experiments demonstrate the efficacy of EDT. The EDT framework\nreduces training and inference costs and surpasses existing transformer-based\ndiffusion models in image synthesis performance, thereby achieving a\nsignificant overall enhancement. With lower FID, EDT-S, EDT-B, and EDT-XL\nattained speed-ups of 3.93x, 2.84x, and 1.92x respectively in the training\nphase, and 2.29x, 2.29x, and 2.22x respectively in inference, compared to the\ncorresponding sizes of MDTv2. The source code is released at\nhttps://github.com/xinwangChen/EDT.\n","authors":["Xinwang Chen","Ning Liu","Yichen Zhu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2410.23788v1.pdf","comment":"Xinwang Chen and Ning Liu are with equal contributions. This paper\n  has been accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.07085v2","updated":"2024-10-31T10:06:16Z","published":"2024-06-11T09:22:39Z","title":"CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor\n  Segmentation","summary":"  Existing promptable segmentation methods in the medical imaging field\nprimarily consider either textual or visual prompts to segment relevant\nobjects, yet they often fall short when addressing anomalies in medical images,\nlike tumors, which may vary greatly in shape, size, and appearance. Recognizing\nthe complexity of medical scenarios and the limitations of textual or visual\nprompts, we propose a novel dual-prompt schema that leverages the complementary\nstrengths of visual and textual prompts for segmenting various organs and\ntumors. Specifically, we introduce CAT, an innovative model that Coordinates\nAnatomical prompts derived from 3D cropped images with Textual prompts enriched\nby medical domain knowledge. The model architecture adopts a general\nquery-based design, where prompt queries facilitate segmentation queries for\nmask prediction. To synergize two types of prompts within a unified framework,\nwe implement a ShareRefiner, which refines both segmentation and prompt queries\nwhile disentangling the two types of prompts. Trained on a consortium of 10\npublic CT datasets, CAT demonstrates superior performance in multiple\nsegmentation tasks. Further validation on a specialized in-house dataset\nreveals the remarkable capacity of segmenting tumors across multiple cancer\nstages. This approach confirms that coordinating multimodal prompts is a\npromising avenue for addressing complex scenarios in the medical domain.\n","authors":["Zhongzhen Huang","Yankai Jiang","Rongzhao Zhang","Shaoting Zhang","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.07085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10897v2","updated":"2024-10-31T10:01:59Z","published":"2024-07-15T16:46:14Z","title":"Optical Diffusion Models for Image Generation","summary":"  Diffusion models generate new samples by progressively decreasing the noise\nfrom the initially provided random distribution. This inference procedure\ngenerally utilizes a trained neural network numerous times to obtain the final\noutput, creating significant latency and energy consumption on digital\nelectronic hardware such as GPUs. In this study, we demonstrate that the\npropagation of a light beam through a semi-transparent medium can be programmed\nto implement a denoising diffusion model on image samples. This framework\nprojects noisy image patterns through passive diffractive optical layers, which\ncollectively only transmit the predicted noise term in the image. The optical\ntransparent layers, which are trained with an online training approach,\nbackpropagating the error to the analytical model of the system, are passive\nand kept the same across different steps of denoising. Hence this method\nenables high-speed image generation with minimal power consumption, benefiting\nfrom the bandwidth and energy efficiency of optical information processing.\n","authors":["Ilker Oguz","Niyazi Ulas Dinc","Mustafa Yildirim","Junjie Ke","Innfarn Yoo","Qifei Wang","Feng Yang","Christophe Moser","Demetri Psaltis"],"pdf_url":"https://arxiv.org/pdf/2407.10897v2.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.23782v1","updated":"2024-10-31T09:55:32Z","published":"2024-10-31T09:55:32Z","title":"Video Token Merging for Long-form Video Understanding","summary":"  As the scale of data and models for video understanding rapidly expand,\nhandling long-form video input in transformer-based models presents a practical\nchallenge. Rather than resorting to input sampling or token dropping, which may\nresult in information loss, token merging shows promising results when used in\ncollaboration with transformers. However, the application of token merging for\nlong-form video processing is not trivial. We begin with the premise that token\nmerging should not rely solely on the similarity of video tokens; the saliency\nof tokens should also be considered. To address this, we explore various video\ntoken merging strategies for long-form video classification, starting with a\nsimple extension of image token merging, moving to region-concentrated merging,\nand finally proposing a learnable video token merging (VTM) algorithm that\ndynamically merges tokens based on their saliency. Extensive experimental\nresults show that we achieve better or comparable performances on the LVU,\nCOIN, and Breakfast datasets. Moreover, our approach significantly reduces\nmemory costs by 84% and boosts throughput by approximately 6.89 times compared\nto baseline algorithms.\n","authors":["Seon-Ho Lee","Jue Wang","Zhikang Zhang","David Fan","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.23782v1.pdf","comment":"21 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23780v1","updated":"2024-10-31T09:53:21Z","published":"2024-10-31T09:53:21Z","title":"Driving by the Rules: A Benchmark for Integrating Traffic Sign\n  Regulations into Vectorized HD Map","summary":"  Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current benchmark datasets concentrate\non lane perception or basic traffic sign recognition, they often overlook the\nintricate task of integrating these regulations into lane operations.\nAddressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over 10,000 annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from\nTraffic Sign, which accurately deciphers regulatory instructions, and 2)\nRule-Lane Correspondence Reasoning, which aligns these rules with their\nrespective lanes. Built upon this benchmark, we provide a multimodal solution\nthat offers a strong baseline for advancing autonomous driving technologies. It\nfills a critical gap in the integration of traffic sign rules, contributing to\nthe development of reliable autonomous navigation systems.\n","authors":["Xinyuan Chang","Maixuan Xue","Xinran Liu","Zheng Pan","Xing Wei"],"pdf_url":"https://arxiv.org/pdf/2410.23780v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.23775v1","updated":"2024-10-31T09:45:00Z","published":"2024-10-31T09:45:00Z","title":"In-Context LoRA for Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., $20\\sim 100$ samples) instead of full-parameter tuning\nwith large datasets. We name our models In-Context LoRA (IC-LoRA). This\napproach requires no modifications to the original DiT models, only changes to\nthe training data. Remarkably, our pipeline generates high-fidelity image sets\nthat better adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Huanzhang Dou","Chen Liang","Yutong Feng","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23775v1.pdf","comment":"Project page: https://ali-vilab.github.io/In-Context-Lora-Page/"},{"id":"http://arxiv.org/abs/2410.14790v2","updated":"2024-10-31T09:34:45Z","published":"2024-10-02T13:42:38Z","title":"SSL-NBV: A Self-Supervised-Learning-Based Next-Best-View algorithm for\n  Efficient 3D Plant Reconstruction by a Robot","summary":"  The 3D reconstruction of plants is challenging due to their complex shape\ncausing many occlusions. Next-Best-View (NBV) methods address this by\niteratively selecting new viewpoints to maximize information gain (IG).\nDeep-learning-based NBV (DL-NBV) methods demonstrate higher computational\nefficiency over classic voxel-based NBV approaches but current methods require\nextensive training using ground-truth plant models, making them impractical for\nreal-world plants. These methods, moreover, rely on offline training with\npre-collected data, limiting adaptability in changing agricultural\nenvironments. This paper proposes a self-supervised learning-based NBV method\n(SSL-NBV) that uses a deep neural network to predict the IG for candidate\nviewpoints. The method allows the robot to gather its own training data during\ntask execution by comparing new 3D sensor data to the earlier gathered data and\nby employing weakly-supervised learning and experience replay for efficient\nonline learning. Comprehensive evaluations were conducted in simulation and\nreal-world environments using cross-validation. The results showed that SSL-NBV\nrequired fewer views for plant reconstruction than non-NBV methods and was over\n800 times faster than a voxel-based method. SSL-NBV reduced training\nannotations by over 90% compared to a baseline DL-NBV. Furthermore, SSL-NBV\ncould adapt to novel scenarios through online fine-tuning. Also using real\nplants, the results showed that the proposed method can learn to effectively\nplan new viewpoints for 3D plant reconstruction. Most importantly, SSL-NBV\nautomated the entire network training and uses continuous online learning,\nallowing it to operate in changing agricultural environments.\n","authors":["Jianchao Ci","Eldert J. van Henten","Xin Wang","Akshay K. Burusa","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2410.14790v2.pdf","comment":"22 pages, 11 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.23767v1","updated":"2024-10-31T09:29:55Z","published":"2024-10-31T09:29:55Z","title":"Open-Set 3D object detection in LiDAR data as an Out-of-Distribution\n  problem","summary":"  3D Object Detection from LiDAR data has achieved industry-ready performance\nin controlled environments through advanced deep learning methods. However,\nthese neural network models are limited by a finite set of inlier object\ncategories. Our work redefines the open-set 3D Object Detection problem in\nLiDAR data as an Out-Of-Distribution (OOD) problem to detect outlier objects.\nThis approach brings additional information in comparison with traditional\nobject detection. We establish a comparative benchmark and show that two-stage\nOOD methods, notably autolabelling, show promising results for 3D OOD Object\nDetection. Our contributions include setting a rigorous evaluation protocol by\nexamining the evaluation of hyperparameters and evaluating strategies for\ngenerating additional data to train an OOD-aware 3D object detector. This\ncomprehensive analysis is essential for developing robust 3D object detection\nsystems that can perform reliably in diverse and unpredictable real-world\nscenarios.\n","authors":["Louis Soum-Fontez","Jean-Emmanuel Deschaud","Fran√ßois Goulette"],"pdf_url":"https://arxiv.org/pdf/2410.23767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23758v1","updated":"2024-10-31T09:25:02Z","published":"2024-10-31T09:25:02Z","title":"Reverse Attitude Statistics Based Star Map Identification Method","summary":"  The star tracker is generally affected by the atmospheric background light\nand the aerodynamic environment when working in near space, which results in\nmissing stars or false stars. Moreover, high-speed maneuvering may cause star\ntrailing, which reduces the accuracy of the star position. To address the\nchallenges for starmap identification, a reverse attitude statistics based\nmethod is proposed to handle position noise, false stars, and missing stars.\nConversely to existing methods which match before solving for attitude, this\nmethod introduces attitude solving into the matching process, and obtains the\nfinal match and the correct attitude simultaneously by frequency statistics.\nFirstly, based on stable angular distance features, the initial matching is\nobtained by utilizing spatial hash indexing. Then, the dual-vector attitude\ndetermination is introduced to calculate potential attitude. Finally, the star\npairs are accurately matched by applying a frequency statistics filtering\nmethod. In addition, Bayesian optimization is employed to find optimal\nparameters under the impact of noises, which is able to enhance the algorithm\nperformance further. In this work, the proposed method is validated in\nsimulation, field test and on-orbit experiment. Compared with the\nstate-of-the-art, the identification rate is improved by more than 14.3%, and\nthe solving time is reduced by over 28.5%.\n","authors":["Shunmei Dong","Qinglong Wang","Haiqing Wang","Qianqian Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23758v1.pdf","comment":"10 pages, 17figures, 4 tables, 4663 words, submitted to IEEE Sensors\n  Journal"},{"id":"http://arxiv.org/abs/2403.09400v3","updated":"2024-10-31T09:21:29Z","published":"2024-03-14T13:50:44Z","title":"ConDiSR: Contrastive Disentanglement and Style Regularization for Single\n  Domain Generalization","summary":"  Medical data often exhibits distribution shifts, which cause test-time\nperformance degradation for deep learning models trained using standard\nsupervised learning pipelines. This challenge is addressed in the field of\nDomain Generalization (DG) with the sub-field of Single Domain Generalization\n(SDG) being specifically interesting due to the privacy- or logistics-related\nissues often associated with medical data. Existing disentanglement-based SDG\nmethods heavily rely on structural information embedded in segmentation masks,\nhowever classification labels do not provide such dense information. This work\nintroduces a novel SDG method aimed at medical image classification that\nleverages channel-wise contrastive disentanglement. It is further enhanced with\nreconstruction-based style regularization to ensure extraction of distinct\nstyle and structure feature representations. We evaluate our method on the\ncomplex task of multicenter histopathology image classification, comparing it\nagainst state-of-the-art (SOTA) SDG baselines. Results demonstrate that our\nmethod surpasses the SOTA by a margin of 1% in average accuracy while also\nshowing more stable performance. This study highlights the importance and\nchallenges of exploring SDG frameworks in the context of the classification\ntask. The code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/ConDiSR\n","authors":["Aleksandr Matsun","Numan Saeed","Fadillah Adamsyah Maani","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.09400v3.pdf","comment":"A flaw was found in the results acquisition"},{"id":"http://arxiv.org/abs/2410.23751v1","updated":"2024-10-31T09:11:56Z","published":"2024-10-31T09:11:56Z","title":"EXACFS -- A CIL Method to mitigate Catastrophic Forgetting","summary":"  Deep neural networks (DNNS) excel at learning from static datasets but\nstruggle with continual learning, where data arrives sequentially. Catastrophic\nforgetting, the phenomenon of forgetting previously learned knowledge, is a\nprimary challenge. This paper introduces EXponentially Averaged Class-wise\nFeature Significance (EXACFS) to mitigate this issue in the class incremental\nlearning (CIL) setting. By estimating the significance of model features for\neach learned class using loss gradients, gradually aging the significance\nthrough the incremental tasks and preserving the significant features through a\ndistillation loss, EXACFS effectively balances remembering old knowledge\n(stability) and learning new knowledge (plasticity). Extensive experiments on\nCIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in\npreserving stability while acquiring plasticity.\n","authors":["S Balasubramanian","M Sai Subramaniam","Sai Sriram Talasu","P Yedu Krishna","Manepalli Pranav Phanindra Sai","Ravi Mukkamala","Darshan Gera"],"pdf_url":"https://arxiv.org/pdf/2410.23751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13495v2","updated":"2024-10-31T09:11:37Z","published":"2024-06-19T12:35:02Z","title":"DF40: Toward Next-Generation Deepfake Detection","summary":"  We propose a new comprehensive benchmark to revolutionize the current\ndeepfake detection field to the next generation. Predominantly, existing works\nidentify top-notch detection algorithms and models by adhering to the common\npractice: training detectors on one specific dataset (e.g., FF++) and testing\nthem on other prevalent deepfake datasets. This protocol is often regarded as a\n\"golden compass\" for navigating SoTA detectors. But can these stand-out\n\"winners\" be truly applied to tackle the myriad of realistic and diverse\ndeepfakes lurking in the real world? If not, what underlying factors contribute\nto this gap? In this work, we found the dataset (both train and test) can be\nthe \"primary culprit\" due to: (1) forgery diversity: Deepfake techniques are\ncommonly referred to as both face forgery and entire image synthesis. Most\nexisting datasets only contain partial types of them, with limited forgery\nmethods implemented; (2) forgery realism: The dominated training dataset, FF++,\ncontains out-of-date forgery techniques from the past four years. \"Honing\nskills\" on these forgeries makes it difficult to guarantee effective detection\ngeneralization toward nowadays' SoTA deepfakes; (3) evaluation protocol: Most\ndetection works perform evaluations on one type, which hinders the development\nof universal deepfake detectors. To address this dilemma, we construct a highly\ndiverse deepfake detection dataset called DF40, which comprises 40 distinct\ndeepfake techniques. We then conduct comprehensive evaluations using 4 standard\nevaluation protocols and 8 representative detection methods, resulting in over\n2,000 evaluations. Through these evaluations, we provide an extensive analysis\nfrom various perspectives, leading to 7 new insightful findings. We also open\nup 4 valuable yet previously underexplored research questions to inspire future\nworks. Our project page is https://github.com/YZY-stack/DF40.\n","authors":["Zhiyuan Yan","Taiping Yao","Shen Chen","Yandan Zhao","Xinghe Fu","Junwei Zhu","Donghao Luo","Chengjie Wang","Shouhong Ding","Yunsheng Wu","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.13495v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2108.05080 by other authors"},{"id":"http://arxiv.org/abs/2405.14702v2","updated":"2024-10-31T09:08:48Z","published":"2024-05-23T15:37:06Z","title":"G3: An Effective and Adaptive Framework for Worldwide Geolocalization\n  Using Large Multi-Modality Models","summary":"  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods. Our code and data are available online for\nreproduction.\n","authors":["Pengyue Jia","Yiding Liu","Xiaopeng Li","Yuhao Wang","Yantong Du","Xiao Han","Xuetao Wei","Shuaiqiang Wang","Dawei Yin","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.14702v2.pdf","comment":"Accepted to NeurIPS2024"},{"id":"http://arxiv.org/abs/2312.05508v3","updated":"2024-10-31T09:04:41Z","published":"2023-12-09T09:08:03Z","title":"Improving Adversarial Robust Fairness via Anti-Bias Soft Label\n  Distillation","summary":"  Adversarial Training (AT) has been widely proved to be an effective method to\nimprove the adversarial robustness against adversarial examples for Deep Neural\nNetworks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD)\nhas demonstrated its superior performance in improving the robustness of small\nstudent models with the guidance of large teacher models. However, both AT and\nARD encounter the robust fairness problem: these models exhibit strong\nrobustness when facing part of classes (easy class), but weak robustness when\nfacing others (hard class). In this paper, we give an in-depth analysis of the\npotential factors and argue that the smoothness degree of samples' soft labels\nfor different classes (i.e., hard class or easy class) will affect the robust\nfairness of DNNs from both empirical observation and theoretical analysis.\nBased on the above finding, we propose an Anti-Bias Soft Label Distillation\n(ABSLD) method to mitigate the adversarial robust fairness problem within the\nframework of Knowledge Distillation (KD). Specifically, ABSLD adaptively\nreduces the student's error risk gap between different classes to achieve\nfairness by adjusting the class-wise smoothness degree of samples' soft labels\nduring the training process, and the smoothness degree of soft labels is\ncontrolled by assigning different temperatures in KD to different classes.\nExtensive experiments demonstrate that ABSLD outperforms state-of-the-art AT,\nARD, and robust fairness methods in the comprehensive metric (Normalized\nStandard Deviation) of robustness and fairness.\n","authors":["Shiji Zhao","Ranjie Duan","Xizhe Wang","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2312.05508v3.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23744v1","updated":"2024-10-31T08:59:34Z","published":"2024-10-31T08:59:34Z","title":"EchoNarrator: Generating natural text explanations for ejection fraction\n  predictions","summary":"  Ejection fraction (EF) of the left ventricle (LV) is considered as one of the\nmost important measurements for diagnosing acute heart failure and can be\nestimated during cardiac ultrasound acquisition. While recent successes in deep\nlearning research successfully estimate EF values, the proposed models often\nlack an explanation for the prediction. However, providing clear and intuitive\nexplanations for clinical measurement predictions would increase the trust of\ncardiologists in these models. In this paper, we explore predicting EF\nmeasurements with Natural Language Explanation (NLE). We propose a model that\nin a single forward pass combines estimation of the LV contour over multiple\nframes, together with a set of modules and routines for computing various\nmotion and shape attributes that are associated with ejection fraction. It then\nfeeds the attributes into a large language model to generate text that helps to\nexplain the network's outcome in a human-like manner. We provide experimental\nevaluation of our explanatory output, as well as EF prediction, and show that\nour model can provide EF comparable to state-of-the-art together with\nmeaningful and accurate natural language explanation to the prediction. The\nproject page can be found at https://github.com/guybenyosef/EchoNarrator .\n","authors":["Sarina Thomas","Qing Cao","Anna Novikova","Daria Kulikova","Guy Ben-Yosef"],"pdf_url":"https://arxiv.org/pdf/2410.23744v1.pdf","comment":"accepted for MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.15223v3","updated":"2024-10-31T08:58:08Z","published":"2024-05-24T05:29:12Z","title":"iVideoGPT: Interactive VideoGPTs are Scalable World Models","summary":"  World models empower model-based agents to interactively explore, reason, and\nplan within imagined environments for real-world decision-making. However, the\nhigh demand for interactivity poses challenges in harnessing recent\nadvancements in video generative models for developing world models at scale.\nThis work introduces Interactive VideoGPT (iVideoGPT), a scalable\nautoregressive transformer framework that integrates multimodal signals--visual\nobservations, actions, and rewards--into a sequence of tokens, facilitating an\ninteractive experience of agents via next-token prediction. iVideoGPT features\na novel compressive tokenization technique that efficiently discretizes\nhigh-dimensional visual observations. Leveraging its scalable architecture, we\nare able to pre-train iVideoGPT on millions of human and robotic manipulation\ntrajectories, establishing a versatile foundation that is adaptable to serve as\ninteractive world models for a wide range of downstream tasks. These include\naction-conditioned video prediction, visual planning, and model-based\nreinforcement learning, where iVideoGPT achieves competitive performance\ncompared with state-of-the-art methods. Our work advances the development of\ninteractive general world models, bridging the gap between generative video\nmodels and practical model-based reinforcement learning applications. Code and\npre-trained models are available at https://thuml.github.io/iVideoGPT.\n","authors":["Jialong Wu","Shaofeng Yin","Ningya Feng","Xu He","Dong Li","Jianye Hao","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2405.15223v3.pdf","comment":"NeurIPS 2024. Code is available at project website:\n  https://thuml.github.io/iVideoGPT"},{"id":"http://arxiv.org/abs/2410.23742v1","updated":"2024-10-31T08:58:00Z","published":"2024-10-31T08:58:00Z","title":"Scaled Inverse Graphics: Efficiently Learning Large Sets of 3D Scenes","summary":"  While the field of inverse graphics has been witnessing continuous growth,\ntechniques devised thus far predominantly focus on learning individual scene\nrepresentations. In contrast, learning large sets of scenes has been a\nconsiderable bottleneck in NeRF developments, as repeatedly applying inverse\ngraphics on a sequence of scenes, though essential for various applications,\nremains largely prohibitive in terms of resource costs. We introduce a\nframework termed \"scaled inverse graphics\", aimed at efficiently learning large\nsets of scene representations, and propose a novel method to this end. It\noperates in two stages: (i) training a compression model on a subset of scenes,\nthen (ii) training NeRF models on the resulting smaller representations,\nthereby reducing the optimization space per new scene. In practice, we compact\nthe representation of scenes by learning NeRFs in a latent space to reduce the\nimage resolution, and sharing information across scenes to reduce NeRF\nrepresentation complexity. We experimentally show that our method presents both\nthe lowest training time and memory footprint in scaled inverse graphics\ncompared to other methods applied independently on each scene. Our codebase is\npublicly available as open-source. Our project page can be found at\nhttps://scaled-ig.github.io .\n","authors":["Karim Kassab","Antoine Schnepf","Jean-Yves Franceschi","Laurent Caraffa","Flavian Vasile","Jeremie Mary","Andrew Comport","Val√©rie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2410.23742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23738v1","updated":"2024-10-31T08:54:23Z","published":"2024-10-31T08:54:23Z","title":"MLLA-UNet: Mamba-like Linear Attention in an Efficient U-Shape Model for\n  Medical Image Segmentation","summary":"  Recent advancements in medical imaging have resulted in more complex and\ndiverse images, with challenges such as high anatomical variability, blurred\ntissue boundaries, low organ contrast, and noise. Traditional segmentation\nmethods struggle to address these challenges, making deep learning approaches,\nparticularly U-shaped architectures, increasingly prominent. However, the\nquadratic complexity of standard self-attention makes Transformers\ncomputationally prohibitive for high-resolution images. To address these\nchallenges, we propose MLLA-UNet (Mamba-Like Linear Attention UNet), a novel\narchitecture that achieves linear computational complexity while maintaining\nhigh segmentation accuracy through its innovative combination of linear\nattention and Mamba-inspired adaptive mechanisms, complemented by an efficient\nsymmetric sampling structure for enhanced feature processing. Our architecture\neffectively preserves essential spatial features while capturing long-range\ndependencies at reduced computational complexity. Additionally, we introduce a\nnovel sampling strategy for multi-scale feature fusion. Experiments demonstrate\nthat MLLA-UNet achieves state-of-the-art performance on six challenging\ndatasets with 24 different segmentation tasks, including but not limited to\nFLARE22, AMOS CT, and ACDC, with an average DSC of 88.32%. These results\nunderscore the superiority of MLLA-UNet over existing methods. Our\ncontributions include the novel 2D segmentation architecture and its empirical\nvalidation. The code is available via https://github.com/csyfjiang/MLLA-UNet.\n","authors":["Yufeng Jiang","Zongxi Li","Xiangyan Chen","Haoran Xie","Jing Cai"],"pdf_url":"https://arxiv.org/pdf/2410.23738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23736v1","updated":"2024-10-31T08:49:05Z","published":"2024-10-31T08:49:05Z","title":"MoTaDual: Modality-Task Dual Alignment for Enhanced Zero-shot Composed\n  Image Retrieval","summary":"  Composed Image Retrieval (CIR) is a challenging vision-language task,\nutilizing bi-modal (image+text) queries to retrieve target images. Despite the\nimpressive performance of supervised CIR, the dependence on costly,\nmanually-labeled triplets limits its scalability and zero-shot capability. To\naddress this issue, zero-shot composed image retrieval (ZS-CIR) is presented\nalong with projection-based approaches. However, such methods face two major\nproblems, i.e., task discrepancy between pre-training (image $\\leftrightarrow$\ntext) and inference (image+text $\\rightarrow$ image), and modality discrepancy.\nThe latter pertains to approaches based on text-only projection training due to\nthe necessity of feature extraction from the reference image during inference.\nIn this paper, we propose a two-stage framework to tackle both discrepancies.\nFirst, to ensure efficiency and scalability, a textual inversion network is\npre-trained on large-scale caption datasets. Subsequently, we put forward\nModality-Task Dual Alignment (MoTaDual) as the second stage, where\nlarge-language models (LLMs) generate triplet data for fine-tuning, and\nadditionally, prompt learning is introduced in a multi-modal context to\neffectively alleviate both modality and task discrepancies. The experimental\nresults show that our MoTaDual achieves the state-of-the-art performance across\nfour widely used ZS-CIR benchmarks, while maintaining low training time and\ncomputational cost. The code will be released soon.\n","authors":["Haiwen Li","Fei Su","Zhicheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.23736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08845v4","updated":"2024-10-31T08:38:26Z","published":"2024-06-13T06:09:22Z","title":"Rethinking Human Evaluation Protocol for Text-to-Video Models: Enhancing\n  Reliability,Reproducibility, and Practicality","summary":"  Recent text-to-video (T2V) technology advancements, as demonstrated by models\nsuch as Gen2, Pika, and Sora, have significantly broadened its applicability\nand popularity. Despite these strides, evaluating these models poses\nsubstantial challenges. Primarily, due to the limitations inherent in automatic\nmetrics, manual evaluation is often considered a superior method for assessing\nT2V generation. However, existing manual evaluation protocols face\nreproducibility, reliability, and practicality issues. To address these\nchallenges, this paper introduces the Text-to-Video Human Evaluation (T2VHE)\nprotocol, a comprehensive and standardized protocol for T2V models. The T2VHE\nprotocol includes well-defined metrics, thorough annotator training, and an\neffective dynamic evaluation module. Experimental results demonstrate that this\nprotocol not only ensures high-quality annotations but can also reduce\nevaluation costs by nearly 50\\%. We will open-source the entire setup of the\nT2VHE protocol, including the complete protocol workflow, the dynamic\nevaluation component details, and the annotation interface code. This will help\ncommunities establish more sophisticated human assessment protocols.\n","authors":["Tianle Zhang","Langtian Ma","Yuchen Yan","Yuchen Zhang","Kai Wang","Yue Yang","Ziyao Guo","Wenqi Shao","Yang You","Yu Qiao","Ping Luo","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08845v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16666v4","updated":"2024-10-31T08:37:22Z","published":"2024-04-25T15:06:58Z","title":"PhyRecon: Physically Plausible Neural Scene Reconstruction","summary":"  We address the issue of physical implausibility in multi-view neural\nreconstruction. While implicit representations have gained popularity in\nmulti-view 3D reconstruction, previous work struggles to yield physically\nplausible results, limiting their utility in domains requiring rigorous\nphysical accuracy. This lack of plausibility stems from the absence of physics\nmodeling in existing methods and their inability to recover intricate\ngeometrical structures. In this paper, we introduce PHYRECON, the first\napproach to leverage both differentiable rendering and differentiable physics\nsimulation to learn implicit surface representations. PHYRECON features a novel\ndifferentiable particle-based physical simulator built on neural implicit\nrepresentations. Central to this design is an efficient transformation between\nSDF-based implicit representations and explicit surface points via our proposed\nSurface Points Marching Cubes (SP-MC), enabling differentiable learning with\nboth rendering and physical losses. Additionally, PHYRECON models both\nrendering and physical uncertainty to identify and compensate for inconsistent\nand inaccurate monocular geometric priors. The physical uncertainty further\nfacilitates physics-guided pixel sampling to enhance the learning of slender\nstructures. By integrating these techniques, our model supports differentiable\njoint modeling of appearance, geometry, and physics. Extensive experiments\ndemonstrate that PHYRECON significantly improves the reconstruction quality.\nOur results also exhibit superior physical stability in physical simulators,\nwith at least a 40% improvement across all datasets, paving the way for future\nphysics-based applications.\n","authors":["Junfeng Ni","Yixin Chen","Bohan Jing","Nan Jiang","Bin Wang","Bo Dai","Puhao Li","Yixin Zhu","Song-Chun Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2404.16666v4.pdf","comment":"NeurIPS'24. Project page: https://phyrecon.github.io/"},{"id":"http://arxiv.org/abs/2410.23730v1","updated":"2024-10-31T08:33:30Z","published":"2024-10-31T08:33:30Z","title":"An Empirical Analysis of GPT-4V's Performance on Fashion Aesthetic\n  Evaluation","summary":"  Fashion aesthetic evaluation is the task of estimating how well the outfits\nworn by individuals in images suit them. In this work, we examine the zero-shot\nperformance of GPT-4V on this task for the first time. We show that its\npredictions align fairly well with human judgments on our datasets, and also\nfind that it struggles with ranking outfits in similar colors. The code is\navailable at https://github.com/st-tech/gpt4v-fashion-aesthetic-evaluation.\n","authors":["Yuki Hirakawa","Takashi Wada","Kazuya Morishita","Ryotaro Shimizu","Takuya Furusawa","Sai Htaung Kham","Yuki Saito"],"pdf_url":"https://arxiv.org/pdf/2410.23730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.13453v2","updated":"2024-10-31T08:29:51Z","published":"2022-03-25T05:27:28Z","title":"Model LEGO: Creating Models Like Disassembling and Assembling Building\n  Blocks","summary":"  With the rapid development of deep learning, the increasing complexity and\nscale of parameters make training a new model increasingly resource-intensive.\nIn this paper, we start from the classic convolutional neural network (CNN) and\nexplore a paradigm that does not require training to obtain new models. Similar\nto the birth of CNN inspired by receptive fields in the biological visual\nsystem, we draw inspiration from the information subsystem pathways in the\nbiological visual system and propose Model Disassembling and Assembling (MDA).\nDuring model disassembling, we introduce the concept of relative contribution\nand propose a component locating technique to extract task-aware components\nfrom trained CNN classifiers. For model assembling, we present the alignment\npadding strategy and parameter scaling strategy to construct a new model\ntailored for a specific task, utilizing the disassembled task-aware components.\nThe entire process is akin to playing with LEGO bricks, enabling arbitrary\nassembly of new models, and providing a novel perspective for model creation\nand reuse. Extensive experiments showcase that task-aware components\ndisassembled from CNN classifiers or new models assembled using these\ncomponents closely match or even surpass the performance of the baseline,\ndemonstrating its promising results for model reuse. Furthermore, MDA exhibits\ndiverse potential applications, with comprehensive experiments exploring model\ndecision route analysis, model compression, knowledge distillation, and more.\nThe code is available at https://github.com/jiaconghu/Model-LEGO.\n","authors":["Jiacong Hu","Jing Gao","Jingwen Ye","Yang Gao","Xingen Wang","Zunlei Feng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2203.13453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22733v2","updated":"2024-10-31T08:26:18Z","published":"2024-10-30T06:39:27Z","title":"ETO:Efficient Transformer-based Local Feature Matching by Organizing\n  Multiple Homography Hypotheses","summary":"  We tackle the efficiency problem of learning local feature matching. Recent\nadvancements have given rise to purely CNN-based and transformer-based\napproaches, each augmented with deep learning techniques. While CNN-based\nmethods often excel in matching speed, transformer-based methods tend to\nprovide more accurate matches. We propose an efficient transformer-based\nnetwork architecture for local feature matching. This technique is built on\nconstructing multiple homography hypotheses to approximate the continuous\ncorrespondence in the real world and uni-directional cross-attention to\naccelerate the refinement. On the YFCC100M dataset, our matching accuracy is\ncompetitive with LoFTR, a state-of-the-art transformer-based architecture,\nwhile the inference speed is boosted to 4 times, even outperforming the\nCNN-based methods. Comprehensive evaluations on other open datasets such as\nMegadepth, ScanNet, and HPatches demonstrate our method's efficacy,\nhighlighting its potential to significantly enhance a wide array of downstream\napplications.\n","authors":["Junjie Ni","Guofeng Zhang","Guanglin Li","Yijin Li","Xinyang Liu","Zhaoyang Huang","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2410.22733v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04555v2","updated":"2024-10-31T08:25:08Z","published":"2024-02-07T03:19:02Z","title":"FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language\n  Foundation Models","summary":"  Semantic mapping based on the supervised object detectors is sensitive to\nimage distribution. In real-world environments, the object detection and\nsegmentation performance can lead to a major drop, preventing the use of\nsemantic mapping in a wider domain. On the other hand, the development of\nvision-language foundation models demonstrates a strong zero-shot\ntransferability across data distribution. It provides an opportunity to\nconstruct generalizable instance-aware semantic maps. Hence, this work explores\nhow to boost instance-aware semantic mapping from object detection generated\nfrom foundation models. We propose a probabilistic label fusion method to\npredict close-set semantic classes from open-set label measurements. An\ninstance refinement module merges the over-segmented instances caused by\ninconsistent segmentation. We integrate all the modules into a unified semantic\nmapping system. Reading a sequence of RGB-D input, our work incrementally\nreconstructs an instance-aware semantic map. We evaluate the zero-shot\nperformance of our method in ScanNet and SceneNN datasets. Our method achieves\n40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation\ntask. It outperforms the traditional semantic mapping method significantly.\n","authors":["Chuhao Liu","Ke Wang","Jieqi Shi","Zhijian Qiao","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2402.04555v2.pdf","comment":"Published in IEEE RAL"},{"id":"http://arxiv.org/abs/2312.10175v3","updated":"2024-10-31T08:10:48Z","published":"2023-12-15T19:57:07Z","title":"UniAR: A Unified model for predicting human Attention and Responses on\n  visual content","summary":"  Progress in human behavior modeling involves understanding both implicit,\nearly-stage perceptual behavior, such as human attention, and explicit,\nlater-stage behavior, such as subjective preferences or likes. Yet most prior\nresearch has focused on modeling implicit and explicit human behavior in\nisolation; and often limited to a specific type of visual content. We propose\nUniAR -- a unified model of human attention and preference behavior across\ndiverse visual content. UniAR leverages a multimodal transformer to predict\nsubjective feedback, such as satisfaction or aesthetic quality, along with the\nunderlying human attention or interaction heatmaps and viewing order. We train\nUniAR on diverse public datasets spanning natural images, webpages, and graphic\ndesigns, and achieve SOTA performance on multiple benchmarks across various\nimage domains and behavior modeling tasks. Potential applications include\nproviding instant feedback on the effectiveness of UIs/visual content, and\nenabling designers and content-creation models to optimize their creation for\nhuman-centric improvements.\n","authors":["Peizhao Li","Junfeng He","Gang Li","Rachit Bhargava","Shaolei Shen","Nachiappan Valliappan","Youwei Liang","Hongxiang Gu","Venky Ramachandran","Golnaz Farhadi","Yang Li","Kai J Kohlhoff","Vidhya Navalpakkam"],"pdf_url":"https://arxiv.org/pdf/2312.10175v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23718v1","updated":"2024-10-31T08:08:54Z","published":"2024-10-31T08:08:54Z","title":"GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D\nassets. To protect the copyright of these assets, digital watermarking\ntechniques can be applied to embed ownership information discreetly within 3DGS\nmodels. However, existing watermarking methods for meshes, point clouds, and\nimplicit radiance fields cannot be directly applied to 3DGS models, as 3DGS\nmodels use explicit 3D Gaussians with distinct structures and do not rely on\nneural networks. Naively embedding the watermark on a pre-trained 3DGS can\ncause obvious distortion in rendered images. In our work, we propose an\nuncertainty-based method that constrains the perturbation of model parameters\nto achieve invisible watermarking for 3DGS. At the message decoding stage, the\ncopyright messages can be reliably extracted from both 3D Gaussians and 2D\nrendered images even under various forms of 3D and 2D distortions. We conduct\nextensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate\nthe effectiveness of our proposed method, demonstrating state-of-the-art\nperformance on both message decoding accuracy and view synthesis quality.\n","authors":["Xiufeng Huang","Ruiqi Li","Yiu-ming Cheung","Ka Chun Cheung","Simon See","Renjie Wan"],"pdf_url":"https://arxiv.org/pdf/2410.23718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22725v2","updated":"2024-10-31T08:08:07Z","published":"2024-10-30T06:17:20Z","title":"One Prompt to Verify Your Models: Black-Box Text-to-Image Models\n  Verification via Non-Transferable Adversarial Attacks","summary":"  Recently, the success of Text-to-Image (T2I) models has led to the rise of\nnumerous third-party platforms, which claim to provide cheaper API services and\nmore flexibility in model options. However, this also raises a new security\nconcern: Are these third-party services truly offering the models they claim?\nTo address this problem, we propose the first T2I model verification method\nnamed Text-to-Image Model Verification via Non-Transferable Adversarial Attacks\n(TVN). The non-transferability of adversarial examples means that these\nexamples are only effective on a target model and ineffective on other models,\nthereby allowing for the verification of the target model. TVN utilizes the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine\nsimilarity of a prompt's text encoding, generating non-transferable adversarial\nprompts. By calculating the CLIP-text scores between the non-transferable\nadversarial prompts without perturbations and the images, we can verify if the\nmodel matches the claimed target model, based on a 3-sigma threshold. The\nexperiments showed that TVN performed well in both closed-set and open-set\nscenarios, achieving a verification accuracy of over 90\\%. Moreover, the\nadversarial prompts generated by TVN significantly reduced the CLIP-text scores\nof the target model, while having little effect on other models.\n","authors":["Ji Guo","Wenbo Jiang","Rui Zhang","Guoming Lu","Hongwei Li"],"pdf_url":"https://arxiv.org/pdf/2410.22725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20568v2","updated":"2024-10-31T07:43:52Z","published":"2024-10-27T19:45:15Z","title":"Detection of adrenal anomalous findings in spinal CT images using multi\n  model graph aggregation","summary":"  Low back pain is the symptom that is the second most frequently reported to\nprimary care physicians, effecting 50 to 80 percent of the population in a\nlifetime, resulting in multiple referrals of patients suffering from back\nproblems, to CT and MRI scans, which are then examined by radiologists. The\nradiologists examining these spinal scans naturally focus on spinal pathologies\nand might miss other types of abnormalities, and in particular, abdominal ones,\nsuch as malignancies. Nevertheless, the patients whose spine was scanned might\nas well have malignant and other abdominal pathologies. Thus, clinicians have\nsuggested the need for computerized assistance and decision support in\nscreening spinal scans for additional abnormalities. In the current study, We\nhave addressed the important case of detecting suspicious lesions in the\nadrenal glands as an example for the overall methodology we have developed. A\npatient CT scan is integrated from multiple slices with an axial orientation.\nOur method determines whether a patient has an abnormal adrenal gland, and\nlocalises the abnormality if it exists. Our method is composed of three deep\nlearning models; each model has a different task for achieving the final goal.\nWe call our compound method the Multi Model Graph Aggregation MMGA method. The\nnovelty in this study is twofold. First, the use, for an important screening\ntask, of CT scans that are originally focused and tuned for imaging the spine,\nwhich were acquired from patients with potential spinal disorders, for\ndetection of a totally different set of abnormalities such as abdominal Adrenal\nglands pathologies. Second, we have built a complex pipeline architecture\ncomposed from three deep learning models that can be utilized for other organs\n(such as the pancreas or the kidney), or for similar applications, but using\nother types of imaging, such as MRI.\n","authors":["Shabalin Carmel","Shenkman Israel","Shelef Ilan","Ben-Arie Gal","Alex Geftler","Shahar Yuval"],"pdf_url":"https://arxiv.org/pdf/2410.20568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10667v2","updated":"2024-10-31T07:43:14Z","published":"2024-04-16T15:43:22Z","title":"VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time","summary":"  We introduce VASA, a framework for generating lifelike talking faces with\nappealing visual affective skills (VAS) given a single static image and a\nspeech audio clip. Our premiere model, VASA-1, is capable of not only\ngenerating lip movements that are exquisitely synchronized with the audio, but\nalso producing a large spectrum of facial nuances and natural head motions that\ncontribute to the perception of authenticity and liveliness. The core\ninnovations include a holistic facial dynamics and head movement generation\nmodel that works in a face latent space, and the development of such an\nexpressive and disentangled face latent space using videos. Through extensive\nexperiments including evaluation on a set of new metrics, we show that our\nmethod significantly outperforms previous methods along various dimensions\ncomprehensively. Our method not only delivers high video quality with realistic\nfacial and head dynamics but also supports the online generation of 512x512\nvideos at up to 40 FPS with negligible starting latency. It paves the way for\nreal-time engagements with lifelike avatars that emulate human conversational\nbehaviors.\n","authors":["Sicheng Xu","Guojun Chen","Yu-Xiao Guo","Jiaolong Yang","Chong Li","Zhenyu Zang","Yizhong Zhang","Xin Tong","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10667v2.pdf","comment":"NeurIPS 2024 (Oral) Camera ready. Project webpage:\n  https://www.microsoft.com/en-us/research/project/vasa-1/"},{"id":"http://arxiv.org/abs/2410.23698v1","updated":"2024-10-31T07:41:13Z","published":"2024-10-31T07:41:13Z","title":"Aggregate-and-Adapt Natural Language Prompts for Downstream\n  Generalization of CLIP","summary":"  Large pretrained vision-language models like CLIP have shown promising\ngeneralization capability, but may struggle in specialized domains (e.g.,\nsatellite imagery) or fine-grained classification (e.g., car models) where the\nvisual concepts are unseen or under-represented during pretraining. Prompt\nlearning offers a parameter-efficient finetuning framework that can adapt CLIP\nto downstream tasks even when limited annotation data are available. In this\npaper, we improve prompt learning by distilling the textual knowledge from\nnatural language prompts (either human- or LLM-generated) to provide rich\npriors for those under-represented concepts. We first obtain a prompt\n``summary'' aligned to each input image via a learned prompt aggregator. Then\nwe jointly train a prompt generator, optimized to produce a prompt embedding\nthat stays close to the aggregated summary while minimizing task loss at the\nsame time. We dub such prompt embedding as Aggregate-and-Adapted Prompt\nEmbedding (AAPE). AAPE is shown to be able to generalize to different\ndownstream data distributions and tasks, including vision-language\nunderstanding tasks (e.g., few-shot classification, VQA) and generation tasks\n(image captioning) where AAPE achieves competitive performance. We also show\nAAPE is particularly helpful to handle non-canonical and OOD examples.\nFurthermore, AAPE learning eliminates LLM-based inference cost as required by\nbaselines, and scales better with data and LLM model size.\n","authors":["Chen Huang","Skyler Seto","Samira Abnar","David Grangier","Navdeep Jaitly","Josh Susskind"],"pdf_url":"https://arxiv.org/pdf/2410.23698v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.09774v2","updated":"2024-10-31T07:40:40Z","published":"2024-09-15T15:46:03Z","title":"Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization","summary":"  Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.\n","authors":["Haoyuan Sun","Bo Xia","Yongzhe Chang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09774v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2410.23690v1","updated":"2024-10-31T07:25:39Z","published":"2024-10-31T07:25:39Z","title":"XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM","summary":"  In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a\nmodular code design and a multi-process running mechanism, providing highly\nreusable foundational modules such as unified dataset management, 3d\nvisualization, algorithm configuration, and metrics evaluation. It can help\ndevelopers quickly build a complete SLAM system, flexibly combine different\nalgorithm modules, and conduct standardized benchmarking for accuracy and\nefficiency comparison. Within this framework, we integrate several\nstate-of-the-art SLAM algorithms with different types, including NeRF and 3DGS\nbased SLAM, and even odometry or reconstruction algorithms, which demonstrates\nthe flexibility and extensibility. We also conduct a comprehensive comparison\nand evaluation of these integrated algorithms, analyzing the characteristics of\neach. Finally, we contribute all the code, configuration and data to the\nopen-source community, which aims to promote the widespread research and\ndevelopment of SLAM technology within the open-source ecosystem.\n","authors":["Xiaomeng Wang","Nan Wang","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21991v2","updated":"2024-10-31T07:24:06Z","published":"2024-10-29T12:22:07Z","title":"From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System","summary":"  Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual-branch architecture, RuleVM\nachieves interpretable coarse-grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises.\n","authors":["Wen-Dong Jiang","Chih-Yung Chang","Hsiang-Chuan Chang","Diptendu Sinha Roy"],"pdf_url":"https://arxiv.org/pdf/2410.21991v2.pdf","comment":"12 pages,7 figures"},{"id":"http://arxiv.org/abs/2410.23687v1","updated":"2024-10-31T07:22:51Z","published":"2024-10-31T07:22:51Z","title":"Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey","summary":"  Adversarial attacks, which manipulate input data to undermine model\navailability and integrity, pose significant security threats during machine\nlearning inference. With the advent of Large Vision-Language Models (LVLMs),\nnew attack vectors, such as cognitive bias, prompt injection, and jailbreak\ntechniques, have emerged. Understanding these attacks is crucial for developing\nmore robust systems and demystifying the inner workings of neural networks.\nHowever, existing reviews often focus on attack classifications and lack\ncomprehensive, in-depth analysis. The research community currently needs: 1)\nunified insights into adversariality, transferability, and generalization; 2)\ndetailed evaluations of existing methods; 3) motivation-driven attack\ncategorizations; and 4) an integrated perspective on both traditional and LVLM\nattacks. This article addresses these gaps by offering a thorough summary of\ntraditional and LVLM adversarial attacks, emphasizing their connections and\ndistinctions, and providing actionable insights for future research.\n","authors":["Chiyu Zhang","Xiaogang Xu","Jiafei Wu","Zhe Liu","Lu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21898v2","updated":"2024-10-31T07:13:08Z","published":"2024-10-29T09:42:54Z","title":"A Longitudinal Analysis of Racial and Gender Bias in New York Times and\n  Fox News Images and Articles","summary":"  The manner in which different racial and gender groups are portrayed in news\ncoverage plays a large role in shaping public opinion. As such, understanding\nhow such groups are portrayed in news media is of notable societal value, and\nhas thus been a significant endeavour in both the computer and social sciences.\nYet, the literature still lacks a longitudinal study examining both the\nfrequency of appearance of different racial and gender groups in online news\narticles, as well as the context in which such groups are discussed. To fill\nthis gap, we propose two machine learning classifiers to detect the race and\nage of a given subject. Next, we compile a dataset of 123,337 images and\n441,321 online news articles from New York Times (NYT) and Fox News (Fox), and\nexamine representation through two computational approaches. Firstly, we\nexamine the frequency and prominence of appearance of racial and gender groups\nin images embedded in news articles, revealing that racial and gender\nminorities are largely under-represented, and when they do appear, they are\nfeatured less prominently compared to majority groups. Furthermore, we find\nthat NYT largely features more images of racial minority groups compared to\nFox. Secondly, we examine both the frequency and context with which racial\nminority groups are presented in article text. This reveals the narrow scope in\nwhich certain racial groups are covered and the frequency with which different\ngroups are presented as victims and/or perpetrators in a given conflict. Taken\ntogether, our analysis contributes to the literature by providing two novel\nopen-source classifiers to detect race and age from images, and shedding light\non the racial and gender biases in news articles from venues on opposite ends\nof the American political spectrum.\n","authors":["Hazem Ibrahim","Nouar AlDahoul","Syed Mustafa Ali Abbasi","Fareed Zaffar","Talal Rahwan","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2410.21898v2.pdf","comment":"13 pages, and 11 figures"},{"id":"http://arxiv.org/abs/2410.12419v2","updated":"2024-10-31T07:11:29Z","published":"2024-10-16T10:04:22Z","title":"Mind the Context: Attention-Guided Weak-to-Strong Consistency for\n  Enhanced Semi-Supervised Medical Image Segmentation","summary":"  Medical image segmentation is a pivotal step in diagnostic and therapeutic\nprocesses, relying on high-quality annotated data that is often challenging and\ncostly to obtain. Semi-supervised learning offers a promising approach to\nenhance model performance by leveraging unlabeled data. Although weak-to-strong\nconsistency is a prevalent method in semi-supervised image segmentation, there\nis a scarcity of research on perturbation strategies specifically tailored for\nsemi-supervised medical image segmentation tasks. To address this challenge,\nthis paper introduces a simple yet efficient semi-supervised learning framework\nnamed Attention-Guided weak-to-strong Consistency Match (AIGCMatch). The\nAIGCMatch framework incorporates attention-guided perturbation strategies at\nboth the image and feature levels to achieve weak-to-strong consistency\nregularization. This method not only preserves the structural information of\nmedical images but also enhances the model's ability to process complex\nsemantic information. Extensive experiments conducted on the ACDC and ISIC-2017\ndatasets have validated the effectiveness of AIGCMatch. Our method achieved a\n90.4\\% Dice score in the 7-case scenario on the ACDC dataset, surpassing the\nstate-of-the-art methods and demonstrating its potential and efficacy in\nclinical settings. Additionally, on the ISIC-2017 dataset, we significantly\noutperformed our baseline, indicating the robustness and generalizability of\nAIGCMatch across different medical image segmentation tasks.\n","authors":["Yuxuan Cheng","Chenxi Shao","Jie Ma","Guoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22817v2","updated":"2024-10-31T07:07:27Z","published":"2024-10-30T08:51:29Z","title":"Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View\n  Synthesis","summary":"  Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from\nsparse-view observations in a feed-forward inference manner, eliminating the\nneed for scene-specific retraining required in conventional 3DGS. However,\nexisting methods rely heavily on epipolar priors, which can be unreliable in\ncomplex realworld scenes, particularly in non-overlapping and occluded regions.\nIn this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based\nmodel for generalizable novel view synthesis that operates independently of\nepipolar line constraints. To enhance multiview feature extraction with 3D\nperception, we employ a selfsupervised Vision Transformer (ViT) with cross-view\ncompletion pre-training on large-scale datasets. Additionally, we introduce an\nIterative Cross-view Gaussians Alignment method to ensure consistent depth\nscales across different views. Our eFreeSplat represents an innovative approach\nfor generalizable novel view synthesis. Different from the existing pure\ngeometry-free methods, eFreeSplat focuses more on achieving epipolar-free\nfeature matching and encoding by providing 3D priors through cross-view\npretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks\nusing the RealEstate10K and ACID datasets. Extensive experiments demonstrate\nthat eFreeSplat surpasses state-of-the-art baselines that rely on epipolar\npriors, achieving superior geometry reconstruction and novel view synthesis\nquality. Project page: https://tatakai1.github.io/efreesplat/.\n","authors":["Zhiyuan Min","Yawei Luo","Jianwen Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.22817v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23677v1","updated":"2024-10-31T06:55:57Z","published":"2024-10-31T06:55:57Z","title":"Wide Two-Layer Networks can Learn from Adversarial Perturbations","summary":"  Adversarial examples have raised several open questions, such as why they can\ndeceive classifiers and transfer between different models. A prevailing\nhypothesis to explain these phenomena suggests that adversarial perturbations\nappear as random noise but contain class-specific features. This hypothesis is\nsupported by the success of perturbation learning, where classifiers trained\nsolely on adversarial examples and the corresponding incorrect labels\ngeneralize well to correctly labeled test data. Although this hypothesis and\nperturbation learning are effective in explaining intriguing properties of\nadversarial examples, their solid theoretical foundation is limited. In this\nstudy, we theoretically explain the counterintuitive success of perturbation\nlearning. We assume wide two-layer networks and the results hold for any data\ndistribution. We prove that adversarial perturbations contain sufficient\nclass-specific features for networks to generalize from them. Moreover, the\npredictions of classifiers trained on mislabeled adversarial examples coincide\nwith those of classifiers trained on correctly labeled clean samples. The code\nis available at https://github.com/s-kumano/perturbation-learning.\n","authors":["Soichiro Kumano","Hiroshi Kera","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2410.23677v1.pdf","comment":"NeurIPS24"},{"id":"http://arxiv.org/abs/2410.23676v1","updated":"2024-10-31T06:55:24Z","published":"2024-10-31T06:55:24Z","title":"Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach","summary":"  Web-scale visual entity recognition, the task of associating images with\ntheir corresponding entities within vast knowledge bases like Wikipedia,\npresents significant challenges due to the lack of clean, large-scale training\ndata. In this paper, we propose a novel methodology to curate such a dataset,\nleveraging a multimodal large language model (LLM) for label verification,\nmetadata generation, and rationale explanation. Instead of relying on the\nmultimodal LLM to directly annotate data, which we found to be suboptimal, we\nprompt it to reason about potential candidate entity labels by accessing\nadditional contextually relevant information (such as Wikipedia), resulting in\nmore accurate annotations. We further use the multimodal LLM to enrich the\ndataset by generating question-answer pairs and a grounded finegrained textual\ndescription (referred to as \"rationale\") that explains the connection between\nimages and their assigned entities. Experiments demonstrate that models trained\non this automatically curated data achieve state-of-the-art performance on\nweb-scale visual entity recognition tasks (e.g. +6.9% improvement in OVEN\nentity task), underscoring the importance of high-quality training data in this\ndomain.\n","authors":["Mathilde Caron","Alireza Fathi","Cordelia Schmid","Ahmet Iscen"],"pdf_url":"https://arxiv.org/pdf/2410.23676v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.12138v3","updated":"2024-10-31T06:38:27Z","published":"2024-02-19T13:38:15Z","title":"Perceiving Longer Sequences With Bi-Directional Cross-Attention\n  Transformers","summary":"  We present a novel bi-directional Transformer architecture (BiXT) which\nscales linearly with input size in terms of computational cost and memory\nconsumption, but does not suffer the drop in performance or limitation to only\none input modality seen with other efficient Transformer-based approaches. BiXT\nis inspired by the Perceiver architectures but replaces iterative attention\nwith an efficient bi-directional cross-attention module in which input tokens\nand latent variables attend to each other simultaneously, leveraging a\nnaturally emerging attention-symmetry between the two. This approach unlocks a\nkey bottleneck experienced by Perceiver-like architectures and enables the\nprocessing and interpretation of both semantics ('what') and location ('where')\nto develop alongside each other over multiple layers -- allowing its direct\napplication to dense and instance-based tasks alike. By combining efficiency\nwith the generality and performance of a full Transformer architecture, BiXT\ncan process longer sequences like point clouds, text or images at higher\nfeature resolutions and achieves competitive performance across a range of\ntasks like point cloud part segmentation, semantic image segmentation, image\nclassification, hierarchical sequence modeling and document retrieval. Our\nexperiments demonstrate that BiXT models outperform larger competitors by\nleveraging longer sequences more efficiently on vision tasks like\nclassification and segmentation, and perform on par with full Transformer\nvariants on sequence modeling and document retrieval -- but require $28\\%$\nfewer FLOPs and are up to $8.4\\times$ faster.\n","authors":["Markus Hiller","Krista A. Ehinger","Tom Drummond"],"pdf_url":"https://arxiv.org/pdf/2402.12138v3.pdf","comment":"Accepted at NeurIPS 2024; Code and models will be available at\n  https://github.com/mrkshllr/BiXT"},{"id":"http://arxiv.org/abs/2410.23663v1","updated":"2024-10-31T06:26:00Z","published":"2024-10-31T06:26:00Z","title":"DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake\n  Detection","summary":"  With the advancement of deepfake generation techniques, the importance of\ndeepfake detection in protecting multimedia content integrity has become\nincreasingly obvious. Recently, temporal inconsistency clues have been explored\nto improve the generalizability of deepfake video detection. According to our\nobservation, the temporal artifacts of forged videos in terms of motion\ninformation usually exhibits quite distinct inconsistency patterns along\nhorizontal and vertical directions, which could be leveraged to improve the\ngeneralizability of detectors. In this paper, a transformer-based framework for\nDiffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits\ndirectional inconsistencies for deepfake video detection. Specifically, DIP\nbegins with a spatiotemporal encoder to represent spatiotemporal information. A\ndirectional inconsistency decoder is adopted accordingly, where direction-aware\nattention and inconsistency diffusion are incorporated to explore potential\ninconsistency patterns and jointly learn the inherent relationships. In\naddition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to\ncontrast spatiotemporally augmented sample pairs and prevent the model from\noverfitting nonessential forgery artifacts. Extensive experiments on several\npublic datasets demonstrate that our method could effectively identify\ndirectional forgery clues and achieve state-of-the-art performance.\n","authors":["Fan Nie","Jiangqun Ni","Jian Zhang","Bin Zhang","Weizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23663v1.pdf","comment":"13 pages, accepted with IEEE Trans. on Multimedia"},{"id":"http://arxiv.org/abs/2410.23658v1","updated":"2024-10-31T06:17:16Z","published":"2024-10-31T06:17:16Z","title":"GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring","summary":"  To train a deblurring network, an appropriate dataset with paired blurry and\nsharp images is essential. Existing datasets collect blurry images either\nsynthetically by aggregating consecutive sharp frames or using sophisticated\ncamera systems to capture real blur. However, these methods offer limited\ndiversity in blur types (blur trajectories) or require extensive human effort\nto reconstruct large-scale datasets, failing to fully reflect real-world blur\nscenarios. To address this, we propose GS-Blur, a dataset of synthesized\nrealistic blurry images created using a novel approach. To this end, we first\nreconstruct 3D scenes from multi-view images using 3D Gaussian Splatting\n(3DGS), then render blurry images by moving the camera view along the randomly\ngenerated motion trajectories. By adopting various camera trajectories in\nreconstructing our GS-Blur, our dataset contains realistic and diverse types of\nblur, offering a large-scale dataset that generalizes well to real-world blur.\nUsing GS-Blur with various deblurring methods, we demonstrate its ability to\ngeneralize effectively compared to previous synthetic or real blur datasets,\nshowing significant improvements in deblurring performance.\n","authors":["Dongwoo Lee","Joonkyu Park","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2410.23658v1.pdf","comment":"Accepted at NeurIPS 2024 Datasets & Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.11187v2","updated":"2024-10-31T06:09:11Z","published":"2024-10-15T02:04:05Z","title":"Multiview Scene Graph","summary":"  A proper scene representation is central to the pursuit of spatial\nintelligence where agents can robustly reconstruct and efficiently understand\n3D scenes. A scene representation is either metric, such as landmark maps in 3D\nreconstruction, 3D bounding boxes in object detection, or voxel grids in\noccupancy prediction, or topological, such as pose graphs with loop closures in\nSLAM or visibility graphs in SfM. In this work, we propose to build Multiview\nScene Graphs (MSG) from unposed images, representing a scene topologically with\ninterconnected place and object nodes. The task of building MSG is challenging\nfor existing representation learning methods since it needs to jointly address\nboth visual place recognition, object detection, and object association from\nimages with limited fields of view and potentially large viewpoint changes. To\nevaluate any method tackling this task, we developed an MSG dataset and\nannotation based on a public 3D dataset. We also propose an evaluation metric\nbased on the intersection-over-union score of MSG edges. Moreover, we develop a\nnovel baseline method built on mainstream pretrained vision models, combining\nvisual place recognition and object association into one Transformer decoder\narchitecture. Experiments demonstrate that our method has superior performance\ncompared to existing relevant baselines.\n","authors":["Juexiao Zhang","Gao Zhu","Sihang Li","Xinhao Liu","Haorui Song","Xinran Tang","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2410.11187v2.pdf","comment":"To be published in NeurIPS 2024. Website at\n  https://ai4ce.github.io/MSG/"},{"id":"http://arxiv.org/abs/2405.14325v3","updated":"2024-10-31T05:47:33Z","published":"2024-05-23T08:55:20Z","title":"Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised\n  Anomaly Detection","summary":"  Recent studies highlighted a practical setting of unsupervised anomaly\ndetection (UAD) that builds a unified model for multi-class images, serving as\nan alternative to the conventional one-class-one-model setup. Despite various\nadvancements addressing this challenging task, the detection performance under\nthe multi-class setting still lags far behind state-of-the-art class-separated\nmodels. Our research aims to bridge this substantial performance gap. In this\npaper, we introduce a minimalistic reconstruction-based anomaly detection\nframework, namely Dinomaly, which leverages pure Transformer architectures\nwithout relying on complex designs, additional modules, or specialized tricks.\nGiven this powerful framework consisted of only Attentions and MLPs, we found\nfour simple components that are essential to multi-class anomaly detection: (1)\nFoundation Transformers that extracts universal and discriminative features,\n(2) Noisy Bottleneck where pre-existing Dropouts do all the noise injection\ntricks, (3) Linear Attention that naturally cannot focus, and (4) Loose\nReconstruction that does not force layer-to-layer and point-by-point\nreconstruction. Extensive experiments are conducted across popular anomaly\ndetection benchmarks including MVTec-AD, VisA, and Real-IAD. Our proposed\nDinomaly achieves impressive image-level AUROC of 99.6%, 98.7%, and 89.3% on\nthe three datasets respectively, which is not only superior to state-of-the-art\nmulti-class UAD methods, but also achieves the most advanced class-separated\nUAD records.\n","authors":["Jia Guo","Shuai Lu","Weihang Zhang","Fang Chen","Hongen Liao","Huiqi Li"],"pdf_url":"https://arxiv.org/pdf/2405.14325v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14596v3","updated":"2024-10-31T05:38:39Z","published":"2024-06-20T17:45:02Z","title":"VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs","summary":"  Large-scale generative language and vision-language models excel in\nin-context learning for decision making. However, they require high-quality\nexemplar demonstrations to be included in their context window. In this work,\nwe ask: Can LLMs and VLMs generate their own examples from generic, sub-optimal\ndemonstrations? We propose In-Context Abstraction Learning (ICAL), a method\nthat builds a memory of multimodal experience from sub-optimal demonstrations\nand human feedback. Given a task demonstration that may contain inefficiencies\nor mistakes, a VLM abstracts the trajectory into a generalized program by\ncorrecting inefficient actions and annotating cognitive abstractions: causal\nrelationships, object state changes, temporal subgoals, and task-relevant\nvisual elements. These abstractions are iteratively improved through human\nfeedback while the agent attempts to execute the trajectory. The resulting\nexamples, when used as exemplars in the prompt, significantly improve\ndecision-making in retrieval-augmented LLM and VLM agents. Moreover, as the\nagent's library of examples grows, it becomes more efficient, relying less on\nhuman feedback and requiring fewer environment interactions per demonstration.\nOur ICAL agent surpasses the state-of-the-art in dialogue-based instruction\nfollowing in TEACh, multimodal web agents in VisualWebArena, and action\nanticipation in Ego4D. In TEACh, we achieve a 12.6% improvement in\ngoal-condition success. In VisualWebArena, our task success rate improves over\nthe SOTA from 14.3% to 22.7% using GPT4V. In Ego4D action forecasting, we\nimprove over few-shot GPT-4V and remain competitive with supervised models. We\nshow finetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such abstractions.\n","authors":["Gabriel Sarch","Lawrence Jang","Michael J. Tarr","William W. Cohen","Kenneth Marino","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2406.14596v3.pdf","comment":"Project website: http://ical-learning.github.io/"},{"id":"http://arxiv.org/abs/2410.23642v1","updated":"2024-10-31T05:29:18Z","published":"2024-10-31T05:29:18Z","title":"Novel Clinical-Grade Prostate Cancer Detection and Grading Model:\n  Development and Prospective Validation Using Real World Data, with\n  Performance Assessment on IHC Requested Cases","summary":"  Artificial intelligence may assist healthcare systems in meeting increasing\ndemand for pathology services while maintaining diagnostic quality and reducing\nturnaround time and costs. We aimed to investigate the performance of an\ninstitutionally developed system for prostate cancer detection, grading, and\nworkflow optimization and to contrast this with commercial alternatives. From\nAugust 2021 to March 2023, we scanned 21,396 slides from 1,147 patients with\npositive biopsies. We developed models for cancer detection, grading, and\nscreening of equivocal cases for IHC ordering. We compared a task-specific\nmodel trained using the PANDA dataset of prostate cancer biopsies with one\nbuilt using features extracted by the general-purpose histology foundation\nmodel, UNI and compare their performance in an unfiltered prospectively\ncollected dataset that reflects our patient population (1737 slides,95\npatients). We evaluated the contributions of a bespoke model designed to\nimprove sensitivity in detecting small cancer foci and scoring of broader\npatterns observed at lower resolution. We found high concordance between the\ndeveloped systems and pathologist reference in detection (AUC 98.5, sensitivity\n95.0, and specificity 97.8), ISUP grading (quadratic Cohen's kappa 0.869),\ngrade group 3 or higher (AUC 97.5, sensitivity 94.9, specificity 96.6) and\ncomparable to published data from commercial systems. Screening could reduce\nIHC ordering for equivocal cases by 44.5% with an overall error rate of 1.8%\n(1.4% false positive, 0.4% false negative rates). Institutions like academic\nmedical centers that have high scanning volumes and report abstraction\ncapabilities can develop accurate computational pathology models for internal\nuse. These models have the potential to aid in quality control role and to\nimprove workflow in the pathology lab to help meet future challenges in\nprostate cancer diagnosis.\n","authors":["Ramin Nateghi","Ruoji Zhou","Madeline Saft","Marina Schnauss","Clayton Neill","Ridwan Alam","Nicole Handa","Mitchell Huang","Eric V Li","Jeffery A Goldstein","Edward M Schaeffer","Menatalla Nadim","Fattaneh Pourakpour","Bogdan Isaila","Christopher Felicelli","Vikas Mehta","Behtash G Nezami","Ashley Ross","Ximing Yang","Lee AD Cooper"],"pdf_url":"https://arxiv.org/pdf/2410.23642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23641v1","updated":"2024-10-31T05:27:58Z","published":"2024-10-31T05:27:58Z","title":"Recovering Complete Actions for Cross-dataset Skeleton Action\n  Recognition","summary":"  Despite huge progress in skeleton-based action recognition, its\ngeneralizability to different domains remains a challenging issue. In this\npaper, to solve the skeleton action generalization problem, we present a\nrecover-and-resample augmentation framework based on a novel complete action\nprior. We observe that human daily actions are confronted with temporal\nmismatch across different datasets, as they are usually partial observations of\ntheir complete action sequences. By recovering complete actions and resampling\nfrom these full sequences, we can generate strong augmentations for unseen\ndomains. At the same time, we discover the nature of general action\ncompleteness within large datasets, indicated by the per-frame diversity over\ntime. This allows us to exploit two assets of transferable knowledge that can\nbe shared across action samples and be helpful for action completion: boundary\nposes for determining the action start, and linear temporal transforms for\ncapturing global action patterns. Therefore, we formulate the recovering stage\nas a two-step stochastic action completion with boundary pose-conditioned\nextrapolation followed by smooth linear transforms. Both the boundary poses and\nlinear transforms can be efficiently learned from the whole dataset via\nclustering. We validate our approach on a cross-dataset setting with three\nskeleton action datasets, outperforming other domain generalization approaches\nby a considerable margin.\n","authors":["Hanchao Liu","Yujiang Li","Tai-Jiang Mu","Shi-Min Hu"],"pdf_url":"https://arxiv.org/pdf/2410.23641v1.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.19137v3","updated":"2024-10-31T05:22:26Z","published":"2024-03-28T04:15:58Z","title":"CLAP4CLIP: Continual Learning with Probabilistic Finetuning for\n  Vision-Language Models","summary":"  Continual learning (CL) aims to help deep neural networks learn new knowledge\nwhile retaining what has been learned. Owing to their powerful\ngeneralizability, pre-trained vision-language models such as Contrastive\nLanguage-Image Pre-training (CLIP) have lately gained traction as practical CL\ncandidates. However, the domain mismatch between the pre-training and the\ndownstream CL tasks often calls for finetuning of the CLIP on the latter. Most\nexisting finetuning methods exhibit deterministic nature. This makes them\noverlook the many possible interactions across the input modalities and deems\nthem unsafe for high-risk tasks requiring reliable uncertainty estimation. To\naddress these, our work proposes Continual LeArning with Probabilistic\nfinetuning (CLAP) - a probabilistic modeling framework over visual-guided text\nfeatures per task, thus providing more calibrated CL finetuning. Unlike recent\ndata-hungry anti-forgetting CL techniques, CLAP alleviates forgetting by\nexploiting the rich pre-trained knowledge of CLIP for weight initialization and\ndistribution regularization of task-specific parameters. Cooperating with the\ndiverse range of existing prompting methods, CLAP can surpass the predominant\ndeterministic finetuning approaches for CL with CLIP. We conclude with\nout-of-the-box applications of superior uncertainty estimation abilities of\nCLAP including novel data detection and exemplar selection within the existing\nCL setups. Our code is available at\n\\url{https://github.com/srvCodes/clap4clip}.\n","authors":["Saurav Jha","Dong Gong","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.19137v3.pdf","comment":"Accepted as a poster at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.14430v3","updated":"2024-10-31T05:14:31Z","published":"2024-05-23T11:00:07Z","title":"PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers\n  Inference","summary":"  This paper presents PipeFusion, an innovative parallel methodology to tackle\nthe high latency issues associated with generating high-resolution images using\ndiffusion transformers (DiTs) models. PipeFusion partitions images into patches\nand the model layers across multiple GPUs. It employs a patch-level pipeline\nparallel strategy to orchestrate communication and computation efficiently. By\ncapitalizing on the high similarity between inputs from successive diffusion\nsteps, PipeFusion reuses one-step stale feature maps to provide context for the\ncurrent pipeline step. This approach notably reduces communication costs\ncompared to existing DiTs inference parallelism, including tensor parallel,\nsequence parallel and DistriFusion. PipeFusion also exhibits superior memory\nefficiency, because it can distribute model parameters across multiple devices,\nmaking it more suitable for DiTs with large parameter sizes, such as Flux.1.\nExperimental results demonstrate that PipeFusion achieves state-of-the-art\nperformance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1\nmodels.Our Source code is available at https://github.com/xdit-project/xDiT.\n","authors":["Jiarui Fang","Jinzhe Pan","Jiannan Wang","Aoyu Li","Xibo Sun"],"pdf_url":"https://arxiv.org/pdf/2405.14430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23629v1","updated":"2024-10-31T04:42:43Z","published":"2024-10-31T04:42:43Z","title":"Posture-Informed Muscular Force Learning for Robust Hand Pressure\n  Estimation","summary":"  We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface\nelectromyography (sEMG) signals. Our approach utilizes detailed spatial\ninformation from 3D hand poses in conjunction with dynamic muscle activity from\nsEMG to enable accurate and robust whole-hand pressure measurements under\ndiverse hand-object interactions. We also developed a multimodal data\ncollection system that combines a pressure glove, an sEMG armband, and a\nmarkerless finger-tracking module. We created a comprehensive dataset from 21\nparticipants, capturing synchronized data of hand posture, sEMG signals, and\nexerted hand pressure across various hand postures and hand-object interaction\nscenarios using our collection system. Our framework enables precise hand\npressure estimation in complex and natural interaction scenarios. Our approach\nsubstantially mitigates the limitations of traditional sEMG-based or\nvision-based methods by integrating 3D hand posture information with sEMG\nsignals. Video demos, data, and code are available online.\n","authors":["Kyungjin Seo","Junghoon Seo","Hanseok Jeong","Sangpil Kim","Sang Ho Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.23629v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23628v1","updated":"2024-10-31T04:34:28Z","published":"2024-10-31T04:34:28Z","title":"Cycle-Constrained Adversarial Denoising Convolutional Network for PET\n  Image Denoising: Multi-Dimensional Validation on Large Datasets with Reader\n  Study and Real Low-Dose Data","summary":"  Positron emission tomography (PET) is a critical tool for diagnosing tumors\nand neurological disorders but poses radiation risks to patients, particularly\nto sensitive populations. While reducing injected radiation dose mitigates this\nrisk, it often compromises image quality. To reconstruct full-dose-quality\nimages from low-dose scans, we propose a Cycle-constrained Adversarial\nDenoising Convolutional Network (Cycle-DCN). This model integrates a noise\npredictor, two discriminators, and a consistency network, and is optimized\nusing a combination of supervised loss, adversarial loss, cycle consistency\nloss, identity loss, and neighboring Structural Similarity Index (SSIM) loss.\nExperiments were conducted on a large dataset consisting of raw PET brain data\nfrom 1,224 patients, acquired using a Siemens Biograph Vision PET/CT scanner.\nEach patient underwent a 120-seconds brain scan. To simulate low-dose PET\nconditions, images were reconstructed from shortened scan durations of 30, 12,\nand 5 seconds, corresponding to 1/4, 1/10, and 1/24 of the full-dose\nacquisition, respectively, using a custom-developed GPU-based image\nreconstruction software. The results show that Cycle-DCN significantly improves\naverage Peak Signal-to-Noise Ratio (PSNR), SSIM, and Normalized Root Mean\nSquare Error (NRMSE) across three dose levels, with improvements of up to 56%,\n35%, and 71%, respectively. Additionally, it achieves contrast-to-noise ratio\n(CNR) and Edge Preservation Index (EPI) values that closely align with\nfull-dose images, effectively preserving image details, tumor shape, and\ncontrast, while resolving issues with blurred edges. The results of reader\nstudies indicated that the images restored by Cycle-DCN consistently received\nthe highest ratings from nuclear medicine physicians, highlighting their strong\nclinical relevance.\n","authors":["Yucun Hou","Fenglin Zhan","Xin Cheng","Chenxi Li","Ziquan Yuan","Runze Liao","Haihao Wang","Jianlang Hua","Jing Wu","Jianyong Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.23628v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.23623v1","updated":"2024-10-31T04:20:47Z","published":"2024-10-31T04:20:47Z","title":"On Learning Multi-Modal Forgery Representation for Diffusion Generated\n  Video Detection","summary":"  Large numbers of synthesized videos from diffusion models pose threats to\ninformation security and authenticity, leading to an increasing demand for\ngenerated content detection. However, existing video-level detection algorithms\nprimarily focus on detecting facial forgeries and often fail to identify\ndiffusion-generated content with a diverse range of semantics. To advance the\nfield of video forensics, we propose an innovative algorithm named Multi-Modal\nDetection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the\nprofound perceptual and comprehensive abilities of Large Multi-modal Models\n(LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's\nmulti-modal space, enhancing its ability to detect unseen forgery content.\nBesides, MM-Det leverages an In-and-Across Frame Attention (IAFA) mechanism for\nfeature augmentation in the spatio-temporal domain. A dynamic fusion strategy\nhelps refine forgery representations for the fusion. Moreover, we construct a\ncomprehensive diffusion video dataset, called Diffusion Video Forensics (DVF),\nacross a wide range of forgery videos. MM-Det achieves state-of-the-art\nperformance in DVF, demonstrating the effectiveness of our algorithm. Both\nsource code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.\n","authors":["Xiufeng Song","Xiao Guo","Jiache Zhang","Qirui Li","Lei Bai","Xiaoming Liu","Guangtao Zhai","Xiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23623v1.pdf","comment":"10 pages, 9 figures"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.23796v1","updated":"2024-10-31T10:27:48Z","published":"2024-10-31T10:27:48Z","title":"Improving snore detection under limited dataset through\n  harmonic/percussive source separation and convolutional neural networks","summary":"  Snoring, an acoustic biomarker commonly observed in individuals with\nObstructive Sleep Apnoea Syndrome (OSAS), holds significant potential for\ndiagnosing and monitoring this recognized clinical disorder. Irrespective of\nsnoring types, most snoring instances exhibit identifiable harmonic patterns\nmanifested through distinctive energy distributions over time. In this work, we\npropose a novel method to differentiate monaural snoring from non-snoring\nsounds by analyzing the harmonic content of the input sound using\nharmonic/percussive sound source separation (HPSS). The resulting feature,\nbased on the harmonic spectrogram from HPSS, is employed as input data for\nconventional neural network architectures, aiming to enhance snoring detection\nperformance even under a limited data learning framework. To evaluate the\nperformance of our proposal, we studied two different scenarios: 1) using a\nlarge dataset of snoring and interfering sounds, and 2) using a reduced\ntraining set composed of around 1% of the data material. In the former\nscenario, the proposed HPSS-based feature provides competitive results compared\nto other input features from the literature. However, the key advantage of the\nproposed method lies in the superior performance of the harmonic spectrogram\nderived from HPSS in a limited data learning context. In this particular\nscenario, using the proposed harmonic feature significantly enhances the\nperformance of all the studied architectures in comparison to the classical\ninput features documented in the existing literature. This finding clearly\ndemonstrates that incorporating harmonic content enables more reliable learning\nof the essential time-frequency characteristics that are prevalent in most\nsnoring sounds, even in scenarios where the amount of training data is limited.\n","authors":["F. D. Gonzalez-Martinez","J. J. Carabias-Orti","F. J. Canadas-Quesada","N. Ruiz-Reyes","D. Martinez-Munoz","S. Garcia-Galan"],"pdf_url":"https://arxiv.org/pdf/2410.23796v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.24223v1","updated":"2024-10-31T17:59:56Z","published":"2024-10-31T17:59:56Z","title":"URAvatar: Universal Relightable Gaussian Codec Avatars","summary":"  We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.\n","authors":["Junxuan Li","Chen Cao","Gabriel Schwartz","Rawal Khirodkar","Christian Richardt","Tomas Simon","Yaser Sheikh","Shunsuke Saito"],"pdf_url":"https://arxiv.org/pdf/2410.24223v1.pdf","comment":"SIGGRAPH Asia 2024. Website:\n  https://junxuan-li.github.io/urgca-website/"},{"id":"http://arxiv.org/abs/2410.24203v1","updated":"2024-10-31T17:57:02Z","published":"2024-10-31T17:57:02Z","title":"DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion","summary":"  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n","authors":["Weicai Ye","Chenhao Ji","Zheng Chen","Junyao Gao","Xiaoshui Huang","Song-Hai Zhang","Wanli Ouyang","Tong He","Cairong Zhao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24203v1.pdf","comment":"NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:\n  https://github.com/zju3dv/DiffPano"},{"id":"http://arxiv.org/abs/2409.06711v2","updated":"2024-10-31T17:48:06Z","published":"2024-08-25T13:14:59Z","title":"Quantized neural network for complex hologram generation","summary":"  Computer-generated holography (CGH) is a promising technology for augmented\nreality displays, such as head-mounted or head-up displays. However, its high\ncomputational demand makes it impractical for implementation. Recent efforts to\nintegrate neural networks into CGH have successfully accelerated computing\nspeed, demonstrating the potential to overcome the trade-off between\ncomputational cost and image quality. Nevertheless, deploying neural\nnetwork-based CGH algorithms on computationally limited embedded systems\nrequires more efficient models with lower computational cost, memory footprint,\nand power consumption. In this study, we developed a lightweight model for\ncomplex hologram generation by introducing neural network quantization.\nSpecifically, we built a model based on tensor holography and quantized it from\n32-bit floating-point precision (FP32) to 8-bit integer precision (INT8). Our\nperformance evaluation shows that the proposed INT8 model achieves hologram\nquality comparable to that of the FP32 model while reducing the model size by\napproximately 70% and increasing the speed fourfold. Additionally, we\nimplemented the INT8 model on a system-on-module to demonstrate its\ndeployability on embedded platforms and high power efficiency.\n","authors":["Yutaka Endo","Minoru Oikawa","Timothy D. Wilkinson","Tomoyoshi Shimobaba","Tomoyoshi Ito"],"pdf_url":"https://arxiv.org/pdf/2409.06711v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.24144v1","updated":"2024-10-31T17:05:44Z","published":"2024-10-31T17:05:44Z","title":"HoloChrome: Polychromatic Illumination for Speckle Reduction in\n  Holographic Near-Eye Displays","summary":"  Holographic displays hold the promise of providing authentic depth cues,\nresulting in enhanced immersive visual experiences for near-eye applications.\nHowever, current holographic displays are hindered by speckle noise, which\nlimits accurate reproduction of color and texture in displayed images. We\npresent HoloChrome, a polychromatic holographic display framework designed to\nmitigate these limitations. HoloChrome utilizes an ultrafast,\nwavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)\narchitecture, enabling the multiplexing of a large set of discrete wavelengths\nacross the visible spectrum. By leveraging spatial separation in our dual-SLM\nsetup, we independently manipulate speckle patterns across multiple\nwavelengths. This novel approach effectively reduces speckle noise through\nincoherent averaging achieved by wavelength multiplexing. Our method is\ncomplementary to existing speckle reduction techniques, offering a new pathway\nto address this challenge. Furthermore, the use of polychromatic illumination\nbroadens the achievable color gamut compared to traditional three-color primary\nholographic displays.\n  Our simulations and tabletop experiments validate that HoloChrome\nsignificantly reduces speckle noise and expands the color gamut. These\nadvancements enhance the performance of holographic near-eye displays, moving\nus closer to practical, immersive next-generation visual experiences.\n","authors":["Florian Schiffers","Grace Kuo","Nathan Matsuda","Douglas Lanman","Oliver Cossairt"],"pdf_url":"https://arxiv.org/pdf/2410.24144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24123v1","updated":"2024-10-31T16:49:10Z","published":"2024-10-31T16:49:10Z","title":"A Practical Style Transfer Pipeline for 3D Animation: Insights from\n  Production R&D","summary":"  Our animation studio has developed a practical style transfer pipeline for\ncreating stylized 3D animation, which is suitable for complex real-world\nproduction. This paper presents the insights from our development process,\nwhere we explored various options to balance quality, artist control, and\nworkload, leading to several key decisions. For example, we chose patch-based\ntexture synthesis over machine learning for better control and to avoid\ntraining data issues. We also addressed specifying style exemplars, managing\nmultiple colors within a scene, controlling outlines and shadows, and reducing\ntemporal noise. These insights were used to further refine our pipeline,\nultimately enabling us to produce an experimental short film showcasing various\nstyles.\n","authors":["Hideki Todo","Yuki Koyama","Kunihiro Sakai","Akihiro Komiya","Jun Kato"],"pdf_url":"https://arxiv.org/pdf/2410.24123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15856v2","updated":"2024-10-31T14:48:23Z","published":"2023-12-26T02:50:42Z","title":"SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields","summary":"  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n","authors":["Kaichen Zhou","Lanqing Hong","Enze Xie","Yongxin Yang","Zhenguo Li","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12282v2","updated":"2024-10-31T12:30:46Z","published":"2024-08-22T10:34:01Z","title":"Subsurface Scattering for 3D Gaussian Splatting","summary":"  3D reconstruction and relighting of objects made from scattering materials\npresent a significant challenge due to the complex light transport beneath the\nsurface. 3D Gaussian Splatting introduced high-quality novel view synthesis at\nreal-time speeds. While 3D Gaussians efficiently approximate an object's\nsurface, they fail to capture the volumetric properties of subsurface\nscattering. We propose a framework for optimizing an object's shape together\nwith the radiance transfer field given multi-view OLAT (one light at a time)\ndata. Our method decomposes the scene into an explicit surface represented as\n3D Gaussians, with a spatially varying BRDF, and an implicit volumetric\nrepresentation of the scattering component. A learned incident light field\naccounts for shadowing. We optimize all parameters jointly via ray-traced\ndifferentiable rendering. Our approach enables material editing, relighting and\nnovel view synthesis at interactive rates. We show successful application on\nsynthetic data and introduce a newly acquired multi-view multi-light dataset of\nobjects in a light-stage setup. Compared to previous work we achieve comparable\nor better results at a fraction of optimization and rendering time while\nenabling detailed control over material attributes. Project page\nhttps://sss.jdihlmann.com/\n","authors":["Jan-Niklas Dihlmann","Arjun Majumdar","Andreas Engelhardt","Raphael Braun","Hendrik P. A. Lensch"],"pdf_url":"https://arxiv.org/pdf/2408.12282v2.pdf","comment":"Project page: https://sss.jdihlmann.com/"},{"id":"http://arxiv.org/abs/2410.23800v1","updated":"2024-10-31T10:35:59Z","published":"2024-10-31T10:35:59Z","title":"SOAR: Self-Occluded Avatar Recovery from a Single Video In the Wild","summary":"  Self-occlusion is common when capturing people in the wild, where the\nperformer do not follow predefined motion scripts. This challenges existing\nmonocular human reconstruction systems that assume full body visibility. We\nintroduce Self-Occluded Avatar Recovery (SOAR), a method for complete human\nreconstruction from partial observations where parts of the body are entirely\nunobserved. SOAR leverages structural normal prior and generative diffusion\nprior to address such an ill-posed reconstruction problem. For structural\nnormal prior, we model human with an reposable surfel model with well-defined\nand easily readable shapes. For generative diffusion prior, we perform an\ninitial reconstruction and refine it using score distillation. On various\nbenchmarks, we show that SOAR performs favorably than state-of-the-art\nreconstruction and generation methods, and on-par comparing to concurrent\nworks. Additional video results and code are available at\nhttps://soar-avatar.github.io/.\n","authors":["Zhuoyang Pan","Angjoo Kanazawa","Hang Gao"],"pdf_url":"https://arxiv.org/pdf/2410.23800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23775v1","updated":"2024-10-31T09:45:00Z","published":"2024-10-31T09:45:00Z","title":"In-Context LoRA for Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 has explored the use of diffusion\ntransformers (DiTs) for task-agnostic image generation by simply concatenating\nattention tokens across images. However, despite substantial computational\nresources, the fidelity of the generated images remains suboptimal. In this\nstudy, we reevaluate and streamline this framework by hypothesizing that\ntext-to-image DiTs inherently possess in-context generation capabilities,\nrequiring only minimal tuning to activate them. Through diverse task\nexperiments, we qualitatively demonstrate that existing text-to-image DiTs can\neffectively perform in-context generation without any tuning. Building on this\ninsight, we propose a remarkably simple pipeline to leverage the in-context\nabilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint\ncaptioning of multiple images, and (3) apply task-specific LoRA tuning using\nsmall datasets (e.g., $20\\sim 100$ samples) instead of full-parameter tuning\nwith large datasets. We name our models In-Context LoRA (IC-LoRA). This\napproach requires no modifications to the original DiT models, only changes to\nthe training data. Remarkably, our pipeline generates high-fidelity image sets\nthat better adhere to prompts. While task-specific in terms of tuning data, our\nframework remains task-agnostic in architecture and pipeline, offering a\npowerful tool for the community and providing valuable insights for further\nresearch on product-level task-agnostic generation systems. We release our\ncode, data, and models at https://github.com/ali-vilab/In-Context-LoRA\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Huanzhang Dou","Chen Liang","Yutong Feng","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23775v1.pdf","comment":"Project page: https://ali-vilab.github.io/In-Context-Lora-Page/"},{"id":"http://arxiv.org/abs/2410.16833v2","updated":"2024-10-31T04:22:43Z","published":"2024-10-22T09:09:15Z","title":"Toroidal density-equalizing map for genus-one surfaces","summary":"  Density-equalizing map is a shape deformation technique originally developed\nfor cartogram creation and sociological data visualization on planar\ngeographical maps. In recent years, there has been an increasing interest in\ndeveloping density-equalizing mapping methods for surface and volumetric\ndomains and applying them to various problems in geometry processing and\nimaging science. However, the existing surface density-equalizing mapping\nmethods are only applicable to surfaces with relatively simple topologies but\nnot surfaces with topological holes. In this work, we develop a novel algorithm\nfor computing density-equalizing maps for toroidal surfaces. In particular,\ndifferent shape deformation effects can be easily achieved by prescribing\ndifferent population functions on the torus and performing diffusion-based\ndeformations on a planar domain with periodic boundary conditions. Furthermore,\nthe proposed toroidal density-equalizing mapping method naturally leads to an\neffective method for computing toroidal parameterizations of genus-one surfaces\nwith controllable shape changes, with the toroidal area-preserving\nparameterization being a prime example. Experimental results are presented to\ndemonstrate the effectiveness of our proposed methods.\n","authors":["Shunyu Yao","Gary P. T. Choi"],"pdf_url":"https://arxiv.org/pdf/2410.16833v2.pdf","comment":null}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.24131v1","updated":"2024-10-31T16:54:33Z","published":"2024-10-31T16:54:33Z","title":"Transit drivers' reflections on the benefits and harms of eye tracking\n  technology","summary":"  Eye tracking technology offers great potential for improving road safety. It\nis already being built into vehicles, namely cars and trucks. When this\ntechnology is integrated into transit service vehicles, employees, i.e., bus\ndrivers, will be subject to being eye tracked on their job. Although there is\nmuch research effort advancing algorithms for eye tracking in transportation,\nless is known about how end users perceive this technology, especially when\ninteracting with it in an employer-mandated context. In this first study of its\nkind, we investigated transit bus operators' perceptions of eye tracking\ntechnology. From a methodological perspective, we introduce a mixed methods\napproach where participants experience the technology first-hand and then\nreflect on their experience while viewing a playback of the recorded data.\nThematic analysis of the interview transcripts reveals interesting potential\nuses of eye tracking in this work context and surfaces transit operators' fears\nand concerns about this technology.\n","authors":["Shaina Murphy","Bryce Grame","Ethan Smith","Siva Srinivasan","Eakta Jain"],"pdf_url":"https://arxiv.org/pdf/2410.24131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24036v1","updated":"2024-10-31T15:34:01Z","published":"2024-10-31T15:34:01Z","title":"The Communal Loom: Integrating Tangible Interaction and Participatory\n  Data Collection for Assessing Well-Being","summary":"  For most health or well-being interventions, the process of evaluation is\ndistinct from the activity itself, both in terms of who is involved, and how\nthe actual data is collected and analyzed. Tangible interaction affords the\nopportunity to combine direct and embodied collaboration with a holistic\napproach to data collection and evaluation. We demonstrate this potential by\ndescribing our experiences designing and using the Communal Loom, an artifact\nfor art therapy that translates quantitative data to collectively woven\nartifacts.\n","authors":["Niti Parikh","Yiran Zhao","Maria Alinea-Bravo","Tapan Parikh"],"pdf_url":"https://arxiv.org/pdf/2410.24036v1.pdf","comment":"Presented at CHI 2022 Tangible Interaction for Supporting Well-being\n  Workshop"},{"id":"http://arxiv.org/abs/2410.24032v1","updated":"2024-10-31T15:30:55Z","published":"2024-10-31T15:30:55Z","title":"Navigating the Unknown: A Chat-Based Collaborative Interface for\n  Personalized Exploratory Tasks","summary":"  The rise of large language models (LLMs) has revolutionized user interactions\nwith knowledge-based systems, enabling chatbots to synthesize vast amounts of\ninformation and assist with complex, exploratory tasks. However, LLM-based\nchatbots often struggle to provide personalized support, particularly when\nusers start with vague queries or lack sufficient contextual information. This\npaper introduces the Collaborative Assistant for Personalized Exploration\n(CARE), a system designed to enhance personalization in exploratory tasks by\ncombining a multi-agent LLM framework with a structured user interface. CARE's\ninterface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling\niterative query refinement and dynamic solution generation. The multi-agent\nframework collaborates to identify both explicit and implicit user needs,\ndelivering tailored, actionable solutions. In a within-subject user study with\n22 participants, CARE was consistently preferred over a baseline LLM chatbot,\nwith users praising its ability to reduce cognitive load, inspire creativity,\nand provide more tailored solutions. Our findings highlight CARE's potential to\ntransform LLM-based systems from passive information retrievers to proactive\npartners in personalized problem-solving and exploration.\n","authors":["Yingzhe Peng","Xiaoting Qin","Zhiyang Zhang","Jue Zhang","Qingwei Lin","Xu Yang","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24028v1","updated":"2024-10-31T15:28:22Z","published":"2024-10-31T15:28:22Z","title":"AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with\n  Generalized Affinity Control","summary":"  The rise of mobile devices equipped with numerous sensors, such as LiDAR and\ncameras, has spurred the adoption of multi-modal deep intelligence for\ndistributed sensing tasks, such as smart cabins and driving assistance.\nHowever, the arrival times of mobile sensory data vary due to modality size and\nnetwork dynamics, which can lead to delays (if waiting for slower data) or\naccuracy decline (if inference proceeds without waiting). Moreover, the\ndiversity and dynamic nature of mobile systems exacerbate this challenge. In\nresponse, we present a shift to \\textit{opportunistic} inference for\nasynchronous distributed multi-modal data, enabling inference as soon as\npartial data arrives. While existing methods focus on optimizing modality\nconsistency and complementarity, known as modal affinity, they lack a\n\\textit{computational} approach to control this affinity in open-world mobile\nenvironments. AdaFlow pioneers the formulation of structured cross-modality\naffinity in mobile contexts using a hierarchical analysis-based normalized\nmatrix. This approach accommodates the diversity and dynamics of modalities,\ngeneralizing across different types and numbers of inputs. Employing an\naffinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible\ndata imputation, adapting to various modalities and downstream tasks without\nretraining. Experiments show that AdaFlow significantly reduces inference\nlatency by up to 79.9\\% and enhances accuracy by up to 61.9\\%, outperforming\nstatus quo approaches.\n","authors":["Fenmin Wu","Sicong Liu","Kehao Zhu","Xiaochen Li","Bin Guo","Zhiwen Yu","Hongkai Wen","Xiangrui Xu","Lehao Wang","Xiangyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23986v1","updated":"2024-10-31T14:38:50Z","published":"2024-10-31T14:38:50Z","title":"Simultaneous Control of Human Hand Joint Positions and Grip Force via\n  HD-EMG and Deep Learning","summary":"  In myoelectric control, simultaneous control of multiple degrees of freedom\ncan be challenging due to the dexterity of the human hand. Numerous studies\nhave focused on hand functionality, however, they only focused on a few degrees\nof freedom. In this paper, a 3DCNN-MLP model is proposed that uses high-density\nsEMG signals to estimate 20 hand joint positions and grip force simultaneously.\nThe deep learning model maps the muscle activity to the hand kinematics and\nkinetics. The proposed models' performance is also evaluated in estimating grip\nforces with real-time resolution. This paper investigated three individual\ndynamic hand movements (2pinch, 3pinch, and fist closing and opening) while\napplying forces in 10% and 30% of the maximum voluntary contraction (MVC). The\nresults demonstrated significant accuracy in estimating kinetics and\nkinematics. The average Euclidean distance across all joints and subjects was\n11.01 $\\pm$ 2.22 mm and the mean absolute error for offline and real-time force\nestimation were found to be 0.8 $\\pm$ 0.33 N and 2.09 $\\pm$ 0.9 N respectively.\nThe results demonstrate that by leveraging high-density sEMG and deep learning,\nit is possible to estimate human hand dynamics (kinematics and kinetics), which\nis a step forward to practical prosthetic hands.\n","authors":["Farnaz Rahimi","Mohammad Ali Badamchizadeh","Raul C. S√Æmpetru","Sehraneh Ghaemi","Bjoern M. Eskofier","Alessandro Del Vecchio"],"pdf_url":"https://arxiv.org/pdf/2410.23986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14808v2","updated":"2024-10-31T14:19:49Z","published":"2024-05-23T17:18:46Z","title":"Implicit Personalization in Language Models: A Systematic Study","summary":"  Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.\n","authors":["Zhijing Jin","Nils Heil","Jiarui Liu","Shehzaad Dhuliawala","Yahang Qi","Bernhard Sch√∂lkopf","Rada Mihalcea","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.14808v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2406.20087v2","updated":"2024-10-31T13:10:12Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2402.04559v3","updated":"2024-10-31T12:59:46Z","published":"2024-02-07T03:37:19Z","title":"Can Large Language Model Agents Simulate Human Trust Behavior?","summary":"  Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in social science and role-playing\napplications. However, one fundamental question remains: can LLM agents really\nsimulate human behavior? In this paper, we focus on one elemental behavior in\nhuman interactions, trust, and investigate whether LLM agents can simulate\nhuman trust behavior. We first find that LLM agents generally exhibit trust\nbehavior, referred to as agent trust, under the framework of Trust Games, which\nare widely recognized in behavioral economics. Then, we discover that GPT-4\nagents manifest high behavioral alignment with humans in terms of trust\nbehavior, indicating the feasibility of simulating human trust behavior with\nLLM agents. In addition, we probe the biases of agent trust and differences in\nagent trust towards other LLM agents and humans. We also explore the intrinsic\nproperties of agent trust under conditions including external manipulations and\nadvanced reasoning strategies. Our study provides new insights into the\nbehaviors of LLM agents and the fundamental analogy between LLMs and humans\nbeyond value alignment. We further illustrate broader implications of our\ndiscoveries for applications where trust is paramount.\n","authors":["Chengxing Xie","Canyu Chen","Feiran Jia","Ziyu Ye","Shiyang Lai","Kai Shu","Jindong Gu","Adel Bibi","Ziniu Hu","David Jurgens","James Evans","Philip Torr","Bernard Ghanem","Guohao Li"],"pdf_url":"https://arxiv.org/pdf/2402.04559v3.pdf","comment":"Accepted to Proceedings of NeurIPS 2024. The first two authors\n  contributed equally. 10 pages for main paper, 56 pages including appendix.\n  Project website: https://agent-trust.camel-ai.org"},{"id":"http://arxiv.org/abs/2410.23807v1","updated":"2024-10-31T10:46:42Z","published":"2024-10-31T10:46:42Z","title":"MAVIL: Design of a Multidimensional Assessment of Visual Data Literacy\n  and its Application in a Representative Survey","summary":"  The ability to read, interpret, and critique data visualizations has mainly\nbeen assessed using data visualization tasks like value retrieval. Although\nevidence on different facets of Visual Data Literacy (VDL) is well established\nin visualization research and includes numeracy, graph familiarity, or\naesthetic elements, they have not been sufficiently considered in ability\nassessments. Here, VDL is considered a multidimensional ability whose facets\ncan be partially self-assessed. We introduce an assessment in which VDL is\ndeconstructed as a process of understanding, in reference to frameworks from\nthe learning sciences. MAVIL, Multidimensional Assessment of Visual Data\nLiteracy, is composed of six ability dimensions: General Impression/Abstract\nThinking, Graph Elements/Familiarity, Aesthetic Perception, Visualization\nCriticism, Data Reading Tasks and Numeracy/Topic Knowledge. MAVIL was designed\nfor general audiences and implemented in a survey (n=438), representative of\nAustria's age groups (18-74 years) and gender split. The survey mirrors the\npopulation's VDL and shows the perception of two climate data visualizations, a\nline and bar chart. We found that $48\\%$ of respondents make mistakes with the\nsimple charts, while $5\\%$ believe that they cannot summarize the visualization\ncontent. About a quarter have deficits in comprehending simple data units, and\n$19-20\\%$ are unfamiliar with each displayed chart type.\n","authors":["Antonia Saske","Torsten M√∂ller","Laura Koesten","Judith Staudner","Sylvia Kritzinger"],"pdf_url":"https://arxiv.org/pdf/2410.23807v1.pdf","comment":"9 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.23803v1","updated":"2024-10-31T10:43:43Z","published":"2024-10-31T10:43:43Z","title":"Generative AI for Accessible and Inclusive Extended Reality","summary":"  Artificial Intelligence-Generated Content (AIGC) has the potential to\ntransform how people build and interact with virtual environments. Within this\npaper, we discuss potential benefits but also challenges that AIGC has for the\ncreation of inclusive and accessible virtual environments. Specifically, we\ntouch upon the decreased need for 3D modeling expertise, benefits of\nsymbolic-only as well as multimodal input, 3D content editing, and 3D model\naccessibility as well as foundation model-specific challenges.\n","authors":["Jens Grubert","Junlong Chen","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2410.23803v1.pdf","comment":"Presented at the CHI 2024 Workshop \"Building a Metaverse for All:\n  Opportunities and Challenges for Future Inclusive and Accessible Virtual\n  Environments\", May 11, 2024, Honolulu, Hawaii"},{"id":"http://arxiv.org/abs/2410.23754v1","updated":"2024-10-31T09:20:20Z","published":"2024-10-31T09:20:20Z","title":"RealMind: Zero-Shot EEG-Based Visual Decoding and Captioning Using\n  Multi-Modal Models","summary":"  Despite significant progress in visual decoding with fMRI data, its high cost\nand low temporal resolution limit widespread applicability. To address these\nchallenges, we introduce RealMind, a novel EEG-based visual decoding framework\nthat leverages multi-modal models to efficiently interpret semantic\ninformation. By integrating semantic and geometric consistency learning,\nRealMind enhances feature alignment, leading to improved decoding performance.\nOur framework achieves a 56.73\\% Top-5 accuracy in a 200-way retrieval task and\na 26.59\\% BLEU-1 score in a 200-way visual captioning task, representing the\nfirst successful attempt at zero-shot visual captioning using EEG data.\nRealMind provides a robust, adaptable, and cost-effective alternative to\nfMRI-based methods, offering scalable solutions for EEG-based visual decoding\nin practical applications.\n","authors":["Dongyang Li","Haoyang Qin","Mingyang Wu","Yuang Cao","Chen Wei","Quanying Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23725v1","updated":"2024-10-31T08:24:37Z","published":"2024-10-31T08:24:37Z","title":"Artificial intelligence to improve clinical coding practice in\n  Scandinavia: a crossover randomized controlled trial","summary":"  \\textbf{Trial design} Crossover randomized controlled trial. \\textbf{Methods}\nAn AI tool, Easy-ICD, was developed to assist clinical coders and was tested\nfor improving both accuracy and time in a user study in Norway and Sweden.\nParticipants were randomly assigned to two groups, and crossed over between\ncoding complex (longer) texts versus simple (shorter) texts, while using our\ntool versus not using our tool. \\textbf{Results} Based on Mann-Whitney U test,\nthe median coding time difference for complex clinical text sequences was 123\nseconds (\\emph{P}\\textless.001, 95\\% CI: 81 to 164), representing a 46\\%\nreduction in median coding time when our tool is used. There was no significant\ntime difference for simpler text sequences. For coding accuracy, the\nimprovement we noted for both complex and simple texts was not significant.\n\\textbf{Conclusions} This study demonstrates the potential of AI to transform\ncommon tasks in clinical workflows, with ostensible positive impacts on work\nefficiencies for complex clinical coding tasks. Further studies within hospital\nworkflows are required before these presumed impacts can be more clearly\nunderstood.\n","authors":["Taridzo Chomutare","Therese Olsen Svenning","Miguel √Ångel Tejedor Hern√°ndez","Phuong Dinh Ngo","Andrius Budrionis","Kaisa Markljung","Lill Irene Hind","Torbj√∏rn Torsvik","Karl √òyvind Mikalsen","Aleksandar Babic","Hercules Dalianis"],"pdf_url":"https://arxiv.org/pdf/2410.23725v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.23639v1","updated":"2024-10-31T05:21:46Z","published":"2024-10-31T05:21:46Z","title":"Biologically-Inspired Technologies: Integrating Brain-Computer Interface\n  and Neuromorphic Computing for Human Digital Twins","summary":"  The integration of the Metaverse into a human-centric ecosystem has\nintensified the need for sophisticated Human Digital Twins (HDTs) that are\ndriven by the multifaceted human data. However, the effective construction of\nHDTs faces significant challenges due to the heterogeneity of data collection\ndevices, the high energy demands associated with processing intricate data, and\nconcerns over the privacy of sensitive information. This work introduces a\nnovel biologically-inspired (bio-inspired) HDT framework that leverages\nBrain-Computer Interface (BCI) sensor technology to capture brain signals as\nthe data source for constructing HDT. By collecting and analyzing these\nsignals, the framework not only minimizes device heterogeneity and enhances\ndata collection efficiency, but also provides richer and more nuanced\nphysiological and psychological data for constructing personalized HDTs. To\nthis end, we further propose a bio-inspired neuromorphic computing learning\nmodel based on the Spiking Neural Network (SNN). This model utilizes discrete\nneural spikes to emulate the way of human brain processes information, thereby\nenhancing the system's ability to process data effectively while reducing\nenergy consumption. Additionally, we integrate a Federated Learning (FL)\nstrategy within the model to strengthen data privacy. We then conduct a case\nstudy to demonstrate the performance of our proposed twofold bio-inspired\nscheme. Finally, we present several challenges and promising directions for\nfuture research of HDTs driven by bio-inspired technologies.\n","authors":["Chen Shang","Jiadong Yu","Dinh Thai Hoang"],"pdf_url":"https://arxiv.org/pdf/2410.23639v1.pdf","comment":"8 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2410.23629v1","updated":"2024-10-31T04:42:43Z","published":"2024-10-31T04:42:43Z","title":"Posture-Informed Muscular Force Learning for Robust Hand Pressure\n  Estimation","summary":"  We present PiMForce, a novel framework that enhances hand pressure estimation\nby leveraging 3D hand posture information to augment forearm surface\nelectromyography (sEMG) signals. Our approach utilizes detailed spatial\ninformation from 3D hand poses in conjunction with dynamic muscle activity from\nsEMG to enable accurate and robust whole-hand pressure measurements under\ndiverse hand-object interactions. We also developed a multimodal data\ncollection system that combines a pressure glove, an sEMG armband, and a\nmarkerless finger-tracking module. We created a comprehensive dataset from 21\nparticipants, capturing synchronized data of hand posture, sEMG signals, and\nexerted hand pressure across various hand postures and hand-object interaction\nscenarios using our collection system. Our framework enables precise hand\npressure estimation in complex and natural interaction scenarios. Our approach\nsubstantially mitigates the limitations of traditional sEMG-based or\nvision-based methods by integrating 3D hand posture information with sEMG\nsignals. Video demos, data, and code are available online.\n","authors":["Kyungjin Seo","Junghoon Seo","Hanseok Jeong","Sangpil Kim","Sang Ho Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.23629v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23627v1","updated":"2024-10-31T04:29:32Z","published":"2024-10-31T04:29:32Z","title":"Col-Con: A Virtual Reality Simulation Testbed for Exploring\n  Collaborative Behaviors in Construction","summary":"  Virtual reality is widely adopted for applications such as training,\neducation, and collaboration. The construction industry, known for its complex\nprojects and numerous personnel involved, relies heavily on effective\ncollaboration. Setting up a real-world construction site for experiments can be\nexpensive and time-consuming, whereas conducting experiments in VR is\nrelatively low-cost, scalable, and efficient. We propose Col-Con, a virtual\nreality simulation testbed for exploring collaborative behaviors in\nconstruction. Col-Con is a multi-user testbed that supports users in completing\ntasks collaboratively. Additionally, Col-Con provides immersive and realistic\nsimulated construction scenes, where real-time voice communication, along with\nsynchronized transformations, animations, sounds, and interactions, enhances\nthe collaborative experience. As a showcase, we implemented a pipe installation\nconstruction task based on Col-Con. A user study demonstrated that Col-Con\nexcels in usability, and participants reported a strong sense of immersion and\ncollaboration. We envision that Col-Con will facilitate research on exploring\nvirtual reality-based collaborative behaviors in construction.\n","authors":["Liuchuan Yu","Ching-Yu Cheng","William F Ranc","Joshua Dow","Michael Szilagyi","Haikun Huang","Sungsoo Ray Hong","Behzad Esmaeili","Lap-Fai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.23627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22041v2","updated":"2024-10-31T02:05:50Z","published":"2024-10-29T13:46:52Z","title":"An LLM-based Simulation Framework for Embodied Conversational Agents in\n  Psychological Counseling","summary":"  Simulation is crucial for validating algorithmic strategies in real-world\nscenarios. While LLM-based social simulation shows promise as a mainstream\ntool, simulating complex scenarios like psychological counseling remains\nchallenging. We present ECAs (short for Embodied Conversational Agents), a\nframework for simulating psychological counseling clients' embodied memory,\nintegrating embodied cognition and counseling theories. We formulate six design\ngoals based on a comprehensive review of psychological counseling theories.\nUsing LLMs, we expand real counseling case data into a nuanced embodied\ncognitive memory space and generate dialogues based on high-frequency\ncounseling questions. We validate our framework using the D4 dataset, with\nevaluations by licensed counselors. Results show our approach significantly\noutperforms baselines in simulation authenticity and necessity. To demonstrate\nscalability, we created a public ECAs dataset through batch simulations. This\nresearch provides valuable insights for future social simulation studies in\npsychological counseling and Embodied Counseling Agents research.\n","authors":["Lixiu Wu","Yuanrong Tang","Qisen Pan","Xianyang Zhan","Yucheng Han","Mingyang You","Lanxi Xiao","Tianhong Wang","Chen Zhong","Jiangtao Gong"],"pdf_url":"https://arxiv.org/pdf/2410.22041v2.pdf","comment":"After careful consideration, we have decided to withdraw this version\n  because there are still several details that need to be adjusted to ensure\n  the accuracy and completeness of our work. We do not have an alternative\n  version in the short term and will resubmit it after the revision is\n  completed"},{"id":"http://arxiv.org/abs/2410.23555v1","updated":"2024-10-31T01:51:41Z","published":"2024-10-31T01:51:41Z","title":"From Context to Action: Analysis of the Impact of State Representation\n  and Context on the Generalization of Multi-Turn Web Navigation Agents","summary":"  Recent advancements in Large Language Model (LLM)-based frameworks have\nextended their capabilities to complex real-world applications, such as\ninteractive web navigation. These systems, driven by user commands, navigate\nweb browsers to complete tasks through multi-turn dialogues, offering both\ninnovative opportunities and significant challenges. Despite the introduction\nof benchmarks for conversational web navigation, a detailed understanding of\nthe key contextual components that influence the performance of these agents\nremains elusive. This study aims to fill this gap by analyzing the various\ncontextual elements crucial to the functioning of web navigation agents. We\ninvestigate the optimization of context management, focusing on the influence\nof interaction history and web page representation. Our work highlights\nimproved agent performance across out-of-distribution scenarios, including\nunseen websites, categories, and geographic locations through effective context\nmanagement. These findings provide insights into the design and optimization of\nLLM-based agents, enabling more accurate and effective web navigation in\nreal-world applications.\n","authors":["Nalin Tiwary","Vardhan Dongre","Sanil Arun Chawla","Ashwin Lamani","Dilek Hakkani-T√ºr"],"pdf_url":"https://arxiv.org/pdf/2410.23555v1.pdf","comment":"10 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.23554v1","updated":"2024-10-31T01:51:23Z","published":"2024-10-31T01:51:23Z","title":"Prosody as a Teaching Signal for Agent Learning: Exploratory Studies and\n  Algorithmic Implications","summary":"  Agent learning from human interaction often relies on explicit signals, but\nimplicit social cues, such as prosody in speech, could provide valuable\ninformation for more effective learning. This paper advocates for the\nintegration of prosody as a teaching signal to enhance agent learning from\nhuman teachers. Through two exploratory studies--one examining voice feedback\nin an interactive reinforcement learning setup and the other analyzing\nrestricted audio from human demonstrations in three Atari games--we demonstrate\nthat prosody carries significant information about task dynamics. Our findings\nsuggest that prosodic features, when coupled with explicit feedback, can\nenhance reinforcement learning outcomes. Moreover, we propose guidelines for\nprosody-sensitive algorithm design and discuss insights into teaching behavior.\nOur work underscores the potential of leveraging prosody as an implicit signal\nfor more efficient agent learning, thus advancing human-agent interaction\nparadigms.\n","authors":["Matilda Knierim","Sahil Jain","Murat Han Aydoƒüan","Kenneth Mitra","Kush Desai","Akanksha Saran","Kim Baraka"],"pdf_url":"https://arxiv.org/pdf/2410.23554v1.pdf","comment":"Published at the 26th ACM International Conference on Multimodal\n  Interaction (ICMI) 2024"},{"id":"http://arxiv.org/abs/2410.23540v1","updated":"2024-10-31T01:02:47Z","published":"2024-10-31T01:02:47Z","title":"Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending","summary":"  Wire bending is a technique used in manufacturing to mass-produce items such\nas clips, mounts, and braces. Wire bending machines like the DIWire by\nPensalabs have made this process accessible for personal fabrication. However,\nsuch machines are controlled using Computer Aided Manufacturing (CAM) software\nwhich is hard to use, making custom design challenging. We present Y-AR, a\nComputer Aided Design (CAD) interface for 3D wire bending. Y-AR uses mixed\nreality to let designers create structures that physically connect to objects\nin the environment. The interface incorporates springs as design primitives\nwhich (1) apply forces to hold objects, and (2) counter-act dimensional\ninaccuracies inherently caused by mid air modeling and measurement errors in\nAR. We demonstrate design workflows to design and fabricate a range of\nmechanisms designed in Y-AR as well as structures made using free-hand design\ntools. In our usability evaluation, all 12 participants successfully designed\nand fabricated a functional bottle holder with Y-AR. 10 out of 12 participants\nfelt that the system aided their design process, rating it above 7 out of 10.\n","authors":["Shuo Feng","Bo Liu"," Yifan"," Shan","Ofer Berman","Harald Haraldsson","Thijs Roumen"],"pdf_url":"https://arxiv.org/pdf/2410.23540v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.24222v1","updated":"2024-10-31T17:59:56Z","published":"2024-10-31T17:59:56Z","title":"Robust Gaussian Processes via Relevance Pursuit","summary":"  Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.\n","authors":["Sebastian Ament","Elizabeth Santorella","David Eriksson","Ben Letham","Maximilian Balandat","Eytan Bakshy"],"pdf_url":"https://arxiv.org/pdf/2410.24222v1.pdf","comment":"NeurIPS 2024 Article"},{"id":"http://arxiv.org/abs/2410.24220v1","updated":"2024-10-31T17:59:53Z","published":"2024-10-31T17:59:53Z","title":"Bridging Geometric States via Geometric Diffusion Bridge","summary":"  The accurate prediction of geometric state evolution in complex systems is\ncritical for advancing scientific domains such as quantum chemistry and\nmaterial modeling. Traditional experimental and computational methods face\nchallenges in terms of environmental constraints and computational demands,\nwhile current deep learning approaches still fall short in terms of precision\nand generality. In this work, we introduce the Geometric Diffusion Bridge\n(GDB), a novel generative modeling framework that accurately bridges initial\nand target geometric states. GDB leverages a probabilistic approach to evolve\ngeometric state distributions, employing an equivariant diffusion bridge\nderived by a modified version of Doob's $h$-transform for connecting geometric\nstates. This tailored diffusion process is anchored by initial and target\ngeometric states as fixed endpoints and governed by equivariant transition\nkernels. Moreover, trajectory data can be seamlessly leveraged in our GDB\nframework by using a chain of equivariant diffusion bridges, providing a more\ndetailed and accurate characterization of evolution dynamics. Theoretically, we\nconduct a thorough examination to confirm our framework's ability to preserve\njoint distributions of geometric states and capability to completely model the\nunderlying dynamics inducing trajectory distributions with negligible error.\nExperimental evaluations across various real-world scenarios show that GDB\nsurpasses existing state-of-the-art approaches, opening up a new pathway for\naccurately bridging geometric states and tackling crucial scientific challenges\nwith improved accuracy and applicability.\n","authors":["Shengjie Luo","Yixian Xu","Di He","Shuxin Zheng","Tie-Yan Liu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24220v1.pdf","comment":"33 pages, 5 tables; NeurIPS 2024 Camera Ready version"},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2410.24216v1","updated":"2024-10-31T17:59:46Z","published":"2024-10-31T17:59:46Z","title":"CaAdam: Improving Adam optimizer using connection aware methods","summary":"  We introduce a new method inspired by Adam that enhances convergence speed\nand achieves better loss function minima. Traditional optimizers, including\nAdam, apply uniform or globally adjusted learning rates across neural networks\nwithout considering their architectural specifics. This architecture-agnostic\napproach is deeply embedded in most deep learning frameworks, where optimizers\nare implemented as standalone modules without direct access to the network's\nstructural information. For instance, in popular frameworks like Keras or\nPyTorch, optimizers operate solely on gradients and parameters, without\nknowledge of layer connectivity or network topology. Our algorithm, CaAdam,\nexplores this overlooked area by introducing connection-aware optimization\nthrough carefully designed proxies of architectural information. We propose\nmultiple scaling methodologies that dynamically adjust learning rates based on\neasily accessible structural properties such as layer depth, connection counts,\nand gradient distributions. This approach enables more granular optimization\nwhile working within the constraints of current deep learning frameworks.\nEmpirical evaluations on standard datasets (e.g., CIFAR-10, Fashion MNIST) show\nthat our method consistently achieves faster convergence and higher accuracy\ncompared to standard Adam optimizer, demonstrating the potential benefits of\nincorporating architectural awareness in optimization strategies.\n","authors":["Remi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2410.24216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24214v1","updated":"2024-10-31T17:59:37Z","published":"2024-10-31T17:59:37Z","title":"ARQ: A Mixed-Precision Quantization Framework for Accurate and\n  Certifiably Robust DNNs","summary":"  Mixed precision quantization has become an important technique for enabling\nthe execution of deep neural networks (DNNs) on limited resource computing\nplatforms. Traditional quantization methods have primarily concentrated on\nmaintaining neural network accuracy, either ignoring the impact of quantization\non the robustness of the network, or using only empirical techniques for\nimproving robustness. In contrast, techniques for robustness certification,\nwhich can provide strong guarantees about the robustness of DNNs have not been\nused during quantization due to their high computation cost.\n  This paper introduces ARQ, an innovative mixed-precision quantization method\nthat not only preserves the clean accuracy of the smoothed classifiers but also\nmaintains their certified robustness. ARQ uses reinforcement learning to find\naccurate and robust DNN quantization, while efficiently leveraging randomized\nsmoothing, a popular class of statistical DNN verification algorithms, to guide\nthe search process.\n  We compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. We demonstrate\nthat ARQ consistently performs better than these baselines across all the\nbenchmarks and the input perturbation levels. In many cases, the performance of\nARQ quantized networks can reach that of the original DNN with floating-point\nweights, but with only 1.5% instructions.\n","authors":["Yuchen Yang","Shubham Ugare","Yifan Zhao","Gagandeep Singh","Sasa Misailovic"],"pdf_url":"https://arxiv.org/pdf/2410.24214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24210v1","updated":"2024-10-31T17:58:41Z","published":"2024-10-31T17:58:41Z","title":"TabM: Advancing Tabular Deep Learning with Parameter-Efficient\n  Ensembling","summary":"  Deep learning architectures for supervised learning on tabular data range\nfrom simple multilayer perceptrons (MLP) to sophisticated Transformers and\nretrieval-augmented methods. This study highlights a major, yet so far\noverlooked opportunity for substantially improving tabular MLPs: namely,\nparameter-efficient ensembling -- a paradigm for implementing an ensemble of\nmodels as one model producing multiple predictions. We start by developing TabM\n-- a simple model based on MLP and our variations of BatchEnsemble (an existing\ntechnique). Then, we perform a large-scale evaluation of tabular DL\narchitectures on public benchmarks in terms of both task performance and\nefficiency, which renders the landscape of tabular DL in a new light.\nGenerally, we show that MLPs, including TabM, form a line of stronger and more\npractical models compared to attention- and retrieval-based architectures. In\nparticular, we find that TabM demonstrates the best performance among tabular\nDL models. Lastly, we conduct an empirical analysis on the ensemble-like nature\nof TabM. For example, we observe that the multiple predictions of TabM are weak\nindividually, but powerful collectively. Overall, our work brings an impactful\ntechnique to tabular DL, analyses its behaviour, and advances the\nperformance-efficiency trade-off with TabM -- a simple and powerful baseline\nfor researchers and practitioners.\n","authors":["Yury Gorishniy","Akim Kotelnikov","Artem Babenko"],"pdf_url":"https://arxiv.org/pdf/2410.24210v1.pdf","comment":"Code: https://github.com/yandex-research/tabm"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24206v1","updated":"2024-10-31T17:58:13Z","published":"2024-10-31T17:58:13Z","title":"Understanding Optimization in Deep Learning with Central Flows","summary":"  Optimization in deep learning remains poorly understood, even in the simple\nsetting of deterministic (i.e. full-batch) training. A key difficulty is that\nmuch of an optimizer's behavior is implicitly determined by complex oscillatory\ndynamics, referred to as the \"edge of stability.\" The main contribution of this\npaper is to show that an optimizer's implicit behavior can be explicitly\ncaptured by a \"central flow:\" a differential equation which models the\ntime-averaged optimization trajectory. We show that these flows can empirically\npredict long-term optimization trajectories of generic neural networks with a\nhigh degree of numerical accuracy. By interpreting these flows, we reveal for\nthe first time 1) the precise sense in which RMSProp adapts to the local loss\nlandscape, and 2) an \"acceleration via regularization\" mechanism, wherein\nadaptive optimizers implicitly navigate towards low-curvature regions in which\nthey can take larger steps. This mechanism is key to the efficacy of these\nadaptive optimizers. Overall, we believe that central flows constitute a\npromising tool for reasoning about optimization in deep learning.\n","authors":["Jeremy M. Cohen","Alex Damian","Ameet Talwalkar","Zico Kolter","Jason D. Lee"],"pdf_url":"https://arxiv.org/pdf/2410.24206v1.pdf","comment":"first two authors contributed equally; author order determined by\n  coin flip"},{"id":"http://arxiv.org/abs/2410.24198v1","updated":"2024-10-31T17:55:13Z","published":"2024-10-31T17:55:13Z","title":"SelfCodeAlign: Self-Alignment for Code Generation","summary":"  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n","authors":["Yuxiang Wei","Federico Cassano","Jiawei Liu","Yifeng Ding","Naman Jain","Zachary Mueller","Harm de Vries","Leandro von Werra","Arjun Guha","Lingming Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24198v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2410.24184v1","updated":"2024-10-31T17:47:01Z","published":"2024-10-31T17:47:01Z","title":"Group Crosscoders for Mechanistic Analysis of Symmetry","summary":"  We introduce group crosscoders, an extension of crosscoders that\nsystematically discover and analyse symmetrical features in neural networks.\nWhile neural networks often develop equivariant representations without\nexplicit architectural constraints, understanding these emergent symmetries has\ntraditionally relied on manual analysis. Group crosscoders automate this\nprocess by performing dictionary learning across transformed versions of inputs\nunder a symmetry group. Applied to InceptionV1's mixed3b layer using the\ndihedral group $\\mathrm{D}_{32}$, our method reveals several key insights:\nFirst, it naturally clusters features into interpretable families that\ncorrespond to previously hypothesised feature types, providing more precise\nseparation than standard sparse autoencoders. Second, our transform block\nanalysis enables the automatic characterisation of feature symmetries,\nrevealing how different geometric features (such as curves versus lines)\nexhibit distinct patterns of invariance and equivariance. These results\ndemonstrate that group crosscoders can provide systematic insights into how\nneural networks represent symmetry, offering a promising new tool for\nmechanistic interpretability.\n","authors":["Liv Gorton"],"pdf_url":"https://arxiv.org/pdf/2410.24184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24178v1","updated":"2024-10-31T17:43:53Z","published":"2024-10-31T17:43:53Z","title":"AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal\n  Properties","summary":"  Anomaly detection is widely used for identifying critical errors and\nsuspicious behaviors, but current methods lack interpretability. We leverage\ncommon properties of existing methods and recent advances in generative models\nto introduce counterfactual explanations for anomaly detection. Given an input,\nwe generate its counterfactual as a diffusion-based repair that shows what a\nnon-anomalous version should have looked like. A key advantage of this approach\nis that it enables a domain-independent formal specification of explainability\ndesiderata, offering a unified framework for generating and evaluating\nexplanations. We demonstrate the effectiveness of our anomaly explainability\nframework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI)\nanomaly datasets. The code used for the experiments is accessible at:\nhttps://github.com/xjiae/arpro.\n","authors":["Xiayan Ji","Anton Xue","Eric Wong","Oleg Sokolsky","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2410.24178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24177v1","updated":"2024-10-31T17:43:13Z","published":"2024-10-31T17:43:13Z","title":"DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models","summary":"  Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.\n","authors":["Heng-Jui Chang","Hongyu Gong","Changhan Wang","James Glass","Yu-An Chung"],"pdf_url":"https://arxiv.org/pdf/2410.24177v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.24169v1","updated":"2024-10-31T17:35:57Z","published":"2024-10-31T17:35:57Z","title":"The Importance of Being Scalable: Improving the Speed and Accuracy of\n  Neural Network Interatomic Potentials Across Chemical Domains","summary":"  Scaling has been critical in improving model performance and generalization\nin machine learning. It involves how a model's performance changes with\nincreases in model size or input data, as well as how efficiently computational\nresources are utilized to support this growth. Despite successes in other\nareas, the study of scaling in Neural Network Interatomic Potentials (NNIPs)\nremains limited. NNIPs act as surrogate models for ab initio quantum mechanical\ncalculations. The dominant paradigm here is to incorporate many physical domain\nconstraints into the model, such as rotational equivariance. We contend that\nthese complex constraints inhibit the scaling ability of NNIPs, and are likely\nto lead to performance plateaus in the long run. In this work, we take an\nalternative approach and start by systematically studying NNIP scaling\nstrategies. Our findings indicate that scaling the model through attention\nmechanisms is efficient and improves model expressivity. These insights\nmotivate us to develop an NNIP architecture designed for scalability: the\nEfficiently Scaled Attention Interatomic Potential (EScAIP). EScAIP leverages a\nmulti-head self-attention formulation within graph neural networks, applying\nattention at the neighbor-level representations. Implemented with\nhighly-optimized attention GPU kernels, EScAIP achieves substantial gains in\nefficiency--at least 10x faster inference, 5x less memory usage--compared to\nexisting NNIPs. EScAIP also achieves state-of-the-art performance on a wide\nrange of datasets including catalysts (OC20 and OC22), molecules (SPICE), and\nmaterials (MPTrj). We emphasize that our approach should be thought of as a\nphilosophy rather than a specific model, representing a proof-of-concept for\ndeveloping general-purpose NNIPs that achieve better expressivity through\nscaling, and continue to scale efficiently with increased computational\nresources and training data.\n","authors":["Eric Qu","Aditi S. Krishnapriyan"],"pdf_url":"https://arxiv.org/pdf/2410.24169v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.04690v3","updated":"2024-10-31T17:32:26Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v3.pdf","comment":"To appear in 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.10796v2","updated":"2024-10-31T17:29:37Z","published":"2024-06-16T03:45:03Z","title":"Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction\n  Data","summary":"  A major challenge in materials science is the determination of the structure\nof nanometer sized objects. Here we present a novel approach that uses a\ngenerative machine learning model based on diffusion processes that is trained\non 45,229 known structures. The model factors both the measured diffraction\npattern as well as relevant statistical priors on the unit cell of atomic\ncluster structures. Conditioned only on the chemical formula and the\ninformation-scarce finite-size broadened powder diffraction pattern, we find\nthat our model, PXRDnet, can successfully solve simulated nanocrystals as small\nas 10 angstroms across 200 materials of varying symmetry and complexity,\nincluding structures from all seven crystal systems. We show that our model can\nsuccessfully and verifiably determine structural candidates four out of five\ntimes, with average error among these candidates being only 7% (as measured by\npost-Rietveld refinement R-factor). Furthermore, PXRDnet is capable of solving\nstructures from noisy diffraction patterns gathered in real-world experiments.\nWe suggest that data driven approaches, bootstrapped from theoretical\nsimulation, will ultimately provide a path towards determining the structure of\npreviously unsolved nano-materials.\n","authors":["Gabe Guo","Tristan Saidi","Maxwell Terban","Michele Valsecchi","Simon JL Billinge","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2406.10796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24166v1","updated":"2024-10-31T17:28:41Z","published":"2024-10-31T17:28:41Z","title":"Approaches to human activity recognition via passive radar","summary":"  The thesis explores novel methods for Human Activity Recognition (HAR) using\npassive radar with a focus on non-intrusive Wi-Fi Channel State Information\n(CSI) data. Traditional HAR approaches often use invasive sensors like cameras\nor wearables, raising privacy issues. This study leverages the non-intrusive\nnature of CSI, using Spiking Neural Networks (SNN) to interpret signal\nvariations caused by human movements. These networks, integrated with symbolic\nreasoning frameworks such as DeepProbLog, enhance the adaptability and\ninterpretability of HAR systems. SNNs offer reduced power consumption, ideal\nfor privacy-sensitive applications. Experimental results demonstrate SNN-based\nneurosymbolic models achieve high accuracy making them a promising alternative\nfor HAR across various domains.\n","authors":["Christian Bresciani","Federico Cerutti","Marco Cominelli"],"pdf_url":"https://arxiv.org/pdf/2410.24166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24164v1","updated":"2024-10-31T17:22:30Z","published":"2024-10-31T17:22:30Z","title":"$œÄ_0$: A Vision-Language-Action Flow Model for General Robot Control","summary":"  Robot learning holds tremendous promise to unlock the full potential of\nflexible, general, and dexterous robot systems, as well as to address some of\nthe deepest questions in artificial intelligence. However, bringing robot\nlearning to the level of generality required for effective real-world systems\nfaces major obstacles in terms of data, generalization, and robustness. In this\npaper, we discuss how generalist robot policies (i.e., robot foundation models)\ncan address these challenges, and how we can design effective generalist robot\npolicies for complex and highly dexterous tasks. We propose a novel flow\nmatching architecture built on top of a pre-trained vision-language model (VLM)\nto inherit Internet-scale semantic knowledge. We then discuss how this model\ncan be trained on a large and diverse dataset from multiple dexterous robot\nplatforms, including single-arm robots, dual-arm robots, and mobile\nmanipulators. We evaluate our model in terms of its ability to perform tasks in\nzero shot after pre-training, follow language instructions from people and from\na high-level VLM policy, and its ability to acquire new skills via fine-tuning.\nOur results cover a wide variety of tasks, such as laundry folding, table\ncleaning, and assembling boxes.\n","authors":["Kevin Black","Noah Brown","Danny Driess","Adnan Esmail","Michael Equi","Chelsea Finn","Niccolo Fusai","Lachy Groom","Karol Hausman","Brian Ichter","Szymon Jakubczak","Tim Jones","Liyiming Ke","Sergey Levine","Adrian Li-Bell","Mohith Mothukuri","Suraj Nair","Karl Pertsch","Lucy Xiaoyang Shi","James Tanner","Quan Vuong","Anna Walling","Haohuan Wang","Ury Zhilinsky"],"pdf_url":"https://arxiv.org/pdf/2410.24164v1.pdf","comment":"See project website for videos:\n  https://physicalintelligence.company/blog/pi0"},{"id":"http://arxiv.org/abs/2410.24162v1","updated":"2024-10-31T17:20:13Z","published":"2024-10-31T17:20:13Z","title":"Conformalized Prediction of Post-Fault Voltage Trajectories Using\n  Pre-trained and Finetuned Attention-Driven Neural Operators","summary":"  This paper proposes a new data-driven methodology for predicting intervals of\npost-fault voltage trajectories in power systems. We begin by introducing the\nQuantile Attention-Fourier Deep Operator Network (QAF-DeepONet), designed to\ncapture the complex dynamics of voltage trajectories and reliably estimate\nquantiles of the target trajectory without any distributional assumptions. The\nproposed operator regression model maps the observed portion of the voltage\ntrajectory to its unobserved post-fault trajectory. Our methodology employs a\npre-training and fine-tuning process to address the challenge of limited data\navailability. To ensure data privacy in learning the pre-trained model, we use\nmerging via federated learning with data from neighboring buses, enabling the\nmodel to learn the underlying voltage dynamics from such buses without directly\nsharing their data. After pre-training, we fine-tune the model with data from\nthe target bus, allowing it to adapt to unique dynamics and operating\nconditions. Finally, we integrate conformal prediction into the fine-tuned\nmodel to ensure coverage guarantees for the predicted intervals. We evaluated\nthe performance of the proposed methodology using the New England 39-bus test\nsystem considering detailed models of voltage and frequency controllers. Two\nmetrics, Prediction Interval Coverage Probability (PICP) and Prediction\nInterval Normalized Average Width (PINAW), are used to numerically assess the\nmodel's performance in predicting intervals. The results show that the proposed\napproach offers practical and reliable uncertainty quantification in predicting\nthe interval of post-fault voltage trajectories.\n","authors":["Amirhossein Mollaali","Gabriel Zufferey","Gonzalo Constante-Flores","Christian Moya","Can Li","Guang Lin","Meng Yue"],"pdf_url":"https://arxiv.org/pdf/2410.24162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12747v2","updated":"2024-10-31T17:18:16Z","published":"2024-06-18T16:07:33Z","title":"TSI-Bench: Benchmarking Time Series Imputation","summary":"  Effective imputation is a crucial preprocessing step for time series\nanalysis. Despite the development of numerous deep learning algorithms for time\nseries imputation, the community lacks standardized and comprehensive benchmark\nplatforms to effectively evaluate imputation performance across different\nsettings. Moreover, although many deep learning forecasting algorithms have\ndemonstrated excellent performance, whether their modelling achievements can be\ntransferred to time series imputation tasks remains unexplored. To bridge these\ngaps, we develop TSI-Bench, the first (to our knowledge) comprehensive\nbenchmark suite for time series imputation utilizing deep learning techniques.\nThe TSI-Bench pipeline standardizes experimental settings to enable fair\nevaluation of imputation algorithms and identification of meaningful insights\ninto the influence of domain-appropriate missing rates and patterns on model\nperformance. Furthermore, TSI-Bench innovatively provides a systematic paradigm\nto tailor time series forecasting algorithms for imputation purposes. Our\nextensive study across 34,804 experiments, 28 algorithms, and 8 datasets with\ndiverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse\ndownstream tasks and potential to unlock future directions in time series\nimputation research and analysis. All source code and experiment logs are\nreleased at https://github.com/WenjieDu/AwesomeImputation.\n","authors":["Wenjie Du","Jun Wang","Linglong Qian","Yiyuan Yang","Zina Ibrahim","Fanxing Liu","Zepu Wang","Haoxin Liu","Zhiyuan Zhao","Yingjie Zhou","Wenjia Wang","Kaize Ding","Yuxuan Liang","B. Aditya Prakash","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.12747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22382v2","updated":"2024-10-31T17:12:27Z","published":"2024-10-29T12:54:55Z","title":"Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference","summary":"  Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.\n","authors":["Chris Lam"],"pdf_url":"https://arxiv.org/pdf/2410.22382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24153v1","updated":"2024-10-31T17:10:57Z","published":"2024-10-31T17:10:57Z","title":"Dense Associative Memory Through the Lens of Random Features","summary":"  Dense Associative Memories are high storage capacity variants of the Hopfield\nnetworks that are capable of storing a large number of memory patterns in the\nweights of the network of a given size. Their common formulations typically\nrequire storing each pattern in a separate set of synaptic weights, which leads\nto the increase of the number of synaptic weights when new patterns are\nintroduced. In this work we propose an alternative formulation of this class of\nmodels using random features, commonly used in kernel methods. In this\nformulation the number of network's parameters remains fixed. At the same time,\nnew memories can be added to the network by modifying existing weights. We show\nthat this novel network closely approximates the energy function and dynamics\nof conventional Dense Associative Memories and shares their desirable\ncomputational properties.\n","authors":["Benjamin Hoover","Duen Horng Chau","Hendrik Strobelt","Parikshit Ram","Dmitry Krotov"],"pdf_url":"https://arxiv.org/pdf/2410.24153v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24145v1","updated":"2024-10-31T17:05:52Z","published":"2024-10-31T17:05:52Z","title":"Conformal prediction of circular data","summary":"  Split conformal prediction techniques are applied to regression problems with\ncircular responses by introducing a suitable conformity score, leading to\nprediction sets with adaptive arc length and finite-sample coverage guarantees\nfor any circular predictive model under exchangeable data. Leveraging the high\nperformance of existing predictive models designed for linear responses, we\nanalyze a general projection procedure that converts any linear response\nregression model into one suitable for circular responses. When random forests\nserve as basis models in this projection procedure, we harness the out-of-bag\ndynamics to eliminate the necessity for a separate calibration sample in the\nconstruction of prediction sets. For synthetic and real datasets the resulting\nprojected random forests model produces more efficient out-of-bag conformal\nprediction sets, with shorter median arc length, when compared to the split\nconformal prediction sets generated by two existing alternative models.\n","authors":["Paulo C. Marques F.","Rinaldo Artes","Helton Graziadei"],"pdf_url":"https://arxiv.org/pdf/2410.24145v1.pdf","comment":"7 pages; 4 figures"},{"id":"http://arxiv.org/abs/2402.07963v3","updated":"2024-10-31T17:05:49Z","published":"2024-02-12T10:32:47Z","title":"SPO: Sequential Monte Carlo Policy Optimisation","summary":"  Leveraging planning during learning and decision-making is central to the\nlong-term development of intelligent agents. Recent works have successfully\ncombined tree-based search methods and self-play learning mechanisms to this\nend. However, these methods typically face scaling challenges due to the\nsequential nature of their search. While practical engineering solutions can\npartly overcome this, they often result in a negative impact on performance. In\nthis paper, we introduce SPO: Sequential Monte Carlo Policy Optimisation, a\nmodel-based reinforcement learning algorithm grounded within the Expectation\nMaximisation (EM) framework. We show that SPO provides robust policy\nimprovement and efficient scaling properties. The sample-based search makes it\ndirectly applicable to both discrete and continuous action spaces without\nmodifications. We demonstrate statistically significant improvements in\nperformance relative to model-free and model-based baselines across both\ncontinuous and discrete environments. Furthermore, the parallel nature of SPO's\nsearch enables effective utilisation of hardware accelerators, yielding\nfavourable scaling laws.\n","authors":["Matthew V Macfarlane","Edan Toledo","Donal Byrne","Paul Duckworth","Alexandre Laterre"],"pdf_url":"https://arxiv.org/pdf/2402.07963v3.pdf","comment":"Accepted to NeurIPS 2024. 34 pages, 3 main figures"},{"id":"http://arxiv.org/abs/2402.18551v2","updated":"2024-10-31T17:01:45Z","published":"2024-02-28T18:34:53Z","title":"Implicit Optimization Bias of Next-Token Prediction in Linear Models","summary":"  We initiate an investigation into the optimization properties of next-token\nprediction (NTP), the dominant training paradigm for modern language models.\nSpecifically, we study the structural properties of the solutions selected by\ngradient-based optimizers among the many possible minimizers of the NTP\nobjective. By framing NTP as cross-entropy minimization across distinct\ncontexts, each tied with a sparse conditional probability distribution across a\nfinite vocabulary of tokens, we introduce \"NTP-separability conditions\" that\nenable reaching the data-entropy lower bound. With this setup, and focusing on\nlinear models with fixed context embeddings, we characterize the optimization\nbias of gradient descent (GD): Within the data subspace defined by the sparsity\npatterns of distinct contexts, GD selects parameters that equate the logits'\ndifferences of in-support tokens to their log-odds. In the orthogonal subspace,\nthe GD parameters diverge in norm and select the direction that maximizes a\nmargin specific to NTP. These findings extend previous research on implicit\nbias in one-hot classification to the NTP setting, highlighting key differences\nand prompting further research into the optimization and generalization\nproperties of NTP, irrespective of the specific architecture used to generate\nthe context embeddings.\n","authors":["Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2402.18551v2.pdf","comment":"v2: fixed typos and writing in various parts; updated figures and\n  future-work section"},{"id":"http://arxiv.org/abs/2407.01079v3","updated":"2024-10-31T16:59:13Z","published":"2024-07-01T08:34:40Z","title":"On Statistical Rates and Provably Efficient Criteria of Latent Diffusion\n  Transformers (DiTs)","summary":"  We investigate the statistical and computational limits of latent Diffusion\nTransformers (DiTs) under the low-dimensional linear latent space assumption.\nStatistically, we study the universal approximation and sample complexity of\nthe DiTs score function, as well as the distribution recovery property of the\ninitial data. Specifically, under mild data assumptions, we derive an\napproximation error bound for the score network of latent DiTs, which is\nsub-linear in the latent space dimension. Additionally, we derive the\ncorresponding sample complexity bound and show that the data distribution\ngenerated from the estimated score function converges toward a proximate area\nof the original one. Computationally, we characterize the hardness of both\nforward inference and backward computation of latent DiTs, assuming the Strong\nExponential Time Hypothesis (SETH). For forward inference, we identify\nefficient criteria for all possible latent DiTs inference algorithms and\nshowcase our theory by pushing the efficiency toward almost-linear time\ninference. For backward computation, we leverage the low-rank structure within\nthe gradient computation of DiTs training for possible algorithmic speedup.\nSpecifically, we show that such speedup achieves almost-linear time latent DiTs\ntraining by casting the DiTs gradient as a series of chained low-rank\napproximations with bounded error. Under the low-dimensional assumption, we\nshow that the statistical rates and the computational efficiency are all\ndominated by the dimension of the subspace, suggesting that latent DiTs have\nthe potential to bypass the challenges associated with the high dimensionality\nof initial data.\n","authors":["Jerry Yao-Chieh Hu","Weimin Wu","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01079v3.pdf","comment":"Accepted at NeurIPS 2024. v3 updated to camera-ready version with\n  many typos fixed; v2 fixed typos, added Fig. 1 and added clarifications"},{"id":"http://arxiv.org/abs/2406.03636v4","updated":"2024-10-31T16:54:30Z","published":"2024-06-05T22:16:19Z","title":"Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource\n  Programming and Formal Languages","summary":"  Recent advances in large language models (LLMs) for code applications have\ndemonstrated remarkable zero-shot fluency and instruction following on\nchallenging code related tasks ranging from test case generation to\nself-repair. Unsurprisingly, however, models struggle to compose syntactically\nvalid programs in programming languages unrepresented in pre-training, referred\nto as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial\nsettings, including domain-specific languages for internal tools, tool-chains\nfor legacy languages, and formal verification frameworks. Inspired by a\ntechnique called natural programming elicitation, we propose designing an\nintermediate language that LLMs \"naturally\" know how to use and which can be\nautomatically compiled to a target VLPL. When LLMs generate code that lies\noutside of this intermediate language, we use compiler techniques to repair the\ncode into programs in the intermediate language. Overall, we introduce\n\\emph{synthetic programming elicitation and compilation} (SPEAC), an approach\nthat enables LLMs to generate syntactically valid code even for VLPLs. We\nempirically evaluate the performance of SPEAC in a case study for the UCLID5\nformal verification language and find that, compared to existing retrieval and\nfine-tuning baselines, SPEAC produces syntactically correct programs more\nfrequently and without sacrificing semantic correctness.\n","authors":["Federico Mora","Justin Wong","Haley Lepe","Sahil Bhatia","Karim Elmaaroufi","George Varghese","Joseph E. Gonzalez","Elizabeth Polgreen","Sanjit A. Seshia"],"pdf_url":"https://arxiv.org/pdf/2406.03636v4.pdf","comment":"14 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.24128v1","updated":"2024-10-31T16:53:20Z","published":"2024-10-31T16:53:20Z","title":"Q-learning for Quantile MDPs: A Decomposition, Performance, and\n  Convergence Analysis","summary":"  In Markov decision processes (MDPs), quantile risk measures such as\nValue-at-Risk are a standard metric for modeling RL agents' preferences for\ncertain outcomes. This paper proposes a new Q-learning algorithm for quantile\noptimization in MDPs with strong convergence and performance guarantees. The\nalgorithm leverages a new, simple dynamic program (DP) decomposition for\nquantile MDPs. Compared with prior work, our DP decomposition requires neither\nknown transition probabilities nor solving complex saddle point equations and\nserves as a suitable foundation for other model-free RL algorithms. Our\nnumerical results in tabular domains show that our Q-learning algorithm\nconverges to its DP variant and outperforms earlier algorithms.\n","authors":["Jia Lin Hau","Erick Delage","Esther Derman","Mohammad Ghavamzadeh","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2410.24128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01903v2","updated":"2024-10-31T16:49:26Z","published":"2024-07-02T03:08:20Z","title":"Text-Aware Diffusion for Policy Learning","summary":"  Training an agent to achieve particular goals or perform desired behaviors is\noften accomplished through reinforcement learning, especially in the absence of\nexpert demonstrations. However, supporting novel goals or behaviors through\nreinforcement learning requires the ad-hoc design of appropriate reward\nfunctions, which quickly becomes intractable. To address this challenge, we\npropose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a\npretrained, frozen text-conditioned diffusion model to compute dense zero-shot\nreward signals for text-aligned policy learning. We hypothesize that\nlarge-scale pretrained generative models encode rich priors that can supervise\na policy to behave not only in a text-aligned manner, but also in alignment\nwith a notion of naturalness summarized from internet-scale training data. In\nour experiments, we demonstrate that TADPoLe is able to learn policies for\nnovel goal-achievement and continuous locomotion behaviors specified by natural\nlanguage, in both Humanoid and Dog environments. The behaviors are learned\nzero-shot without ground-truth rewards or expert demonstrations, and are\nqualitatively more natural according to human evaluation. We further show that\nTADPoLe performs competitively when applied to robotic manipulation tasks in\nthe Meta-World environment, without having access to any in-domain\ndemonstrations.\n","authors":["Calvin Luo","Mandy He","Zilai Zeng","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07451v2","updated":"2024-10-31T16:48:40Z","published":"2024-06-11T16:57:48Z","title":"An Optimism-based Approach to Online Evaluation of Generative Models","summary":"  Existing frameworks for evaluating and comparing generative models typically\ntarget an offline setting, where the evaluator has access to full batches of\ndata produced by the models. However, in many practical scenarios, the goal is\nto identify the best model using the fewest generated samples to minimize the\ncosts of querying data from the models. Such an online comparison is\nchallenging with current offline assessment methods. In this work, we propose\nan online evaluation framework to find the generative model that maximizes a\nstandard assessment score among a group of available models. Our method uses an\noptimism-based multi-armed bandit framework to identify the model producing\ndata with the highest evaluation score, quantifying the quality and diversity\nof generated data. Specifically, we study the online assessment of generative\nmodels based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS)\nmetrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper\nconfidence bound approach in online learning. We prove sub-linear regret bounds\nfor these algorithms and present numerical results on standard image datasets,\ndemonstrating their effectiveness in identifying the score-maximizing\ngenerative model.\n","authors":["Xiaoyan Hu","Ho-fung Leung","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2406.07451v2.pdf","comment":"arXiv version"},{"id":"http://arxiv.org/abs/2410.24117v1","updated":"2024-10-31T16:46:52Z","published":"2024-10-31T16:46:52Z","title":"Repository-Level Compositional Code Translation and Validation","summary":"  Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.\n","authors":["Ali Reza Ibrahimzada","Kaiyao Ke","Mrigank Pawagi","Muhammad Salman Abid","Rangeet Pan","Saurabh Sinha","Reyhaneh Jabbarvand"],"pdf_url":"https://arxiv.org/pdf/2410.24117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24116v1","updated":"2024-10-31T16:46:23Z","published":"2024-10-31T16:46:23Z","title":"AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization","summary":"  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n","authors":["Amir Kazemi","Qurat ul ain Fatima","Volodymyr Kindratenko","Christopher Tessum"],"pdf_url":"https://arxiv.org/pdf/2410.24116v1.pdf","comment":"19 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.07307v3","updated":"2024-10-31T16:39:11Z","published":"2024-02-11T21:12:21Z","title":"Self-Calibrating Conformal Prediction","summary":"  In machine learning, model calibration and predictive inference are essential\nfor producing reliable predictions and quantifying uncertainty to support\ndecision-making. Recognizing the complementary roles of point and interval\npredictions, we introduce Self-Calibrating Conformal Prediction, a method that\ncombines Venn-Abers calibration and conformal prediction to deliver calibrated\npoint predictions alongside prediction intervals with finite-sample validity\nconditional on these predictions. To achieve this, we extend the original\nVenn-Abers procedure from binary classification to regression. Our theoretical\nframework supports analyzing conformal prediction methods that involve\ncalibrating model predictions and subsequently constructing conditionally valid\nprediction intervals on the same data, where the conditioning set or conformity\nscores may depend on the calibrated predictions. Real-data experiments show\nthat our method improves interval efficiency through model calibration and\noffers a practical alternative to feature-conditional validity.\n","authors":["Lars van der Laan","Ahmed M. Alaa"],"pdf_url":"https://arxiv.org/pdf/2402.07307v3.pdf","comment":"Accepted to Neurips 2024. Preprint previously titled Self-Consistent\n  Conformal Prediction.\n  https://openreview.net/forum?id=BJ6HkT7qIk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2024%2FConference%2FAuthors%23your-submissions)"},{"id":"http://arxiv.org/abs/2410.24108v1","updated":"2024-10-31T16:38:51Z","published":"2024-10-31T16:38:51Z","title":"Reinforcement Learning Gradients as Vitamin for Online Finetuning\n  Decision Transformers","summary":"  Decision Transformers have recently emerged as a new and compelling paradigm\nfor offline Reinforcement Learning (RL), completing a trajectory in an\nautoregressive way. While improvements have been made to overcome initial\nshortcomings, online finetuning of decision transformers has been surprisingly\nunder-explored. The widely adopted state-of-the-art Online Decision Transformer\n(ODT) still struggles when pretrained with low-reward offline data. In this\npaper, we theoretically analyze the online-finetuning of the decision\ntransformer, showing that the commonly used Return-To-Go (RTG) that's far from\nthe expected return hampers the online fine-tuning process. This problem,\nhowever, is well-addressed by the value function and advantage of standard RL\nalgorithms. As suggested by our analysis, in our experiments, we hence find\nthat simply adding TD3 gradients to the finetuning process of ODT effectively\nimproves the online finetuning performance of ODT, especially if ODT is\npretrained with low-reward offline data. These findings provide new directions\nto further improve decision transformers.\n","authors":["Kai Yan","Alexander G. Schwing","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.24108v1.pdf","comment":"Accepted as NeurIPS 2024 spotlight. 33 pages, 26 figures"},{"id":"http://arxiv.org/abs/2410.24106v1","updated":"2024-10-31T16:37:25Z","published":"2024-10-31T16:37:25Z","title":"On Sampling Strategies for Spectral Model Sharding","summary":"  The problem of heterogeneous clients in federated learning has recently drawn\na lot of attention. Spectral model sharding, i.e., partitioning the model\nparameters into low-rank matrices based on the singular value decomposition,\nhas been one of the proposed solutions for more efficient on-device training in\nsuch settings. In this work, we present two sampling strategies for such\nsharding, obtained as solutions to specific optimization problems. The first\nproduces unbiased estimators of the original weights, while the second aims to\nminimize the squared approximation error. We discuss how both of these\nestimators can be incorporated in the federated learning loop and practical\nconsiderations that arise during local training. Empirically, we demonstrate\nthat both of these methods can lead to improved performance on various commonly\nused datasets.\n","authors":["Denis Korzhenkov","Christos Louizos"],"pdf_url":"https://arxiv.org/pdf/2410.24106v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.15892v3","updated":"2024-10-31T16:36:27Z","published":"2024-07-22T01:52:30Z","title":"Mini-Sequence Transformer: Optimizing Intermediate Memory for Long\n  Sequences Training","summary":"  We introduce Mini-Sequence Transformer (MsT), a simple and effective\nmethodology for highly efficient and accurate LLM training with extremely long\nsequences. MsT partitions input sequences and iteratively processes\nmini-sequences to reduce intermediate memory usage. Integrated with activation\nrecomputation, it enables significant memory savings in both forward and\nbackward passes. In experiments with the Llama3-8B model, with MsT, we measure\nno degradation in throughput or convergence even with 12x longer sequences than\nstandard implementations. MsT is fully general, implementation-agnostic, and\nrequires minimal code changes to integrate with existing LLM training\nframeworks. Integrated with the huggingface library, MsT successfully extends\nthe maximum context length of Qwen, Mistral, and Gemma-2 by 12-24x.\n","authors":["Cheng Luo","Jiawei Zhao","Zhuoming Chen","Beidi Chen","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2407.15892v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14919v2","updated":"2024-10-31T16:36:14Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nstate-of-the-art performance in image generation by leveraging only a\npretrained diffusion model, without requiring any training data. However, the\nultimate performance of SiD is constrained by the accuracy with which the\npretrained model captures the true data scores at different stages of the\ndiffusion process. In this paper, we introduce SiDA (SiD with Adversarial\nLoss), which not only enhances generation quality but also improves\ndistillation efficiency by incorporating real images and adversarial loss. SiDA\nutilizes the encoder from the generator's score network as a discriminator,\nboosting its ability to distinguish between real images and those generated by\nSiD. The adversarial loss is batch-normalized within each GPU and then combined\nwith the original SiD loss. This integration effectively incorporates the\naverage \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA\nto distill a single-step generator either from scratch or by fine-tuning an\nexisting one. SiDA converges significantly faster than its predecessor when\ntrained from scratch, and swiftly improves upon the original model's\nperformance after an initial warmup period during fine-tuning from a\npre-distilled SiD generator. This one-step adversarial distillation method\nestablishes new benchmarks in generation performance when distilling EDM\ndiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving\nFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when\ndistilling EDM2 models trained on ImageNet (512x512), surpassing even the\nlargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156\nfor EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,\ndemonstrating significant improvements across all model sizes. Our open-source\ncode will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24105v1","updated":"2024-10-31T16:34:03Z","published":"2024-10-31T16:34:03Z","title":"Matchmaker: Self-Improving Large Language Model Programs for Schema\n  Matching","summary":"  Schema matching -- the task of finding matches between attributes across\ndisparate data sources with different tables and hierarchies -- is critical for\ncreating interoperable machine learning (ML)-ready data. Addressing this\nfundamental data-centric problem has wide implications, especially in domains\nlike healthcare, finance and e-commerce -- but also has the potential to\nbenefit ML models more generally, by increasing the data available for ML model\ntraining. However, schema matching is a challenging ML task due to\nstructural/hierarchical and semantic heterogeneity between different schemas.\nPrevious ML approaches to automate schema matching have either required\nsignificant labeled data for model training, which is often unrealistic or\nsuffer from poor zero-shot performance. To this end, we propose Matchmaker - a\ncompositional language model program for schema matching, comprised of\ncandidate generation, refinement and confidence scoring. Matchmaker also\nself-improves in a zero-shot manner without the need for labeled demonstrations\nvia a novel optimization approach, which constructs synthetic in-context\ndemonstrations to guide the language model's reasoning process. Empirically, we\ndemonstrate on real-world medical schema matching benchmarks that Matchmaker\noutperforms previous ML-based approaches, highlighting its potential to\naccelerate data integration and interoperability of ML-ready data.\n","authors":["Nabeel Seedat","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2410.24105v1.pdf","comment":"Accepted to NeurIPS 2024, GenAI for Health Workshop and Table\n  Representation Learning Workshop"},{"id":"http://arxiv.org/abs/2410.24104v1","updated":"2024-10-31T16:33:40Z","published":"2024-10-31T16:33:40Z","title":"Clustering to Minimize Cluster-Aware Norm Objectives","summary":"  We initiate the study of the following general clustering problem. We seek to\npartition a given set $P$ of data points into $k$ clusters by finding a set $X$\nof $k$ centers and assigning each data point to one of the centers. The cost of\na cluster, represented by a center $x\\in X$, is a monotone, symmetric norm $f$\n(inner norm) of the vector of distances of points assigned to $x$. The goal is\nto minimize a norm $g$ (outer norm) of the vector of cluster costs. This\nproblem, which we call $(f,g)$-Clustering, generalizes many fundamental\nclustering problems such as $k$-Center, $k$-Median , Min-Sum of Radii, and\nMin-Load $k$-Clustering . A recent line of research (Chakrabarty, Swamy\n[STOC'19]) studies norm objectives that are oblivious to the cluster structure\nsuch as $k$-Median and $k$-Center. In contrast, our problem models\ncluster-aware objectives including Min-Sum of Radii and Min-Load\n$k$-Clustering.\n  Our main results are as follows. First, we design a constant-factor\napproximation algorithm for $(\\textsf{top}_\\ell,\\mathcal{L}_1)$-Clustering\nwhere the inner norm ($\\textsf{top}_\\ell$) sums over the $\\ell$ largest\ndistances. Second, we design a constant-factor approximation\\ for\n$(\\mathcal{L}_\\infty,\\textsf{Ord})$-Clustering where the outer norm is a convex\ncombination of $\\textsf{top}_\\ell$ norms (ordered weighted norm).\n","authors":["Martin G. Herold","Evangelos Kipouridis","Joachim Spoerhase"],"pdf_url":"https://arxiv.org/pdf/2410.24104v1.pdf","comment":"accepted at SODA 2025"},{"id":"http://arxiv.org/abs/2410.24100v1","updated":"2024-10-31T16:30:08Z","published":"2024-10-31T16:30:08Z","title":"Benchmark Data Repositories for Better Benchmarking","summary":"  In machine learning research, it is common to evaluate algorithms via their\nperformance on standard benchmark datasets. While a growing body of work\nestablishes guidelines for -- and levies criticisms at -- data and benchmarking\npractices in machine learning, comparatively less attention has been paid to\nthe data repositories where these datasets are stored, documented, and shared.\nIn this paper, we analyze the landscape of these $\\textit{benchmark data\nrepositories}$ and the role they can play in improving benchmarking. This role\nincludes addressing issues with both datasets themselves (e.g.,\nrepresentational harms, construct validity) and the manner in which evaluation\nis carried out using such datasets (e.g., overemphasis on a few datasets and\nmetrics, lack of reproducibility). To this end, we identify and discuss a set\nof considerations surrounding the design and use of benchmark data\nrepositories, with a focus on improving benchmarking practices in machine\nlearning.\n","authors":["Rachel Longjohn","Markelle Kelly","Sameer Singh","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2410.24100v1.pdf","comment":"Accepted to NeurIPS Datasets and Benchmarks 2024"},{"id":"http://arxiv.org/abs/2403.11574v2","updated":"2024-10-31T16:29:10Z","published":"2024-03-18T08:50:30Z","title":"Offline Multitask Representation Learning for Reinforcement Learning","summary":"  We study offline multitask representation learning in reinforcement learning\n(RL), where a learner is provided with an offline dataset from different tasks\nthat share a common representation and is asked to learn the shared\nrepresentation. We theoretically investigate offline multitask low-rank RL, and\npropose a new algorithm called MORL for offline multitask representation\nlearning. Furthermore, we examine downstream RL in reward-free, offline and\nonline scenarios, where a new task is introduced to the agent that shares the\nsame representation as the upstream offline tasks. Our theoretical results\ndemonstrate the benefits of using the learned representation from the upstream\noffline task instead of directly learning the representation of the low-rank\nmodel.\n","authors":["Haque Ishfaq","Thanh Nguyen-Tang","Songtao Feng","Raman Arora","Mengdi Wang","Ming Yin","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2403.11574v2.pdf","comment":"Accepted to 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.24096v1","updated":"2024-10-31T16:28:33Z","published":"2024-10-31T16:28:33Z","title":"Progressive Safeguards for Safe and Model-Agnostic Reinforcement\n  Learning","summary":"  In this paper we propose a formal, model-agnostic meta-learning framework for\nsafe reinforcement learning. Our framework is inspired by how parents safeguard\ntheir children across a progression of increasingly riskier tasks, imparting a\nsense of safety that is carried over from task to task. We model this as a\nmeta-learning process where each task is synchronized with a safeguard that\nmonitors safety and provides a reward signal to the agent. The safeguard is\nimplemented as a finite-state machine based on a safety specification; the\nreward signal is formally shaped around this specification. The safety\nspecification and its corresponding safeguard can be arbitrarily complex and\nnon-Markovian, which adds flexibility to the training process and\nexplainability to the learned policy. The design of the safeguard is manual but\nit is high-level and model-agnostic, which gives rise to an end-to-end safe\nlearning approach with wide applicability, from pixel-level game control to\nlanguage model fine-tuning. Starting from a given set of safety specifications\n(tasks), we train a model such that it can adapt to new specifications using\nonly a small number of training samples. This is made possible by our method\nfor efficiently transferring safety bias between tasks, which effectively\nminimizes the number of safety violations. We evaluate our framework in a\nMinecraft-inspired Gridworld, a VizDoom game environment, and an LLM\nfine-tuning application. Agents trained with our approach achieve near-minimal\nsafety violations, while baselines are shown to underperform.\n","authors":["Nabil Omi","Hosein Hasanbeig","Hiteshi Sharma","Sriram K. Rajamani","Siddhartha Sen"],"pdf_url":"https://arxiv.org/pdf/2410.24096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24091v1","updated":"2024-10-31T16:22:53Z","published":"2024-10-31T16:22:53Z","title":"3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing","summary":"  Tactile and visual perception are both crucial for humans to perform\nfine-grained interactions with their environment. Developing similar\nmulti-modal sensing capabilities for robots can significantly enhance and\nexpand their manipulation skills. This paper introduces \\textbf{3D-ViTac}, a\nmulti-modal sensing and learning system designed for dexterous bimanual\nmanipulation. Our system features tactile sensors equipped with dense sensing\nunits, each covering an area of 3$mm^2$. These sensors are low-cost and\nflexible, providing detailed and extensive coverage of physical contacts,\neffectively complementing visual information. To integrate tactile and visual\ndata, we fuse them into a unified 3D representation space that preserves their\n3D structures and spatial relationships. The multi-modal representation can\nthen be coupled with diffusion policies for imitation learning. Through\nconcrete hardware experiments, we demonstrate that even low-cost robots can\nperform precise manipulations and significantly outperform vision-only\npolicies, particularly in safe interactions with fragile items and executing\nlong-horizon tasks involving in-hand manipulation. Our project page is\navailable at \\url{https://binghao-huang.github.io/3D-ViTac/}.\n","authors":["Binghao Huang","Yixuan Wang","Xinyi Yang","Yiyue Luo","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.24091v1.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.24089v1","updated":"2024-10-31T16:21:41Z","published":"2024-10-31T16:21:41Z","title":"Demystifying Linear MDPs and Novel Dynamics Aggregation Framework","summary":"  In this work, we prove that, in linear MDPs, the feature dimension $d$ is\nlower bounded by $S/U$ in order to aptly represent transition probabilities,\nwhere $S$ is the size of the state space and $U$ is the maximum size of\ndirectly reachable states. Hence, $d$ can still scale with $S$ depending on the\ndirect reachability of the environment. To address this limitation of linear\nMDPs, we propose a novel structural aggregation framework based on dynamics,\nnamed as the \"dynamics aggregation\". For this newly proposed framework, we\ndesign a provably efficient hierarchical reinforcement learning algorithm in\nlinear function approximation that leverages aggregated sub-structures. Our\nproposed algorithm exhibits statistical efficiency, achieving a regret of $\n\\tilde{O} ( d_{\\psi}^{3/2} H^{3/2}\\sqrt{ N T} )$, where $d_{\\psi}$ represents\nthe feature dimension of aggregated subMDPs and $N$ signifies the number of\naggregated subMDPs. We establish that the condition $d_{\\psi}^3 N \\ll d^{3}$ is\nreadily met in most real-world environments with hierarchical structures,\nenabling a substantial improvement in the regret bound compared to LSVI-UCB,\nwhich enjoys a regret of $ \\tilde{O} (d^{3/2} H^{3/2} \\sqrt{ T})$. To the best\nof our knowledge, this work presents the first HRL algorithm with linear\nfunction approximation that offers provable guarantees.\n","authors":["Joongkyu Lee","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2410.24089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24087v1","updated":"2024-10-31T16:20:04Z","published":"2024-10-31T16:20:04Z","title":"In-Context Fine-Tuning for Time-Series Foundation Models","summary":"  Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.\n","authors":["Abhimanyu Das","Matthew Faw","Rajat Sen","Yichen Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.24087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24079v1","updated":"2024-10-31T16:16:18Z","published":"2024-10-31T16:16:18Z","title":"Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models","summary":"  Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences.\n","authors":["Jinlin Lai","Daniel Sheldon","Justin Domke"],"pdf_url":"https://arxiv.org/pdf/2410.24079v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2409.17446v2","updated":"2024-10-31T16:16:00Z","published":"2024-09-26T00:38:18Z","title":"Efficient Federated Learning against Heterogeneous and Non-stationary\n  Client Unavailability","summary":"  Addressing intermittent client availability is critical for the real-world\ndeployment of federated learning algorithms. Most prior work either overlooks\nthe potential non-stationarity in the dynamics of client unavailability or\nrequires substantial memory/computation overhead. We study federated learning\nin the presence of heterogeneous and non-stationary client availability, which\nmay occur when the deployment environments are uncertain, or the clients are\nmobile. The impacts of heterogeneity and non-stationarity on client\nunavailability can be significant, as we illustrate using FedAvg, the most\nwidely adopted federated learning algorithm. We propose FedAPM, which includes\nnovel algorithmic structures that (i) compensate for missed computations due to\nunavailability with only $O(1)$ additional memory and computation with respect\nto standard FedAvg, and (ii) evenly diffuse local updates within the federated\nlearning system through implicit gossiping, despite being agnostic to\nnon-stationary dynamics. We show that FedAPM converges to a stationary point of\neven non-convex objectives while achieving the desired linear speedup property.\nWe corroborate our analysis with numerical experiments over diversified client\nunavailability dynamics on real-world data sets.\n","authors":["Ming Xiang","Stratis Ioannidis","Edmund Yeh","Carlee Joe-Wong","Lili Su"],"pdf_url":"https://arxiv.org/pdf/2409.17446v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17518v2","updated":"2024-10-31T16:10:42Z","published":"2024-10-23T02:50:05Z","title":"Univariate Conditional Variational Autoencoder for Morphogenic Patterns\n  Design in Frontal Polymerization-Based Manufacturing","summary":"  Under some initial and boundary conditions, the rapid reaction-thermal\ndiffusion process taking place during frontal polymerization (FP) destabilizes\nthe planar mode of front propagation, leading to spatially varying, complex\nhierarchical patterns in thermoset polymeric materials. Although modern\nreaction-diffusion models can predict the patterns resulting from unstable FP,\nthe inverse design of patterns, which aims to retrieve process conditions that\nproduce a desired pattern, remains an open challenge due to the non-unique and\nnon-intuitive mapping between process conditions and manufactured patterns. In\nthis work, we propose a probabilistic generative model named univariate\nconditional variational autoencoder (UcVAE) for the inverse design of\nhierarchical patterns in FP-based manufacturing. Unlike the cVAE, which encodes\nboth the design space and the design target, the UcVAE encodes only the design\nspace. In the encoder of the UcVAE, the number of training parameters is\nsignificantly reduced compared to the cVAE, resulting in a shorter training\ntime while maintaining comparable performance. Given desired pattern images,\nthe trained UcVAE can generate multiple process condition solutions that\nproduce high-fidelity hierarchical patterns.\n","authors":["Qibang Liu","Pengfei Cai","Diab Abueidda","Sagar Vyas","Seid Koric","Rafael Gomez-Bombarelli","Philippe Geubelle"],"pdf_url":"https://arxiv.org/pdf/2410.17518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24071v1","updated":"2024-10-31T16:07:22Z","published":"2024-10-31T16:07:22Z","title":"Local Linearity: the Key for No-regret Reinforcement Learning in\n  Continuous MDPs","summary":"  Achieving the no-regret property for Reinforcement Learning (RL) problems in\ncontinuous state and action-space environments is one of the major open\nproblems in the field. Existing solutions either work under very specific\nassumptions or achieve bounds that are vacuous in some regimes. Furthermore,\nmany structural assumptions are known to suffer from a provably unavoidable\nexponential dependence on the time horizon $H$ in the regret, which makes any\npossible solution unfeasible in practice. In this paper, we identify local\nlinearity as the feature that makes Markov Decision Processes (MDPs) both\nlearnable (sublinear regret) and feasible (regret that is polynomial in $H$).\nWe define a novel MDP representation class, namely Locally Linearizable MDPs,\ngeneralizing other representation classes like Linear MDPs and MDPS with low\ninherent Belmman error. Then, i) we introduce Cinderella, a no-regret algorithm\nfor this general representation class, and ii) we show that all known learnable\nand feasible MDP families are representable in this class. We first show that\nall known feasible MDPs belong to a family that we call Mildly Smooth MDPs.\nThen, we show how any mildly smooth MDP can be represented as a Locally\nLinearizable MDP by an appropriate choice of representation. This way,\nCinderella is shown to achieve state-of-the-art regret bounds for all\npreviously known (and some new) continuous MDPs for which RL is learnable and\nfeasible.\n","authors":["Davide Maran","Alberto Maria Metelli","Matteo Papini","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2410.24071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24070v1","updated":"2024-10-31T16:07:21Z","published":"2024-10-31T16:07:21Z","title":"Dynamical similarity analysis uniquely captures how computations develop\n  in RNNs","summary":"  Methods for analyzing representations in neural systems are increasingly\npopular tools in neuroscience and mechanistic interpretability. Measures\ncomparing neural activations across conditions, architectures, and species give\nscalable ways to understand information transformation within different neural\nnetworks. However, recent findings show that some metrics respond to spurious\nsignals, leading to misleading results. Establishing benchmark test cases is\nthus essential for identifying the most reliable metric and potential\nimprovements. We propose that compositional learning in recurrent neural\nnetworks (RNNs) can provide a test case for dynamical representation alignment\nmetrics. Implementing this case allows us to evaluate if metrics can identify\nrepresentations that develop throughout learning and determine if\nrepresentations identified by metrics reflect the network's actual\ncomputations. Building both attractor and RNN based test cases, we show that\nthe recently proposed Dynamical Similarity Analysis (DSA) is more noise robust\nand reliably identifies behaviorally relevant representations compared to prior\nmetrics (Procrustes, CKA). We also demonstrate how such test cases can extend\nbeyond metric evaluation to study new architectures. Specifically, testing DSA\nin modern (Mamba) state space models suggests that these models, unlike RNNs,\nmay not require changes in recurrent dynamics due to their expressive hidden\nstates. Overall, we develop test cases that showcase how DSA's enhanced ability\nto detect dynamical motifs makes it highly effective for identifying ongoing\ncomputations in RNNs and revealing how networks learn tasks.\n","authors":["Quentin Guilhot","Jascha Achterberg","Micha≈Ç W√≥jcik","Rui Ponte Costa"],"pdf_url":"https://arxiv.org/pdf/2410.24070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19238v2","updated":"2024-10-31T16:06:22Z","published":"2024-06-27T15:01:53Z","title":"Revealing Fine-Grained Values and Opinions in Large Language Models","summary":"  Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.\n","authors":["Dustin Wright","Arnav Arora","Nadav Borenstein","Srishti Yadav","Serge Belongie","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19238v2.pdf","comment":"Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2312.02119v3","updated":"2024-10-31T15:57:42Z","published":"2023-12-04T18:49:23Z","title":"Tree of Attacks: Jailbreaking Black-Box LLMs Automatically","summary":"  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.\n","authors":["Anay Mehrotra","Manolis Zampetakis","Paul Kassianik","Blaine Nelson","Hyrum Anderson","Yaron Singer","Amin Karbasi"],"pdf_url":"https://arxiv.org/pdf/2312.02119v3.pdf","comment":"Accepted for presentation at NeurIPS 2024. Code:\n  https://github.com/RICommunity/TAP"},{"id":"http://arxiv.org/abs/2402.10095v3","updated":"2024-10-31T15:57:39Z","published":"2024-02-15T16:49:42Z","title":"Classification Diffusion Models: Revitalizing Density Ratio Estimation","summary":"  A prominent family of methods for learning data distributions relies on\ndensity ratio estimation (DRE), where a model is trained to $\\textit{classify}$\nbetween data samples and samples from some reference distribution. DRE-based\nmodels can directly output the likelihood for any given input, a highly desired\nproperty that is lacking in most generative techniques. Nevertheless, to date,\nDRE methods have failed in accurately capturing the distributions of complex\nhigh-dimensional data, like images, and have thus been drawing reduced research\nattention in recent years. In this work we present $\\textit{classification\ndiffusion models}$ (CDMs), a DRE-based generative method that adopts the\nformalism of denoising diffusion models (DDMs) while making use of a classifier\nthat predicts the level of noise added to a clean signal. Our method is based\non an analytical connection that we derive between the MSE-optimal denoiser for\nremoving white Gaussian noise and the cross-entropy-optimal classifier for\npredicting the noise level. Our method is the first DRE-based technique that\ncan successfully generate images beyond the MNIST dataset. Furthermore, it can\noutput the likelihood of any input in a single forward pass, achieving\nstate-of-the-art negative log likelihood (NLL) among methods with this\nproperty. Code is available on the project's webpage in\nhttps://shaharYadin.github.io/CDM/ .\n","authors":["Shahar Yadin","Noam Elata","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2402.10095v3.pdf","comment":"Accepted for NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24060v1","updated":"2024-10-31T15:57:04Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24059v1","updated":"2024-10-31T15:56:50Z","published":"2024-10-31T15:56:50Z","title":"Identifying General Mechanism Shifts in Linear Causal Representations","summary":"  We consider the linear causal representation learning setting where we\nobserve a linear mixing of $d$ unknown latent factors, which follow a linear\nstructural causal model. Recent work has shown that it is possible to recover\nthe latent factors as well as the underlying structural causal model over them,\nup to permutation and scaling, provided that we have at least $d$ environments,\neach of which corresponds to perfect interventions on a single latent node\n(factor). After this powerful result, a key open problem faced by the community\nhas been to relax these conditions: allow for coarser than perfect single-node\ninterventions, and allow for fewer than $d$ of them, since the number of latent\nfactors $d$ could be very large. In this work, we consider precisely such a\nsetting, where we allow a smaller than $d$ number of environments, and also\nallow for very coarse interventions that can very coarsely \\textit{change the\nentire causal graph over the latent factors}. On the flip side, we relax what\nwe wish to extract to simply the \\textit{list of nodes that have shifted\nbetween one or more environments}. We provide a surprising identifiability\nresult that it is indeed possible, under some very mild standard assumptions,\nto identify the set of shifted nodes. Our identifiability proof moreover is a\nconstructive one: we explicitly provide necessary and sufficient conditions for\na node to be a shifted node, and show that we can check these conditions given\nobserved data. Our algorithm lends itself very naturally to the sample setting\nwhere instead of just interventional distributions, we are provided datasets of\nsamples from each of these distributions. We corroborate our results on both\nsynthetic experiments as well as an interesting psychometric dataset. The code\ncan be found at https://github.com/TianyuCodings/iLCS.\n","authors":["Tianyu Chen","Kevin Bello","Francesco Locatello","Bryon Aragam","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.24059v1.pdf","comment":"NeuIPS 2024"},{"id":"http://arxiv.org/abs/2402.01093v2","updated":"2024-10-31T15:56:08Z","published":"2024-02-02T01:45:18Z","title":"Need a Small Specialized Language Model? Plan Early!","summary":"  Large language models are versatile tools but are not suitable for small\ninference budgets. Small models have more efficient inference, but their lower\ncapacity means that their performance can be good only if one limits their\nscope to a specialized domain. This paper explores how to get good specialized\nsmall language models using a large, generic, pretraining set and a limited\namount of specialized data. We consider two scenarios, depending on whether (i)\none can afford pretraining a model for each specialization task, or (ii) one\nwants to cheaply adapt a single pretrained model for each task. In the first\nscenario, we propose an effective solution based on importance sampling: we\nresample the pretraining set to imitate the specialization data and train a\nsmall model on it. In the second scenario, we propose a novel architecture,\nprojected networks (PN). PN is a large network whose parameters can be linearly\nprojected into a small network for specialization. For both scenarios, we\ndemonstrate the empirical effectiveness of our solutions across various\ndomains, training set sizes, and training budgets.\n","authors":["David Grangier","Angelos Katharopoulos","Pierre Ablin","Awni Hannun"],"pdf_url":"https://arxiv.org/pdf/2402.01093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24058v1","updated":"2024-10-31T15:56:06Z","published":"2024-10-31T15:56:06Z","title":"Natural gradient and parameter estimation for quantum Boltzmann machines","summary":"  Thermal states play a fundamental role in various areas of physics, and they\nare becoming increasingly important in quantum information science, with\napplications related to semi-definite programming, quantum Boltzmann machine\nlearning, Hamiltonian learning, and the related task of estimating the\nparameters of a Hamiltonian. Here we establish formulas underlying the basic\ngeometry of parameterized thermal states, and we delineate quantum algorithms\nfor estimating the values of these formulas. More specifically, we prove\nformulas for the Fisher--Bures and Kubo--Mori information matrices of\nparameterized thermal states, and our quantum algorithms for estimating their\nmatrix elements involve a combination of classical sampling, Hamiltonian\nsimulation, and the Hadamard test. These results have applications in\ndeveloping a natural gradient descent algorithm for quantum Boltzmann machine\nlearning, which takes into account the geometry of thermal states, and in\nestablishing fundamental limitations on the ability to estimate the parameters\nof a Hamiltonian, when given access to thermal-state samples. For the latter\ntask, and for the special case of estimating a single parameter, we sketch an\nalgorithm that realizes a measurement that is asymptotically optimal for the\nestimation task. We finally stress that the natural gradient descent algorithm\ndeveloped here can be used for any machine learning problem that employs the\nquantum Boltzmann machine ansatz.\n","authors":["Dhrumil Patel","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2410.24058v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.18946v2","updated":"2024-10-31T15:53:15Z","published":"2024-09-27T17:46:05Z","title":"Unconditional stability of a recurrent neural circuit implementing\n  divisive normalization","summary":"  Stability in recurrent neural models poses a significant challenge,\nparticularly in developing biologically plausible neurodynamical models that\ncan be seamlessly trained. Traditional cortical circuit models are notoriously\ndifficult to train due to expansive nonlinearities in the dynamical system,\nleading to an optimization problem with nonlinear stability constraints that\nare difficult to impose. Conversely, recurrent neural networks (RNNs) excel in\ntasks involving sequential data but lack biological plausibility and\ninterpretability. In this work, we address these challenges by linking dynamic\ndivisive normalization (DN) to the stability of ORGaNICs, a biologically\nplausible recurrent cortical circuit model that dynamically achieves DN and\nthat has been shown to simulate a wide range of neurophysiological phenomena.\nBy using the indirect method of Lyapunov, we prove the remarkable property of\nunconditional local stability for an arbitrary-dimensional ORGaNICs circuit\nwhen the recurrent weight matrix is the identity. We thus connect ORGaNICs to a\nsystem of coupled damped harmonic oscillators, which enables us to derive the\ncircuit's energy function, providing a normative principle of what the circuit,\nand individual neurons, aim to accomplish. Further, for a generic recurrent\nweight matrix, we prove the stability of the 2D model and demonstrate\nempirically that stability holds in higher dimensions. Finally, we show that\nORGaNICs can be trained by backpropagation through time without gradient\nclipping/scaling, thanks to its intrinsic stability property and adaptive time\nconstants, which address the problems of exploding, vanishing, and oscillating\ngradients. By evaluating the model's performance on RNN benchmarks, we find\nthat ORGaNICs outperform alternative neurodynamical models on static image\nclassification tasks and perform comparably to LSTMs on sequential tasks.\n","authors":["Shivang Rawat","David J. Heeger","Stefano Martiniani"],"pdf_url":"https://arxiv.org/pdf/2409.18946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08557v2","updated":"2024-10-31T15:52:52Z","published":"2023-11-14T21:39:15Z","title":"Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges","summary":"  Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study reviews recent developments in low-light pedestrian\ndetection approaches. It systematically categorizes and analyses various\nalgorithms from region-based to non-region-based and graph-based learning\nmethodologies by highlighting their methodologies, implementation issues, and\nchallenges. It also outlines the key benchmark datasets that can be used for\nresearch and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations.\n","authors":["Thangarajah Akilan","Hrishikesh Vachhani"],"pdf_url":"https://arxiv.org/pdf/2311.08557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16978v2","updated":"2024-10-31T15:50:21Z","published":"2024-05-27T09:21:40Z","title":"OSLO: One-Shot Label-Only Membership Inference Attacks","summary":"  We introduce One-Shot Label-Only (OSLO) membership inference attacks (MIAs),\nwhich accurately infer a given sample's membership in a target model's training\nset with high precision using just \\emph{a single query}, where the target\nmodel only returns the predicted hard label. This is in contrast to\nstate-of-the-art label-only attacks which require $\\sim6000$ queries, yet get\nattack precisions lower than OSLO's. OSLO leverages transfer-based black-box\nadversarial attacks. The core idea is that a member sample exhibits more\nresistance to adversarial perturbations than a non-member. We compare OSLO\nagainst state-of-the-art label-only attacks and demonstrate that, despite\nrequiring only one query, our method significantly outperforms previous attacks\nin terms of precision and true positive rate (TPR) under the same false\npositive rates (FPR). For example, compared to previous label-only MIAs, OSLO\nachieves a TPR that is at least 7$\\times$ higher under a 1\\% FPR and at least\n22$\\times$ higher under a 0.1\\% FPR on CIFAR100 for a ResNet18 model. We\nevaluated multiple defense mechanisms against OSLO.\n","authors":["Yuefeng Peng","Jaechul Roh","Subhransu Maji","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2405.16978v2.pdf","comment":"To appear at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24055v1","updated":"2024-10-31T15:48:36Z","published":"2024-10-31T15:48:36Z","title":"Advanced Predictive Quality Assessment for Ultrasonic Additive\n  Manufacturing with Deep Learning Model","summary":"  Ultrasonic Additive Manufacturing (UAM) employs ultrasonic welding to bond\nsimilar or dissimilar metal foils to a substrate, resulting in solid,\nconsolidated metal components. However, certain processing conditions can lead\nto inter-layer defects, affecting the final product's quality. This study\ndevelops a method to monitor in-process quality using deep learning-based\nconvolutional neural networks (CNNs). The CNN models were evaluated on their\nability to classify samples with and without embedded thermocouples across five\npower levels (300W, 600W, 900W, 1200W, 1500W) using thermal images with\nsupervised labeling. Four distinct CNN classification models were created for\ndifferent scenarios including without (baseline) and with thermocouples, only\nwithout thermocouples across power levels, only with thermocouples across power\nlevels, and combined without and with thermocouples across power levels. The\nmodels achieved 98.29% accuracy on combined baseline and thermocouple images,\n97.10% for baseline images across power levels, 97.43% for thermocouple images,\nand 97.27% for both types across power levels. The high accuracy, above 97%,\ndemonstrates the system's effectiveness in identifying and classifying\nconditions within the UAM process, providing a reliable tool for quality\nassurance and process control in manufacturing environments.\n","authors":["Lokendra Poudel","Sushant Jha","Ryan Meeker","Duy-Nhat Phan","Rahul Bhowmik"],"pdf_url":"https://arxiv.org/pdf/2410.24055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24054v1","updated":"2024-10-31T15:48:34Z","published":"2024-10-31T15:48:34Z","title":"EigenVI: score-based variational inference with orthogonal function\n  expansions","summary":"  We develop EigenVI, an eigenvalue-based approach for black-box variational\ninference (BBVI). EigenVI constructs its variational approximations from\northogonal function expansions. For distributions over $\\mathbb{R}^D$, the\nlowest order term in these expansions provides a Gaussian variational\napproximation, while higher-order terms provide a systematic way to model\nnon-Gaussianity. These approximations are flexible enough to model complex\ndistributions (multimodal, asymmetric), but they are simple enough that one can\ncalculate their low-order moments and draw samples from them. EigenVI can also\nmodel other types of random variables (e.g., nonnegative, bounded) by\nconstructing variational approximations from different families of orthogonal\nfunctions. Within these families, EigenVI computes the variational\napproximation that best matches the score function of the target distribution\nby minimizing a stochastic estimate of the Fisher divergence. Notably, this\noptimization reduces to solving a minimum eigenvalue problem, so that EigenVI\neffectively sidesteps the iterative gradient-based optimizations that are\nrequired for many other BBVI algorithms. (Gradient-based methods can be\nsensitive to learning rates, termination criteria, and other tunable\nhyperparameters.) We use EigenVI to approximate a variety of target\ndistributions, including a benchmark suite of Bayesian models from posteriordb.\nOn these distributions, we find that EigenVI is more accurate than existing\nmethods for Gaussian BBVI.\n","authors":["Diana Cai","Chirag Modi","Charles C. Margossian","Robert M. Gower","David M. Blei","Lawrence K. Saul"],"pdf_url":"https://arxiv.org/pdf/2410.24054v1.pdf","comment":"25 pages, 9 figures. Advances in Neural Information Processing\n  Systems (NeurIPS), 2024"},{"id":"http://arxiv.org/abs/2410.24052v1","updated":"2024-10-31T15:47:50Z","published":"2024-10-31T15:47:50Z","title":"Attention is All You Need to Optimize Wind Farm Operations and\n  Maintenance","summary":"  Operations and maintenance (O&M) is a fundamental problem in wind energy\nsystems with far reaching implications for reliability and profitability.\nOptimizing O&M is a multi-faceted decision optimization problem that requires a\ncareful balancing act across turbine level failure risks, operational revenues,\nand maintenance crew logistics. The resulting O&M problems are typically solved\nusing large-scale mixed integer programming (MIP) models, which yield\ncomputationally challenging problems that require either long-solution times,\nor heuristics to reach a solution. To address this problem, we introduce a\nnovel decision-making framework for wind farm O&M that builds on a multi-head\nattention (MHA) models, an emerging artificial intelligence methods that are\nspecifically designed to learn in rich and complex problem settings. The\ndevelopment of proposed MHA framework incorporates a number of modeling\ninnovations that allows explicit embedding of MIP models within an MHA\nstructure. The proposed MHA model (i) significantly reduces the solution time\nfrom hours to seconds, (ii) guarantees feasibility of the proposed solutions\nconsidering complex constraints that are omnipresent in wind farm O&M, (iii)\nresults in significant solution quality compared to the conventional MIP\nformulations, and (iv) exhibits significant transfer learning capability across\ndifferent problem settings.\n","authors":["Iman Kazemian","Murat Yildirim","Paritosh Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.24052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24050v1","updated":"2024-10-31T15:46:10Z","published":"2024-10-31T15:46:10Z","title":"A Visual Case Study of the Training Dynamics in Neural Networks","summary":"  This paper introduces a visual sandbox designed to explore the training\ndynamics of a small-scale transformer model, with the embedding dimension\nconstrained to $d=2$. This restriction allows for a comprehensive\ntwo-dimensional visualization of each layer's dynamics. Through this approach,\nwe gain insights into training dynamics, circuit transferability, and the\ncauses of loss spikes, including those induced by the high curvature of\nnormalization layers. We propose strategies to mitigate these spikes,\ndemonstrating how good visualization facilitates the design of innovative ideas\nof practical interest. Additionally, we believe our sandbox could assist\ntheoreticians in assessing essential training dynamics mechanisms and\nintegrating them into future theories. The code is available at\nhttps://github.com/facebookresearch/pal.\n","authors":["Ambroise Odonnat","Wassim Bouaziz","Vivien Cabannes"],"pdf_url":"https://arxiv.org/pdf/2410.24050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09168v2","updated":"2024-10-31T15:44:36Z","published":"2024-06-13T14:30:35Z","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution","summary":"  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes at the cellular\nand subcellular levels. Scanning confocal microscopy allows the capture of\nhigh-quality images from thick three-dimensional (3D) samples, yet suffers from\nwell-known limitations such as photobleaching and phototoxicity of specimens\ncaused by intense light exposure, limiting its applications. Cellular damage\ncan be alleviated by changing imaging parameters to reduce light exposure,\noften at the expense of image quality. Machine/deep learning methods for\nsingle-image super-resolution (SISR) can be applied to restore image quality by\nupscaling lower-resolution (LR) images to yield high-resolution images (HR).\nThese SISR methods have been successfully applied to photo-realistic images due\npartly to the abundance of publicly available data. In contrast, the lack of\npublicly available data partly limits their application and success in scanning\nconfocal microscopy. In this paper, we introduce a large scanning confocal\nmicroscopy dataset named SR-CACO-2 that is comprised of low- and\nhigh-resolution image pairs marked for three different fluorescent markers. It\nallows the evaluation of performance of SISR methods on three different\nupscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell\nline Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured\nwith four resolutions and three markers, forming 9,937 image patches for SISR\nmethods. We provide benchmarking results for 16 state-of-the-art methods of the\nmain SISR families. Results show that these methods have limited success in\nproducing high-resolution textures. The dataset is freely accessible under a\nCreative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained\nweights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.\n","authors":["Soufiane Belharbi","Mara KM Whitford","Phuong Hoang","Shakeeb Murtaza","Luke McCaffrey","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2406.09168v2.pdf","comment":"27 pages, 15 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.18457v3","updated":"2024-10-31T15:44:22Z","published":"2024-05-28T16:58:37Z","title":"Improving Linear System Solvers for Hyperparameter Optimisation in\n  Iterative Gaussian Processes","summary":"  Scaling hyperparameter optimisation to very large datasets remains an open\nproblem in the Gaussian process community. This paper focuses on iterative\nmethods, which use linear system solvers, like conjugate gradients, alternating\nprojections or stochastic gradient descent, to construct an estimate of the\nmarginal likelihood gradient. We discuss three key improvements which are\napplicable across solvers: (i) a pathwise gradient estimator, which reduces the\nrequired number of solver iterations and amortises the computational cost of\nmaking predictions, (ii) warm starting linear system solvers with the solution\nfrom the previous step, which leads to faster solver convergence at the cost of\nnegligible bias, (iii) early stopping linear system solvers after a limited\ncomputational budget, which synergises with warm starting, allowing solver\nprogress to accumulate over multiple marginal likelihood steps. These\ntechniques provide speed-ups of up to $72\\times$ when solving to tolerance, and\ndecrease the average residual norm by up to $7\\times$ when stopping early.\n","authors":["Jihao Andreas Lin","Shreyas Padhy","Bruno Mlodozeniec","Javier Antor√°n","Jos√© Miguel Hern√°ndez-Lobato"],"pdf_url":"https://arxiv.org/pdf/2405.18457v3.pdf","comment":"Advances in Neural Information Processing Systems 2024"},{"id":"http://arxiv.org/abs/2311.08376v3","updated":"2024-10-31T15:42:51Z","published":"2023-11-14T18:41:28Z","title":"Ensemble sampling for linear bandits: small ensembles suffice","summary":"  We provide the first useful and rigorous analysis of ensemble sampling for\nthe stochastic linear bandit setting. In particular, we show that, under\nstandard assumptions, for a $d$-dimensional stochastic linear bandit with an\ninteraction horizon $T$, ensemble sampling with an ensemble of size of order\n$\\smash{d \\log T}$ incurs regret at most of the order $\\smash{(d \\log T)^{5/2}\n\\sqrt{T}}$. Ours is the first result in any structured setting not to require\nthe size of the ensemble to scale linearly with $T$ -- which defeats the\npurpose of ensemble sampling -- while obtaining near $\\smash{\\sqrt{T}}$ order\nregret. Ours is also the first result that allows infinite action sets.\n","authors":["David Janz","Alexander E. Litvak","Csaba Szepesv√°ri"],"pdf_url":"https://arxiv.org/pdf/2311.08376v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24046v1","updated":"2024-10-31T15:42:24Z","published":"2024-10-31T15:42:24Z","title":"Deep Learning with HM-VGG: AI Strategies for Multi-modal Image Analysis","summary":"  This study introduces the Hybrid Multi-modal VGG (HM-VGG) model, a\ncutting-edge deep learning approach for the early diagnosis of glaucoma. The\nHM-VGG model utilizes an attention mechanism to process Visual Field (VF) data,\nenabling the extraction of key features that are vital for identifying early\nsigns of glaucoma. Despite the common reliance on large annotated datasets, the\nHM-VGG model excels in scenarios with limited data, achieving remarkable\nresults with small sample sizes. The model's performance is underscored by its\nhigh metrics in Precision, Accuracy, and F1-Score, indicating its potential for\nreal-world application in glaucoma detection. The paper also discusses the\nchallenges associated with ophthalmic image analysis, particularly the\ndifficulty of obtaining large volumes of annotated data. It highlights the\nimportance of moving beyond single-modality data, such as VF or Optical\nCoherence Tomography (OCT) images alone, to a multimodal approach that can\nprovide a richer, more comprehensive dataset. This integration of different\ndata types is shown to significantly enhance diagnostic accuracy. The HM- VGG\nmodel offers a promising tool for doctors, streamlining the diagnostic process\nand improving patient outcomes. Furthermore, its applicability extends to\ntelemedicine and mobile healthcare, making diagnostic services more accessible.\nThe research presented in this paper is a significant step forward in the field\nof medical image processing and has profound implications for clinical\nophthalmology.\n","authors":["Junliang Du","Yiru Cang","Tong Zhou","Jiacheng Hu","Weijie He"],"pdf_url":"https://arxiv.org/pdf/2410.24046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22114v2","updated":"2024-10-31T15:34:35Z","published":"2024-10-29T15:16:02Z","title":"Policy Gradient for Robust Markov Decision Processes","summary":"  We develop a generic policy gradient method with the global optimality\nguarantee for robust Markov Decision Processes (MDPs). While policy gradient\nmethods are widely used for solving dynamic decision problems due to their\nscalable and efficient nature, adapting these methods to account for model\nambiguity has been challenging, often making it impractical to learn robust\npolicies. This paper introduces a novel policy gradient method, Double-Loop\nRobust Policy Mirror Descent (DRPMD), for solving robust MDPs. DRPMD employs a\ngeneral mirror descent update rule for the policy optimization with adaptive\ntolerance per iteration, guaranteeing convergence to a globally optimal policy.\nWe provide a comprehensive analysis of DRPMD, including new convergence results\nunder both direct and softmax parameterizations, and provide novel insights\ninto the inner problem solution through Transition Mirror Ascent (TMA).\nAdditionally, we propose innovative parametric transition kernels for both\ndiscrete and continuous state-action spaces, broadening the applicability of\nour approach. Empirical results validate the robustness and global convergence\nof DRPMD across various challenging robust MDP settings.\n","authors":["Qiuhao Wang","Shaohang Xu","Chin Pang Ho","Marek Petrik"],"pdf_url":"https://arxiv.org/pdf/2410.22114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24035v1","updated":"2024-10-31T15:32:32Z","published":"2024-10-31T15:32:32Z","title":"State- and context-dependent robotic manipulation and grasping via\n  uncertainty-aware imitation learning","summary":"  Generating context-adaptive manipulation and grasping actions is a\nchallenging problem in robotics. Classical planning and control algorithms tend\nto be inflexible with regard to parameterization by external variables such as\nobject shapes. In contrast, Learning from Demonstration (LfD) approaches, due\nto their nature as function approximators, allow for introducing external\nvariables to modulate policies in response to the environment. In this paper,\nwe utilize this property by introducing an LfD approach to acquire\ncontext-dependent grasping and manipulation strategies. We treat the problem as\na kernel-based function approximation, where the kernel inputs include generic\ncontext variables describing task-dependent parameters such as the object\nshape. We build on existing work on policy fusion with uncertainty\nquantification to propose a state-dependent approach that automatically returns\nto demonstrations, avoiding unpredictable behavior while smoothly adapting to\ncontext changes. The approach is evaluated against the LASA handwriting dataset\nand on a real 7-DoF robot in two scenarios: adaptation to slippage while\ngrasping and manipulating a deformable food item.\n","authors":["Tim R. Winter","Ashok M. Sundaram","Werner Friedl","Maximo A. Roa","Freek Stulp","Jo√£o Silv√©rio"],"pdf_url":"https://arxiv.org/pdf/2410.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10490v2","updated":"2024-10-31T15:28:49Z","published":"2024-06-15T04:03:12Z","title":"Active, anytime-valid risk controlling prediction sets","summary":"  Rigorously establishing the safety of black-box machine learning models\nconcerning critical risk measures is important for providing guarantees about\nmodel behavior. Recently, Bates et. al. (JACM '24) introduced the notion of a\nrisk controlling prediction set (RCPS) for producing prediction sets that are\nstatistically guaranteed low risk from machine learning models. Our method\nextends this notion to the sequential setting, where we provide guarantees even\nwhen the data is collected adaptively, and ensures that the risk guarantee is\nanytime-valid, i.e., simultaneously holds at all time steps. Further, we\npropose a framework for constructing RCPSes for active labeling, i.e., allowing\none to use a labeling policy that chooses whether to query the true label for\neach received data point and ensures that the expected proportion of data\npoints whose labels are queried are below a predetermined label budget. We also\ndescribe how to use predictors (i.e., the machine learning model for which we\nprovide risk control guarantees) to further improve the utility of our RCPSes\nby estimating the expected risk conditioned on the covariates. We characterize\nthe optimal choices of label policy and predictor under a fixed label budget\nand show a regret result that relates the estimation error of the optimal\nlabeling policy and predictor to the wealth process that underlies our RCPSes.\nLastly, we present practical ways of formulating label policies and empirically\nshow that our label policies use fewer labels to reach higher utility than\nnaive baseline labeling strategies on both simulations and real data.\n","authors":["Ziyu Xu","Nikos Karampatziakis","Paul Mineiro"],"pdf_url":"https://arxiv.org/pdf/2406.10490v2.pdf","comment":"22 pages, 3 figures. Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.24029v1","updated":"2024-10-31T15:28:26Z","published":"2024-10-31T15:28:26Z","title":"Joint Training for Selective Prediction","summary":"  Classifier models are prevalent in natural language processing (NLP), often\nwith high accuracy. Yet in real world settings, human-in-the-loop systems can\nfoster trust in model outputs and even higher performance. Selective Prediction\n(SP) methods determine when to adopt a classifier's output versus defer to a\nhuman. Previous SP approaches have addressed how to improve softmax as a\nmeasure of model confidence, or have developed separate confidence estimators.\nOne previous method involves learning a deferral model based on engineered\nfeatures. We introduce a novel joint-training approach that simultaneously\noptimizes learned representations used by the classifier module and a learned\ndeferral policy. Our results on four classification tasks demonstrate that\njoint training not only leads to better SP outcomes over two strong baselines,\nbut also improves the performance of both modules.\n","authors":["Zhaohui Li","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2410.24029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24028v1","updated":"2024-10-31T15:28:22Z","published":"2024-10-31T15:28:22Z","title":"AdaFlow: Opportunistic Inference on Asynchronous Mobile Data with\n  Generalized Affinity Control","summary":"  The rise of mobile devices equipped with numerous sensors, such as LiDAR and\ncameras, has spurred the adoption of multi-modal deep intelligence for\ndistributed sensing tasks, such as smart cabins and driving assistance.\nHowever, the arrival times of mobile sensory data vary due to modality size and\nnetwork dynamics, which can lead to delays (if waiting for slower data) or\naccuracy decline (if inference proceeds without waiting). Moreover, the\ndiversity and dynamic nature of mobile systems exacerbate this challenge. In\nresponse, we present a shift to \\textit{opportunistic} inference for\nasynchronous distributed multi-modal data, enabling inference as soon as\npartial data arrives. While existing methods focus on optimizing modality\nconsistency and complementarity, known as modal affinity, they lack a\n\\textit{computational} approach to control this affinity in open-world mobile\nenvironments. AdaFlow pioneers the formulation of structured cross-modality\naffinity in mobile contexts using a hierarchical analysis-based normalized\nmatrix. This approach accommodates the diversity and dynamics of modalities,\ngeneralizing across different types and numbers of inputs. Employing an\naffinity attention-based conditional GAN (ACGAN), AdaFlow facilitates flexible\ndata imputation, adapting to various modalities and downstream tasks without\nretraining. Experiments show that AdaFlow significantly reduces inference\nlatency by up to 79.9\\% and enhances accuracy by up to 61.9\\%, outperforming\nstatus quo approaches.\n","authors":["Fenmin Wu","Sicong Liu","Kehao Zhu","Xiaochen Li","Bin Guo","Zhiwen Yu","Hongkai Wen","Xiangrui Xu","Lehao Wang","Xiangyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10881v2","updated":"2024-10-31T15:27:40Z","published":"2024-04-16T20:01:10Z","title":"Differentially Private Optimization with Sparse Gradients","summary":"  Motivated by applications of large embedding models, we study differentially\nprivate (DP) optimization problems under sparsity of individual gradients. We\nstart with new near-optimal bounds for the classic mean estimation problem but\nwith sparse data, improving upon existing algorithms particularly for the\nhigh-dimensional regime. Building on this, we obtain pure- and approximate-DP\nalgorithms with almost optimal rates for stochastic convex optimization with\nsparse gradients; the former represents the first nearly dimension-independent\nrates for this problem. Finally, we study the approximation of stationary\npoints for the empirical loss in approximate-DP optimization and obtain rates\nthat depend on sparsity instead of dimension, modulo polylogarithmic factors.\n","authors":["Badih Ghazi","Crist√≥bal Guzm√°n","Pritish Kamath","Ravi Kumar","Pasin Manurangsi"],"pdf_url":"https://arxiv.org/pdf/2404.10881v2.pdf","comment":"Minor corrections and re-structuring of the presentation"},{"id":"http://arxiv.org/abs/2410.24023v1","updated":"2024-10-31T15:23:34Z","published":"2024-10-31T15:23:34Z","title":"Approximate attention with MLP: a pruning strategy for attention-based\n  model in multivariate time series forecasting","summary":"  Attention-based architectures have become ubiquitous in time series\nforecasting tasks, including spatio-temporal (STF) and long-term time series\nforecasting (LTSF). Yet, our understanding of the reasons for their\neffectiveness remains limited. This work proposes a new way to understand\nself-attention networks: we have shown empirically that the entire attention\nmechanism in the encoder can be reduced to an MLP formed by feedforward,\nskip-connection, and layer normalization operations for temporal and/or spatial\nmodeling in multivariate time series forecasting. Specifically, the Q, K, and V\nprojection, the attention score calculation, the dot-product between the\nattention score and the V, and the final projection can be removed from the\nattention-based networks without significantly degrading the performance that\nthe given network remains the top-tier compared to other SOTA methods. For\nspatio-temporal networks, the MLP-replace-attention network achieves a\nreduction in FLOPS of $62.579\\%$ with a loss in performance less than $2.5\\%$;\nfor LTSF, a reduction in FLOPs of $42.233\\%$ with a loss in performance less\nthan $2\\%$.\n","authors":["Suhan Guo","Jiahong Deng","Yi Wei","Hui Dou","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.24023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16510v2","updated":"2024-10-31T15:23:13Z","published":"2023-03-29T07:36:54Z","title":"Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms\n  for Optimization under Orthogonality Constraints","summary":"  Orthogonality constraints naturally appear in many machine learning problems,\nfrom principal component analysis to robust neural network training. They are\nusually solved using Riemannian optimization algorithms, which minimize the\nobjective function while enforcing the constraint. However, enforcing the\northogonality constraint can be the most time-consuming operation in such\nalgorithms. Recently, Ablin & Peyr\\'e (2022) proposed the landing algorithm, a\nmethod with cheap iterations that does not enforce the orthogonality\nconstraints but is attracted towards the manifold in a smooth manner. This\narticle provides new practical and theoretical developments for the landing\nalgorithm. First, the method is extended to the Stiefel manifold, the set of\nrectangular orthogonal matrices. We also consider stochastic and variance\nreduction algorithms when the cost function is an average of many functions. We\ndemonstrate that all these methods have the same rate of convergence as their\nRiemannian counterparts that exactly enforce the constraint, and converge to\nthe manifold. Finally, our experiments demonstrate the promise of our approach\nto an array of machine-learning problems that involve orthogonality\nconstraints.\n","authors":["Pierre Ablin","Simon Vary","Bin Gao","P. -A. Absil"],"pdf_url":"https://arxiv.org/pdf/2303.16510v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24022v1","updated":"2024-10-31T15:22:03Z","published":"2024-10-31T15:22:03Z","title":"SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced\n  Protein Sequence Representation","summary":"  Proteins, essential to biological systems, perform functions intricately\nlinked to their three-dimensional structures. Understanding the relationship\nbetween protein structures and their amino acid sequences remains a core\nchallenge in protein modeling. While traditional protein foundation models\nbenefit from pre-training on vast unlabeled datasets, they often struggle to\ncapture critical co-evolutionary information, which evolutionary-based methods\nexcel at. In this study, we introduce a novel pre-training strategy for protein\nfoundation models that emphasizes the interactions among amino acid residues to\nenhance the extraction of both short-range and long-range co-evolutionary\nfeatures from sequence data. Trained on a large-scale protein sequence dataset,\nour model demonstrates superior generalization ability, outperforming\nestablished baselines of similar size, including the ESM model, across diverse\ndownstream tasks. Experimental results confirm the model's effectiveness in\nintegrating co-evolutionary information, marking a significant step forward in\nprotein sequence-based modeling.\n","authors":["Liang He","Peiran Jin","Yaosen Min","Shufang Xie","Lijun Wu","Tao Qin","Xiaozhuan Liang","Kaiyuan Gao","Yuliang Jiang","Tie-Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24018v1","updated":"2024-10-31T15:20:43Z","published":"2024-10-31T15:20:43Z","title":"Bayesian-guided Label Mapping for Visual Reprogramming","summary":"  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained\nvision models by adapting their input or output interfaces to solve downstream\ntasks whose labels (i.e., downstream labels) might be totally different from\nthe labels associated with the pretrained models (i.e., pretrained labels).\nWhen adapting the output interface, label mapping methods transform the\npretrained labels to downstream labels by establishing a gradient-free\none-to-one correspondence between the two sets of labels. However, in this\npaper, we reveal that one-to-one mappings may overlook the complex relationship\nbetween pretrained and downstream labels. Motivated by this observation, we\npropose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an\niteratively-updated probabilistic label mapping matrix, with each element\nquantifying a pairwise relationship between pretrained and downstream labels.\nThe assignment of values to the constructed matrix is guided by Bayesian\nconditional probability, considering the joint distribution of the downstream\nlabels and the labels predicted by the pretrained model on downstream samples.\nExperiments conducted on both pretrained vision models (e.g., ResNeXt) and\nvision-language models (e.g., CLIP) demonstrate the superior performance of BLM\nover existing label mapping methods. The success of BLM also offers a\nprobabilistic lens through which to understand and analyze the effectiveness of\nVR. Our code is available at https://github.com/tmlr-group/BayesianLM.\n","authors":["Chengyi Cai","Zesheng Ye","Lei Feng","Jianzhong Qi","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.24018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24016v1","updated":"2024-10-31T15:18:37Z","published":"2024-10-31T15:18:37Z","title":"Maximum Entropy Hindsight Experience Replay","summary":"  Hindsight experience replay (HER) is well-known to accelerate goal-based\nreinforcement learning (RL). While HER is generally applied to off-policy RL\nalgorithms, we previously showed that HER can also accelerate on-policy\nalgorithms, such as proximal policy optimization (PPO), for goal-based\nPredator-Prey environments. Here, we show that we can improve the previous\nPPO-HER algorithm by selectively applying HER in a principled manner.\n","authors":["Douglas C. Crowder","Matthew L. Trappett","Darrien M. McKenzie","Frances S. Chance"],"pdf_url":"https://arxiv.org/pdf/2410.24016v1.pdf","comment":"11 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2410.24012v1","updated":"2024-10-31T15:13:11Z","published":"2024-10-31T15:13:11Z","title":"Diffusion Twigs with Loop Guidance for Conditional Graph Generation","summary":"  We introduce a novel score-based diffusion framework named Twigs that\nincorporates multiple co-evolving flows for enriching conditional generation\ntasks. Specifically, a central or trunk diffusion process is associated with a\nprimary variable (e.g., graph structure), and additional offshoot or stem\nprocesses are dedicated to dependent variables (e.g., graph properties or\nlabels). A new strategy, which we call loop guidance, effectively orchestrates\nthe flow of information between the trunk and the stem processes during\nsampling. This approach allows us to uncover intricate interactions and\ndependencies, and unlock new generative capabilities. We provide extensive\nexperiments to demonstrate strong performance gains of the proposed method over\ncontemporary baselines in the context of conditional graph generation,\nunderscoring the potential of Twigs in challenging generative tasks such as\ninverse molecular design and molecular optimization.\n","authors":["Giangiacomo Mercatali","Yogesh Verma","Andre Freitas","Vikas Garg"],"pdf_url":"https://arxiv.org/pdf/2410.24012v1.pdf","comment":"NeurIPS 2024. Code is available at\n  https://github.com/Aalto-QuML/Diffusion_twigs"},{"id":"http://arxiv.org/abs/2410.24006v1","updated":"2024-10-31T15:09:36Z","published":"2024-10-31T15:09:36Z","title":"DiffPAD: Denoising Diffusion-based Adversarial Patch Decontamination","summary":"  In the ever-evolving adversarial machine learning landscape, developing\neffective defenses against patch attacks has become a critical challenge,\nnecessitating reliable solutions to safeguard real-world AI systems. Although\ndiffusion models have shown remarkable capacity in image synthesis and have\nbeen recently utilized to counter $\\ell_p$-norm bounded attacks, their\npotential in mitigating localized patch attacks remains largely underexplored.\nIn this work, we propose DiffPAD, a novel framework that harnesses the power of\ndiffusion models for adversarial patch decontamination. DiffPAD first performs\nsuper-resolution restoration on downsampled input images, then adopts\nbinarization, dynamic thresholding scheme and sliding window for effective\nlocalization of adversarial patches. Such a design is inspired by the\ntheoretically derived correlation between patch size and diffusion restoration\nerror that is generalized across diverse patch attack scenarios. Finally,\nDiffPAD applies inpainting techniques to the original input images with the\nestimated patch region being masked. By integrating closed-form solutions for\nsuper-resolution restoration and image inpainting into the conditional reverse\nsampling process of a pre-trained diffusion model, DiffPAD obviates the need\nfor text guidance or fine-tuning. Through comprehensive experiments, we\ndemonstrate that DiffPAD not only achieves state-of-the-art adversarial\nrobustness against patch attacks but also excels in recovering naturalistic\nimages without patch remnants.\n","authors":["Jia Fu","Xiao Zhang","Sepideh Pashami","Fatemeh Rahimian","Anders Holst"],"pdf_url":"https://arxiv.org/pdf/2410.24006v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2410.24005v1","updated":"2024-10-31T15:06:16Z","published":"2024-10-31T15:06:16Z","title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models","summary":"  The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.\n","authors":["Paulius Rauba","Nabeel Seedat","Max Ruiz Luyten","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2410.24005v1.pdf","comment":"Presented at the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024). *Rauba & Seedat contributed equally"},{"id":"http://arxiv.org/abs/2405.15010v2","updated":"2024-10-31T15:03:06Z","published":"2024-05-23T19:29:38Z","title":"Parameter-free Clipped Gradient Descent Meets Polyak","summary":"  Gradient descent and its variants are de facto standard algorithms for\ntraining machine learning models. As gradient descent is sensitive to its\nhyperparameters, we need to tune the hyperparameters carefully using a grid\nsearch. However, the method is time-consuming, particularly when multiple\nhyperparameters exist. Therefore, recent studies have analyzed parameter-free\nmethods that adjust the hyperparameters on the fly. However, the existing work\nis limited to investigations of parameter-free methods for the stepsize, and\nparameter-free methods for other hyperparameters have not been explored. For\ninstance, although the gradient clipping threshold is a crucial hyperparameter\nin addition to the stepsize for preventing gradient explosion issues, none of\nthe existing studies have investigated parameter-free methods for clipped\ngradient descent. Therefore, in this study, we investigate the parameter-free\nmethods for clipped gradient descent. Specifically, we propose Inexact Polyak\nStepsize, which converges to the optimal solution without any hyperparameters\ntuning, and its convergence rate is asymptotically independent of $L$ under\n$L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to\nthat of clipped gradient descent with well-tuned hyperparameters. We\nnumerically validated our convergence results using a synthetic function and\ndemonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT,\nand T5.\n","authors":["Yuki Takezawa","Han Bao","Ryoma Sato","Kenta Niwa","Makoto Yamada"],"pdf_url":"https://arxiv.org/pdf/2405.15010v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23996v1","updated":"2024-10-31T14:57:31Z","published":"2024-10-31T14:57:31Z","title":"An Information Criterion for Controlled Disentanglement of Multimodal\n  Data","summary":"  Multimodal representation learning seeks to relate and decompose information\ninherent in multiple modalities. By disentangling modality-specific information\nfrom information that is shared across modalities, we can improve\ninterpretability and robustness and enable downstream tasks such as the\ngeneration of counterfactual outcomes. Separating the two types of information\nis challenging since they are often deeply entangled in many real-world\napplications. We propose Disentangled Self-Supervised Learning\n(DisentangledSSL), a novel self-supervised approach for learning disentangled\nrepresentations. We present a comprehensive analysis of the optimality of each\ndisentangled representation, particularly focusing on the scenario not covered\nin prior work where the so-called Minimum Necessary Information (MNI) point is\nnot attainable. We demonstrate that DisentangledSSL successfully learns shared\nand modality-specific features on multiple synthetic and real-world datasets\nand consistently outperforms baselines on various downstream tasks, including\nprediction tasks for vision-language data, as well as molecule-phenotype\nretrieval tasks for biological data.\n","authors":["Chenyu Wang","Sharut Gupta","Xinyi Zhang","Sana Tonekaboni","Stefanie Jegelka","Tommi Jaakkola","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2410.23996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01158v3","updated":"2024-10-31T14:53:43Z","published":"2023-06-01T21:31:59Z","title":"Heterogeneous Knowledge for Augmented Modular Reinforcement Learning","summary":"  Existing modular Reinforcement Learning (RL) architectures are generally\nbased on reusable components, also allowing for \"plug-and-play\" integration.\nHowever, these modules are homogeneous in nature - in fact, they essentially\nprovide policies obtained via RL through the maximization of individual reward\nfunctions. Consequently, such solutions still lack the ability to integrate and\nprocess multiple types of information (i.e., heterogeneous knowledge\nrepresentations), such as rules, sub-goals, and skills from various sources. In\nthis paper, we discuss several practical examples of heterogeneous knowledge\nand propose Augmented Modular Reinforcement Learning (AMRL) to address these\nlimitations. Our framework uses a selector to combine heterogeneous modules and\nseamlessly incorporate different types of knowledge representations and\nprocessing mechanisms. Our results demonstrate the performance and efficiency\nimprovements, also in terms of generalization, that can be achieved by\naugmenting traditional modular RL with heterogeneous knowledge sources and\nprocessing mechanisms. Finally, we examine the safety, robustness, and\ninterpretability issues stemming from the introduction of knowledge\nheterogeneity.\n","authors":["Lorenz Wolf","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2306.01158v3.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.00002v3","updated":"2024-10-31T14:52:28Z","published":"2024-04-09T14:08:06Z","title":"Kermut: Composite kernel regression for protein variant effects","summary":"  Reliable prediction of protein variant effects is crucial for both protein\noptimization and for advancing biological understanding. For practical use in\nprotein engineering, it is important that we can also provide reliable\nuncertainty estimates for our predictions, and while prediction accuracy has\nseen much progress in recent years, uncertainty metrics are rarely reported. We\nhere provide a Gaussian process regression model, Kermut, with a novel\ncomposite kernel for modeling mutation similarity, which obtains\nstate-of-the-art performance for supervised protein variant effect prediction\nwhile also offering estimates of uncertainty through its posterior. An analysis\nof the quality of the uncertainty estimates demonstrates that our model\nprovides meaningful levels of overall calibration, but that instance-specific\nuncertainty calibration remains more challenging.\n","authors":["Peter M√∏rch Groth","Mads Herbert Kerrn","Lars Olsen","Jesper Salomon","Wouter Boomsma"],"pdf_url":"https://arxiv.org/pdf/2407.00002v3.pdf","comment":"Accepted to NeurIPS 2024 as Spotlight"},{"id":"http://arxiv.org/abs/2410.23994v1","updated":"2024-10-31T14:52:01Z","published":"2024-10-31T14:52:01Z","title":"Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using\n  Discrete State Space Diffusion Model","summary":"  Sequential recommendation (SR) aims to predict items that users may be\ninterested in based on their historical behavior sequences. We revisit SR from\na novel information-theoretic perspective and find that conventional sequential\nmodeling methods fail to adequately capture the randomness and unpredictability\nof user behavior. Inspired by fuzzy information processing theory, this paper\nintroduces the DDSR model, which uses fuzzy sets of interaction sequences to\novercome the limitations and better capture the evolution of users' real\ninterests. Formally based on diffusion transition processes in discrete state\nspaces, which is unlike common diffusion models such as DDPM that operate in\ncontinuous domains. It is better suited for discrete data, using structured\ntransitions instead of arbitrary noise introduction to avoid information loss.\nAdditionally, to address the inefficiency of matrix transformations due to the\nvast discrete space, we use semantic labels derived from quantization or RQ-VAE\nto replace item IDs, enhancing efficiency and improving cold start issues.\nTesting on three public benchmark datasets shows that DDSR outperforms existing\nstate-of-the-art methods in various settings, demonstrating its potential and\neffectiveness in handling SR tasks.\n","authors":["Wenjia Xie","Hao Wang","Luankang Zhang","Rui Zhou","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.23994v1.pdf","comment":"NeurIPS'2024, 10 pages"},{"id":"http://arxiv.org/abs/2410.23992v1","updated":"2024-10-31T14:51:09Z","published":"2024-10-31T14:51:09Z","title":"Ada-MSHyper: Adaptive Multi-Scale Hypergraph Transformer for Time Series\n  Forecasting","summary":"  Although transformer-based methods have achieved great success in multi-scale\ntemporal pattern interaction modeling, two key challenges limit their further\ndevelopment: (1) Individual time points contain less semantic information, and\nleveraging attention to model pair-wise interactions may cause the information\nutilization bottleneck. (2) Multiple inherent temporal variations (e.g.,\nrising, falling, and fluctuating) entangled in temporal patterns. To this end,\nwe propose Adaptive Multi-Scale Hypergraph Transformer (Ada-MSHyper) for time\nseries forecasting. Specifically, an adaptive hypergraph learning module is\ndesigned to provide foundations for modeling group-wise interactions, then a\nmulti-scale interaction module is introduced to promote more comprehensive\npattern interactions at different scales. In addition, a node and hyperedge\nconstraint mechanism is introduced to cluster nodes with similar semantic\ninformation and differentiate the temporal variations within each scales.\nExtensive experiments on 11 real-world datasets demonstrate that Ada-MSHyper\nachieves state-of-the-art performance, reducing prediction errors by an average\nof 4.56%, 10.38%, and 4.97% in MSE for long-range, short-range, and\nultra-long-range time series forecasting, respectively. Code is available at\nhttps://github.com/shangzongjiang/Ada-MSHyper.\n","authors":["Zongjiang Shang","Ling Chen","Binqing wu","Dongliang Cui"],"pdf_url":"https://arxiv.org/pdf/2410.23992v1.pdf","comment":"Accepted by NeurIPS, 21 pages, and 8 figures"},{"id":"http://arxiv.org/abs/2409.17692v2","updated":"2024-10-31T14:38:27Z","published":"2024-09-26T09:57:16Z","title":"MIO: A Foundation Model on Multimodal Tokens","summary":"  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n","authors":["Zekun Wang","King Zhu","Chunpu Xu","Wangchunshu Zhou","Jiaheng Liu","Yibo Zhang","Jiashuo Wang","Ning Shi","Siyu Li","Yizhi Li","Haoran Que","Zhaoxiang Zhang","Yuanxing Zhang","Ge Zhang","Ke Xu","Jie Fu","Wenhao Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17692v2.pdf","comment":"Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO"},{"id":"http://arxiv.org/abs/2403.03333v3","updated":"2024-10-31T14:37:27Z","published":"2024-03-05T21:34:23Z","title":"Federated Learning over Connected Modes","summary":"  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the parameter space of\nneural networks, which we call solution simplex. We propose federated learning\nover connected modes (\\textsc{Floco}), where clients are assigned local\nsubregions in this simplex based on their gradient signals, and together learn\nthe shared global solution simplex. This allows personalization of the client\nmodels to fit their local distributions within the degrees of freedom in the\nsolution simplex and homogenizes the update signals for the global simplex\ntraining. Our experiments show that \\textsc{Floco} accelerates the global\ntraining process, and significantly improves the local accuracy with minimal\ncomputational overhead in cross-silo federated learning settings.\n","authors":["Dennis Grinwald","Philipp Wiesner","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2403.03333v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22637v2","updated":"2024-10-31T14:35:31Z","published":"2024-10-30T02:04:23Z","title":"Consistency Diffusion Bridge Models","summary":"  Diffusion models (DMs) have become the dominant paradigm of generative\nmodeling in a variety of domains by learning stochastic processes from noise to\ndata. Recently, diffusion denoising bridge models (DDBMs), a new formulation of\ngenerative modeling that builds stochastic processes between fixed data\nendpoints based on a reference diffusion process, have achieved empirical\nsuccess across tasks with coupled data distribution, such as image-to-image\ntranslation. However, DDBM's sampling process typically requires hundreds of\nnetwork evaluations to achieve decent performance, which may impede their\npractical deployment due to high computational demands. In this work, inspired\nby the recent advance of consistency models in DMs, we tackle this problem by\nlearning the consistency function of the probability-flow ordinary differential\nequation (PF-ODE) of DDBMs, which directly predicts the solution at a starting\nstep given any point on the ODE trajectory. Based on a dedicated general-form\nODE solver, we propose two paradigms: consistency bridge distillation and\nconsistency bridge training, which is flexible to apply on DDBMs with broad\ndesign choices. Experimental results show that our proposed method could sample\n$4\\times$ to $50\\times$ faster than the base DDBM and produce better visual\nquality given the same step in various tasks with pixel resolution ranging from\n$64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such\nas semantic interpolation in the data space.\n","authors":["Guande He","Kaiwen Zheng","Jianfei Chen","Fan Bao","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22637v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2310.17638v3","updated":"2024-10-31T14:35:05Z","published":"2023-10-26T17:53:24Z","title":"Generative Fractional Diffusion Models","summary":"  We introduce the first continuous-time score-based generative model that\nleverages fractional diffusion processes for its underlying dynamics. Although\ndiffusion models have excelled at capturing data distributions, they still\nsuffer from various limitations such as slow convergence, mode-collapse on\nimbalanced data, and lack of diversity. These issues are partially linked to\nthe use of light-tailed Brownian motion (BM) with independent increments. In\nthis paper, we replace BM with an approximation of its non-Markovian\ncounterpart, fractional Brownian motion (fBM), characterized by correlated\nincrements and Hurst index $H \\in (0,1)$, where $H=0.5$ recovers the classical\nBM. To ensure tractable inference and learning, we employ a recently\npopularized Markov approximation of fBM (MA-fBM) and derive its reverse-time\nmodel, resulting in generative fractional diffusion models (GFDM). We\ncharacterize the forward dynamics using a continuous reparameterization trick\nand propose augmented score matching to efficiently learn the score function,\nwhich is partly known in closed form, at minimal added cost. The ability to\ndrive our diffusion model via MA-fBM offers flexibility and control. $H \\leq\n0.5$ enters the regime of rough paths whereas $H>0.5$ regularizes diffusion\npaths and invokes long-term memory. The Markov approximation allows added\ncontrol by varying the number of Markov processes linearly combined to\napproximate fBM. Our evaluations on real image datasets demonstrate that GFDM\nachieves greater pixel-wise diversity and enhanced image quality, as indicated\nby a lower FID, offering a promising alternative to traditional diffusion\nmodels\n","authors":["Gabriel Nobis","Maximilian Springenberg","Marco Aversa","Michael Detzel","Rembert Daems","Roderick Murray-Smith","Shinichi Nakajima","Sebastian Lapuschkin","Stefano Ermon","Tolga Birdal","Manfred Opper","Christoph Knochenhauer","Luis Oala","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2310.17638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19534v4","updated":"2024-10-31T14:32:28Z","published":"2024-05-29T21:29:44Z","title":"Preference Learning Algorithms Do Not Learn Preference Rankings","summary":"  Preference learning algorithms (e.g., RLHF and DPO) are frequently used to\nsteer LLMs to produce generations that are more preferred by humans, but our\nunderstanding of their inner workings is still limited. In this work, we study\nthe conventional wisdom that preference learning trains models to assign higher\nlikelihoods to more preferred outputs than less preferred outputs, measured via\nranking accuracy. Surprisingly, we find that most state-of-the-art\npreference-tuned models achieve a ranking accuracy of less than 60% on common\npreference datasets. We furthermore derive the idealized ranking accuracy that\na preference-tuned LLM would achieve if it optimized the DPO or RLHF objective\nperfectly. We demonstrate that existing models exhibit a significant alignment\ngap -- i.e., a gap between the observed and idealized ranking accuracies. We\nattribute this discrepancy to the DPO objective, which is empirically and\ntheoretically ill-suited to fix even mild ranking errors in the reference\nmodel, and derive a simple and efficient formula for quantifying the difficulty\nof learning a given preference datapoint. Finally, we demonstrate that ranking\naccuracy strongly correlates with the empirically popular win rate metric when\nthe model is close to the reference model used in the objective, shedding\nfurther light on the differences between on-policy (e.g., RLHF) and off-policy\n(e.g., DPO) preference learning algorithms.\n","authors":["Angelica Chen","Sadhika Malladi","Lily H. Zhang","Xinyi Chen","Qiuyi Zhang","Rajesh Ranganath","Kyunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2405.19534v4.pdf","comment":"NeurIPS 2024 camera-ready"},{"id":"http://arxiv.org/abs/2407.00114v2","updated":"2024-10-31T14:27:50Z","published":"2024-06-27T13:46:11Z","title":"OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents","summary":"  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.\n","authors":["Zihao Wang","Shaofei Cai","Zhancun Mu","Haowei Lin","Ceyao Zhang","Xuejie Liu","Qing Li","Anji Liu","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2407.00114v2.pdf","comment":"accepted on NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23970v1","updated":"2024-10-31T14:25:55Z","published":"2024-10-31T14:25:55Z","title":"TrAct: Making First-layer Pre-Activations Trainable","summary":"  We consider the training of the first layer of vision models and notice the\nclear relationship between pixel values and gradient update magnitudes: the\ngradients arriving at the weights of a first layer are by definition directly\nproportional to (normalized) input pixel values. Thus, an image with low\ncontrast has a smaller impact on learning than an image with higher contrast,\nand a very bright or very dark image has a stronger impact on the weights than\nan image with moderate brightness. In this work, we propose performing gradient\ndescent on the embeddings produced by the first layer of the model. However,\nswitching to discrete inputs with an embedding layer is not a reasonable option\nfor vision models. Thus, we propose the conceptual procedure of (i) a gradient\ndescent step on first layer activations to construct an activation proposal,\nand (ii) finding the optimal weights of the first layer, i.e., those weights\nwhich minimize the squared distance to the activation proposal. We provide a\nclosed form solution of the procedure and adjust it for robust stochastic\ntraining while computing everything efficiently. Empirically, we find that\nTrAct (Training Activations) speeds up training by factors between 1.25x and 4x\nwhile requiring only a small computational overhead. We demonstrate the utility\nof TrAct with different optimizers for a range of different vision models\nincluding convolutional and transformer architectures.\n","authors":["Felix Petersen","Christian Borgelt","Stefano Ermon"],"pdf_url":"https://arxiv.org/pdf/2410.23970v1.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.06246v2","updated":"2024-10-31T14:24:45Z","published":"2024-06-10T13:23:00Z","title":"Data-Efficient Learning with Neural Programs","summary":"  Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.\n","authors":["Alaia Solko-Breslin","Seewon Choi","Ziyang Li","Neelay Velingker","Rajeev Alur","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2406.06246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23969v1","updated":"2024-10-31T14:22:52Z","published":"2024-10-31T14:22:52Z","title":"Interactive proofs for verifying (quantum) learning and testing","summary":"  We consider the problem of testing and learning from data in the presence of\nresource constraints, such as limited memory or weak data access, which place\nlimitations on the efficiency and feasibility of testing or learning. In\nparticular, we ask the following question: Could a resource-constrained\nlearner/tester use interaction with a resource-unconstrained but untrusted\nparty to solve a learning or testing problem more efficiently than they could\nwithout such an interaction? In this work, we answer this question both\nabstractly and for concrete problems, in two complementary ways: For a wide\nvariety of scenarios, we prove that a resource-constrained learner cannot gain\nany advantage through classical interaction with an untrusted prover. As a\nspecial case, we show that for the vast majority of testing and learning\nproblems in which quantum memory is a meaningful resource, a memory-constrained\nquantum algorithm cannot overcome its limitations via classical communication\nwith a memory-unconstrained quantum prover. In contrast, when quantum\ncommunication is allowed, we construct a variety of interactive proof\nprotocols, for specific learning and testing problems, which allow\nmemory-constrained quantum verifiers to gain significant advantages through\ndelegation to untrusted provers. These results highlight both the limitations\nand potential of delegating learning and testing problems to resource-rich but\nuntrusted third parties.\n","authors":["Matthias C. Caro","Jens Eisert","Marcel Hinsche","Marios Ioannou","Alexander Nietner","Ryan Sweke"],"pdf_url":"https://arxiv.org/pdf/2410.23969v1.pdf","comment":"12 + 33 + 13 pages; 1 table; 2 figures"},{"id":"http://arxiv.org/abs/2405.14808v2","updated":"2024-10-31T14:19:49Z","published":"2024-05-23T17:18:46Z","title":"Implicit Personalization in Language Models: A Systematic Study","summary":"  Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.\n","authors":["Zhijing Jin","Nils Heil","Jiarui Liu","Shehzaad Dhuliawala","Yahang Qi","Bernhard Sch√∂lkopf","Rada Mihalcea","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.14808v2.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.21076v2","updated":"2024-10-31T14:16:36Z","published":"2024-10-28T14:40:01Z","title":"Accelerated Bayesian parameter estimation and model selection for\n  gravitational waves with normalizing flows","summary":"  We present an accelerated pipeline, based on high-performance computing\ntechniques and normalizing flows, for joint Bayesian parameter estimation and\nmodel selection and demonstrate its efficiency in gravitational wave\nastrophysics. We integrate the Jim inference toolkit, a normalizing\nflow-enhanced Markov chain Monte Carlo (MCMC) sampler, with the learned\nharmonic mean estimator. Our Bayesian evidence estimates run on $1$ GPU are\nconsistent with traditional nested sampling techniques run on $16$ CPU cores,\nwhile reducing the computation time by factors of $5\\times$ and $15\\times$ for\n$4$-dimensional and $11$-dimensional gravitational wave inference problems,\nrespectively. Our code is available in well-tested and thoroughly documented\nopen-source packages, ensuring accessibility and reproducibility for the wider\nresearch community.\n","authors":["Alicja Polanska","Thibeau Wouters","Peter T. H. Pang","Kaze K. W. Wong","Jason D. McEwen"],"pdf_url":"https://arxiv.org/pdf/2410.21076v2.pdf","comment":"accepted to NeurIPS 2024 workshop on Machine Learning and the\n  Physical Sciences"},{"id":"http://arxiv.org/abs/2410.23955v1","updated":"2024-10-31T14:09:05Z","published":"2024-10-31T14:09:05Z","title":"An Empirical Analysis of Speech Self-Supervised Learning at Multiple\n  Resolutions","summary":"  Self-supervised learning (SSL) models have become crucial in speech\nprocessing, with recent advancements concentrating on developing architectures\nthat capture representations across multiple timescales. The primary goal of\nthese multi-scale architectures is to exploit the hierarchical nature of\nspeech, where lower-resolution components aim to capture representations that\nalign with increasingly abstract concepts (e.g., from phones to words to\nsentences). Although multi-scale approaches have demonstrated some improvements\nover single-scale models, the precise reasons for these enhancements have poor\nempirical support. In this study, we present an initial analysis of layer-wise\nrepresentations in multi-scale architectures, with a focus on Canonical\nCorrelation Analysis (CCA) and Mutual Information (MI). We apply this analysis\nto Multi-Resolution HuBERT (MR-HuBERT) and find that (1) the improved\nperformance on SUPERB tasks is primarily due to the auxiliary low-resolution\nloss rather than the downsampling itself, and (2) downsampling to lower\nresolutions neither improves downstream performance nor correlates with\nhigher-level information (e.g., words), though it does improve computational\nefficiency. These findings challenge assumptions about the multi-scale nature\nof MR-HuBERT and motivate the importance of disentangling computational\nefficiency from learning better representations.\n","authors":["Theo Clark","Benedetta Cevoli","Eloy de Jong","Timofey Abramski","Jamie Dougherty"],"pdf_url":"https://arxiv.org/pdf/2410.23955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23953v1","updated":"2024-10-31T14:07:26Z","published":"2024-10-31T14:07:26Z","title":"Representative Social Choice: From Learning Theory to AI Alignment","summary":"  Social choice theory is the study of preference aggregation across a\npopulation, used both in mechanism design for human agents and in the\ndemocratic alignment of language models. In this study, we propose the\nrepresentative social choice framework for the modeling of democratic\nrepresentation in collective decisions, where the number of issues and\nindividuals are too large for mechanisms to consider all preferences directly.\nThese scenarios are widespread in real-world decision-making processes, such as\njury trials, indirect elections, legislation processes, corporate governance,\nand, more recently, language model alignment. In representative social choice,\nthe population is represented by a finite sample of individual-issue pairs\nbased on which social choice decisions are made. We show that many of the\ndeepest questions in representative social choice can be naturally formulated\nas statistical learning problems, and prove the generalization properties of\nsocial choice mechanisms using the theory of machine learning. We further\nformulate axioms for representative social choice, and prove Arrow-like\nimpossibility theorems with new combinatorial tools of analysis. Our framework\nintroduces the representative approach to social choice, opening up research\ndirections at the intersection of social choice, learning theory, and AI\nalignment.\n","authors":["Tianyi Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23953v1.pdf","comment":"Full version (20 pages). Under review. An excerpt was previously\n  accepted to NeurIPS 2024 Pluralistic Alignment Workshop"},{"id":"http://arxiv.org/abs/2410.23952v1","updated":"2024-10-31T14:06:43Z","published":"2024-10-31T14:06:43Z","title":"Scalable Kernel Inverse Optimization","summary":"  Inverse Optimization (IO) is a framework for learning the unknown objective\nfunction of an expert decision-maker from a past dataset. In this paper, we\nextend the hypothesis class of IO objective functions to a reproducing kernel\nHilbert space (RKHS), thereby enhancing feature representation to an\ninfinite-dimensional space. We demonstrate that a variant of the representer\ntheorem holds for a specific training loss, allowing the reformulation of the\nproblem as a finite-dimensional convex optimization program. To address\nscalability issues commonly associated with kernel methods, we propose the\nSequential Selection Optimization (SSO) algorithm to efficiently train the\nproposed Kernel Inverse Optimization (KIO) model. Finally, we validate the\ngeneralization capabilities of the proposed KIO model and the effectiveness of\nthe SSO algorithm through learning-from-demonstration tasks on the MuJoCo\nbenchmark.\n","authors":["Youyuan Long","Tolga Ok","Pedro Zattoni Scroccaro","Peyman Mohajerin Esfahani"],"pdf_url":"https://arxiv.org/pdf/2410.23952v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23949v1","updated":"2024-10-31T14:04:33Z","published":"2024-10-31T14:04:33Z","title":"Deep Learning Frameworks for Cognitive Radio Networks: Review and Open\n  Research Challenges","summary":"  Deep learning has been proven to be a powerful tool for addressing the most\nsignificant issues in cognitive radio networks, such as spectrum sensing,\nspectrum sharing, resource allocation, and security attacks. The utilization of\ndeep learning techniques in cognitive radio networks can significantly enhance\nthe network's capability to adapt to changing environments and improve the\noverall system's efficiency and reliability. As the demand for higher data\nrates and connectivity increases, B5G/6G wireless networks are expected to\nenable new services and applications significantly. Therefore, the significance\nof deep learning in addressing cognitive radio network challenges cannot be\noverstated. This review article provides valuable insights into potential\nsolutions that can serve as a foundation for the development of future B5G/6G\nservices. By leveraging the power of deep learning, cognitive radio networks\ncan pave the way for the next generation of wireless networks capable of\nmeeting the ever-increasing demands for higher data rates, improved\nreliability, and security.\n","authors":["Senthil Kumar Jagatheesaperumal","Ijaz Ahmad","Marko H√∂yhty√§","Suleman Khan","Andrei Gurtov"],"pdf_url":"https://arxiv.org/pdf/2410.23949v1.pdf","comment":"The article has been accepted for publication in \"Journal of Network\n  and Computer Applications\" during October 2024"},{"id":"http://arxiv.org/abs/2410.23948v1","updated":"2024-10-31T14:03:37Z","published":"2024-10-31T14:03:37Z","title":"Transformers to Predict the Applicability of Symbolic Integration\n  Routines","summary":"  Symbolic integration is a fundamental problem in mathematics: we consider how\nmachine learning may be used to optimise this task in a Computer Algebra System\n(CAS). We train transformers that predict whether a particular integration\nmethod will be successful, and compare against the existing human-made\nheuristics (called guards) that perform this task in a leading CAS. We find the\ntransformer can outperform these guards, gaining up to 30% accuracy and 70%\nprecision. We further show that the inference time of the transformer is\ninconsequential which shows that it is well-suited to include as a guard in a\nCAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions\nthat the transformer is making. If guided by a subject-matter expert, the\ntechnique can explain some of the predictions based on the input tokens, which\ncan lead to further optimisations.\n","authors":["Rashid Barket","Uzma Shafiq","Matthew England","Juergen Gerhard"],"pdf_url":"https://arxiv.org/pdf/2410.23948v1.pdf","comment":"10 pages, 5 figures, to be published in NeurIPS 2024 MATH-AI Workshop"},{"id":"http://arxiv.org/abs/2410.23940v1","updated":"2024-10-31T13:54:37Z","published":"2024-10-31T13:54:37Z","title":"Quantum Deep Equilibrium Models","summary":"  The feasibility of variational quantum algorithms, the most popular\ncorrespondent of neural networks on noisy, near-term quantum hardware, is\nhighly impacted by the circuit depth of the involved parametrized quantum\ncircuits (PQCs). Higher depth increases expressivity, but also results in a\ndetrimental accumulation of errors. Furthermore, the number of parameters\ninvolved in the PQC significantly influences the performance through the\nnecessary number of measurements to evaluate gradients, which scales linearly\nwith the number of parameters.\n  Motivated by this, we look at deep equilibrium models (DEQs), which mimic an\ninfinite-depth, weight-tied network using a fraction of the memory by employing\na root solver to find the fixed points of the network. In this work, we present\nQuantum Deep Equilibrium Models (QDEQs): a training paradigm that learns\nparameters of a quantum machine learning model given by a PQC using DEQs. To\nour knowledge, no work has yet explored the application of DEQs to QML models.\nWe apply QDEQs to find the parameters of a quantum circuit in two settings: the\nfirst involves classifying MNIST-4 digits with 4 qubits; the second extends it\nto 10 classes of MNIST, FashionMNIST and CIFAR. We find that QDEQ is not only\ncompetitive with comparable existing baseline models, but also achieves higher\nperformance than a network with 5 times more layers. This demonstrates that the\nQDEQ paradigm can be used to develop significantly more shallow quantum\ncircuits for a given task, something which is essential for the utility of\nnear-term quantum computers.\n  Our code is available at https://github.com/martaskrt/qdeq.\n","authors":["Philipp Schleich","Marta Skreta","Lasse B. Kristensen","Rodrigo A. Vargas-Hern√°ndez","Al√°n Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2410.23940v1.pdf","comment":"To be published in NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23938v1","updated":"2024-10-31T13:52:59Z","published":"2024-10-31T13:52:59Z","title":"Learning Macroscopic Dynamics from Partial Microscopic Observations","summary":"  Macroscopic observables of a system are of keen interest in real applications\nsuch as the design of novel materials. Current methods rely on microscopic\ntrajectory simulations, where the forces on all microscopic coordinates need to\nbe computed or measured. However, this can be computationally prohibitive for\nrealistic systems. In this paper, we propose a method to learn macroscopic\ndynamics requiring only force computations on a subset of the microscopic\ncoordinates. Our method relies on a sparsity assumption: the force on each\nmicroscopic coordinate relies only on a small number of other coordinates. The\nmain idea of our approach is to map the training procedure on the macroscopic\ncoordinates back to the microscopic coordinates, on which partial force\ncomputations can be used as stochastic estimation to update model parameters.\nWe provide a theoretical justification of this under suitable conditions. We\ndemonstrate the accuracy, force computation efficiency, and robustness of our\nmethod on learning macroscopic closure models from a variety of microscopic\nsystems, including those modeled by partial differential equations or molecular\ndynamics simulations.\n","authors":["Mengyi Chen","Qianxiao Li"],"pdf_url":"https://arxiv.org/pdf/2410.23938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23937v1","updated":"2024-10-31T13:51:59Z","published":"2024-10-31T13:51:59Z","title":"Robust Sparse Regression with Non-Isotropic Designs","summary":"  We develop a technique to design efficiently computable estimators for sparse\nlinear regression in the simultaneous presence of two adversaries: oblivious\nand adaptive. We design several robust algorithms that outperform the state of\nthe art even in the special case when oblivious adversary simply adds Gaussian\nnoise. In particular, we provide a polynomial-time algorithm that with high\nprobability recovers the signal up to error $O(\\sqrt{\\varepsilon})$ as long as\nthe number of samples $n \\ge \\tilde{O}(k^2/\\varepsilon)$, only assuming some\nbounds on the third and the fourth moments of the distribution ${D}$ of the\ndesign.\n  In addition, prior to this work, even in the special case of Gaussian design\nand noise, no polynomial time algorithm was known to achieve error\n$o(\\sqrt{\\varepsilon})$ in the sparse setting $n < d^2$. We show that under\nsome assumptions on the fourth and the eighth moments of ${D}$, there is a\npolynomial-time algorithm that achieves error $o(\\sqrt{\\varepsilon})$ as long\nas $n \\ge \\tilde{O}(k^4 / \\varepsilon^3)$. For Gaussian distribution, this\nalgorithm achieves error $O(\\varepsilon^{3/4})$. Moreover, our algorithm\nachieves error $o(\\sqrt{\\varepsilon})$ for all log-concave distributions if\n$\\varepsilon \\le 1/\\text{polylog(d)}$.\n  Our algorithms are based on the filtering of the covariates that uses\nsum-of-squares relaxations, and weighted Huber loss minimization with $\\ell_1$\nregularizer. We provide a novel analysis of weighted penalized Huber loss that\nis suitable for heavy-tailed designs in the presence of two adversaries.\nFurthermore, we complement our algorithmic results with Statistical Query lower\nbounds, providing evidence that our estimators are likely to have nearly\noptimal sample complexity.\n","authors":["Chih-Hung Liu","Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2410.23937v1.pdf","comment":"NeurIPS 2024; Authors have equal contribution"},{"id":"http://arxiv.org/abs/2410.22086v2","updated":"2024-10-31T13:50:02Z","published":"2024-10-29T14:41:44Z","title":"Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate","summary":"  Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.\n","authors":["Zhiqi Bu","Xiaomeng Jin","Bhanukiran Vinzamuri","Anil Ramakrishna","Kai-Wei Chang","Volkan Cevher","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2410.22086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22938v2","updated":"2024-10-31T13:39:20Z","published":"2024-10-30T11:47:40Z","title":"DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic\n  Signal Control with Missing Data","summary":"  The application of reinforcement learning in traffic signal control (TSC) has\nbeen extensively researched and yielded notable achievements. However, most\nexisting works for TSC assume that traffic data from all surrounding\nintersections is fully and continuously available through sensors. In\nreal-world applications, this assumption often fails due to sensor malfunctions\nor data loss, making TSC with missing data a critical challenge. To meet the\nneeds of practical applications, we introduce DiffLight, a novel conditional\ndiffusion model for TSC under data-missing scenarios in the offline setting.\nSpecifically, we integrate two essential sub-tasks, i.e., traffic data\nimputation and decision-making, by leveraging a Partial Rewards Conditioned\nDiffusion (PRCD) model to prevent missing rewards from interfering with the\nlearning process. Meanwhile, to effectively capture the spatial-temporal\ndependencies among intersections, we design a Spatial-Temporal transFormer\n(STFormer) architecture. In addition, we propose a Diffusion Communication\nMechanism (DCM) to promote better communication and control performance under\ndata-missing scenarios. Extensive experiments on five datasets with various\ndata-missing scenarios demonstrate that DiffLight is an effective controller to\naddress TSC with missing data. The code of DiffLight is released at\nhttps://github.com/lokol5579/DiffLight-release.\n","authors":["Hanyang Chen","Yang Jiang","Shengnan Guo","Xiaowei Mao","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.22938v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22193v2","updated":"2024-10-31T13:37:43Z","published":"2024-10-29T16:31:40Z","title":"GoRINNs: Godunov-Riemann Informed Neural Networks for Learning\n  Hyperbolic Conservation Laws","summary":"  We present GoRINNs: numerical analysis-informed neural networks for the\nsolution of inverse problems of non-linear systems of conservation laws.\nGoRINNs are based on high-resolution Godunov schemes for the solution of the\nRiemann problem in hyperbolic Partial Differential Equations (PDEs). In\ncontrast to other existing machine learning methods that learn the numerical\nfluxes of conservative Finite Volume methods, GoRINNs learn the physical flux\nfunction per se. Due to their structure, GoRINNs provide interpretable,\nconservative schemes, that learn the solution operator on the basis of\napproximate Riemann solvers that satisfy the Rankine-Hugoniot condition. The\nperformance of GoRINNs is assessed via four benchmark problems, namely the\nBurgers', the Shallow Water, the Lighthill-Whitham-Richards and the\nPayne-Whitham traffic flow models. The solution profiles of these PDEs exhibit\nshock waves, rarefactions and/or contact discontinuities at finite times. We\ndemonstrate that GoRINNs provide a very high accuracy both in the smooth and\ndiscontinuous regions.\n","authors":["Dimitrios G. Patsatzis","Mario di Bernardo","Lucia Russo","Constantinos Siettos"],"pdf_url":"https://arxiv.org/pdf/2410.22193v2.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23922v1","updated":"2024-10-31T13:32:39Z","published":"2024-10-31T13:32:39Z","title":"Analyzing & Reducing the Need for Learning Rate Warmup in GPT Training","summary":"  Learning Rate Warmup is a popular heuristic for training neural networks,\nespecially at larger batch sizes, despite limited understanding of its\nbenefits. Warmup decreases the update size $\\Delta \\mathbf{w}_t = \\eta_t\n\\mathbf{u}_t$ early in training by using lower values for the learning rate\n$\\eta_t$. In this work we argue that warmup benefits training by keeping the\noverall size of $\\Delta \\mathbf{w}_t$ limited, counteracting large initial\nvalues of $\\mathbf{u}_t$. Focusing on small-scale GPT training with AdamW/Lion,\nwe explore the following question: Why and by which criteria are early updates\n$\\mathbf{u}_t$ too large? We analyze different metrics for the update size\nincluding the $\\ell_2$-norm, resulting directional change, and impact on the\nrepresentations of the network, providing a new perspective on warmup. In\nparticular, we find that warmup helps counteract large angular updates as well\nas a limited critical batch size early in training. Finally, we show that the\nneed for warmup can be significantly reduced or eliminated by modifying the\noptimizer to explicitly normalize $\\mathbf{u}_t$ based on the aforementioned\nmetrics.\n","authors":["Atli Kosson","Bettina Messmer","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2410.23922v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23918v1","updated":"2024-10-31T13:26:11Z","published":"2024-10-31T13:26:11Z","title":"BitStack: Fine-Grained Size Control for Compressed Large Language Models\n  in Variable Memory Environments","summary":"  Large language models (LLMs) have revolutionized numerous applications, yet\ntheir deployment remains challenged by memory constraints on local devices.\nWhile scaling laws have enhanced LLM capabilities, the primary bottleneck has\nshifted from \\textit{capability} to \\textit{availability}, emphasizing the need\nfor efficient memory management. Traditional compression methods, such as\nquantization, often require predefined compression ratios and separate\ncompression processes for each setting, complicating deployment in variable\nmemory environments. In this paper, we introduce \\textbf{BitStack}, a novel,\ntraining-free weight compression approach that enables megabyte-level\ntrade-offs between memory usage and model performance. By leveraging weight\ndecomposition, BitStack can dynamically adjust the model size with minimal\ntransmission between running memory and storage devices. Our approach\niteratively decomposes weight matrices while considering the significance of\neach parameter, resulting in an approximately 1-bit per parameter residual\nblock in each decomposition iteration. These blocks are sorted and stacked in\nstorage as basic transmission units, with different quantities loaded based on\ncurrent memory availability. Extensive experiments across a wide range of tasks\ndemonstrate that, despite offering fine-grained size control, BitStack\nconsistently matches or surpasses strong quantization baselines, particularly\nat extreme compression ratios. To the best of our knowledge, this is the first\ndecomposition-based method that effectively bridges the gap to practical\ncompression techniques like quantization. Code is available at\nhttps://github.com/xinghaow99/BitStack.\n","authors":["Xinghao Wang","Pengyu Wang","Bo Wang","Dong Zhang","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.23918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09160v3","updated":"2024-10-31T13:20:42Z","published":"2023-02-17T22:15:20Z","title":"Identifying Equivalent Training Dynamics","summary":"  Study of the nonlinear evolution deep neural network (DNN) parameters undergo\nduring training has uncovered regimes of distinct dynamical behavior. While a\ndetailed understanding of these phenomena has the potential to advance\nimprovements in training efficiency and robustness, the lack of methods for\nidentifying when DNN models have equivalent dynamics limits the insight that\ncan be gained from prior work. Topological conjugacy, a notion from dynamical\nsystems theory, provides a precise definition of dynamical equivalence,\noffering a possible route to address this need. However, topological\nconjugacies have historically been challenging to compute. By leveraging\nadvances in Koopman operator theory, we develop a framework for identifying\nconjugate and non-conjugate training dynamics. To validate our approach, we\ndemonstrate that comparing Koopman eigenvalues can correctly identify a known\nequivalence between online mirror descent and online gradient descent. We then\nutilize our approach to: (a) identify non-conjugate training dynamics between\nshallow and wide fully connected neural networks; (b) characterize the early\nphase of training dynamics in convolutional neural networks; (c) uncover\nnon-conjugate training dynamics in Transformers that do and do not undergo\ngrokking. Our results, across a range of DNN architectures, illustrate the\nflexibility of our framework and highlight its potential for shedding new light\non training dynamics.\n","authors":["William T. Redman","Juan M. Bello-Rivas","Maria Fonoberova","Ryan Mohr","Ioannis G. Kevrekidis","Igor Meziƒá"],"pdf_url":"https://arxiv.org/pdf/2302.09160v3.pdf","comment":"23 pages, 5 figures, 6 supplemental figures"},{"id":"http://arxiv.org/abs/2410.23912v1","updated":"2024-10-31T13:17:53Z","published":"2024-10-31T13:17:53Z","title":"RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for\n  Self-Taught Reasoner","summary":"  The reasoning abilities of large language models (LLMs) have improved with\nchain-of-thought (CoT) prompting, allowing models to solve complex tasks in a\nstepwise manner. However, training CoT capabilities requires detailed reasoning\ndata, which is often scarce. The self-taught reasoner (STaR) framework\naddresses this by using reinforcement learning to automatically generate\nreasoning steps, reducing reliance on human-labeled data. Although STaR and its\nvariants have demonstrated empirical success, a theoretical foundation\nexplaining these improvements is lacking. This work provides a theoretical\nframework for understanding the effectiveness of reinforcement learning on CoT\nreasoning and STaR. Our contributions are: (1) an analysis of policy\nimprovement, showing why LLM reasoning improves iteratively with STaR; (2)\nconditions for convergence to an optimal reasoning policy; (3) an examination\nof STaR's robustness, explaining how it can improve reasoning even when\nincorporating occasional incorrect steps; and (4) criteria for the quality of\npre-trained models necessary to initiate effective reasoning improvement. This\nframework aims to bridge empirical findings with theoretical insights,\nadvancing reinforcement learning approaches for reasoning in LLMs.\n","authors":["Fu-Chieh Chang","Yu-Ting Lee","Hui-Ying Shih","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.23912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20087v2","updated":"2024-10-31T13:10:12Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v2.pdf","comment":"NeurIPS 2024 Track on Datasets and Benchmarks (Spotlight)"},{"id":"http://arxiv.org/abs/2303.15124v2","updated":"2024-10-31T13:10:11Z","published":"2023-03-27T11:56:20Z","title":"Blind Inpainting with Object-aware Discrimination for Artificial Marker\n  Removal","summary":"  Medical images often incorporate doctor-added markers that can hinder\nAI-based diagnosis. This issue highlights the need of inpainting techniques to\nrestore the corrupted visual contents. However, existing methods require manual\nmask annotation as input, limiting the application scenarios. In this paper, we\npropose a novel blind inpainting method that automatically reconstructs visual\ncontents within the corrupted regions without mask input as guidance. Our model\nincludes a blind reconstruction network and an object-aware discriminator for\nadversarial training. The reconstruction network contains two branches that\npredict corrupted regions in images and simultaneously restore the missing\nvisual contents. Leveraging the potent recognition capability of a dense object\ndetector, the object-aware discriminator ensures markers undetectable after\ninpainting. Thus, the restored images closely resemble the clean ones. We\nevaluate our method on three datasets of various medical imaging modalities,\nconfirming better performance over other state-of-the-art methods.\n","authors":["Xuechen Guo","Wenhao Hu","Chiming Ni","Wenhao Chai","Shiyan Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.15124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23903v1","updated":"2024-10-31T13:05:46Z","published":"2024-10-31T13:05:46Z","title":"Neural Network Verification with PyRAT","summary":"  As AI systems are becoming more and more popular and used in various critical\ndomains (health, transport, energy, ...), the need to provide guarantees and\ntrust of their safety is undeniable. To this end, we present PyRAT, a tool\nbased on abstract interpretation to verify the safety and the robustness of\nneural networks. In this paper, we describe the different abstractions used by\nPyRAT to find the reachable states of a neural network starting from its input\nas well as the main features of the tool to provide fast and accurate analysis\nof neural networks. PyRAT has already been used in several collaborations to\nensure safety guarantees, with its second place at the VNN-Comp 2024 showcasing\nits performance.\n","authors":["Augustin Lemesle","Julien Lehmann","Tristan Le Gall"],"pdf_url":"https://arxiv.org/pdf/2410.23903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09451v2","updated":"2024-10-31T13:02:43Z","published":"2024-06-12T15:51:00Z","title":"Enhancing Activity Recognition After Stroke: Generative Adversarial\n  Networks for Kinematic Data Augmentation","summary":"  The generalizability of machine learning (ML) models for wearable monitoring\nin stroke rehabilitation is often constrained by the limited scale and\nheterogeneity of available data. Data augmentation addresses this challenge by\nadding computationally derived data to real data to enrich the variability\nrepresented in the training set. Traditional augmentation methods, such as\nrotation, permutation, and time-warping, have shown some benefits in improving\nclassifier performance, but often fail to produce realistic training examples.\nThis study employs Conditional Generative Adversarial Networks (cGANs) to\ncreate synthetic kinematic data from a publicly available dataset, closely\nmimicking the experimentally measured reaching movements of stroke survivors.\nThis approach not only captures the complex temporal dynamics and common\nmovement patterns after stroke, but also significantly enhances the training\ndataset. By training deep learning models on both synthetic and experimental\ndata, we enhanced task classification accuracy: models incorporating synthetic\ndata attained an overall accuracy of 80.0%, significantly higher than the 66.1%\nseen in models trained solely with real data. These improvements allow for more\nprecise task classification, offering clinicians the potential to monitor\npatient progress more accurately and tailor rehabilitation interventions more\neffectively.\n","authors":["Aaron J. Hadley","Christopher L. Pulliam"],"pdf_url":"https://arxiv.org/pdf/2406.09451v2.pdf","comment":"13 pages, 5 figures, 3 tables; resubmitted to Sensors"},{"id":"http://arxiv.org/abs/2407.04491v2","updated":"2024-10-31T13:02:36Z","published":"2024-07-05T13:29:30Z","title":"Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular\n  Data","summary":"  For classification and regression on tabular data, the dominance of\ngradient-boosted decision trees (GBDTs) has recently been challenged by often\nmuch slower deep learning methods with extensive hyperparameter tuning. We\naddress this discrepancy by introducing (a) RealMLP, an improved multilayer\nperceptron (MLP), and (b) strong meta-tuned default parameters for GBDTs and\nRealMLP. We tune RealMLP and the default parameters on a meta-train benchmark\nwith 118 datasets and compare them to hyperparameter-optimized versions on a\ndisjoint meta-test benchmark with 90 datasets, as well as the GBDT-friendly\nbenchmark by Grinsztajn et al. (2022). Our benchmark results on medium-to-large\ntabular datasets (1K--500K samples) show that RealMLP offers a favorable\ntime-accuracy tradeoff compared to other neural baselines and is competitive\nwith GBDTs in terms of benchmark scores. Moreover, a combination of RealMLP and\nGBDTs with improved default parameters can achieve excellent results without\nhyperparameter tuning. Finally, we demonstrate that some of RealMLP's\nimprovements can also considerably improve the performance of TabR with default\nparameters.\n","authors":["David Holzm√ºller","L√©o Grinsztajn","Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2407.04491v2.pdf","comment":"NeurIPS 2024. Changes in v2: more baselines, more ablations,\n  introduced RealTabR-D, updated Grinstzajn benchmark protocol, updated\n  text/figures. Code is available at github.com/dholzmueller/pytabkit"},{"id":"http://arxiv.org/abs/2407.15580v2","updated":"2024-10-31T12:59:49Z","published":"2024-07-22T12:16:56Z","title":"Annealed Multiple Choice Learning: Overcoming limitations of\n  Winner-takes-all with annealing","summary":"  We introduce Annealed Multiple Choice Learning (aMCL) which combines\nsimulated annealing with MCL. MCL is a learning framework handling ambiguous\ntasks by predicting a small set of plausible hypotheses. These hypotheses are\ntrained using the Winner-takes-all (WTA) scheme, which promotes the diversity\nof the predictions. However, this scheme may converge toward an arbitrarily\nsuboptimal local minimum, due to the greedy nature of WTA. We overcome this\nlimitation using annealing, which enhances the exploration of the hypothesis\nspace during training. We leverage insights from statistical physics and\ninformation theory to provide a detailed description of the model training\ntrajectory. Additionally, we validate our algorithm by extensive experiments on\nsynthetic datasets, on the standard UCI benchmark, and on speech separation.\n","authors":["David Perera","Victor Letzelter","Th√©o Mariotte","Adrien Cort√©s","Mickael Chen","Slim Essid","Ga√´l Richard"],"pdf_url":"https://arxiv.org/pdf/2407.15580v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.20745v2","updated":"2024-10-31T12:54:46Z","published":"2024-10-28T05:25:47Z","title":"Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large\n  Language Models","summary":"  Online shopping is a complex multi-task, few-shot learning problem with a\nwide and evolving range of entities, relations, and tasks. However, existing\nmodels and benchmarks are commonly tailored to specific tasks, falling short of\ncapturing the full complexity of online shopping. Large Language Models (LLMs),\nwith their multi-task and few-shot learning abilities, have the potential to\nprofoundly transform online shopping by alleviating task-specific engineering\nefforts and by providing users with interactive conversations. Despite the\npotential, LLMs face unique challenges in online shopping, such as\ndomain-specific concepts, implicit knowledge, and heterogeneous user behaviors.\nMotivated by the potential and challenges, we propose Shopping MMLU, a diverse\nmulti-task online shopping benchmark derived from real-world Amazon data.\nShopping MMLU consists of 57 tasks covering 4 major shopping skills: concept\nunderstanding, knowledge reasoning, user behavior alignment, and\nmulti-linguality, and can thus comprehensively evaluate the abilities of LLMs\nas general shop assistants. With Shopping MMLU, we benchmark over 20 existing\nLLMs and uncover valuable insights about practices and prospects of building\nversatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at\nhttps://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we\nhost a competition in KDD Cup 2024 with over 500 participating teams. The\nwinning solutions and the associated workshop can be accessed at our website\nhttps://amazon-kddcup24.github.io/.\n","authors":["Yilun Jin","Zheng Li","Chenwei Zhang","Tianyu Cao","Yifan Gao","Pratik Jayarao","Mao Li","Xin Liu","Ritesh Sarkhel","Xianfeng Tang","Haodong Wang","Zhengyang Wang","Wenju Xu","Jingfeng Yang","Qingyu Yin","Xian Li","Priyanka Nigam","Yi Xu","Kai Chen","Qiang Yang","Meng Jiang","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2410.20745v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track Accepted. Modified typos\n  in Figure 9"},{"id":"http://arxiv.org/abs/2410.23894v1","updated":"2024-10-31T12:53:56Z","published":"2024-10-31T12:53:56Z","title":"Metamorphic Malware Evolution: The Potential and Peril of Large Language\n  Models","summary":"  Code metamorphism refers to a computer programming exercise wherein the\nprogram modifies its own code (partial or entire) consistently and\nautomatically while retaining its core functionality. This technique is often\nused for online performance optimization and automated crash recovery in\ncertain mission-critical applications. However, the technique has been\nmisappropriated by malware creators to bypass signature-based detection\nmeasures instituted by anti-malware engines. However, current code mutation\nengines used by threat actors offer only a limited degree of mutation, which is\nfrequently detectable via static code analysis. The advent of large language\nmodels (LLMs), such as ChatGPT 4.0 and Google Bard may lead to a significant\nevolution in this landscape. These models have demonstrated a level of\nalgorithm comprehension and code synthesis capability that closely resembles\nhuman abilities. This advancement has sparked concerns among experts that such\nmodels could be exploited by threat actors to generate sophisticated\nmetamorphic malware. This paper explores the potential of several prominent\nLLMs for software code mutation that may be used to reconstruct (with mutation)\nexisting malware code bases or create new forms of embedded mutation engines\nfor next-gen metamorphic malwares. In this work, we introduce a framework for\ncreating self-testing program mutation engines based on LLM/Transformer-based\nmodels. The proposed framework serves as an essential tool in testing next-gen\nmetamorphic malware detection engines.\n","authors":["Pooria Madani"],"pdf_url":"https://arxiv.org/pdf/2410.23894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23893v1","updated":"2024-10-31T12:53:53Z","published":"2024-10-31T12:53:53Z","title":"DiffBatt: A Diffusion Model for Battery Degradation Prediction and\n  Synthesis","summary":"  Battery degradation remains a critical challenge in the pursuit of green\ntechnologies and sustainable energy solutions. Despite significant research\nefforts, predicting battery capacity loss accurately remains a formidable task\ndue to its complex nature, influenced by both aging and cycling behaviors. To\naddress this challenge, we introduce a novel general-purpose model for battery\ndegradation prediction and synthesis, DiffBatt. Leveraging an innovative\ncombination of conditional and unconditional diffusion models with\nclassifier-free guidance and transformer architecture, DiffBatt achieves high\nexpressivity and scalability. DiffBatt operates as a probabilistic model to\ncapture uncertainty in aging behaviors and a generative model to simulate\nbattery degradation. The performance of the model excels in prediction tasks\nwhile also enabling the generation of synthetic degradation curves,\nfacilitating enhanced model training by data augmentation. In the remaining\nuseful life prediction task, DiffBatt provides accurate results with a mean\nRMSE of 196 cycles across all datasets, outperforming all other models and\ndemonstrating superior generalizability. This work represents an important step\ntowards developing foundational models for battery degradation.\n","authors":["Hamidreza Eivazi","Andr√© Hebenbrock","Raphael Ginster","Steffen Bl√∂meke","Stefan Wittek","Christoph Hermann","Thomas S. Spengler","Thomas Turek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2410.23893v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.02910v2","updated":"2024-10-31T12:52:55Z","published":"2023-02-06T16:29:50Z","title":"An Empirical Analysis of Fairness Notions under Differential Privacy","summary":"  Recent works have shown that selecting an optimal model architecture suited\nto the differential privacy setting is necessary to achieve the best possible\nutility for a given privacy budget using differentially private stochastic\ngradient descent (DP-SGD)(Tramer and Boneh 2020; Cheng et al. 2022). In light\nof these findings, we empirically analyse how different fairness notions,\nbelonging to distinct classes of statistical fairness criteria (independence,\nseparation and sufficiency), are impacted when one selects a model architecture\nsuitable for DP-SGD, optimized for utility. Using standard datasets from ML\nfairness literature, we show using a rigorous experimental protocol, that by\nselecting the optimal model architecture for DP-SGD, the differences across\ngroups concerning the relevant fairness metrics (demographic parity, equalized\nodds and predictive parity) more often decrease or are negligibly impacted,\ncompared to the non-private baseline, for which optimal model architecture has\nalso been selected to maximize utility. These findings challenge the\nunderstanding that differential privacy will necessarily exacerbate unfairness\nin deep learning models trained on biased datasets.\n","authors":["Anderson Santana de Oliveira","Caelin Kaplan","Khawla Mallat","Tanmay Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2302.02910v2.pdf","comment":"Accepted for oral presentation at the The Fourth AAAI Workshop on\n  Privacy-Preserving Artificial Intelligence (PPAI-23)\n  https://aaai-ppai23.github.io/#accepted_papers"},{"id":"http://arxiv.org/abs/2410.23889v1","updated":"2024-10-31T12:51:40Z","published":"2024-10-31T12:51:40Z","title":"GEPS: Boosting Generalization in Parametric PDE Neural Solvers through\n  Adaptive Conditioning","summary":"  Solving parametric partial differential equations (PDEs) presents significant\nchallenges for data-driven methods due to the sensitivity of spatio-temporal\ndynamics to variations in PDE parameters. Machine learning approaches often\nstruggle to capture this variability. To address this, data-driven approaches\nlearn parametric PDEs by sampling a very large variety of trajectories with\nvarying PDE parameters. We first show that incorporating conditioning\nmechanisms for learning parametric PDEs is essential and that among them,\n$\\textit{adaptive conditioning}$, allows stronger generalization. As existing\nadaptive conditioning methods do not scale well with respect to the number of\nparameters to adapt in the neural solver, we propose GEPS, a simple adaptation\nmechanism to boost GEneralization in Pde Solvers via a first-order optimization\nand low-rank rapid adaptation of a small set of context parameters. We\ndemonstrate the versatility of our approach for both fully data-driven and for\nphysics-aware neural solvers. Validation performed on a whole range of\nspatio-temporal forecasting problems demonstrates excellent performance for\ngeneralizing to unseen conditions including initial conditions, PDE\ncoefficients, forcing terms and solution domain. $\\textit{Project page}$:\nhttps://geps-project.github.io\n","authors":["Armand Kassa√Ø Koupa√Ø","Jorge Misfut Benet","Yuan Yin","Jean-No√´l Vittaut","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2410.23889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23884v1","updated":"2024-10-31T12:48:58Z","published":"2024-10-31T12:48:58Z","title":"Failure Modes of LLMs for Causal Reasoning on Narratives","summary":"  In this work, we investigate the causal reasoning abilities of large language\nmodels (LLMs) through the representative problem of inferring causal\nrelationships from narratives. We find that even state-of-the-art language\nmodels rely on unreliable shortcuts, both in terms of the narrative\npresentation and their parametric knowledge. For example, LLMs tend to\ndetermine causal relationships based on the topological ordering of events\n(i.e., earlier events cause later ones), resulting in lower performance\nwhenever events are not narrated in their exact causal order. Similarly, we\ndemonstrate that LLMs struggle with long-term causal reasoning and often fail\nwhen the narratives are long and contain many events. Additionally, we show\nLLMs appear to rely heavily on their parametric knowledge at the expense of\nreasoning over the provided narrative. This degrades their abilities whenever\nthe narrative opposes parametric knowledge. We extensively validate these\nfailure modes through carefully controlled synthetic experiments, as well as\nevaluations on real-world narratives. Finally, we observe that explicitly\ngenerating a causal graph generally improves performance while naive\nchain-of-thought is ineffective. Collectively, our results distill precise\nfailure modes of current state-of-the-art models and can pave the way for\nfuture techniques to enhance causal reasoning in LLMs.\n","authors":["Khurram Yamin","Shantanu Gupta","Gaurav R. Ghosal","Zachary C. Lipton","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2410.23884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14762v3","updated":"2024-10-31T12:46:43Z","published":"2024-05-23T16:30:51Z","title":"Neural Pfaffians: Solving Many Many-Electron Schr√∂dinger Equations","summary":"  Neural wave functions accomplished unprecedented accuracies in approximating\nthe ground state of many-electron systems, though at a high computational cost.\nRecent works proposed amortizing the cost by learning generalized wave\nfunctions across different structures and compounds instead of solving each\nproblem independently. Enforcing the permutation antisymmetry of electrons in\nsuch generalized neural wave functions remained challenging as existing methods\nrequire discrete orbital selection via non-learnable hand-crafted algorithms.\nThis work tackles the problem by defining overparametrized, fully learnable\nneural wave functions suitable for generalization across molecules. We achieve\nthis by relying on Pfaffians rather than Slater determinants. The Pfaffian\nallows us to enforce the antisymmetry on arbitrary electronic systems without\nany constraint on electronic spin configurations or molecular structure. Our\nempirical evaluation finds that a single neural Pfaffian calculates the ground\nstate and ionization energies with chemical accuracy across various systems. On\nthe TinyMol dataset, we outperform the `gold-standard' CCSD(T) CBS reference\nenergies by 1.9m$E_h$ and reduce energy errors compared to previous generalized\nneural wave functions by up to an order of magnitude.\n","authors":["Nicholas Gao","Stephan G√ºnnemann"],"pdf_url":"https://arxiv.org/pdf/2405.14762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.15199v3","updated":"2024-10-31T12:44:50Z","published":"2024-04-23T16:35:14Z","title":"Reinforcement Learning with Adaptive Regularization for Safe Control of\n  Critical Systems","summary":"  Reinforcement Learning (RL) is a powerful method for controlling dynamic\nsystems, but its learning mechanism can lead to unpredictable actions that\nundermine the safety of critical systems. Here, we propose RL with Adaptive\nRegularization (RL-AR), an algorithm that enables safe RL exploration by\ncombining the RL policy with a policy regularizer that hard-codes the safety\nconstraints. RL-AR performs policy combination via a \"focus module,\" which\ndetermines the appropriate combination depending on the state--relying more on\nthe safe policy regularizer for less-exploited states while allowing unbiased\nconvergence for well-exploited states. In a series of critical control\napplications, we demonstrate that RL-AR not only ensures safety during training\nbut also achieves a return competitive with the standards of model-free RL that\ndisregards safety.\n","authors":["Haozhe Tian","Homayoun Hamedmoghadam","Robert Shorten","Pietro Ferraro"],"pdf_url":"https://arxiv.org/pdf/2404.15199v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23881v1","updated":"2024-10-31T12:44:07Z","published":"2024-10-31T12:44:07Z","title":"DynaSplit: A Hardware-Software Co-Design Framework for Energy-Aware\n  Inference on Edge","summary":"  The deployment of ML models on edge devices is challenged by limited\ncomputational resources and energy availability. While split computing enables\nthe decomposition of large neural networks (NNs) and allows partial computation\non both edge and cloud devices, identifying the most suitable split layer and\nhardware configurations is a non-trivial task. This process is in fact hindered\nby the large configuration space, the non-linear dependencies between software\nand hardware parameters, the heterogeneous hardware and energy characteristics,\nand the dynamic workload conditions. To overcome this challenge, we propose\nDynaSplit, a two-phase framework that dynamically configures parameters across\nboth software (i.e., split layer) and hardware (e.g., accelerator usage, CPU\nfrequency). During the Offline Phase, we solve a multi-objective optimization\nproblem with a meta-heuristic approach to discover optimal settings. During the\nOnline Phase, a scheduling algorithm identifies the most suitable settings for\nan incoming inference request and configures the system accordingly. We\nevaluate DynaSplit using popular pre-trained NNs on a real-world testbed.\nExperimental results show a reduction in energy consumption up to 72% compared\nto cloud-only computation, while meeting ~90% of user request's latency\nthreshold compared to baselines.\n","authors":["Daniel May","Alessandro Tundo","Shashikant Ilager","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2410.23881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23880v1","updated":"2024-10-31T12:40:38Z","published":"2024-10-31T12:40:38Z","title":"Directly Optimizing Explanations for Desired Properties","summary":"  When explaining black-box machine learning models, it's often important for\nexplanations to have certain desirable properties. Most existing methods\n`encourage' desirable properties in their construction of explanations. In this\nwork, we demonstrate that these forms of encouragement do not consistently\ncreate explanations with the properties that are supposedly being targeted.\nMoreover, they do not allow for any control over which properties are\nprioritized when different properties are at odds with each other. We propose\nto directly optimize explanations for desired properties. Our direct approach\nnot only produces explanations with optimal properties more consistently but\nalso empowers users to control trade-offs between different properties,\nallowing them to create explanations with exactly what is needed for a\nparticular task.\n","authors":["Hiwot Belay Tadesse","Alihan H√ºy√ºk","Weiwei Pan","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2410.23880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03941v2","updated":"2024-10-31T12:27:30Z","published":"2024-02-06T12:18:54Z","title":"Discovery of the Hidden World with Large Language Models","summary":"  Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.\n","authors":["Chenxi Liu","Yongqiang Chen","Tongliang Liu","Mingming Gong","James Cheng","Bo Han","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.03941v2.pdf","comment":"NeurIPS 2024; Chenxi and Yongqiang contributed equally; 59 pages, 72\n  figures; Project page: https://causalcoat.github.io/"},{"id":"http://arxiv.org/abs/2404.12404v3","updated":"2024-10-31T12:27:14Z","published":"2024-04-15T17:49:16Z","title":"EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n","authors":["Jinhee Kim","Taesung Kim","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2404.12404v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2402.06255v4","updated":"2024-10-31T12:24:14Z","published":"2024-02-09T09:09:39Z","title":"Fight Back Against Jailbreaking via Prompt Adversarial Tuning","summary":"  While Large Language Models (LLMs) have achieved tremendous success in\nvarious applications, they are also susceptible to jailbreaking attacks.\nSeveral primary defense strategies have been proposed to protect LLMs from\nproducing harmful information, mostly focusing on model fine-tuning or\nheuristical defense designs. However, how to achieve intrinsic robustness\nthrough prompt optimization remains an open problem. In this paper, motivated\nby adversarial training paradigms for achieving reliable robustness, we propose\nan approach named Prompt Adversarial Tuning (PAT) that trains a prompt control\nattached to the user prompt as a guard prefix. To achieve our defense goal\nwhilst maintaining natural performance, we optimize the control prompt with\nboth adversarial and benign prompts. Comprehensive experiments show that our\nmethod is effective against both grey-box and black-box attacks, reducing the\nsuccess rate of advanced attacks to nearly 0%, while maintaining the model's\nutility on the benign task and incurring only negligible computational\noverhead, charting a new perspective for future explorations in LLM security.\nOur code is available at https://github.com/PKU-ML/PAT.\n","authors":["Yichuan Mo","Yuji Wang","Zeming Wei","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.06255v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09695v2","updated":"2024-10-31T12:22:50Z","published":"2024-10-13T02:10:26Z","title":"Can In-context Learning Really Generalize to Out-of-distribution Tasks?","summary":"  In this work, we explore the mechanism of in-context learning (ICL) on\nout-of-distribution (OOD) tasks that were not encountered during training. To\nachieve this, we conduct synthetic experiments where the objective is to learn\nOOD mathematical functions through ICL using a GPT-2 model. We reveal that\nTransformers may struggle to learn OOD task functions through ICL.\nSpecifically, ICL performance resembles implementing a function within the\npretraining hypothesis space and optimizing it with gradient descent based on\nthe in-context examples. Additionally, we investigate ICL's well-documented\nability to learn unseen abstract labels in context. We demonstrate that such\nability only manifests in the scenarios without distributional shifts and,\ntherefore, may not serve as evidence of new-task-learning ability. Furthermore,\nwe assess ICL's performance on OOD tasks when the model is pretrained on\nmultiple tasks. Both empirical and theoretical analyses demonstrate the\nexistence of the \\textbf{low-test-error preference} of ICL, where it tends to\nimplement the pretraining function that yields low test error in the testing\ncontext. We validate this through numerical experiments. This new theoretical\nresult, combined with our empirical findings, elucidates the mechanism of ICL\nin addressing OOD tasks.\n","authors":["Qixun Wang","Yifei Wang","Yisen Wang","Xianghua Ying"],"pdf_url":"https://arxiv.org/pdf/2410.09695v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2410.23870v1","updated":"2024-10-31T12:22:19Z","published":"2024-10-31T12:22:19Z","title":"Noise as a Double-Edged Sword: Reinforcement Learning Exploits\n  Randomized Defenses in Neural Networks","summary":"  This study investigates a counterintuitive phenomenon in adversarial machine\nlearning: the potential for noise-based defenses to inadvertently aid evasion\nattacks in certain scenarios. While randomness is often employed as a defensive\nstrategy against adversarial examples, our research reveals that this approach\ncan sometimes backfire, particularly when facing adaptive attackers using\nreinforcement learning (RL). Our findings show that in specific cases,\nespecially with visually noisy classes, the introduction of noise in the\nclassifier's confidence values can be exploited by the RL attacker, leading to\na significant increase in evasion success rates. In some instances, the\nnoise-based defense scenario outperformed other strategies by up to 20\\% on a\nsubset of classes. However, this effect was not consistent across all\nclassifiers tested, highlighting the complexity of the interaction between\nnoise-based defenses and different models. These results suggest that in some\ncases, noise-based defenses can inadvertently create an adversarial training\nloop beneficial to the RL attacker. Our study emphasizes the need for a more\nnuanced approach to defensive strategies in adversarial machine learning,\nparticularly in safety-critical applications. It challenges the assumption that\nrandomness universally enhances defense against evasion attacks and highlights\nthe importance of considering adaptive, RL-based attackers when designing\nrobust defense mechanisms.\n","authors":["Steve Bakos","Pooria Madani","Heidar Davoudi"],"pdf_url":"https://arxiv.org/pdf/2410.23870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16965v4","updated":"2024-10-31T12:21:32Z","published":"2023-09-29T04:23:58Z","title":"Controlling Continuous Relaxation for Combinatorial Optimization","summary":"  Unsupervised learning (UL)-based solvers for combinatorial optimization (CO)\ntrain a neural network that generates a soft solution by directly optimizing\nthe CO objective using a continuous relaxation strategy. These solvers offer\nseveral advantages over traditional methods and other learning-based methods,\nparticularly for large-scale CO problems. However, UL-based solvers face two\npractical issues: (I) an optimization issue, where UL-based solvers are easily\ntrapped at local optima, and (II) a rounding issue, where UL-based solvers\nrequire artificial post-learning rounding from the continuous space back to the\noriginal discrete space, undermining the robustness of the results. This study\nproposes a Continuous Relaxation Annealing (CRA) strategy, an effective\nrounding-free learning method for UL-based solvers. CRA introduces a penalty\nterm that dynamically shifts from prioritizing continuous solutions,\neffectively smoothing the non-convexity of the objective function, to enforcing\ndiscreteness, eliminating artificial rounding. Experimental results demonstrate\nthat CRA significantly enhances the performance of UL-based solvers,\noutperforming existing UL-based solvers and greedy algorithms in complex CO\nproblems. Additionally, CRA effectively eliminates artificial rounding and\naccelerates the learning process.\n","authors":["Yuma Ichikawa"],"pdf_url":"https://arxiv.org/pdf/2309.16965v4.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.23867v1","updated":"2024-10-31T12:20:36Z","published":"2024-10-31T12:20:36Z","title":"QuACK: A Multipurpose Queuing Algorithm for Cooperative $k$-Armed\n  Bandits","summary":"  We study the cooperative stochastic $k$-armed bandit problem, where a network\nof $m$ agents collaborate to find the optimal action. In contrast to most prior\nwork on this problem, which focuses on extending a specific algorithm to the\nmulti-agent setting, we provide a black-box reduction that allows us to extend\nany single-agent bandit algorithm to the multi-agent setting. Under mild\nassumptions on the bandit environment, we prove that our reduction transfers\nthe regret guarantees of the single-agent algorithm to the multi-agent setting.\nThese guarantees are tight in subgaussian environments, in that using a near\nminimax optimal single-player algorithm is near minimax optimal in the\nmulti-player setting up to an additive graph-dependent quantity. Our reduction\nand theoretical results are also general, and apply to many different bandit\nsettings. By plugging in appropriate single-player algorithms, we can easily\ndevelop provably efficient algorithms for many multi-player settings such as\nheavy-tailed bandits, duelling bandits and bandits with local differential\nprivacy, among others. Experimentally, our approach is competitive with or\noutperforms specialised multi-agent algorithms.\n","authors":["Benjamin Howson","Sarah Filippi","Ciara Pike-Burke"],"pdf_url":"https://arxiv.org/pdf/2410.23867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16173v2","updated":"2024-10-31T12:20:16Z","published":"2024-05-25T10:45:46Z","title":"Diffusion-based Reinforcement Learning via Q-weighted Variational Policy\n  Optimization","summary":"  Diffusion models have garnered widespread attention in Reinforcement Learning\n(RL) for their powerful expressiveness and multimodality. It has been verified\nthat utilizing diffusion policies can significantly improve the performance of\nRL algorithms in continuous control tasks by overcoming the limitations of\nunimodal policies, such as Gaussian policies, and providing the agent with\nenhanced exploration capabilities. However, existing works mainly focus on the\napplication of diffusion policies in offline RL, while their incorporation into\nonline RL is less investigated. The training objective of the diffusion model,\nknown as the variational lower bound, cannot be optimized directly in online RL\ndue to the unavailability of 'good' actions. This leads to difficulties in\nconducting diffusion policy improvement. To overcome this, we propose a novel\nmodel-free diffusion-based online RL algorithm, Q-weighted Variational Policy\nOptimization (QVPO). Specifically, we introduce the Q-weighted variational\nloss, which can be proved to be a tight lower bound of the policy objective in\nonline RL under certain conditions. To fulfill these conditions, the Q-weight\ntransformation functions are introduced for general scenarios. Additionally, to\nfurther enhance the exploration capability of the diffusion policy, we design a\nspecial entropy regularization term. We also develop an efficient behavior\npolicy to enhance sample efficiency by reducing the variance of the diffusion\npolicy during online interactions. Consequently, the QVPO algorithm leverages\nthe exploration capabilities and multimodality of diffusion policies,\npreventing the RL agent from converging to a sub-optimal policy. To verify the\neffectiveness of QVPO, we conduct comprehensive experiments on MuJoCo\nbenchmarks. The final results demonstrate that QVPO achieves state-of-the-art\nperformance on both cumulative reward and sample efficiency.\n","authors":["Shutong Ding","Ke Hu","Zhenhao Zhang","Kan Ren","Weinan Zhang","Jingyi Yu","Jingya Wang","Ye Shi"],"pdf_url":"https://arxiv.org/pdf/2405.16173v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2312.10112v3","updated":"2024-10-31T12:19:37Z","published":"2023-12-15T09:09:25Z","title":"NM-FlowGAN: Modeling sRGB Noise without Paired Images using a Hybrid\n  Approach of Normalizing Flows and GAN","summary":"  Modeling and synthesizing real sRGB noise is crucial for various low-level\nvision tasks, such as building datasets for training image denoising systems.\nThe distribution of real sRGB noise is highly complex and affected by a\nmultitude of factors, making its accurate modeling extremely challenging.\nTherefore, recent studies have proposed methods that employ data-driven\ngenerative models, such as Generative Adversarial Networks (GAN) and\nNormalizing Flows. These studies achieve more accurate modeling of sRGB noise\ncompared to traditional noise modeling methods. However, there are performance\nlimitations due to the inherent characteristics of each generative model. To\naddress this issue, we propose NM-FlowGAN, a hybrid approach that exploits the\nstrengths of both GAN and Normalizing Flows. We combine pixel-wise noise\nmodeling networks based on Normalizing Flows and spatial correlation modeling\nnetworks based on GAN. Specifically, the pixel-wise noise modeling network\nleverages the high training stability of Normalizing Flows to capture noise\ncharacteristics that are affected by a multitude of factors, and the spatial\ncorrelation networks efficiently model pixel-to-pixel relationships. In\nparticular, unlike recent methods that rely on paired noisy images, our method\nsynthesizes noise using clean images and factors that affect noise\ncharacteristics, such as easily obtainable parameters like camera type and ISO\nsettings, making it applicable to various fields where obtaining noisy-clean\nimage pairs is not feasible. In our experiments, our NM-FlowGAN outperforms\nother baselines in the sRGB noise synthesis task. Moreover, the denoising\nneural network trained with synthesized image pairs from our model shows\nsuperior performance compared to other baselines. Our code is available at:\n\\url{https://github.com/YoungJooHan/NM-FlowGAN}.\n","authors":["Young Joo Han","Ha-Jin Yu"],"pdf_url":"https://arxiv.org/pdf/2312.10112v3.pdf","comment":"13 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2410.23862v1","updated":"2024-10-31T12:13:11Z","published":"2024-10-31T12:13:11Z","title":"$œà$DAG: Projected Stochastic Approximation Iteration for DAG\n  Structure Learning","summary":"  Learning the structure of Directed Acyclic Graphs (DAGs) presents a\nsignificant challenge due to the vast combinatorial search space of possible\ngraphs, which scales exponentially with the number of nodes. Recent\nadvancements have redefined this problem as a continuous optimization task by\nincorporating differentiable acyclicity constraints. These methods commonly\nrely on algebraic characterizations of DAGs, such as matrix exponentials, to\nenable the use of gradient-based optimization techniques. Despite these\ninnovations, existing methods often face optimization difficulties due to the\nhighly non-convex nature of DAG constraints and the per-iteration computational\ncomplexity. In this work, we present a novel framework for learning DAGs,\nemploying a Stochastic Approximation approach integrated with Stochastic\nGradient Descent (SGD)-based optimization techniques. Our framework introduces\nnew projection methods tailored to efficiently enforce DAG constraints,\nensuring that the algorithm converges to a feasible local minimum. With its low\niteration complexity, the proposed method is well-suited for handling\nlarge-scale problems with improved computational efficiency. We demonstrate the\neffectiveness and scalability of our framework through comprehensive\nexperimental evaluations, which confirm its superior performance across various\nsettings.\n","authors":["Klea Ziu","Slavom√≠r Hanzely","Loka Li","Kun Zhang","Martin Tak√°ƒç","Dmitry Kamzolov"],"pdf_url":"https://arxiv.org/pdf/2410.23862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11512v3","updated":"2024-10-31T12:10:30Z","published":"2023-05-19T08:22:23Z","title":"Enriching Disentanglement: From Logical Definitions to Quantitative\n  Metrics","summary":"  Disentangling the explanatory factors in complex data is a promising approach\nfor generalizable and data-efficient representation learning. While a variety\nof quantitative metrics for learning and evaluating disentangled\nrepresentations have been proposed, it remains unclear what properties these\nmetrics truly quantify. In this work, we establish algebraic relationships\nbetween logical definitions and quantitative metrics to derive theoretically\ngrounded disentanglement metrics. Concretely, we introduce a compositional\napproach for converting a higher-order predicate into a real-valued quantity by\nreplacing (i) equality with a strict premetric, (ii) the Heyting algebra of\nbinary truth values with a quantale of continuous values, and (iii) quantifiers\nwith aggregators. The metrics induced by logical definitions have strong\ntheoretical guarantees, and some of them are easily differentiable and can be\nused as learning objectives directly. Finally, we empirically demonstrate the\neffectiveness of the proposed metrics by isolating different aspects of\ndisentangled representations.\n","authors":["Yivan Zhang","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2305.11512v3.pdf","comment":"Neural Information Processing Systems 2024"},{"id":"http://arxiv.org/abs/2410.23858v1","updated":"2024-10-31T12:07:52Z","published":"2024-10-31T12:07:52Z","title":"Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable\n  Machine Learning Potential","summary":"  A neural network-based machine learning potential energy surface (PES)\nexpressed in a matrix product operator (NN-MPO) is proposed. The MPO form\nenables efficient evaluation of high-dimensional integrals that arise in\nsolving the time-dependent and time-independent Schr\\\"odinger equation and\neffectively overcomes the so-called curse of dimensionality. This starkly\ncontrasts with other neural network-based machine learning PES methods, such as\nmulti-layer perceptrons (MLPs), where evaluating high-dimensional integrals is\nnot straightforward due to the fully connected topology in their backbone\narchitecture. Nevertheless, the NN-MPO retains the high representational\ncapacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a\ntest mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled\nsix-dimensional ab initio PES, using only 625 training points distributed\nacross a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is\navailable at https://github.com/KenHino/Pompon.\n","authors":["Kentaro Hino","Yuki Kurashige"],"pdf_url":"https://arxiv.org/pdf/2410.23858v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.23856v1","updated":"2024-10-31T12:07:44Z","published":"2024-10-31T12:07:44Z","title":"Can Language Models Perform Robust Reasoning in Chain-of-thought\n  Prompting with Noisy Rationales?","summary":"  This paper investigates an under-explored challenge in large language models\n(LLMs): chain-of-thought prompting with noisy rationales, which include\nirrelevant or inaccurate reasoning thoughts within examples used for in-context\nlearning. We construct NoRa dataset that is tailored to evaluate the robustness\nof reasoning in the presence of noisy rationales. Our findings on NoRa dataset\nreveal a prevalent vulnerability to such noise among current LLMs, with\nexisting robust methods like self-correction and self-consistency showing\nlimited efficacy. Notably, compared to prompting with clean rationales, base\nLLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more\ndrastically by 2.2%-40.4% with inaccurate thoughts.\n  Addressing this challenge necessitates external supervision that should be\naccessible in practice. Here, we propose the method of contrastive denoising\nwith noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning\ncapabilities by contrasting noisy rationales with only one clean rationale,\nwhich can be the minimal requirement for denoising-purpose prompting. This\nmethod follows a principle of exploration and exploitation: (1) rephrasing and\nselecting rationales in the input space to achieve explicit denoising and (2)\nexploring diverse reasoning paths and voting on answers in the output space.\nEmpirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy\nover the base model and shows significantly stronger denoising capabilities\nthan baseline methods. The source code is publicly available at:\nhttps://github.com/tmlr-group/NoisyRationales.\n","authors":["Zhanke Zhou","Rong Tao","Jianing Zhu","Yiwen Luo","Zengmao Wang","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2410.23856v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23855v1","updated":"2024-10-31T12:05:21Z","published":"2024-10-31T12:05:21Z","title":"RAGraph: A General Retrieval-Augmented Graph Learning Framework","summary":"  Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability.\n","authors":["Xinke Jiang","Rihong Qiu","Yongxin Xu","Wentao Zhang","Yichen Zhu","Ruizhe Zhang","Yuchen Fang","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23855v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23854v1","updated":"2024-10-31T12:04:30Z","published":"2024-10-31T12:04:30Z","title":"Airway Labeling Meets Clinical Applications: Reflecting Topology\n  Consistency and Outliers via Learnable Attentions","summary":"  Accurate airway anatomical labeling is crucial for clinicians to identify and\nnavigate complex bronchial structures during bronchoscopy. Automatic airway\nanatomical labeling is challenging due to significant individual variability\nand anatomical variations. Previous methods are prone to generate inconsistent\npredictions, which is harmful for preoperative planning and intraoperative\nnavigation. This paper aims to address these challenges by proposing a novel\nmethod that enhances topological consistency and improves the detection of\nabnormal airway branches.\n  We propose a novel approach incorporating two modules: the Soft Subtree\nConsistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module\nconstructs a soft subtree to capture clinically relevant topological\nrelationships, allowing for flexible feature aggregation within and across\nsubtrees. The ABS module facilitates the interaction between node features and\nprototypes to distinguish abnormal branches, preventing the erroneous\naggregation of features between normal and abnormal nodes.\n  Evaluated on a challenging dataset characterized by severe airway distortion\nand atrophy, our method achieves superior performance compared to\nstate-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the\nsegmental level and an 83.7% accuracy at the subsegmental level, representing a\n1.4% increase in subsegmental accuracy and a 3.1% increase in topological\nconsistency. Notably, the method demonstrates reliable performance in cases\nwith disease-induced airway deformities, ensuring consistent and accurate\nlabeling.\n","authors":["Chenyu Li","Minghui Zhang","Chuyan Zhang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2410.23854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05368v3","updated":"2024-10-31T12:04:12Z","published":"2023-05-09T12:03:42Z","title":"Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive\n  Residual Module","summary":"  Graph Neural Networks (GNNs), a type of neural network that can learn from\ngraph-structured data through neighborhood information aggregation, have shown\nsuperior performance in various downstream tasks. However, as the number of\nlayers increases, node representations become indistinguishable, which is known\nas over-smoothing. To address this issue, many residual methods have emerged.\nIn this paper, we focus on the over-smoothing issue and related residual\nmethods. Firstly, we revisit over-smoothing from the perspective of overlapping\nneighborhood subgraphs, and based on this, we explain how residual methods can\nalleviate over-smoothing by integrating multiple orders neighborhood subgraphs\nto avoid the indistinguishability of the single high-order neighborhood\nsubgraphs. Additionally, we reveal the drawbacks of previous residual methods,\nsuch as the lack of node adaptability and severe loss of high-order\nneighborhood subgraph information, and propose a\n\\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We\ntheoretically demonstrate that PSNR can alleviate the drawbacks of previous\nresidual methods. Furthermore, extensive experiments verify the superiority of\nthe PSNR module in fully observed node classification and missing feature\nscenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.\n","authors":["Jingbo Zhou","Yixuan Du","Ruqiong Zhang","Jun Xia","Zhizhi Yu","Zelin Zang","Di Jin","Carl Yang","Rui Zhang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2305.05368v3.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.23846v1","updated":"2024-10-31T11:55:30Z","published":"2024-10-31T11:55:30Z","title":"Case ID detection based on time series data -- the mining use case","summary":"  Process mining gains increasing popularity in business process analysis, also\nin heavy industry. It requires a specific data format called an event log, with\nthe basic structure including a case identifier (case ID), activity (event)\nname, and timestamp. In the case of industrial processes, data is very often\nprovided by a monitoring system as time series of low level sensor readings.\nThis data cannot be directly used for process mining since there is no explicit\nmarking of activities in the event log, and sometimes, case ID is not provided.\nWe propose a novel rule-based algorithm for identification patterns, based on\nthe identification of significant changes in short-term mean values of selected\nvariable to detect case ID. We present our solution on the mining use case. We\ncompare computed results (identified patterns) with expert labels of the same\ndataset. Experiments show that the developed algorithm in the most of the cases\ncorrectly detects IDs in datasets with and without outliers reaching F1 score\nvalues: 96.8% and 97% respectively. We also evaluate our algorithm on dataset\nfrom manufacturing domain reaching value 92.6% for F1 score.\n","authors":["Edyta Brzychczy","Tomasz Pe≈Çech-Pilichowski","Ziemowit Dworakowski"],"pdf_url":"https://arxiv.org/pdf/2410.23846v1.pdf","comment":"Presented at EdbA'24 - Fifth International Workshop on Event Data and\n  Behavioral Analytics, ICPM 2024, Kopenhagen, Denmark"},{"id":"http://arxiv.org/abs/2402.08406v3","updated":"2024-10-31T11:47:25Z","published":"2024-02-13T12:11:40Z","title":"Transition Constrained Bayesian Optimization via Markov Decision\n  Processes","summary":"  Bayesian optimization is a methodology to optimize black-box functions.\nTraditionally, it focuses on the setting where you can arbitrarily query the\nsearch space. However, many real-life problems do not offer this flexibility;\nin particular, the search space of the next query may depend on previous ones.\nExample challenges arise in the physical sciences in the form of local movement\nconstraints, required monotonicity in certain variables, and transitions\ninfluencing the accuracy of measurements. Altogether, such transition\nconstraints necessitate a form of planning. This work extends classical\nBayesian optimization via the framework of Markov Decision Processes. We\niteratively solve a tractable linearization of our utility function using\nreinforcement learning to obtain a policy that plans ahead for the entire\nhorizon. This is a parallel to the optimization of an acquisition function in\npolicy space. The resulting policy is potentially history-dependent and\nnon-Markovian. We showcase applications in chemical reactor optimization,\ninformative path planning, machine calibration, and other synthetic examples.\n","authors":["Jose Pablo Folch","Calvin Tsay","Robert M Lee","Behrang Shafei","Weronika Ormaniec","Andreas Krause","Mark van der Wilk","Ruth Misener","Mojm√≠r Mutn√Ω"],"pdf_url":"https://arxiv.org/pdf/2402.08406v3.pdf","comment":"10 pages main, 34 pages total, 17 figures, 2 tables. Accepted to\n  NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23840v1","updated":"2024-10-31T11:46:48Z","published":"2024-10-31T11:46:48Z","title":"Deterministic Exploration via Stationary Bellman Error Maximization","summary":"  Exploration is a crucial and distinctive aspect of reinforcement learning\n(RL) that remains a fundamental open problem. Several methods have been\nproposed to tackle this challenge. Commonly used methods inject random noise\ndirectly into the actions, indirectly via entropy maximization, or add\nintrinsic rewards that encourage the agent to steer to novel regions of the\nstate space. Another previously seen idea is to use the Bellman error as a\nseparate optimization objective for exploration. In this paper, we introduce\nthree modifications to stabilize the latter and arrive at a deterministic\nexploration policy. Our separate exploration agent is informed about the state\nof the exploitation, thus enabling it to account for previous experiences.\nFurther components are introduced to make the exploration objective agnostic\ntoward the episode length and to mitigate instability introduced by\nfar-off-policy learning. Our experimental results show that our approach can\noutperform $\\varepsilon$-greedy in dense and sparse reward settings.\n","authors":["Sebastian Griesbach","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2410.23840v1.pdf","comment":"Published at the 17th European Workshop for Reinforcement Learning"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2409.11393v2","updated":"2024-10-31T11:07:11Z","published":"2024-09-17T17:54:17Z","title":"LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless\n  Integration of Multi Active/Passive Core-Agents","summary":"  In an era where vast amounts of data are collected and processed from diverse\nsources, there is a growing demand to develop sophisticated AI systems capable\nof intelligently fusing and analyzing this information. To address these\nchallenges, researchers have turned towards integrating tools into LLM-powered\nagents to enhance the overall information fusion process. However, the\nconjunction of these technologies and the proposed enhancements in several\nstate-of-the-art works followed a non-unified software architecture resulting\nin a lack of modularity and terminological inconsistencies among researchers.\nTo address these issues, we propose a novel LLM-based Agent Unified Modeling\nFramework (LLM-Agent-UMF) that aims to establish a clear foundation for agent\ndevelopment from both functional and software architectural perspectives. Our\nframework distinguishes between the different components of an LLM-based agent,\nsetting LLMs, and tools apart from a new element, the core-agent, playing the\nrole of the central coordinator of the agent. This pivotal entity comprises\nfive modules: planning, memory, profile, action, and security - the latter\noften neglected in previous works. By classifying core-agents into passive and\nactive types based on their authoritative natures, we propose various\nmulti-core agent architectures that combine unique characteristics of\ndistinctive agents to tackle complex tasks more efficiently. We evaluate our\nframework by applying it to thirteen state-of-the-art agents, thereby\ndemonstrating its alignment with their functionalities and clarifying the\noverlooked architectural aspects. Moreover, we thoroughly assess five of our\nproposed architectures through the integration of existing agents into new\nhybrid active/passive core-agents architectures. This analysis provides\ninsights into potential improvements and highlights challenges involved in\ncombining specific agents.\n","authors":["Amine Ben Hassouna","Hana Chaari","Ines Belhaj"],"pdf_url":"https://arxiv.org/pdf/2409.11393v2.pdf","comment":"36 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.23761v1","updated":"2024-10-31T09:26:37Z","published":"2024-10-31T09:26:37Z","title":"Abstract Continuation Semantics for Multiparty Interactions in Process\n  Calculi based on CCS","summary":"  We develop denotational and operational semantics designed with continuations\nfor process calculi based on CCS extended with mechanisms offering support for\nmultiparty interactions. We investigate the abstractness of this continuation\nsemantics. We show that our continuation-based denotational models are weakly\nabstract with respect to the corresponding operational models.\n","authors":["Eneia Nicolae Todoran","Gabriel Ciobanu"],"pdf_url":"https://arxiv.org/pdf/2410.23761v1.pdf","comment":"In Proceedings FROM 2024, arXiv:2410.23020"},{"id":"http://arxiv.org/abs/2204.07481v3","updated":"2024-10-31T08:41:45Z","published":"2022-04-15T14:31:17Z","title":"Knowledge Equivalence in Digital Twins of Intelligent Systems","summary":"  A digital twin contains up-to-date data-driven models of the physical world\nbeing studied and can use simulation to optimise the physical world. However,\nthe analysis made by the digital twin is valid and reliable only when the model\nis equivalent to the physical world. Maintaining such an equivalent model is\nchallenging, especially when the physical systems being modelled are\nintelligent and autonomous. The paper focuses in particular on digital twin\nmodels of intelligent systems where the systems are knowledge-aware but with\nlimited capability. The digital twin improves the acting of the physical system\nat a meta-level by accumulating more knowledge in the simulated environment.\nThe modelling of such an intelligent physical system requires replicating the\nknowledge-awareness capability in the virtual space. Novel equivalence\nmaintaining techniques are needed, especially in synchronising the knowledge\nbetween the model and the physical system. This paper proposes the notion of\nknowledge equivalence and an equivalence maintaining approach by knowledge\ncomparison and updates. A quantitative analysis of the proposed approach\nconfirms that compared to state equivalence, knowledge equivalence maintenance\ncan tolerate deviation thus reducing unnecessary updates and achieve more\nPareto efficient solutions for the trade-off between update overhead and\nsimulation reliability.\n","authors":["Nan Zhang","Rami Bahsoon","Nikos Tziritas","Georgios Theodoropoulos"],"pdf_url":"https://arxiv.org/pdf/2204.07481v3.pdf","comment":"35 pages, 16 figures. In ACM Transactions on Modeling and Computer\n  Simulation (TOMACS)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.23883v1","updated":"2024-10-31T12:45:54Z","published":"2024-10-31T12:45:54Z","title":"'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue","summary":"  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n","authors":["Rena Gao","Xuetong Wu","Siwen Luo","Caren Han","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23883v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.23861v1","updated":"2024-10-31T12:11:17Z","published":"2024-10-31T12:11:17Z","title":"Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models","summary":"  Large Multimodal Models (LMMs) have demonstrated the ability to interact with\nhumans under real-world conditions by combining Large Language Models (LLMs)\nand modality encoders to align multimodal information (visual and auditory)\nwith text. However, such models raise new safety challenges of whether models\nthat are safety-aligned on text also exhibit consistent safeguards for\nmultimodal inputs. Despite recent safety-alignment research on vision LMMs, the\nsafety of audio LMMs remains under-explored. In this work, we comprehensively\nred team the safety of five advanced audio LMMs under three settings: (i)\nharmful questions in both audio and text formats, (ii) harmful questions in\ntext format accompanied by distracting non-speech audio, and (iii)\nspeech-specific jailbreaks. Our results under these settings demonstrate that\nopen-source audio LMMs suffer an average attack success rate of 69.14% on\nharmful audio questions, and exhibit safety vulnerabilities when distracted\nwith non-speech audio noise. Our speech-specific jailbreaks on Gemini-1.5-Pro\nachieve an attack success rate of 70.67% on the harmful query benchmark. We\nprovide insights on what could cause these reported safety-misalignments.\nWarning: this paper contains offensive examples.\n","authors":["Hao Yang","Lizhen Qu","Ehsan Shareghi","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2410.23861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23663v1","updated":"2024-10-31T06:26:00Z","published":"2024-10-31T06:26:00Z","title":"DIP: Diffusion Learning of Inconsistency Pattern for General DeepFake\n  Detection","summary":"  With the advancement of deepfake generation techniques, the importance of\ndeepfake detection in protecting multimedia content integrity has become\nincreasingly obvious. Recently, temporal inconsistency clues have been explored\nto improve the generalizability of deepfake video detection. According to our\nobservation, the temporal artifacts of forged videos in terms of motion\ninformation usually exhibits quite distinct inconsistency patterns along\nhorizontal and vertical directions, which could be leveraged to improve the\ngeneralizability of detectors. In this paper, a transformer-based framework for\nDiffusion Learning of Inconsistency Pattern (DIP) is proposed, which exploits\ndirectional inconsistencies for deepfake video detection. Specifically, DIP\nbegins with a spatiotemporal encoder to represent spatiotemporal information. A\ndirectional inconsistency decoder is adopted accordingly, where direction-aware\nattention and inconsistency diffusion are incorporated to explore potential\ninconsistency patterns and jointly learn the inherent relationships. In\naddition, the SpatioTemporal Invariant Loss (STI Loss) is introduced to\ncontrast spatiotemporally augmented sample pairs and prevent the model from\noverfitting nonessential forgery artifacts. Extensive experiments on several\npublic datasets demonstrate that our method could effectively identify\ndirectional forgery clues and achieve state-of-the-art performance.\n","authors":["Fan Nie","Jiangqun Ni","Jian Zhang","Bin Zhang","Weizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23663v1.pdf","comment":"13 pages, accepted with IEEE Trans. on Multimedia"},{"id":"http://arxiv.org/abs/2410.23230v2","updated":"2024-10-31T04:20:22Z","published":"2024-10-30T17:18:53Z","title":"Aligning Audio-Visual Joint Representations with an Agentic Workflow","summary":"  Visual content and accompanied audio signals naturally formulate a joint\nrepresentation to improve audio-visual (AV) related applications. While studies\ndevelop various AV representation learning frameworks, the importance of AV\ndata alignment is usually undermined for achieving high-quality representation.\nWe observe that an audio signal may contain background noise interference.\nAlso, non-synchronization may appear between audio and video streams. These\nnon-strict data alignment limits representation quality and downgrade\napplication performance. In this paper, we propose to improve AV joint\nrepresentations from a data-centric perspective by aligning audio signals to\nvisual data. Our alignment is conducted in an agentic workflow controlled by an\nLLM-based assistant named AVAgent. For each input AV data pair, our AVAgent\nuses a multi-modal LLM to convert audio and visual data into language\ndescriptions separately (i.e., tool use). Then, AVAgent reasons whether this\npaired data is aligned well and plans to edit the audio signal if needed (i.e.,\nplanning). The audio editing is executed by predefined actions that filter\nnoise or augment data. Moreover, we use a VLM to evaluate how modified audio\nsignals match the visual content and provide feedback to AVAgent (i.e.,\nreflection). The tool use, planning, and reflection steps operate cyclically to\nbecome an agentic workflow where audio signals are gradually aligned to visual\ncontent. To this end, existing methods can directly leverage the aligned AV\ndata via our agentic workflow to improve AV joint representations. The\nexperimental results comprehensively demonstrate the state-of-the-art\nperformance of the proposed approach against previous baselines in diverse\ndownstream tasks.\n","authors":["Shentong Mo","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2410.23230v2.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2410.24226v1","updated":"2024-10-31T17:59:59Z","published":"2024-10-31T17:59:59Z","title":"Tensegrity Robot Proprioceptive State Estimation with Geometric\n  Constraints","summary":"  Tensegrity robots, characterized by a synergistic assembly of rigid rods and\nelastic cables, form robust structures that are resistant to impacts. However,\nthis design introduces complexities in kinematics and dynamics, complicating\ncontrol and state estimation. This work presents a novel proprioceptive state\nestimator for tensegrity robots. The estimator initially uses the geometric\nconstraints of 3-bar prism tensegrity structures, combined with IMU and motor\nencoder measurements, to reconstruct the robot's shape and orientation. It then\nemploys a contact-aided invariant extended Kalman filter with forward\nkinematics to estimate the global position and orientation of the tensegrity\nrobot. The state estimator's accuracy is assessed against ground truth data in\nboth simulated environments and real-world tensegrity robot applications. It\nachieves an average drift percentage of 4.2%, comparable to the state\nestimation performance of traditional rigid robots. This state estimator\nadvances the state of the art in tensegrity robot state estimation and has the\npotential to run in real-time using onboard sensors, paving the way for full\nautonomy of tensegrity robots in unstructured environments.\n","authors":["Wenzhe Tong","Tzu-Yuan Lin","Jonathan Mi","Yicheng Jiang","Maani Ghaffari","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.24226v1.pdf","comment":"Preprint; 8 pages, 11 figures, 2 tables; Code at\n  https://github.com/Jonathan-Twz/tensegrity-robot-state-estimator"},{"id":"http://arxiv.org/abs/2410.24221v1","updated":"2024-10-31T17:59:55Z","published":"2024-10-31T17:59:55Z","title":"EgoMimic: Scaling Imitation Learning via Egocentric Video","summary":"  The scale and diversity of demonstration data required for imitation learning\nis a significant challenge. We present EgoMimic, a full-stack framework which\nscales manipulation via human embodiment data, specifically egocentric human\nvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) a\nsystem to capture human embodiment data using the ergonomic Project Aria\nglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap\nto human data, (3) cross-domain data alignment techniques, and (4) an imitation\nlearning architecture that co-trains on human and robot data. Compared to prior\nworks that only extract high-level intent from human videos, our approach\ntreats human and robot data equally as embodied demonstration data and learns a\nunified policy from both data sources. EgoMimic achieves significant\nimprovement on a diverse set of long-horizon, single-arm and bimanual\nmanipulation tasks over state-of-the-art imitation learning methods and enables\ngeneralization to entirely new scenes. Finally, we show a favorable scaling\ntrend for EgoMimic, where adding 1 hour of additional hand data is\nsignificantly more valuable than 1 hour of additional robot data. Videos and\nadditional information can be found at https://egomimic.github.io/\n","authors":["Simar Kareer","Dhruv Patel","Ryan Punamiya","Pranay Mathur","Shuo Cheng","Chen Wang","Judy Hoffman","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.24221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24218v1","updated":"2024-10-31T17:59:52Z","published":"2024-10-31T17:59:52Z","title":"Teaching Embodied Reinforcement Learning Agents: Informativeness and\n  Diversity of Language Use","summary":"  In real-world scenarios, it is desirable for embodied agents to have the\nability to leverage human language to gain explicit or implicit knowledge for\nlearning tasks. Despite recent progress, most previous approaches adopt simple\nlow-level instructions as language inputs, which may not reflect natural human\ncommunication. It's not clear how to incorporate rich language use to\nfacilitate task learning. To address this question, this paper studies\ndifferent types of language inputs in facilitating reinforcement learning (RL)\nembodied agents. More specifically, we examine how different levels of language\ninformativeness (i.e., feedback on past behaviors and future guidance) and\ndiversity (i.e., variation of language expressions) impact agent learning and\ninference. Our empirical results based on four RL benchmarks demonstrate that\nagents trained with diverse and informative language feedback can achieve\nenhanced generalization and fast adaptation to new tasks. These findings\nhighlight the pivotal role of language use in teaching embodied agents new\ntasks in an open world. Project website:\nhttps://github.com/sled-group/Teachable_RL\n","authors":["Jiajun Xi","Yinong He","Jianing Yang","Yinpei Dai","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2410.24218v1.pdf","comment":"EMNLP 2024 Main. Project website:\n  https://github.com/sled-group/Teachable_RL"},{"id":"http://arxiv.org/abs/2406.15349v2","updated":"2024-10-31T17:58:34Z","published":"2024-06-21T17:59:02Z","title":"NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking","summary":"  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n","authors":["Daniel Dauner","Marcel Hallgarten","Tianyu Li","Xinshuo Weng","Zhiyu Huang","Zetong Yang","Hongyang Li","Igor Gilitschenski","Boris Ivanovic","Marco Pavone","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2406.15349v2.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.24205v1","updated":"2024-10-31T17:57:51Z","published":"2024-10-31T17:57:51Z","title":"Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision Probability\n  and Zone Connectivity","summary":"  Path planning in high-dimensional spaces poses significant challenges,\nparticularly in achieving both time efficiency and a fair success rate. To\naddress these issues, we introduce a novel path-planning algorithm, Zonal\nRL-RRT, that leverages kd-tree partitioning to segment the map into zones while\naddressing zone connectivity, ensuring seamless transitions between zones. By\nbreaking down the complex environment into multiple zones and using Q-learning\nas the high-level decision-maker, our algorithm achieves a 3x improvement in\ntime efficiency compared to basic sampling methods such as RRT and RRT* in\nforest-like maps. Our approach outperforms heuristic-guided methods like BIT*\nand Informed RRT* by 1.5x in terms of runtime while maintaining robust and\nreliable success rates across 2D to 6D environments. Compared to learning-based\nmethods like NeuralRRT* and MPNetSMP, as well as the heuristic RRT*J, our\nalgorithm demonstrates, on average, 1.5x better performance in the same\nenvironments. We also evaluate the effectiveness of our approach through\nsimulations of the UR10e arm manipulator in the MuJoCo environment. A key\nobservation of our approach lies in its use of zone partitioning and\nReinforcement Learning (RL) for adaptive high-level planning allowing the\nalgorithm to accommodate flexible policies across diverse environments, making\nit a versatile tool for advanced path planning.\n","authors":["AmirMohammad Tahmasbi","MohammadSaleh Faghfoorian","Saeed Khodaygan","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2410.24205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24203v1","updated":"2024-10-31T17:57:02Z","published":"2024-10-31T17:57:02Z","title":"DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion","summary":"  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n","authors":["Weicai Ye","Chenhao Ji","Zheng Chen","Junyao Gao","Xiaoshui Huang","Song-Hai Zhang","Wanli Ouyang","Tong He","Cairong Zhao","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.24203v1.pdf","comment":"NeurIPS2024, Project: https://github.com/zju3dv/DiffPano; Code:\n  https://github.com/zju3dv/DiffPano"},{"id":"http://arxiv.org/abs/2410.24196v1","updated":"2024-10-31T17:54:33Z","published":"2024-10-31T17:54:33Z","title":"A Sagittal Planar Ankle-Foot Prosthesis with Powered Plantarflexion and\n  Socket Alignment","summary":"  Powered ankle-foot prostheses can often reduce the energy cost of walking by\nassisting with push-off. However, focus on providing mechanical work may lead\nto ignoring or exacerbating common issues with chronic pain, irritation,\npressure ulcer development, and eventual osteoarthritis in persons with\namputation. This paper presents the design and validation of a novel\ntranstibial prosthesis informed by predictive biomechanical simulations of gait\nwhich minimize a combination of user effort and interaction loading from the\nprosthesis socket. From these findings, the device was designed with a\nnon-biomimetic anterior-posterior translation degree of freedom with a 10 cm\nrange of motion which is primarily position-controlled to change the alignment\nof the prosthetic foot with the residual limb. The system is both mobile and\ntethered, with the batteries, actuators, and majority of electronics located in\na small backpack. Mechanical loads are transmitted through cables to the\nprosthesis, minimizing the distal mass carriage required. We measured torque\nand force sensing accuracy, open loop actuator performance, closed loop torque\nand position control bandwidth, and torque and position tracking error during\nwalking. The system is capable of producing up to 160 N-m of plantarflexion\ntorque and 394 N of AP translation force with a closed loop control bandwidth\nof about 7 Hz in both degrees of freedom. Torque tracking during walking was\naccurate within about 10 N-m but position tracking was substantially affected\nby phase lag, possibly due to cable slack in the bidirectional mechanism. The\nprototype was capable of replicating our simulated prosthesis dynamics during\ngait and offers useful insights into the advantages and the practical\nconsiderations of using predictive biomechanical simulation as a design tool\nfor wearable robots.\n","authors":["Mark A. Price","Frank C. Sup IV"],"pdf_url":"https://arxiv.org/pdf/2410.24196v1.pdf","comment":"9 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.24185v1","updated":"2024-10-31T17:48:45Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v1.pdf","comment":"Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2302.12610v3","updated":"2024-10-31T17:22:32Z","published":"2023-02-24T12:54:18Z","title":"A Joint Modeling of Vision-Language-Action for Target-oriented Grasping\n  in Clutter","summary":"  We focus on the task of language-conditioned grasping in clutter, in which a\nrobot is supposed to grasp the target object based on a language instruction.\nPrevious works separately conduct visual grounding to localize the target\nobject, and generate a grasp for that object. However, these works require\nobject labels or visual attributes for grounding, which calls for handcrafted\nrules in planner and restricts the range of language instructions. In this\npaper, we propose to jointly model vision, language and action with\nobject-centric representation. Our method is applicable under more flexible\nlanguage instructions, and not limited by visual grounding error. Besides, by\nutilizing the powerful priors from the pre-trained multi-modal model and grasp\nmodel, sample efficiency is effectively improved and the sim2real problem is\nrelived without additional data for transfer. A series of experiments carried\nout in simulation and real world indicate that our method can achieve better\ntask success rate by less times of motion under more flexible language\ninstructions. Moreover, our method is capable of generalizing better to\nscenarios with unseen objects and language instructions. Our code is available\nat https://github.com/xukechun/Vision-Language-Grasping\n","authors":["Kechun Xu","Shuqi Zhao","Zhongxiang Zhou","Zizhang Li","Huaijin Pi","Yue Wang","Rong Xiong"],"pdf_url":"https://arxiv.org/pdf/2302.12610v3.pdf","comment":"Accepted by ICRA 2023"},{"id":"http://arxiv.org/abs/2410.24164v1","updated":"2024-10-31T17:22:30Z","published":"2024-10-31T17:22:30Z","title":"$œÄ_0$: A Vision-Language-Action Flow Model for General Robot Control","summary":"  Robot learning holds tremendous promise to unlock the full potential of\nflexible, general, and dexterous robot systems, as well as to address some of\nthe deepest questions in artificial intelligence. However, bringing robot\nlearning to the level of generality required for effective real-world systems\nfaces major obstacles in terms of data, generalization, and robustness. In this\npaper, we discuss how generalist robot policies (i.e., robot foundation models)\ncan address these challenges, and how we can design effective generalist robot\npolicies for complex and highly dexterous tasks. We propose a novel flow\nmatching architecture built on top of a pre-trained vision-language model (VLM)\nto inherit Internet-scale semantic knowledge. We then discuss how this model\ncan be trained on a large and diverse dataset from multiple dexterous robot\nplatforms, including single-arm robots, dual-arm robots, and mobile\nmanipulators. We evaluate our model in terms of its ability to perform tasks in\nzero shot after pre-training, follow language instructions from people and from\na high-level VLM policy, and its ability to acquire new skills via fine-tuning.\nOur results cover a wide variety of tasks, such as laundry folding, table\ncleaning, and assembling boxes.\n","authors":["Kevin Black","Noah Brown","Danny Driess","Adnan Esmail","Michael Equi","Chelsea Finn","Niccolo Fusai","Lachy Groom","Karol Hausman","Brian Ichter","Szymon Jakubczak","Tim Jones","Liyiming Ke","Sergey Levine","Adrian Li-Bell","Mohith Mothukuri","Suraj Nair","Karl Pertsch","Lucy Xiaoyang Shi","James Tanner","Quan Vuong","Anna Walling","Haohuan Wang","Ury Zhilinsky"],"pdf_url":"https://arxiv.org/pdf/2410.24164v1.pdf","comment":"See project website for videos:\n  https://physicalintelligence.company/blog/pi0"},{"id":"http://arxiv.org/abs/2406.00885v2","updated":"2024-10-31T17:12:57Z","published":"2024-06-02T22:40:05Z","title":"Visual place recognition for aerial imagery: A survey","summary":"  Aerial imagery and its direct application to visual localization is an\nessential problem for many Robotics and Computer Vision tasks. While Global\nNavigation Satellite Systems (GNSS) are the standard default solution for\nsolving the aerial localization problem, it is subject to a number of\nlimitations, such as, signal instability or solution unreliability that make\nthis option not so desirable. Consequently, visual geolocalization is emerging\nas a viable alternative. However, adapting Visual Place Recognition (VPR) task\nto aerial imagery presents significant challenges, including weather variations\nand repetitive patterns. Current VPR reviews largely neglect the specific\ncontext of aerial data. This paper introduces a methodology tailored for\nevaluating VPR techniques specifically in the domain of aerial imagery,\nproviding a comprehensive assessment of various methods and their performance.\nHowever, we not only compare various VPR methods, but also demonstrate the\nimportance of selecting appropriate zoom and overlap levels when constructing\nmap tiles to achieve maximum efficiency of VPR algorithms in the case of aerial\nimagery. The code is available on our GitHub repository --\nhttps://github.com/prime-slam/aero-vloc.\n","authors":["Ivan Moskalenko","Anastasiia Kornilova","Gonzalo Ferrer"],"pdf_url":"https://arxiv.org/pdf/2406.00885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24152v1","updated":"2024-10-31T17:10:01Z","published":"2024-10-31T17:10:01Z","title":"Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning","summary":"  The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods.\n","authors":["Jiaqi Liu","Chengkai Xu","Peng Hang","Jian Sun","Mingyu Ding","Wei Zhan","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2410.24152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24091v1","updated":"2024-10-31T16:22:53Z","published":"2024-10-31T16:22:53Z","title":"3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing","summary":"  Tactile and visual perception are both crucial for humans to perform\nfine-grained interactions with their environment. Developing similar\nmulti-modal sensing capabilities for robots can significantly enhance and\nexpand their manipulation skills. This paper introduces \\textbf{3D-ViTac}, a\nmulti-modal sensing and learning system designed for dexterous bimanual\nmanipulation. Our system features tactile sensors equipped with dense sensing\nunits, each covering an area of 3$mm^2$. These sensors are low-cost and\nflexible, providing detailed and extensive coverage of physical contacts,\neffectively complementing visual information. To integrate tactile and visual\ndata, we fuse them into a unified 3D representation space that preserves their\n3D structures and spatial relationships. The multi-modal representation can\nthen be coupled with diffusion policies for imitation learning. Through\nconcrete hardware experiments, we demonstrate that even low-cost robots can\nperform precise manipulations and significantly outperform vision-only\npolicies, particularly in safe interactions with fragile items and executing\nlong-horizon tasks involving in-hand manipulation. Our project page is\navailable at \\url{https://binghao-huang.github.io/3D-ViTac/}.\n","authors":["Binghao Huang","Yixuan Wang","Xinyi Yang","Yiyue Luo","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2410.24091v1.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2410.24090v1","updated":"2024-10-31T16:22:23Z","published":"2024-10-31T16:22:23Z","title":"Sparsh: Self-supervised touch representations for vision-based tactile\n  sensing","summary":"  In this work, we introduce general purpose touch representations for the\nincreasingly accessible class of vision-based tactile sensors. Such sensors\nhave led to many recent advances in robot manipulation as they markedly\ncomplement vision, yet solutions today often rely on task and sensor specific\nhandcrafted perception models. Collecting real data at scale with task centric\nground truth labels, like contact forces and slip, is a challenge further\ncompounded by sensors of various form factor differing in aspects like lighting\nand gel markings. To tackle this we turn to self-supervised learning (SSL) that\nhas demonstrated remarkable performance in computer vision. We present Sparsh,\na family of SSL models that can support various vision-based tactile sensors,\nalleviating the need for custom labels through pre-training on 460k+ tactile\nimages with masking and self-distillation in pixel and latent spaces. We also\nbuild TacBench, to facilitate standardized benchmarking across sensors and\nmodels, comprising of six tasks ranging from comprehending tactile properties\nto enabling physical perception and manipulation planning. In evaluations, we\nfind that SSL pre-training for touch representation outperforms task and\nsensor-specific end-to-end training by 95.1% on average over TacBench, and\nSparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the\nmerits of learning in latent space for tactile images. Project page:\nhttps://sparsh-ssl.github.io/\n","authors":["Carolina Higuera","Akash Sharma","Chaithanya Krishna Bodduluri","Taosha Fan","Patrick Lancaster","Mrinal Kalakrishnan","Michael Kaess","Byron Boots","Mike Lambeta","Tingfan Wu","Mustafa Mukadam"],"pdf_url":"https://arxiv.org/pdf/2410.24090v1.pdf","comment":"Conference on Robot Learning (CoRL), 2024"},{"id":"http://arxiv.org/abs/2410.24035v1","updated":"2024-10-31T15:32:32Z","published":"2024-10-31T15:32:32Z","title":"State- and context-dependent robotic manipulation and grasping via\n  uncertainty-aware imitation learning","summary":"  Generating context-adaptive manipulation and grasping actions is a\nchallenging problem in robotics. Classical planning and control algorithms tend\nto be inflexible with regard to parameterization by external variables such as\nobject shapes. In contrast, Learning from Demonstration (LfD) approaches, due\nto their nature as function approximators, allow for introducing external\nvariables to modulate policies in response to the environment. In this paper,\nwe utilize this property by introducing an LfD approach to acquire\ncontext-dependent grasping and manipulation strategies. We treat the problem as\na kernel-based function approximation, where the kernel inputs include generic\ncontext variables describing task-dependent parameters such as the object\nshape. We build on existing work on policy fusion with uncertainty\nquantification to propose a state-dependent approach that automatically returns\nto demonstrations, avoiding unpredictable behavior while smoothly adapting to\ncontext changes. The approach is evaluated against the LASA handwriting dataset\nand on a real 7-DoF robot in two scenarios: adaptation to slippage while\ngrasping and manipulating a deformable food item.\n","authors":["Tim R. Winter","Ashok M. Sundaram","Werner Friedl","Maximo A. Roa","Freek Stulp","Jo√£o Silv√©rio"],"pdf_url":"https://arxiv.org/pdf/2410.24035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22931v2","updated":"2024-10-31T15:04:25Z","published":"2024-10-30T11:37:47Z","title":"GPTR: Gaussian Process Trajectory Representation for Continuous-Time\n  Motion Estimation","summary":"  Continuous-time trajectory representation has gained significant popularity\nin recent years, as it offers an elegant formulation that allows the fusion of\na larger number of sensors and sensing modalities, overcoming limitations of\ntraditional discrete-time frameworks. To bolster the adoption of the\ncontinuous-time paradigm, we propose a so-called Gaussian Process Trajectory\nRepresentation (GPTR) framework for continuous-time motion estimation (CTME)\ntasks. Our approach stands out by employing a third-order random jerk model,\nfeaturing closed-form expressions for both rotational and translational state\nderivatives. This model provides smooth, continuous trajectory representations\nthat are crucial for precise estimation of complex motion. To support the wider\nrobotics and computer vision communities, we have made the source code for GPTR\navailable as a light-weight header-only library. This format was chosen for its\nease of integration, allowing developers to incorporate GPTR into existing\nsystems without needing extensive code modifications. Moreover, we also provide\na set of optimization examples with LiDAR, camera, IMU, UWB factors, and\nclosed-form analytical Jacobians under the proposed GP framework. Our\nexperiments demonstrate the efficacy and efficiency of GP-based trajectory\nrepresentation in various motion estimation tasks, and the examples can serve\nas the prototype to help researchers quickly develop future applications such\nas batch optimization, calibration, sensor fusion, trajectory planning, etc.,\nwith continuous-time trajectory representation. Our project is accessible at\nhttps://github.com/brytsknguyen/gptr .\n","authors":["Thien-Minh Nguyen","Ziyu Cao","Kailai Li","Shenghai Yuan","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2410.22931v2.pdf","comment":"The source code has been released. All feedbacks are welcome"},{"id":"http://arxiv.org/abs/2211.15656v3","updated":"2024-10-31T15:01:41Z","published":"2022-11-28T18:59:02Z","title":"SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation","summary":"  High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.\n","authors":["Hao Dong","Weihao Gu","Xianjing Zhang","Jintao Xu","Rui Ai","Huimin Lu","Juho Kannala","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2211.15656v3.pdf","comment":"ICRA 2024"},{"id":"http://arxiv.org/abs/2407.12582v2","updated":"2024-10-31T14:37:42Z","published":"2024-07-17T14:09:46Z","title":"Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection","summary":"  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n","authors":["Hu Cao","Zehua Zhang","Yan Xia","Xinyi Li","Jiahao Xia","Guang Chen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.12582v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2410.23978v1","updated":"2024-10-31T14:31:53Z","published":"2024-10-31T14:31:53Z","title":"GAMap: Zero-Shot Object Goal Navigation with Multi-Scale\n  Geometric-Affordance Guidance","summary":"  Zero-Shot Object Goal Navigation (ZS-OGN) enables robots or agents to\nnavigate toward objects of unseen categories without object-specific training.\nTraditional approaches often leverage categorical semantic information for\nnavigation guidance, which struggles when only objects are partially observed\nor detailed and functional representations of the environment are lacking. To\nresolve the above two issues, we propose \\textit{Geometric-part and Affordance\nMaps} (GAMap), a novel method that integrates object parts and affordance\nattributes as navigation guidance. Our method includes a multi-scale scoring\napproach to capture geometric-part and affordance attributes of objects at\ndifferent scales. Comprehensive experiments conducted on HM3D and Gibson\nbenchmark datasets demonstrate improvements in Success Rate and Success\nweighted by Path Length, underscoring the efficacy of our geometric-part and\naffordance-guided navigation approach in enhancing robot autonomy and\nversatility, without any additional object-specific training or fine-tuning\nwith the semantics of unseen objects and/or the locomotions of the robot.\n","authors":["Shuaihang Yuan","Hao Huang","Yu Hao","Congcong Wen","Anthony Tzes","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.23978v1.pdf","comment":"16 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.23968v1","updated":"2024-10-31T14:22:20Z","published":"2024-10-31T14:22:20Z","title":"EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable\n  Robot Task Planning","summary":"  Recent advances in Large Language Models (LLMs) have helped facilitate\nexciting progress for robotic planning in real, open-world environments. 3D\nscene graphs (3DSGs) offer a promising environment representation for grounding\nsuch LLM-based planners as they are compact and semantically rich. However, as\nthe robot's environment scales (e.g., number of entities tracked) and the\ncomplexity of scene graph information increases (e.g., maintaining more\nattributes), providing the 3DSG as-is to an LLM-based planner quickly becomes\ninfeasible due to input token count limits and attentional biases present in\nLLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods\nthat retrieve query-relevant document chunks for LLM question and answering, we\nadapt the paradigm for our embodied domain. Specifically, we propose a 3D scene\nsubgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based\nplanner with for executing natural language robotic tasks. Notably, our\nretrieved subgraphs adapt to changes in the environment as well as changes in\ntask-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's\nability to significantly reduce input token counts (by an order of magnitude)\nand planning time (up to 70% reduction in average time per planning step) while\nimproving success rates on AI2Thor simulated household tasks with a single-arm,\nmobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with\na manipulator to highlight the performance benefits for robot deployment at the\nedge in real environments.\n","authors":["Meghan Booker","Grayson Byrd","Bethany Kemp","Aurora Schmidt","Corban Rivera"],"pdf_url":"https://arxiv.org/pdf/2410.23968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23963v1","updated":"2024-10-31T14:15:54Z","published":"2024-10-31T14:15:54Z","title":"Exploiting Information Theory for Intuitive Robot Programming of Manual\n  Activities","summary":"  Observational learning is a promising approach to enable people without\nexpertise in programming to transfer skills to robots in a user-friendly\nmanner, since it mirrors how humans learn new behaviors by observing others.\nMany existing methods focus on instructing robots to mimic human trajectories,\nbut motion-level strategies often pose challenges in skills generalization\nacross diverse environments. This paper proposes a novel framework that allows\nrobots to achieve a \\textit{higher-level} understanding of human-demonstrated\nmanual tasks recorded in RGB videos. By recognizing the task structure and\ngoals, robots generalize what observed to unseen scenarios. We found our task\nrepresentation on Shannon's Information Theory (IT), which is applied for the\nfirst time to manual tasks. IT helps extract the active scene elements and\nquantify the information shared between hands and objects. We exploit scene\ngraph properties to encode the extracted interaction features in a compact\nstructure and segment the demonstration into blocks, streamlining the\ngeneration of Behavior Trees for robot replicas. Experiments validated the\neffectiveness of IT to automatically generate robot execution plans from a\nsingle human demonstration. Additionally, we provide HANDSOME, an open-source\ndataset of HAND Skills demOnstrated by Multi-subjEcts, to promote further\nresearch and evaluation in this field.\n","authors":["Elena Merlo","Marta Lagomarsino","Edoardo Lamon","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2410.23963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13321v2","updated":"2024-10-31T13:42:58Z","published":"2024-03-20T05:57:20Z","title":"Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow\n  below a Quadrotor as a Turbulent Jet","summary":"  The widespread adoption of quadrotors for diverse applications, from\nagriculture to public safety, necessitates an understanding of the aerodynamic\ndisturbances they create. This paper introduces a computationally lightweight\nmodel for estimating the time-averaged magnitude of the induced flow below\nquadrotors in hover. Unlike related approaches that rely on expensive\ncomputational fluid dynamics (CFD) simulations or drone specific time-consuming\nempirical measurements, our method leverages classical theory from turbulent\nflows. By analyzing over 16 hours of flight data from drones of varying sizes\nwithin a large motion capture system, we show for the first time that the\ncombined flow from all drone propellers is well-approximated by a turbulent jet\nafter 2.5 drone-diameters below the vehicle. Using a novel normalization and\nscaling, we experimentally identify model parameters that describe a unified\nmean velocity field below differently sized quadrotors. The model, which\nrequires only the drone's mass, propeller size, and drone size for\ncalculations, accurately describes the far-field airflow over a long-range in a\nvery large volume which is impractical to simulate using CFD. Our model offers\na practical tool for ensuring safer operations near humans, optimizing sensor\nplacements and drone control in multi-agent scenarios. We demonstrate the\nlatter by designing a controller that compensates for the downwash of another\ndrone, leading to a four times lower altitude deviation when passing below.\n","authors":["Leonard Bauersfeld","Koen Muller","Dominic Ziegler","Filippo Coletti","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.13321v2.pdf","comment":"7+1 pages"},{"id":"http://arxiv.org/abs/2410.23929v1","updated":"2024-10-31T13:36:04Z","published":"2024-10-31T13:36:04Z","title":"Redundant Observer-Based Tracking Control for Object Extraction Using a\n  Cable Connected UAV","summary":"  A new disturbance observer based control scheme is developed for a quadrotor\nunder the concurrent disturbances from a lightweight elastic tether cable and a\nlumped vertical disturbance. This elastic tether is unusual as it creates a\ndisturbance proportional to the multicopter's translational movement. This\npaper takes an observer-based approach to estimate the stiffness coefficient of\nthe cable and uses the system model to update the estimates of the external\nforces, which are then compensated in the control action. Given that the\ntethered cable force affects both horizontal channels of the quadrotor and is\nalso coupled with the vertical channel, the proposed disturbance observer is\nconstructed to exploit the redundant measurements across all three channels to\njointly estimate the cable stiffness and the vertical disturbance. A\npseudo-inverse method is used to determine the observer gain functions, such\nthat the estimation of the two quantities is decoupled and stable. Compared to\nstandard disturbance observers which assume nearly constant disturbances, the\nproposed approach can quickly adjust its total force estimate as the tethered\nquadrotor changes its position or tautness of the tether. This is applied to\ntwo experiments - a tracking performance test where the multicopter moves under\na constant tether strain, and an object extraction test. In the second test,\nthe multicopter manipulates a nonlinear mechanism mimicking the extraction of a\nwedged object. In both cases, the proposed approach shows significant\nimprovement over standard Disturbance Observer and Extended State Observer\napproaches. A video summary of the experiments can be found at\nhttps://youtu.be/9gKr13WTj-k.\n","authors":["Benjamin J. Marshall","Yunda Yan","James Knowles","Chenguang Yang","Cunjia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23916v1","updated":"2024-10-31T13:23:10Z","published":"2024-10-31T13:23:10Z","title":"Transformer-based Model Predictive Control: Trajectory Optimization via\n  Sequence Modeling","summary":"  Model predictive control (MPC) has established itself as the primary\nmethodology for constrained control, enabling general-purpose robot autonomy in\ndiverse real-world scenarios. However, for most problems of interest, MPC\nrelies on the recursive solution of highly non-convex trajectory optimization\nproblems, leading to high computational complexity and strong dependency on\ninitialization. In this work, we present a unified framework to combine the\nmain strengths of optimization-based and learning-based methods for MPC. Our\napproach entails embedding high-capacity, transformer-based neural network\nmodels within the optimization process for trajectory generation, whereby the\ntransformer provides a near-optimal initial guess, or target plan, to a\nnon-convex optimization problem. Our experiments, performed in simulation and\nthe real world onboard a free flyer platform, demonstrate the capabilities of\nour framework to improve MPC convergence and runtime. Compared to purely\noptimization-based approaches, results show that our approach can improve\ntrajectory generation performance by up to 75%, reduce the number of solver\niterations by up to 45%, and improve overall MPC runtime by 7x without loss in\nperformance.\n","authors":["Davide Celestini","Daniele Gammelli","Tommaso Guffanti","Simone D'Amico","Elisa Capello","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2410.23916v1.pdf","comment":"8 pages, 7 figures. Datasets, videos and code available at:\n  https://transformermpc.github.io"},{"id":"http://arxiv.org/abs/2408.14791v3","updated":"2024-10-31T12:23:42Z","published":"2024-08-27T05:53:02Z","title":"Optimizing Structured Data Processing through Robotic Process Automation","summary":"  Robotic Process Automation (RPA) has emerged as a game-changing technology in\ndata extraction, revolutionizing the way organizations process and analyze\nlarge volumes of documents such as invoices, purchase orders, and payment\nadvices. This study investigates the use of RPA for structured data extraction\nand evaluates its advantages over manual processes. By comparing\nhuman-performed tasks with those executed by RPA software bots, we assess\nefficiency and accuracy in data extraction from invoices, focusing on the\neffectiveness of the RPA system. Through four distinct scenarios involving\nvarying numbers of invoices, we measure efficiency in terms of time and effort\nrequired for task completion, as well as accuracy by comparing error rates\nbetween manual and RPA processes. Our findings highlight the significant\nefficiency gains achieved by RPA, with bots completing tasks in significantly\nless time compared to manual efforts across all cases. Moreover, the RPA system\nconsistently achieves perfect accuracy, mitigating the risk of errors and\nenhancing process reliability. These results underscore the transformative\npotential of RPA in optimizing operational efficiency, reducing human labor\ncosts, and improving overall business performance.\n","authors":["Vivek Bhardwaj","Ajit Noonia","Sandeep Chaurasia","Mukesh Kumar","Abdulnaser Rashid","Mohamed Tahar Ben Othman"],"pdf_url":"https://arxiv.org/pdf/2408.14791v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23860v1","updated":"2024-10-31T12:10:20Z","published":"2024-10-31T12:10:20Z","title":"Analysing the Interplay of Vision and Touch for Dexterous Insertion\n  Tasks","summary":"  Robotic insertion tasks remain challenging due to uncertainties in perception\nand the need for precise control, particularly in unstructured environments.\nWhile humans seamlessly combine vision and touch for such tasks, effectively\nintegrating these modalities in robotic systems is still an open problem. Our\nwork presents an extensive analysis of the interplay between visual and tactile\nfeedback during dexterous insertion tasks, showing that tactile sensing can\ngreatly enhance success rates on challenging insertions with tight tolerances\nand varied hole orientations that vision alone cannot solve. These findings\nprovide valuable insights for designing more effective multi-modal robotic\ncontrol systems and highlight the critical role of tactile feedback in\ncontact-rich manipulation tasks.\n","authors":["Janis Lenz","Theo Gruner","Daniel Palenicek","Tim Schneider","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2410.23860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14927v3","updated":"2024-10-31T11:32:19Z","published":"2024-06-21T07:37:17Z","title":"GIC: Gaussian-Informed Continuum for Physical Property Identification\n  and Simulation","summary":"  This paper studies the problem of estimating physical properties (system\nidentification) through visual observations. To facilitate geometry-aware\nguidance in physical property estimation, we introduce a novel hybrid framework\nthat leverages 3D Gaussian representation to not only capture explicit shapes\nbut also enable the simulated continuum to render object masks as 2D shape\nsurrogates during training. We propose a new dynamic 3D Gaussian framework\nbased on motion factorization to recover the object as 3D Gaussian point sets\nacross different time states. Furthermore, we develop a coarse-to-fine filling\nstrategy to generate the density fields of the object from the Gaussian\nreconstruction, allowing for the extraction of object continuums along with\ntheir surfaces and the integration of Gaussian attributes into these continuum.\nIn addition to the extracted object surfaces, the Gaussian-informed continuum\nalso enables the rendering of object masks during simulations, serving as\n2D-shape guidance for physical property estimation. Extensive experimental\nevaluations demonstrate that our pipeline achieves state-of-the-art performance\nacross multiple benchmarks and metrics. Additionally, we illustrate the\neffectiveness of the proposed method through real-world demonstrations,\nshowcasing its practical utility. Our project page is at\nhttps://jukgei.github.io/project/gic.\n","authors":["Junhao Cai","Yuji Yang","Weihao Yuan","Yisheng He","Zilong Dong","Liefeng Bo","Hui Cheng","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14927v3.pdf","comment":"21 pages, 8 figures, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2304.06923v5","updated":"2024-10-31T09:26:18Z","published":"2023-04-14T05:01:10Z","title":"An NMPC-ECBF Framework for Dynamic Motion Planning and Execution in\n  vision-based Human-Robot Collaboration","summary":"  To enable safe and effective human-robot collaboration (HRC) in smart\nmanufacturing, seamless integration of sensing, cognition, and prediction into\nthe robot controller is critical for real-time awareness, response, and\ncommunication inside a heterogeneous environment (robots, humans, and\nequipment). The proposed approach takes advantage of the prediction\ncapabilities of nonlinear model predictive control (NMPC) to execute a safe\npath planning based on feedback from a vision system. In order to satisfy the\nrequirement of real-time path planning, an embedded solver based on a penalty\nmethod is applied. However, due to tight sampling times NMPC solutions are\napproximate, and hence the safety of the system cannot be guaranteed. To\naddress this we formulate a novel safety-critical paradigm with an exponential\ncontrol barrier function (ECBF) used as a safety filter. We also design a\nsimple human-robot collaboration scenario using V-REP to evaluate the\nperformance of the proposed controller and investigate whether integrating\nhuman pose prediction can help with safe and efficient collaboration. The robot\nuses OptiTrack cameras for perception and dynamically generates collision-free\ntrajectories to the predicted target interactive position. Results for a number\nof different configurations confirm the efficiency of the proposed motion\nplanning and execution framework. It yields a 19.8% reduction in execution time\nfor the HRC task considered.\n","authors":["Dianhao Zhang","Mien Van","Pantelis Sopasakis","Se√°n McLoone"],"pdf_url":"https://arxiv.org/pdf/2304.06923v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23747v1","updated":"2024-10-31T09:02:39Z","published":"2024-10-31T09:02:39Z","title":"A Comprehensive Review of Current Robot- Based Pollinators in Greenhouse\n  Farming","summary":"  The decline of bee and wind-based pollination systems in greenhouses due to\ncontrolled environments and limited access has boost the importance of finding\nalternative pollination methods. Robotic based pollination systems have emerged\nas a promising solution, ensuring adequate crop yield even in challenging\npollination scenarios. This paper presents a comprehensive review of the\ncurrent robotic-based pollinators employed in greenhouses. The review\ncategorizes pollinator technologies into major categories such as air-jet,\nwater-jet, linear actuator, ultrasonic wave, and air-liquid spray, each\nsuitable for specific crop pollination requirements. However, these\ntechnologies are often tailored to particular crops, limiting their\nversatility. The advancement of science and technology has led to the\nintegration of automated pollination technology, encompassing information\ntechnology, automatic perception, detection, control, and operation. This\nintegration not only reduces labor costs but also fosters the ongoing progress\nof modern agriculture by refining technology, enhancing automation, and\npromoting intelligence in agricultural practices. Finally, the challenges\nencountered in design of pollinator are addressed, and a forward-looking\nperspective is taken towards future developments, aiming to contribute to the\nsustainable advancement of this technology.\n","authors":["Rajmeet Singh","lakmal Seneviratne","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2410.23747v1.pdf","comment":"20 pages, 21 figures"},{"id":"http://arxiv.org/abs/2405.15223v3","updated":"2024-10-31T08:58:08Z","published":"2024-05-24T05:29:12Z","title":"iVideoGPT: Interactive VideoGPTs are Scalable World Models","summary":"  World models empower model-based agents to interactively explore, reason, and\nplan within imagined environments for real-world decision-making. However, the\nhigh demand for interactivity poses challenges in harnessing recent\nadvancements in video generative models for developing world models at scale.\nThis work introduces Interactive VideoGPT (iVideoGPT), a scalable\nautoregressive transformer framework that integrates multimodal signals--visual\nobservations, actions, and rewards--into a sequence of tokens, facilitating an\ninteractive experience of agents via next-token prediction. iVideoGPT features\na novel compressive tokenization technique that efficiently discretizes\nhigh-dimensional visual observations. Leveraging its scalable architecture, we\nare able to pre-train iVideoGPT on millions of human and robotic manipulation\ntrajectories, establishing a versatile foundation that is adaptable to serve as\ninteractive world models for a wide range of downstream tasks. These include\naction-conditioned video prediction, visual planning, and model-based\nreinforcement learning, where iVideoGPT achieves competitive performance\ncompared with state-of-the-art methods. Our work advances the development of\ninteractive general world models, bridging the gap between generative video\nmodels and practical model-based reinforcement learning applications. Code and\npre-trained models are available at https://thuml.github.io/iVideoGPT.\n","authors":["Jialong Wu","Shaofeng Yin","Ningya Feng","Xu He","Dong Li","Jianye Hao","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2405.15223v3.pdf","comment":"NeurIPS 2024. Code is available at project website:\n  https://thuml.github.io/iVideoGPT"},{"id":"http://arxiv.org/abs/2402.04555v2","updated":"2024-10-31T08:25:08Z","published":"2024-02-07T03:19:02Z","title":"FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language\n  Foundation Models","summary":"  Semantic mapping based on the supervised object detectors is sensitive to\nimage distribution. In real-world environments, the object detection and\nsegmentation performance can lead to a major drop, preventing the use of\nsemantic mapping in a wider domain. On the other hand, the development of\nvision-language foundation models demonstrates a strong zero-shot\ntransferability across data distribution. It provides an opportunity to\nconstruct generalizable instance-aware semantic maps. Hence, this work explores\nhow to boost instance-aware semantic mapping from object detection generated\nfrom foundation models. We propose a probabilistic label fusion method to\npredict close-set semantic classes from open-set label measurements. An\ninstance refinement module merges the over-segmented instances caused by\ninconsistent segmentation. We integrate all the modules into a unified semantic\nmapping system. Reading a sequence of RGB-D input, our work incrementally\nreconstructs an instance-aware semantic map. We evaluate the zero-shot\nperformance of our method in ScanNet and SceneNN datasets. Our method achieves\n40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation\ntask. It outperforms the traditional semantic mapping method significantly.\n","authors":["Chuhao Liu","Ke Wang","Jieqi Shi","Zhijian Qiao","Shaojie Shen"],"pdf_url":"https://arxiv.org/pdf/2402.04555v2.pdf","comment":"Published in IEEE RAL"},{"id":"http://arxiv.org/abs/2410.23722v1","updated":"2024-10-31T08:19:08Z","published":"2024-10-31T08:19:08Z","title":"Features characterizing safe aerial-aquatic robots","summary":"  This paper underscores the importance of environmental monitoring, and\nspecifically of freshwater ecosystems, which play a critical role in sustaining\nlife and global economy. Despite their importance, insufficient data\navailability prevents a comprehensive understanding of these ecosystems,\nthereby impeding informed decision-making concerning their preservation.\nAerial-aquatic robots are identified as effective tools for freshwater sensing,\noffering rapid deployment and avoiding the need of using ships and manned\nteams.\n  To advance the field of aerial aquatic robots, this paper conducts a\ncomprehensive review of air-water transitions focusing on the water entry\nstrategy of existing prototypes. This analysis also highlights the safety risks\nassociated with each transition and proposes a set of design requirements\nrelating to robots' tasks, mission objectives, and safety measures. To further\nexplore the proposed design requirements, we present a novel robot with VTOL\ncapability, enabling seamless air water transitions.\n","authors":["Andrea Giordano","Luca Romanello","Diego Perez Gonzalez","Mirko Kovac","Sophie F. Armanini"],"pdf_url":"https://arxiv.org/pdf/2410.23722v1.pdf","comment":"Peer-reviewed and accepted in IEEE Ubiquitous Robots 2024, New York\n  City"},{"id":"http://arxiv.org/abs/2404.09200v2","updated":"2024-10-31T07:46:38Z","published":"2024-04-14T09:29:37Z","title":"Tube RRT*: Efficient Homotopic Path Planning for Swarm Robotics\n  Passing-Through Large-Scale Obstacle Environments","summary":"  Recently, the concept of homotopic trajectory planning has emerged as a novel\nsolution to navigation in large-scale obstacle environments for swarm robotics,\noffering a wide ranging of applications. However, it lacks an efficient\nhomotopic path planning method in large-scale obstacle environments. This paper\nintroduces Tube RRT*, an innovative homotopic path planning method that builds\nupon and improves the Rapidly-exploring Random Tree (RRT) algorithm. Tube RRT*\nis specifically designed to generate homotopic paths, strategically considering\ngap volume and path length to mitigate swarm congestion and ensure agile\nnavigation. Through comprehensive simulations and experiments, the\neffectiveness of Tube RRT* is validated.\n","authors":["Pengda Mao","Shuli Lv","Quan Quan"],"pdf_url":"https://arxiv.org/pdf/2404.09200v2.pdf","comment":"8 pages, 8 figures, submitted to RA-L"},{"id":"http://arxiv.org/abs/2410.23701v1","updated":"2024-10-31T07:45:12Z","published":"2024-10-31T07:45:12Z","title":"Get a Grip: Multi-Finger Grasp Evaluation at Scale Enables Robust\n  Sim-to-Real Transfer","summary":"  This work explores conditions under which multi-finger grasping algorithms\ncan attain robust sim-to-real transfer. While numerous large datasets\nfacilitate learning generative models for multi-finger grasping at scale,\nreliable real-world dexterous grasping remains challenging, with most methods\ndegrading when deployed on hardware. An alternate strategy is to use\ndiscriminative grasp evaluation models for grasp selection and refinement,\nconditioned on real-world sensor measurements. This paradigm has produced\nstate-of-the-art results for vision-based parallel-jaw grasping, but remains\nunproven in the multi-finger setting. In this work, we find that existing\ndatasets and methods have been insufficient for training discriminitive models\nfor multi-finger grasping. To train grasp evaluators at scale, datasets must\nprovide on the order of millions of grasps, including both positive and\nnegative examples, with corresponding visual data resembling measurements at\ninference time. To that end, we release a new, open-source dataset of 3.5M\ngrasps on 4.3K objects annotated with RGB images, point clouds, and trained\nNeRFs. Leveraging this dataset, we train vision-based grasp evaluators that\noutperform both analytic and generative modeling-based baselines on extensive\nsimulated and real-world trials across a diverse range of objects. We show via\nnumerous ablations that the key factor for performance is indeed the evaluator,\nand that its quality degrades as the dataset shrinks, demonstrating the\nimportance of our new dataset. Project website at:\nhttps://sites.google.com/view/get-a-grip-dataset.\n","authors":["Tyler Ga Wei Lum","Albert H. Li","Preston Culbertson","Krishnan Srinivasan","Aaron D. Ames","Mac Schwager","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2410.23701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23690v1","updated":"2024-10-31T07:25:39Z","published":"2024-10-31T07:25:39Z","title":"XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM","summary":"  In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a\nmodular code design and a multi-process running mechanism, providing highly\nreusable foundational modules such as unified dataset management, 3d\nvisualization, algorithm configuration, and metrics evaluation. It can help\ndevelopers quickly build a complete SLAM system, flexibly combine different\nalgorithm modules, and conduct standardized benchmarking for accuracy and\nefficiency comparison. Within this framework, we integrate several\nstate-of-the-art SLAM algorithms with different types, including NeRF and 3DGS\nbased SLAM, and even odometry or reconstruction algorithms, which demonstrates\nthe flexibility and extensibility. We also conduct a comprehensive comparison\nand evaluation of these integrated algorithms, analyzing the characteristics of\neach. Finally, we contribute all the code, configuration and data to the\nopen-source community, which aims to promote the widespread research and\ndevelopment of SLAM technology within the open-source ecosystem.\n","authors":["Xiaomeng Wang","Nan Wang","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14329v2","updated":"2024-10-31T07:16:46Z","published":"2023-08-28T06:17:15Z","title":"End-to-End Driving via Self-Supervised Imitation Learning Using Camera\n  and LiDAR Data","summary":"  In autonomous driving, the end-to-end (E2E) driving approach that predicts\nvehicle control signals directly from sensor data is rapidly gaining attention.\nTo learn a safe E2E driving system, one needs an extensive amount of driving\ndata and human intervention. Vehicle control data is constructed by many hours\nof human driving, and it is challenging to construct large vehicle control\ndatasets. Often, publicly available driving datasets are collected with limited\ndriving scenes, and collecting vehicle control data is only available by\nvehicle manufacturers. To address these challenges, this letter proposes the\nfirst fully self-supervised learning framework, self-supervised imitation\nlearning (SSIL), for E2E driving, based on the self-supervised regression\nlearning framework. The proposed SSIL framework can learn E2E driving networks\nwithout using driving command data. To construct pseudo steering angle data,\nproposed SSIL predicts a pseudo target from the vehicle's poses at the current\nand previous time points that are estimated with light detection and ranging\nsensors. In addition, we propose two modified E2E driving networks that predict\ndriving commands depending on high-level instruction. Our numerical experiments\nwith three different benchmark datasets demonstrate that the proposed SSIL\nframework achieves very comparable E2E driving accuracy with the supervised\nlearning counterpart.\n","authors":["Jin Bok Park","Jinkyu Lee","Muhyun Back","Hyunmin Han","David T. Ma","Sang Min Won","Sung Soo Hwang","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2308.14329v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23682v1","updated":"2024-10-31T07:12:35Z","published":"2024-10-31T07:12:35Z","title":"CubiXMusashi: Fusion of Wire-Driven CubiX and Musculoskeletal Humanoid\n  Musashi toward Unlimited Performance","summary":"  Humanoids exhibit a wide variety in terms of joint configuration, actuators,\nand degrees of freedom, resulting in different achievable movements and tasks\nfor each type. Particularly, musculoskeletal humanoids are developed to closely\nemulate human body structure and movement functions, consisting of a skeletal\nframework driven by numerous muscle actuators. The redundant arrangement of\nmuscles relative to the skeletal degrees of freedom has been used to represent\nthe flexible and complex body movements observed in humans. However, due to\nthis flexible body and high degrees of freedom, modeling, simulation, and\ncontrol become extremely challenging, limiting the feasible movements and\ntasks. In this study, we integrate the musculoskeletal humanoid Musashi with\nthe wire-driven robot CubiX, capable of connecting to the environment, to form\nCubiXMusashi. This combination addresses the shortcomings of traditional\nmusculoskeletal humanoids and enables movements beyond the capabilities of\nother humanoids. CubiXMusashi connects to the environment with wires and drives\nby winding them, successfully achieving movements such as pull-up, rising from\na lying pose, and mid-air kicking, which are difficult for Musashi alone. This\nconcept demonstrates that various humanoids, not limited to musculoskeletal\nhumanoids, can mitigate their physical constraints and acquire new abilities by\nconnecting to the environment and driving through wires.\n","authors":["Shintaro Inoue","Kento Kawaharazuka","Temma Suzuki","Sota Yuzaki","Yoshimoto Ribayashi","Yuta Sahara","Kei Okada"],"pdf_url":"https://arxiv.org/pdf/2410.23682v1.pdf","comment":"Accepted Humanoids2024, website -\n  https://shin0805.github.io/cubixmusashi/, YouTube -\n  https://youtu.be/IvzP98-r_mo"},{"id":"http://arxiv.org/abs/2410.23643v1","updated":"2024-10-31T05:29:30Z","published":"2024-10-31T05:29:30Z","title":"SceneComplete: Open-World 3D Scene Completion in Complex Real World\n  Environments for Robot Manipulation","summary":"  Careful robot manipulation in every-day cluttered environments requires an\naccurate understanding of the 3D scene, in order to grasp and place objects\nstably and reliably and to avoid mistakenly colliding with other objects. In\ngeneral, we must construct such a 3D interpretation of a complex scene based on\nlimited input, such as a single RGB-D image. We describe SceneComplete, a\nsystem for constructing a complete, segmented, 3D model of a scene from a\nsingle view. It provides a novel pipeline for composing general-purpose\npretrained perception modules (vision-language, segmentation, image-inpainting,\nimage-to-3D, and pose-estimation) to obtain high-accuracy results. We\ndemonstrate its accuracy and effectiveness with respect to ground-truth models\nin a large benchmark dataset and show that its accurate whole-object\nreconstruction enables robust grasp proposal generation, including for a\ndexterous hand.\n","authors":["Aditya Agarwal","Gaurav Singh","Bipasha Sen","Tom√°s Lozano-P√©rez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2410.23643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23640v1","updated":"2024-10-31T05:25:11Z","published":"2024-10-31T05:25:11Z","title":"SuctionPrompt: Visual-assisted Robotic Picking with a Suction Cup Using\n  Vision-Language Models and Facile Hardware Design","summary":"  The development of large language models and vision-language models (VLMs)\nhas resulted in the increasing use of robotic systems in various fields.\nHowever, the effective integration of these models into real-world robotic\ntasks is a key challenge. We developed a versatile robotic system called\nSuctionPrompt that utilizes prompting techniques of VLMs combined with 3D\ndetections to perform product-picking tasks in diverse and dynamic\nenvironments. Our method highlights the importance of integrating 3D spatial\ninformation with adaptive action planning to enable robots to approach and\nmanipulate objects in novel environments. In the validation experiments, the\nsystem accurately selected suction points 75.4%, and achieved a 65.0% success\nrate in picking common items. This study highlights the effectiveness of VLMs\nin robotic manipulation tasks, even with simple 3D processing.\n","authors":["Tomohiro Motoda","Takahide Kitamura","Ryo Hanai","Yukiyasu Domae"],"pdf_url":"https://arxiv.org/pdf/2410.23640v1.pdf","comment":"11 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.23634v1","updated":"2024-10-31T05:01:20Z","published":"2024-10-31T05:01:20Z","title":"Tiny Learning-Based MPC for Multirotors: Solver-Aware Learning for\n  Efficient Embedded Predictive Control","summary":"  Tiny aerial robots show promise for applications like environmental\nmonitoring and search-and-rescue but face challenges in control due to their\nlimited computing power and complex dynamics. Model Predictive Control (MPC)\ncan achieve agile trajectory tracking and handle constraints. Although current\nlearning-based MPC methods, such as Gaussian Process (GP) MPC, improve control\nperformance by learning residual dynamics, they are computationally demanding,\nlimiting their onboard application on tiny robots. This paper introduces Tiny\nLearning-Based Model Predictive Control (LB MPC), a novel framework for\nresource-constrained micro multirotor platforms. By exploiting multirotor\ndynamics' structure and developing an efficient solver, our approach enables\nhigh-rate control at 100 Hz on a Crazyflie 2.1 with a Teensy 4.0\nmicrocontroller. We demonstrate a 23\\% average improvement in tracking\nperformance over existing embedded MPC methods, achieving the first onboard\nimplementation of learning-based MPC on a tiny multirotor (53 g).\n","authors":["Babak Akbari","Justin Frank","Melissa Greeff"],"pdf_url":"https://arxiv.org/pdf/2410.23634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05421v3","updated":"2024-10-31T04:53:19Z","published":"2024-02-08T05:26:40Z","title":"DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement\n  and Imitation Learning","summary":"  This paper introduces DiffTORI, which utilizes Differentiable Trajectory\nOptimization as the policy representation to generate actions for deep\nReinforcement and Imitation learning. Trajectory optimization is a powerful and\nwidely used algorithm in control, parameterized by a cost and a dynamics\nfunction. The key to our approach is to leverage the recent progress in\ndifferentiable trajectory optimization, which enables computing the gradients\nof the loss with respect to the parameters of trajectory optimization. As a\nresult, the cost and dynamics functions of trajectory optimization can be\nlearned end-to-end. DiffTORI addresses the ``objective mismatch'' issue of\nprior model-based RL algorithms, as the dynamics model in DiffTORI is learned\nto directly maximize task performance by differentiating the policy gradient\nloss through the trajectory optimization process. We further benchmark DiffTORI\nfor imitation learning on standard robotic manipulation task suites with\nhigh-dimensional sensory observations and compare our method to feed-forward\npolicy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15\nmodel-based RL tasks and 35 imitation learning tasks with high-dimensional\nimage and point cloud inputs, DiffTORI outperforms prior state-of-the-art\nmethods in both domains.\n","authors":["Weikang Wan","Ziyu Wang","Yufei Wang","Zackory Erickson","David Held"],"pdf_url":"https://arxiv.org/pdf/2402.05421v3.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2309.09017v2","updated":"2024-10-31T04:11:54Z","published":"2023-09-16T15:11:34Z","title":"Triple Regression for Camera Agnostic Sim2Real Robot Grasping and\n  Manipulation Tasks","summary":"  Sim2Real (Simulation to Reality) techniques have gained prominence in robotic\nmanipulation and motion planning due to their ability to enhance success rates\nby enabling agents to test and evaluate various policies and trajectories. In\nthis paper, we investigate the advantages of integrating Sim2Real into robotic\nframeworks. We introduce the Triple Regression Sim2Real framework, which\nconstructs a real-time digital twin. This twin serves as a replica of reality\nto simulate and evaluate multiple plans before their execution in real-world\nscenarios. Our triple regression approach addresses the reality gap by: (1)\nmitigating projection errors between real and simulated camera perspectives\nthrough the first two regression models, and (2) detecting discrepancies in\nrobot control using the third regression model. Experiments on 6-DoF grasp and\nmanipulation tasks (where the gripper can approach from any direction)\nhighlight the effectiveness of our framework. Remarkably, with only RGB input\nimages, our method achieves state-of-the-art success rates. This research\nadvances efficient robot training methods and sets the stage for rapid\nadvancements in robotics and automation.\n","authors":["Yuanhong Zeng","Yizhou Zhao","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/2309.09017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20927v3","updated":"2024-10-31T03:50:22Z","published":"2024-10-28T11:12:00Z","title":"VLMimic: Vision Language Models are Visual Imitation Learner for\n  Fine-grained Actions","summary":"  Visual imitation learning (VIL) provides an efficient and intuitive strategy\nfor robotic systems to acquire novel skills. Recent advancements in Vision\nLanguage Models (VLMs) have demonstrated remarkable performance in vision and\nlanguage reasoning capabilities for VIL tasks. Despite the progress, current\nVIL methods naively employ VLMs to learn high-level plans from human videos,\nrelying on pre-defined motion primitives for executing physical interactions,\nwhich remains a major bottleneck. In this work, we present VLMimic, a novel\nparadigm that harnesses VLMs to directly learn even fine-grained action levels,\nonly given a limited number of human videos. Specifically, VLMimic first\ngrounds object-centric movements from human videos, and learns skills using\nhierarchical constraint representations, facilitating the derivation of skills\nwith fine-grained action levels from limited human videos. These skills are\nrefined and updated through an iterative comparison strategy, enabling\nefficient adaptation to unseen environments. Our extensive experiments exhibit\nthat our VLMimic, using only 5 human videos, yields significant improvements of\nover 27% and 21% in RLBench and real-world manipulation tasks, and surpasses\nbaselines by over 37% in long-horizon tasks.\n","authors":["Guanyan Chen","Meiling Wang","Te Cui","Yao Mu","Haoyang Lu","Tianxing Zhou","Zicai Peng","Mengxiao Hu","Haizhou Li","Yuan Li","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2410.20927v3.pdf","comment":"accepted for publication in the 38th Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2406.12095v2","updated":"2024-10-31T03:23:39Z","published":"2024-06-17T21:15:13Z","title":"DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by\n  Distilling Neural Fields and Foundation Model Features","summary":"  We propose DistillNeRF, a self-supervised learning framework addressing the\nchallenge of understanding 3D environments from limited 2D observations in\noutdoor autonomous driving scenes. Our method is a generalizable feedforward\nmodel that predicts a rich neural scene representation from sparse,\nsingle-frame multi-view camera inputs with limited view overlap, and is trained\nself-supervised with differentiable rendering to reconstruct RGB, depth, or\nfeature images. Our first insight is to exploit per-scene optimized Neural\nRadiance Fields (NeRFs) by generating dense depth and virtual camera targets\nfrom them, which helps our model to learn enhanced 3D geometry from sparse\nnon-overlapping image inputs. Second, to learn a semantically rich 3D\nrepresentation, we propose distilling features from pre-trained 2D foundation\nmodels, such as CLIP or DINOv2, thereby enabling various downstream tasks\nwithout the need for costly 3D human annotations. To leverage these two\ninsights, we introduce a novel model architecture with a two-stage\nlift-splat-shoot encoder and a parameterized sparse hierarchical voxel\nrepresentation. Experimental results on the NuScenes and Waymo NOTR datasets\ndemonstrate that DistillNeRF significantly outperforms existing comparable\nstate-of-the-art self-supervised methods for scene reconstruction, novel view\nsynthesis, and depth estimation; and it allows for competitive zero-shot 3D\nsemantic occupancy prediction, as well as open-world scene understanding\nthrough distilled foundation model features. Demos and code will be available\nat https://distillnerf.github.io/.\n","authors":["Letian Wang","Seung Wook Kim","Jiawei Yang","Cunjun Yu","Boris Ivanovic","Steven L. Waslander","Yue Wang","Sanja Fidler","Marco Pavone","Peter Karkus"],"pdf_url":"https://arxiv.org/pdf/2406.12095v2.pdf","comment":"Accepted by Advances in Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.23586v1","updated":"2024-10-31T02:56:19Z","published":"2024-10-31T02:56:19Z","title":"Multi-Robot Pursuit in Parameterized Formation via Imitation Learning","summary":"  This paper studies the problem of multi-robot pursuit of how to coordinate a\ngroup of defending robots to capture a faster attacker before it enters a\nprotected area. Such operation for defending robots is challenging due to the\nunknown avoidance strategy and higher speed of the attacker, coupled with the\nlimited communication capabilities of defenders. To solve this problem, we\npropose a parameterized formation controller that allows defending robots to\nadapt their formation shape using five adjustable parameters. Moreover, we\ndevelop an imitation-learning based approach integrated with model predictive\ncontrol to optimize these shape parameters. We make full use of these two\ntechniques to enhance the capture capabilities of defending robots through\nongoing training. Both simulation and experiment are provided to verify the\neffectiveness and robustness of our proposed controller. Simulation results\nshow that defending robots can rapidly learn an effective strategy for\ncapturing the attacker, and moreover the learned strategy remains effective\nacross varying numbers of defenders. Experiment results on real robot platforms\nfurther validated these findings.\n","authors":["Jinyong Chen","Rui Zhou","Zhaozong Wang","Yunjie Zhang","Guibin Sun"],"pdf_url":"https://arxiv.org/pdf/2410.23586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02444v2","updated":"2024-10-31T02:49:27Z","published":"2024-08-05T13:08:53Z","title":"RIs-Calib: An Open-Source Spatiotemporal Calibrator for Multiple 3D\n  Radars and IMUs Based on Continuous-Time Estimation","summary":"  Aided inertial navigation system (INS), typically consisting of an inertial\nmeasurement unit (IMU) and an exteroceptive sensor, has been widely accepted as\na feasible solution for navigation. Compared with vision-aided and LiDAR-aided\nINS, radar-aided INS could achieve better performance in adverse weather\nconditions since the radar utilizes low-frequency measuring signals with less\nattenuation effect in atmospheric gases and rain. For such a radar-aided INS,\naccurate spatiotemporal transformation is a fundamental prerequisite to\nachieving optimal information fusion. In this work, we present RIs-Calib: a\nspatiotemporal calibrator for multiple 3D radars and IMUs based on\ncontinuous-time estimation, which enables accurate spatiotemporal calibration\nand does not require any additional artificial infrastructure or prior\nknowledge. Our approach starts with a rigorous and robust procedure for state\ninitialization, followed by batch optimizations, where all parameters can be\nrefined to global optimal states steadily. We validate and evaluate RIs-Calib\non both simulated and real-world experiments, and the results demonstrate that\nRIs-Calib is capable of accurate and consistent calibration. We open-source our\nimplementations at (https://github.com/Unsigned-Long/RIs-Calib) to benefit the\nresearch community.\n","authors":["Shuolong Chen","Xingxing Li","Shengyu Li","Yuxuan Zhou","Shiwen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23581v1","updated":"2024-10-31T02:47:42Z","published":"2024-10-31T02:47:42Z","title":"Distributed Formation Shape Control of Identity-less Robot Swarms","summary":"  Different from most of the formation strategies where robots require unique\nlabels to identify topological neighbors to satisfy the predefined shape\nconstraints, we here study the problem of identity-less distributed shape\nformation in homogeneous swarms, which is rarely studied in the literature. The\nabsence of identities creates a unique challenge: how to design appropriate\ntarget formations and local behaviors that are suitable for identity-less\nformation shape control. To address this challenge, we propose the following\nnovel results. First, to avoid using unique identities, we propose a dynamic\nformation description method and solve the formation consensus of robots in a\nlocally distributed manner. Second, to handle identity-less distributed\nformations, we propose a fully distributed control law for homogeneous swarms\nbased on locally sensed information. While the existing methods are applicable\nto simple cases where the target formation is stationary, ours can tackle more\ngeneral maneuvering formations such as translation, rotation, or even shape\ndeformation. Both numerical simulation and flight experiment are presented to\nverify the effectiveness and robustness of our proposed formation strategy.\n","authors":["Guibin Sun","Yang Xu","Kexin Liu","Jinhu L√º"],"pdf_url":"https://arxiv.org/pdf/2410.23581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19291v2","updated":"2024-10-31T02:43:00Z","published":"2024-05-29T17:19:15Z","title":"Grasp as You Say: Language-guided Dexterous Grasp Generation","summary":"  This paper explores a novel task \"Dexterous Grasp as You Say\" (DexGYS),\nenabling robots to perform dexterous grasping based on human commands expressed\nin natural language. However, the development of this field is hindered by the\nlack of datasets with natural human guidance; thus, we propose a\nlanguage-guided dexterous grasp dataset, named DexGYSNet, offering high-quality\ndexterous grasp annotations along with flexible and fine-grained human language\nguidance. Our dataset construction is cost-efficient, with the carefully-design\nhand-object interaction retargeting strategy, and the LLM-assisted language\nguidance annotation system. Equipped with this dataset, we introduce the\nDexGYSGrasp framework for generating dexterous grasps based on human language\ninstructions, with the capability of producing grasps that are intent-aligned,\nhigh quality and diversity. To achieve this capability, our framework\ndecomposes the complex learning process into two manageable progressive\nobjectives and introduce two components to realize them. The first component\nlearns the grasp distribution focusing on intention alignment and generation\ndiversity. And the second component refines the grasp quality while maintaining\nintention consistency. Extensive experiments are conducted on DexGYSNet and\nreal world environments for validation.\n","authors":["Yi-Lin Wei","Jian-Jian Jiang","Chengyi Xing","Xian-Tuo Tan","Xiao-Ming Wu","Hao Li","Mark Cutkosky","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.19291v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2409.07116v2","updated":"2024-10-31T02:38:49Z","published":"2024-09-11T09:09:25Z","title":"iKalibr-RGBD: Partially-Specialized Target-Free Visual-Inertial\n  Spatiotemporal Calibration For RGBDs via Continuous-Time Velocity Estimation","summary":"  Visual-inertial systems have been widely studied and applied in the last two\ndecades (from the early 2000s to the present), mainly due to their low cost and\npower consumption, small footprint, and high availability. Such a trend\nsimultaneously leads to a large amount of visual-inertial calibration methods\nbeing presented, as accurate spatiotemporal parameters between sensors are a\nprerequisite for visual-inertial fusion. In our previous work, i.e., iKalibr, a\ncontinuous-time-based visual-inertial calibration method was proposed as a part\nof one-shot multi-sensor resilient spatiotemporal calibration. While requiring\nno artificial target brings considerable convenience, computationally expensive\npose estimation is demanded in initialization and batch optimization, limiting\nits availability. Fortunately, this could be vastly improved for the RGBDs with\nadditional depth information, by employing mapping-free ego-velocity estimation\ninstead of mapping-based pose estimation. In this paper, we present the\ncontinuous-time ego-velocity estimation-based RGBD-inertial spatiotemporal\ncalibration, termed as iKalibr-RGBD, which is also targetless but\ncomputationally efficient. The general pipeline of iKalibr-RGBD is inherited\nfrom iKalibr, composed of a rigorous initialization procedure and several\ncontinuous-time batch optimizations. The implementation of iKalibr-RGBD is\nopen-sourced at (https://github.com/Unsigned-Long/iKalibr) to benefit the\nresearch community.\n","authors":["Shuolong Chen","Xingxing Li","Shengyu Li","Yuxuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.07116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23571v1","updated":"2024-10-31T02:27:25Z","published":"2024-10-31T02:27:25Z","title":"Dual Agent Learning Based Aerial Trajectory Tracking","summary":"  This paper presents a novel reinforcement learning framework for trajectory\ntracking of unmanned aerial vehicles in cluttered environments using a\ndual-agent architecture. Traditional optimization methods for trajectory\ntracking face significant computational challenges and lack robustness in\ndynamic environments. Our approach employs deep reinforcement learning (RL) to\novercome these limitations, leveraging 3D pointcloud data to perceive the\nenvironment without relying on memory-intensive obstacle representations like\noccupancy grids. The proposed system features two RL agents: one for predicting\nUAV velocities to follow a reference trajectory and another for managing\ncollision avoidance in the presence of obstacles. This architecture ensures\nreal-time performance and adaptability to uncertainties. We demonstrate the\nefficacy of our approach through simulated and real-world experiments,\nhighlighting improvements over state-of-the-art RL and optimization-based\nmethods. Additionally, a curriculum learning paradigm is employed to scale the\nalgorithms to more complex environments, ensuring robust trajectory tracking\nand obstacle avoidance in both static and dynamic scenarios.\n","authors":["Shaswat Garg","Houman Masnavi","Baris Fidan","Farrokh Janabi-Sharifi"],"pdf_url":"https://arxiv.org/pdf/2410.23571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00769v3","updated":"2024-10-31T02:07:10Z","published":"2024-03-31T18:51:52Z","title":"An Active Perception Game for Robust Information Gathering","summary":"  Active perception approaches select future viewpoints by using some estimate\nof the information gain. An inaccurate estimate can be detrimental in critical\nsituations, e.g., locating a person in distress. However the true information\ngained can only be calculated post hoc, i.e., after the observation is\nrealized. We present an approach for estimating the discrepancy between the\ninformation gain (which is the average over putative future observations) and\nthe true information gain. The key idea is to analyze the mathematical\nrelationship between active perception and the estimation error of the\ninformation gain in a game-theoretic setting. Using this, we develop an online\nestimation approach that achieves sub-linear regret (in the number of\ntime-steps) for the estimation of the true information gain and reduces the\nsub-optimality of active perception systems.\n  We demonstrate our approach for active perception using a comprehensive set\nof experiments on: (a) different types of environments, including a quadrotor\nin a photorealistic simulation, real-world robotic data, and real-world\nexperiments with ground robots exploring indoor and outdoor scenes; (b)\ndifferent types of robotic perception data; and (c) different map\nrepresentations. On average, our approach reduces information gain estimation\nerrors by 42%, increases the information gain by 7%, PSNR by 5%, and semantic\naccuracy (measured as the number of objects that are localized correctly) by\n6%. In real-world experiments with a Jackal ground robot, our approach\ndemonstrated complex trajectories to explore occluded regions.\n","authors":["Siming He","Yuezhan Tao","Igor Spasojevic","Vijay Kumar","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2404.00769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23535v1","updated":"2024-10-31T00:56:08Z","published":"2024-10-31T00:56:08Z","title":"Simulating User Agents for Embodied Conversational-AI","summary":"  Embodied agents designed to assist users with tasks must engage in natural\nlanguage interactions, interpret instructions, execute actions, and communicate\neffectively to resolve issues. However, collecting large-scale, diverse\ndatasets of situated human-robot dialogues to train and evaluate such agents is\nexpensive, labor-intensive, and time-consuming. To address this challenge, we\npropose building a large language model (LLM)-based user agent that can\nsimulate user behavior during interactions with an embodied agent in a virtual\nenvironment. Given a user goal (e.g., make breakfast), at each time step, the\nuser agent may observe\" the robot actions or speak\" to either intervene with\nthe robot or answer questions. Such a user agent assists in improving the\nscalability and efficiency of embodied dialogues dataset generation and is\ncritical for enhancing and evaluating the robot's interaction and task\ncompletion ability, as well as for research in reinforcement learning using AI\nfeedback. We evaluate our user agent's ability to generate human-like behaviors\nby comparing its simulated dialogues with the TEACh dataset. We perform three\nexperiments: zero-shot prompting to predict dialogue acts, few-shot prompting,\nand fine-tuning on the TEACh training subset. Results show the LLM-based user\nagent achieves an F-measure of 42% with zero-shot prompting and 43.4% with\nfew-shot prompting in mimicking human speaking behavior. Through fine-tuning,\nperformance in deciding when to speak remained stable, while deciding what to\nsay improved from 51.1% to 62.5%. These findings showcase the feasibility of\nthe proposed approach for assessing and enhancing the effectiveness of robot\ntask completion through natural language communication.\n","authors":["Daniel Philipov","Vardhan Dongre","Gokhan Tur","Dilek Hakkani-T√ºr"],"pdf_url":"https://arxiv.org/pdf/2410.23535v1.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.23522v1","updated":"2024-10-31T00:08:36Z","published":"2024-10-31T00:08:36Z","title":"LBurst: Learning-Based Robotic Burst Feature Extraction for 3D\n  Reconstruction in Low Light","summary":"  Drones have revolutionized the fields of aerial imaging, mapping, and\ndisaster recovery. However, the deployment of drones in low-light conditions is\nconstrained by the image quality produced by their on-board cameras. In this\npaper, we present a learning architecture for improving 3D reconstructions in\nlow-light conditions by finding features in a burst. Our approach enhances\nvisual reconstruction by detecting and describing high quality true features\nand less spurious features in low signal-to-noise ratio images. We demonstrate\nthat our method is capable of handling challenging scenes in millilux\nillumination, making it a significant step towards drones operating at night\nand in extremely low-light applications such as underground mining and search\nand rescue operations.\n","authors":["Ahalya Ravendran","Mitch Bryson","Donald G. Dansereau"],"pdf_url":"https://arxiv.org/pdf/2410.23522v1.pdf","comment":"7 pages, 8 figures, 3 tables, for associated project page, see\n  https://roboticimaging.org/Projects/LBurst/"}]},"2024-10-30T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.23288v1","updated":"2024-10-30T17:59:34Z","published":"2024-10-30T17:59:34Z","title":"Computing the bridge length: the key ingredient in a continuous isometry\n  classification of periodic point sets","summary":"  The fundamental model of any periodic crystal is a periodic set of points at\nall atomic centres. Since crystal structures are determined in a rigid form,\ntheir strongest equivalence is rigid motion (composition of translations and\nrotations) or isometry (also including reflections). The recent isometry\nclassification of periodic point sets used a complete invariant isoset whose\nsize essentially depends on the bridge length, defined as the minimum `jump'\nthat suffices to connect any points in the given set.\n  We propose a practical algorithm to compute the bridge length of any periodic\npoint set given by a motif of points in a periodically translated unit cell.\nThe algorithm has been tested on a large crystal dataset and is required for an\nefficient continuous classification of all periodic crystals. The exact\ncomputation of the bridge length is a key step to realising the inverse design\nof materials from new invariant values.\n","authors":["Jonathan McManus","Vitaliy Kurlin"],"pdf_url":"https://arxiv.org/pdf/2410.23288v1.pdf","comment":"22 pages, 4 figures, the latest version is maintained at\n  http://kurlin.org/projects/periodic-geometry/bridge-length.pdf"},{"id":"http://arxiv.org/abs/2410.23109v1","updated":"2024-10-30T15:20:10Z","published":"2024-10-30T15:20:10Z","title":"NASM: Neural Anisotropic Surface Meshing","summary":"  This paper introduces a new learning-based method, NASM, for anisotropic\nsurface meshing. Our key idea is to propose a graph neural network to embed an\ninput mesh into a high-dimensional (high-d) Euclidean embedding space to\npreserve curvature-based anisotropic metric by using a dot product loss between\nhigh-d edge vectors. This can dramatically reduce the computational time and\nincrease the scalability. Then, we propose a novel feature-sensitive remeshing\non the generated high-d embedding to automatically capture sharp geometric\nfeatures. We define a high-d normal metric, and then derive an automatic\ndifferentiation on a high-d centroidal Voronoi tessellation (CVT) optimization\nwith the normal metric to simultaneously preserve geometric features and\ncurvature anisotropy that exhibit in the original 3D shapes. To our knowledge,\nthis is the first time that a deep learning framework and a large dataset are\nproposed to construct a high-d Euclidean embedding space for 3D anisotropic\nsurface meshing. Experimental results are evaluated and compared with the\nstate-of-the-art in anisotropic surface meshing on a large number of surface\nmodels from Thingi10K dataset as well as tested on extensive unseen 3D shapes\nfrom Multi-Garment Network dataset and FAUST human dataset.\n","authors":["Hongbo Li","Haikuan Zhu","Sikai Zhong","Ningna Wang","Cheng Lin","Xiaohu Guo","Shiqing Xin","Wenping Wang","Jing Hua","Zichun Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23109v1.pdf","comment":"SIGGRAPH Asia 2024 (Conference Track)"},{"id":"http://arxiv.org/abs/2410.19024v2","updated":"2024-10-30T12:30:20Z","published":"2024-10-24T11:32:41Z","title":"On a Geometric Interpretation Of the Subset Sum Problem","summary":"  For $S \\in \\mathbb{N}^n$ and $T \\in \\mathbb{N}$, the Subset Sum Problem (SSP)\n$\\exists^? x \\in \\{0,1\\}^n $ such that $S^T\\cdot x = T$ can be interpreted as\nthe problem of deciding whether the intersection of the positive unit hypercube\n$Q_n = [0,1]^n$ with the hyperplane $S^T\\cdot \\left(x - \\frac{S}{\\|S\\|^2 }\\cdot\nT \\right) = 0$ contains at least a vertex. In this paper, we give an algorithm\nof complexity $\\mathcal{O}\\left( \\frac{1}{\\epsilon}\\cdot n^b \\right)$, for some\nabsolute constant $b$, which either proves that there are no vertices in a slab\nof thickness $\\epsilon$ either finds a vertex in the slab of thickness $4\\cdot\n\\epsilon$. It is shown that any vertex $P$ in a slab of thickness $\\epsilon$\nmeets $\\left| \\frac{S^T\\cdot P}{T} - 1 \\right| \\leq \\epsilon$, therefore making\nthe proposed algorithm a FPTAS for the SSP. The results are then applied to the\nstudy of the so called Simultaneous Subset-Sum Problem (SSSP).\n","authors":["Marius Costandin"],"pdf_url":"https://arxiv.org/pdf/2410.19024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22722v1","updated":"2024-10-30T06:11:08Z","published":"2024-10-30T06:11:08Z","title":"Enhancing binary classification: A new stacking method via leveraging\n  computational geometry","summary":"  Stacking, a potent ensemble learning method, leverages a meta-model to\nharness the strengths of multiple base models, thereby enhancing prediction\naccuracy. Traditional stacking techniques typically utilize established\nlearning models, such as logistic regression, as the meta-model. This paper\nintroduces a novel approach that integrates computational geometry techniques,\nspecifically solving the maximum weighted rectangle problem, to develop a new\nmeta-model for binary classification. Our method is evaluated on multiple open\ndatasets, with statistical analysis showing its stability and demonstrating\nimprovements in accuracy compared to current state-of-the-art stacking methods\nwith out-of-fold predictions. This new stacking method also boasts two\nsignificant advantages: enhanced interpretability and the elimination of\nhyperparameter tuning for the meta-model, thus increasing its practicality.\nThese merits make our method highly applicable not only in stacking ensemble\nlearning but also in various real-world applications, such as hospital health\nevaluation scoring and bank credit scoring systems, offering a fresh evaluation\nperspective.\n","authors":["Wei Wu","Liang Tang","Zhongjie Zhao","Chung-Piaw Teo"],"pdf_url":"https://arxiv.org/pdf/2410.22722v1.pdf","comment":"11 pages"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2405.17700v2","updated":"2024-10-30T21:24:32Z","published":"2024-05-27T23:16:52Z","title":"Learning Social Welfare Functions","summary":"  Is it possible to understand or imitate a policy maker's rationale by looking\nat past decisions they made? We formalize this question as the problem of\nlearning social welfare functions belonging to the well-studied family of power\nmean functions. We focus on two learning tasks; in the first, the input is\nvectors of utilities of an action (decision or policy) for individuals in a\ngroup and their associated social welfare as judged by a policy maker, whereas\nin the second, the input is pairwise comparisons between the welfares\nassociated with a given pair of utility vectors. We show that power mean\nfunctions are learnable with polynomial sample complexity in both cases, even\nif the comparisons are social welfare information is noisy. Finally, we design\npractical algorithms for these tasks and evaluate their performance.\n","authors":["Kanad Shrikar Pardeshi","Itai Shapira","Ariel D. Procaccia","Aarti Singh"],"pdf_url":"https://arxiv.org/pdf/2405.17700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07067v3","updated":"2024-10-30T21:17:49Z","published":"2024-02-10T23:49:49Z","title":"Learning the Expected Core of Strictly Convex Stochastic Cooperative\n  Games","summary":"  Reward allocation, also known as the credit assignment problem, has been an\nimportant topic in economics, engineering, and machine learning. An important\nconcept in reward allocation is the core, which is the set of stable\nallocations where no agent has the motivation to deviate from the grand\ncoalition. In previous works, computing the core requires either knowledge of\nthe reward function in deterministic games or the reward distribution in\nstochastic games. However, this is unrealistic, as the reward function or\ndistribution is often only partially known and may be subject to uncertainty.\nIn this paper, we consider the core learning problem in stochastic cooperative\ngames, where the reward distribution is unknown. Our goal is to learn the\nexpected core, that is, the set of allocations that are stable in expectation,\ngiven an oracle that returns a stochastic reward for an enquired coalition each\nround. Within the class of strictly convex games, we present an algorithm named\n\\texttt{Common-Points-Picking} that returns a point in the expected core given\na polynomial number of samples, with high probability. To analyse the\nalgorithm, we develop a new extension of the separation hyperplane theorem for\nmultiple convex sets.\n","authors":["Nam Phuong Tran","The Anh Ta","Shuqing Shi","Debmalya Mandal","Yali Du","Long Tran-Thanh"],"pdf_url":"https://arxiv.org/pdf/2402.07067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23416v1","updated":"2024-10-30T19:45:51Z","published":"2024-10-30T19:45:51Z","title":"Temporal Fair Division","summary":"  We study temporal fair division, whereby a set of agents are allocated a\n(possibly different) set of goods on each day for a period of days. We study\nthis setting, as well as a number of its special cases formed by the\nrestrictions to two agents, same goods on each day, identical preferences, or\ncombinations thereof, and chart out the landscape of achieving two types of\nfairness guarantees simultaneously: fairness on each day (per day) and fairness\nover time (up to each day, or the weaker version, overall).\n  In the most general setting, we prove that there always exists an allocation\nthat is stochastically-dominant envy-free up to one good (SD-EF1) per day and\nproportional up to one good (PROP1) overall, and when all the agents have\nidentical preferences, we show that SD-EF1 per day and SD-EF1 overall can be\nguaranteed. For the case of two agents, we prove that SD-EF1 per day and EF1 up\nto each day can be guaranteed using an envy balancing technique. We provide\ncounterexamples for other combinations that establish our results as among the\nbest guarantees possible, but also leaving open some tantalizing questions.\n","authors":["Benjamin Cookson","Soroush Ebadian","Nisarg Shah"],"pdf_url":"https://arxiv.org/pdf/2410.23416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23398v1","updated":"2024-10-30T19:03:33Z","published":"2024-10-30T19:03:33Z","title":"On the Optimality of Dilated Entropy and Lower Bounds for Online\n  Learning in Extensive-Form Games","summary":"  First-order methods (FOMs) are arguably the most scalable algorithms for\nequilibrium computation in large extensive-form games. To operationalize these\nmethods, a distance-generating function, acting as a regularizer for the\nstrategy space, must be chosen. The ratio between the strong convexity modulus\nand the diameter of the regularizer is a key parameter in the analysis of FOMs.\nA natural question is then: what is the optimal distance-generating function\nfor extensive-form decision spaces? In this paper, we make a number of\ncontributions, ultimately establishing that the weight-one dilated entropy\n(DilEnt) distance-generating function is optimal up to logarithmic factors. The\nDilEnt regularizer is notable due to its iterate-equivalence with Kernelized\nOMWU (KOMWU) -- the algorithm with state-of-the-art dependence on the game tree\nsize in extensive-form games -- when used in conjunction with the online mirror\ndescent (OMD) algorithm. However, the standard analysis for OMD is unable to\nestablish such a result; the only current analysis is by appealing to the\niterate equivalence to KOMWU. We close this gap by introducing a pair of\nprimal-dual treeplex norms, which we contend form the natural analytic\nviewpoint for studying the strong convexity of DilEnt. Using these norm pairs,\nwe recover the diameter-to-strong-convexity ratio that predicts the same\nperformance as KOMWU. Along with a new regret lower bound for online learning\nin sequence-form strategy spaces, we show that this ratio is nearly optimal.\nFinally, we showcase our analytic techniques by refining the analysis of\nClairvoyant OMD when paired with DilEnt, establishing an $\\mathcal{O}(n \\log\n|\\mathcal{V}| \\log T/T)$ approximation rate to coarse correlated equilibrium in\n$n$-player games, where $|\\mathcal{V}|$ is the number of reduced normal-form\nstrategies of the players, establishing the new state of the art.\n","authors":["Zhiyuan Fan","Christian Kroer","Gabriele Farina"],"pdf_url":"https://arxiv.org/pdf/2410.23398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23396v1","updated":"2024-10-30T18:59:02Z","published":"2024-10-30T18:59:02Z","title":"Adaptive Network Intervention for Complex Systems: A Hierarchical Graph\n  Reinforcement Learning Approach","summary":"  Effective governance and steering of behavior in complex multi-agent systems\n(MAS) are essential for managing system-wide outcomes, particularly in\nenvironments where interactions are structured by dynamic networks. In many\napplications, the goal is to promote pro-social behavior among agents, where\nnetwork structure plays a pivotal role in shaping these interactions. This\npaper introduces a Hierarchical Graph Reinforcement Learning (HGRL) framework\nthat governs such systems through targeted interventions in the network\nstructure. Operating within the constraints of limited managerial authority,\nthe HGRL framework demonstrates superior performance across a range of\nenvironmental conditions, outperforming established baseline methods. Our\nfindings highlight the critical influence of agent-to-agent learning (social\nlearning) on system behavior: under low social learning, the HGRL manager\npreserves cooperation, forming robust core-periphery networks dominated by\ncooperators. In contrast, high social learning accelerates defection, leading\nto sparser, chain-like networks. Additionally, the study underscores the\nimportance of the system manager's authority level in preventing system-wide\nfailures, such as agent rebellion or collapse, positioning HGRL as a powerful\ntool for dynamic network-based governance.\n","authors":["Qiliang Chen","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2410.23396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05241v4","updated":"2024-10-30T18:37:57Z","published":"2024-08-05T20:49:48Z","title":"Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models","summary":"  As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these\nstate-of-the-art models increases commensurately. However, their deployment is\ncostly both in terms of processing power and time. In this paper, we\ninvestigate the feasibility of creating smaller, highly-performing specialized\nalgorithms by way of fine-tuning. To do this, we first present a large\npre-trained model with 20 unique scenarios that combine different social\ncontexts with games of varying social dilemmas, record its answers, and use\nthem for Q&A fine-tuning on a smaller model of the same family. Our focus is on\nin-context game-theoretic decision-making, the same domain within which human\ninteraction occurs and that requires both a theory of mind (or a semblance\nthereof) and an understanding of social dynamics. The smaller model is\ntherefore trained not just on the answers provided, but also on the motivations\nprovided by the larger model, which should contain advice and guidelines to\nnavigate both strategic dilemmas and social cues. We find that the fine-tuned\nsmaller language model consistently bridged the gap in performance between the\nsmaller pre-trained version of the model and its larger relative and that its\nimprovements extended in areas and contexts beyond the ones provided in the\ntraining examples, including on out-of-sample scenarios that include completely\ndifferent game structures. On average for all games, through fine-tuning, the\nsmaller model showed a 46% improvement measured as alignment towards the\nbehavior of the larger model, with 100% representing indistinguishable\nbehavior. When presented with out-of-sample social contexts and games, the\nfine-tuned model still displays remarkable levels of alignment, reaching an\nimprovement of 18% and 28% respectively.\n","authors":["Nunzio Lore","Sepehr Ilami","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2408.05241v4.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23273v1","updated":"2024-10-30T17:53:49Z","published":"2024-10-30T17:53:49Z","title":"Proportional Fairness in Non-Centroid Clustering","summary":"  We revisit the recently developed framework of proportionally fair\nclustering, where the goal is to provide group fairness guarantees that become\nstronger for groups of data points (agents) that are large and cohesive. Prior\nwork applies this framework to centroid clustering, where the loss of an agent\nis its distance to the centroid assigned to its cluster. We expand the\nframework to non-centroid clustering, where the loss of an agent is a function\nof the other agents in its cluster, by adapting two proportional fairness\ncriteria -- the core and its relaxation, fully justified representation (FJR)\n-- to this setting.\n  We show that the core can be approximated only under structured loss\nfunctions, and even then, the best approximation we are able to establish,\nusing an adaptation of the GreedyCapture algorithm developed for centroid\nclustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a\nnatural loss function. In contrast, we design a new (inefficient) algorithm,\nGreedyCohesiveClustering, which achieves the relaxation FJR exactly under\narbitrary loss functions, and show that the efficient GreedyCapture algorithm\nachieves a constant approximation of FJR. We also design an efficient auditing\nalgorithm, which estimates the FJR approximation of any given clustering\nsolution up to a constant factor. Our experiments on real data suggest that\ntraditional clustering algorithms are highly unfair, whereas GreedyCapture is\nconsiderably fairer and incurs only a modest loss in common clustering\nobjectives.\n","authors":["Ioannis Caragiannis","Evi Micha","Nisarg Shah"],"pdf_url":"https://arxiv.org/pdf/2410.23273v1.pdf","comment":"A preliminary version appeared at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23243v1","updated":"2024-10-30T17:28:59Z","published":"2024-10-30T17:28:59Z","title":"Carrot and Stick: Eliciting Comparison Data and Beyond","summary":"  Comparison data elicited from people are fundamental to many machine learning\ntasks, including reinforcement learning from human feedback for large language\nmodels and estimating ranking models. They are typically subjective and not\ndirectly verifiable. How to truthfully elicit such comparison data from\nrational individuals? We design peer prediction mechanisms for eliciting\ncomparison data using a bonus-penalty payment. Our design leverages on the\nstrong stochastic transitivity for comparison data to create symmetrically\nstrongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian\nNash equilibrium, and 2) yields the highest payment among all symmetric\nequilibria. Each individual only needs to evaluate one pair of items and report\nher comparison in our mechanism.\n  We further extend the bonus-penalty payment concept to eliciting networked\ndata, designing a symmetrically strongly truthful mechanism when agents'\nprivate signals are sampled according to the Ising models. We provide the\nnecessary and sufficient conditions for our bonus-penalty payment to have\ntruth-telling as a strict Bayesian Nash equilibrium. Experiments on two\nreal-world datasets further support our theoretical discoveries.\n","authors":["Yiling Chen","Shi Feng","Fang-Yi Yu"],"pdf_url":"https://arxiv.org/pdf/2410.23243v1.pdf","comment":"33 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.23223v1","updated":"2024-10-30T17:13:02Z","published":"2024-10-30T17:13:02Z","title":"COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General\n  Preferences","summary":"  Many alignment methods, including reinforcement learning from human feedback\n(RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to\ncapture the full range of general human preferences. To achieve robust\nalignment with general preferences, we model the alignment problem as a\ntwo-player zero-sum game, where the Nash equilibrium policy guarantees a 50%\nwin rate against any competing policy. However, previous algorithms for finding\nthe Nash policy either diverge or converge to a Nash policy in a modified game,\neven in a simple synthetic setting, thereby failing to maintain the 50% win\nrate guarantee against all other policies. We propose a meta-algorithm,\nConvergent Meta Alignment Algorithm (COMAL), for language model alignment with\ngeneral preferences, inspired by convergent algorithms in game theory.\nTheoretically, we prove that our meta-algorithm converges to an exact Nash\npolicy in the last iterate. Additionally, our meta-algorithm is simple and can\nbe integrated with many existing methods designed for RLHF and preference\noptimization with minimal changes. Experimental results demonstrate the\neffectiveness of the proposed framework when combined with existing preference\npolicy optimization methods.\n","authors":["Yixin Liu","Argyris Oikonomou","Weiqiang Zheng","Yang Cai","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2410.23223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16745v2","updated":"2024-10-30T17:10:52Z","published":"2024-06-24T15:53:11Z","title":"Bandits with Preference Feedback: A Stackelberg Game Perspective","summary":"  Bandits with preference feedback present a powerful tool for optimizing\nunknown target functions when only pairwise comparisons are allowed instead of\ndirect value queries. This model allows for incorporating human feedback into\nonline inference and optimization and has been employed in systems for\nfine-tuning large language models. The problem is well understood in simplified\nsettings with linear target functions or over finite small domains that limit\npractical interest. Taking the next step, we consider infinite domains and\nnonlinear (kernelized) rewards. In this setting, selecting a pair of actions is\nquite challenging and requires balancing exploration and exploitation at two\nlevels: within the pair, and along the iterations of the algorithm. We propose\nMAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and\nchooses action pairs that are informative and yield favorable rewards.\nMAXMINLCB consistently outperforms existing algorithms and satisfies an\nanytime-valid rate-optimal regret guarantee. This is due to our novel\npreference-based confidence sequences for kernelized logistic estimators.\n","authors":["Barna P√°sztor","Parnian Kassraie","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2406.16745v2.pdf","comment":"In Proceedings of the 38th Conference on Neural Information\n  Processing Systems (NeurIPS), 30 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.10372v3","updated":"2024-10-30T16:45:15Z","published":"2024-09-16T15:15:51Z","title":"Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation","summary":"  This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.\n","authors":["Qiliang Chen","Sepehr Ilami","Nunzio Lore","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2409.10372v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17694v2","updated":"2024-10-30T15:58:27Z","published":"2024-05-27T23:02:24Z","title":"Bias Detection Via Signaling","summary":"  We introduce and study the problem of detecting whether an agent is updating\ntheir prior beliefs given new evidence in an optimal way that is Bayesian, or\nwhether they are biased towards their own prior. In our model, biased agents\nform posterior beliefs that are a convex combination of their prior and the\nBayesian posterior, where the more biased an agent is, the closer their\nposterior is to the prior. Since we often cannot observe the agent's beliefs\ndirectly, we take an approach inspired by information design. Specifically, we\nmeasure an agent's bias by designing a signaling scheme and observing the\nactions they take in response to different signals, assuming that they are\nmaximizing their own expected utility; our goal is to detect bias with a\nminimum number of signals. Our main results include a characterization of\nscenarios where a single signal suffices and a computationally efficient\nalgorithm to compute optimal signaling schemes.\n","authors":["Yiling Chen","Tao Lin","Ariel D. Procaccia","Aaditya Ramdas","Itai Shapira"],"pdf_url":"https://arxiv.org/pdf/2405.17694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23137v1","updated":"2024-10-30T15:52:15Z","published":"2024-10-30T15:52:15Z","title":"Fair Division with Market Values","summary":"  We introduce a model of fair division with market values, where indivisible\ngoods must be partitioned among agents with (additive) subjective valuations,\nand each good additionally has a market value. The market valuation can be\nviewed as a separate additive valuation that holds identically across all the\nagents. We seek allocations that are simultaneously fair with respect to the\nsubjective valuations and with respect to the market valuation.\n  We show that an allocation that satisfies stochastically-dominant\nenvy-freeness up to one good (SD-EF1) with respect to both the subjective\nvaluations and the market valuation does not always exist, but the weaker\nguarantee of EF1 with respect to the subjective valuations along with SD-EF1\nwith respect to the market valuation can be guaranteed. We also study a number\nof other guarantees such as Pareto optimality, EFX, and MMS. In addition, we\nexplore non-additive valuations and extend our model to cake-cutting. Along the\nway, we identify several tantalizing open questions.\n","authors":["Siddharth Barman","Soroush Ebadian","Mohamad Latifian","Nisarg Shah"],"pdf_url":"https://arxiv.org/pdf/2410.23137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22992v1","updated":"2024-10-30T13:17:38Z","published":"2024-10-30T13:17:38Z","title":"Dynamic Matching with Post-allocation Service and its Application to\n  Refugee Resettlement","summary":"  Motivated by our collaboration with a major refugee resettlement agency in\nthe U.S., we study a dynamic matching problem where each new arrival (a refugee\ncase) must be matched immediately and irrevocably to one of the static\nresources (a location with a fixed annual quota). In addition to consuming the\nstatic resource, each case requires post-allocation service from a server, such\nas a translator. Given the time-consuming nature of service, a server may not\nbe available at a given time, thus we refer to it as a dynamic resource. Upon\nmatching, the case will wait to avail service in a first-come-first-serve\nmanner. Bursty matching to a location may result in undesirable congestion at\nits corresponding server. Consequently, the central planner (the agency) faces\na dynamic matching problem with an objective that combines the matching reward\n(captured by pair-specific employment outcomes) with the cost for congestion\nfor dynamic resources and over-allocation for the static ones. Motivated by the\nobserved fluctuations in the composition of refugee pools across the years, we\ndesign algorithms that do not rely on distributional knowledge constructed\nbased on past years' data. To that end, we develop learning-based algorithms\nthat are asymptotically optimal in certain regimes, easy to interpret, and\ncomputationally fast. Our design is based on learning the dual variables of the\nunderlying optimization problem; however, the main challenge lies in the\ntime-varying nature of the dual variables associated with dynamic resources. To\novercome this challenge, our theoretical development brings together techniques\nfrom Lyapunov analysis, adversarial online learning, and stochastic\noptimization. On the application side, when tested on real data from our\npartner agency, our method outperforms existing ones making it a viable\ncandidate for replacing the current practice upon experimentation.\n","authors":["Kirk Bansak","Soonbong Lee","Vahideh Manshadi","Rad Niazadeh","Elisabeth Paulson"],"pdf_url":"https://arxiv.org/pdf/2410.22992v1.pdf","comment":"Preliminary conference version appeared in ACM Economics and\n  Computation (EC 2024)"},{"id":"http://arxiv.org/abs/2410.22912v1","updated":"2024-10-30T11:09:31Z","published":"2024-10-30T11:09:31Z","title":"Self-optimization in distributed manufacturing systems using Modular\n  State-based Stackelberg Games","summary":"  In this study, we introduce Modular State-based Stackelberg Games (Mod-SbSG),\na novel game structure developed for distributed self-learning in modular\nmanufacturing systems. Mod-SbSG enhances cooperative decision-making among\nself-learning agents within production systems by integrating State-based\nPotential Games (SbPG) with Stackelberg games. This hierarchical structure\nassigns more important modules of the manufacturing system a first-mover\nadvantage, while less important modules respond optimally to the leaders'\ndecisions. This decision-making process differs from typical multi-agent\nlearning algorithms in manufacturing systems, where decisions are made\nsimultaneously. We provide convergence guarantees for the novel game structure\nand design learning algorithms to account for the hierarchical game structure.\nWe further analyse the effects of single-leader/multiple-follower and\nmultiple-leader/multiple-follower scenarios within a Mod-SbSG. To assess its\neffectiveness, we implement and test Mod-SbSG in an industrial control setting\nusing two laboratory-scale testbeds featuring sequential and serial-parallel\nprocesses. The proposed approach delivers promising results compared to the\nvanilla SbPG, which reduces overflow by 97.1%, and in some cases, prevents\noverflow entirely. Additionally, it decreases power consumption by 5-13% while\nsatisfying the production demand, which significantly improves potential\n(global objective) values.\n","authors":["Steve Yuwono","Ahmar Kamal Hussain","Dorothea Schwung","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2410.22912v1.pdf","comment":"This pre-print was submitted to Journal of Manufacturing Systems on\n  October 30, 2024"},{"id":"http://arxiv.org/abs/2410.22776v1","updated":"2024-10-30T07:44:14Z","published":"2024-10-30T07:44:14Z","title":"Conflux-PSRO: Effectively Leveraging Collective Advantages in Policy\n  Space Response Oracles","summary":"  Policy Space Response Oracle (PSRO) with policy population construction has\nbeen demonstrated as an effective method for approximating Nash Equilibrium\n(NE) in zero-sum games. Existing studies have attempted to improve diversity in\npolicy space, primarily by incorporating diversity regularization into the Best\nResponse (BR). However, these methods cause the BR to deviate from maximizing\nrewards, easily resulting in a population that favors diversity over\nperformance, even when diversity is not always necessary. Consequently,\nexploitability is difficult to reduce until policies are fully explored,\nespecially in complex games. In this paper, we propose Conflux-PSRO, which\nfully exploits the diversity of the population by adaptively selecting and\ntraining policies at state-level. Specifically, Conflux-PSRO identifies useful\npolicies from the existing population and employs a routing policy to select\nthe most appropriate policies at each decision point, while simultaneously\ntraining them to enhance their effectiveness. Compared to the single-policy BR\nof traditional PSRO and its diversity-improved variants, the BR generated by\nConflux-PSRO not only leverages the specialized expertise of diverse policies\nbut also synergistically enhances overall performance. Our experiments on\nvarious environments demonstrate that Conflux-PSRO significantly improves the\nutility of BRs and reduces exploitability compared to existing methods.\n","authors":["Yucong Huang","Jiesong Lian","Mingzhi Wang","Chengdong Ma","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2410.22776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22765v1","updated":"2024-10-30T07:30:42Z","published":"2024-10-30T07:30:42Z","title":"Combinatorial Diffusion Auction Design","summary":"  Diffusion auction design for combinatorial settings is a long-standing\nchallenge. One difficulty is that we cannot directly extend the solutions for\nsimpler settings to combinatorial settings (like extending the Vickrey auction\nto VCG in the traditional settings). In this paper, we propose a different\napproach to leverage the diffusion auctions for single-item settings. We design\na combinatorial diffusion auction framework which can use any desirable\nsingle-item diffusion auction to produce a combinatorial auction to satisfy\nincentive compatibility (IC), individual rationality (IR), and weak budget\nbalance (WBB).\n","authors":["Xuanyu Li","Miao Li","Yuhan Cao","Dengji Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.22765v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.22762v1","updated":"2024-10-30T07:29:48Z","published":"2024-10-30T07:29:48Z","title":"A Game-Theoretic Approach for Security Control Selection","summary":"  Selecting the combination of security controls that will most effectively\nprotect a system's assets is a difficult task. If the wrong controls are\nselected, the system may be left vulnerable to cyber-attacks that can impact\nthe confidentiality, integrity and availability of critical data and services.\nIn practical settings, it is not possible to select and implement every control\npossible. Instead considerations, such as budget, effectiveness, and\ndependencies among various controls, must be considered to choose a combination\nof security controls that best achieve a set of system security objectives. In\nthis paper, we propose a game-theoretic approach for selecting effective\ncombinations of security controls based on expected attacker profiles and a set\nbudget. The control selection problem is set up as a two-person zero-sum\none-shot game. Valid control combinations for selection are generated using an\nalgebraic formalism to account for dependencies among selected controls. We\ndemonstrate the proposed approach on an illustrative financial system used in\ngovernment departments under four different scenarios. The results illustrate\nhow a security analyst can use the proposed approach to guide and support\ndecision-making in the control selection activity when developing secure\nsystems.\n","authors":["Dylan L√©veill√©","Jason Jaskolka"],"pdf_url":"https://arxiv.org/pdf/2410.22762v1.pdf","comment":"In Proceedings GandALF 2024, arXiv:2410.21884"},{"id":"http://arxiv.org/abs/2410.22760v1","updated":"2024-10-30T07:28:47Z","published":"2024-10-30T07:28:47Z","title":"Reactive Synthesis for Expected Impacts","summary":"  As business processes become increasingly complex, effectively modeling\ndecision points, their likelihood, and resource consumption is crucial for\noptimizing operations. To address this challenge, this paper introduces a\nformal extension of the Business Process Model and Notation (BPMN) that\nincorporates choices, probabilities, and impacts, referred to as BPMN+CPI. This\nextension is motivated by the growing emphasis on precise control within\nbusiness process management, where carefully selecting decision pathways in\nrepeated instances is crucial for conforming to certain standards of multiple\nresource consumption and environmental impacts. In this context we deal with\nthe problem of synthesizing a strategy (if any) that guarantees that the\nexpected impacts on repeated execution of the input process are below a given\nthreshold. We show that this problem belongs to PSPACE complexity class;\nmoreover we provide an effective procedure for computing a strategy (if\npresent).\n","authors":["Emanuele Chini","Pietro Sala","Andrea Simonetti","Omid Zare"],"pdf_url":"https://arxiv.org/pdf/2410.22760v1.pdf","comment":"In Proceedings GandALF 2024, arXiv:2410.21884"},{"id":"http://arxiv.org/abs/2311.18339v2","updated":"2024-10-30T02:02:23Z","published":"2023-11-30T08:18:37Z","title":"Tight Bounds for The Price of Fairness","summary":"  A central decision maker (CDM), who seeks an efficient allocation of scarce\nresources among a finite number of players, often has to incorporate fairness\ncriteria to avoid unfair outcomes. Indeed, the Price of Fairness (POF), a term\ncoined in the seminal work by Bertsimas et al. (2011), refers to the efficiency\nloss due to the incorporation of fairness criteria into the allocation method.\nQuantifying the POF would help the CDM strike an appropriate balance between\nefficiency and fairness. In this paper we improve upon existing results in the\nliterature, by providing tight bounds for the POF for the proportional fairness\ncriterion for any $n$, when the maximum achievable utilities of the players are\nequal or are not equal. Further, while Bertsimas et al. (2011) have already\nderived a tight bound for the max-min fairness criterion for the case that all\nplayers have equal maximum achievable utilities, we also provide a tight bound\nin scenarios where these utilities are not equal. For both criteria, we\ncharacterize the conditions where the POF reaches its peak and provide the\nsupremum bounds of our bounds over all maximum achievable utility vectors,\nwhich are shown to be asymptotically strictly smaller than the supremum of the\nBertsimas et al. (2011) bounds. Finally, we investigate the sensitivity of our\nbounds and the bounds in Bertsimas et al. (2011) for the POF to the variability\nof the maximum achievable utilities.\n","authors":["Yifeng Cao","Yichuan Ding","Daniel Granot"],"pdf_url":"https://arxiv.org/pdf/2311.18339v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.23517v1","updated":"2024-10-30T23:58:11Z","published":"2024-10-30T23:58:11Z","title":"On Heterogeneous Ising Machines","summary":"  Ising machines are effective solvers for complex combinatorial optimization\nproblems. The idea is mapping the optimal solution(s) to a combinatorial\noptimization problem to the minimum energy state(s) of a physical system, which\nnaturally converges to and stabilizes at a minimum energy state upon\nperturbance. The underlying mathematical abstraction, the Ising model, was\noriginally developed to explain dynamic behavior of ferromagnetic materials and\nwas shown to generalize to numerous other physical systems. In a generic\noptimization problem, each variable can interact with another in different\nways. At the same time, problem sizes of practical importance are growing very\nfast. Unfortunately, both the number and connectivity of spins in hardware are\nsubject to fundamental physical limits. Different problems feature different\ninteraction patterns between variables which may not always directly match the\nnetwork topology supported by a specific Ising machine. In the presence of a\nmismatch, emulating generic interactions using the machine topology is usually\npossible, however, comes at the cost of additional physical spins to facilitate\nthe mapping. Furthermore, mismatches in the problem vs. hardware connectivity\nrender even more physical spins necessary. Combinatorial optimization problems\nof practical importance come with diverse connectivity patterns, which a rigid\nnetwork topology in hardware cannot efficiently cover. To bridge the gap\nbetween application demand and hardware resources, in analogy to classical\nheterogeneous chip multiprocessors, in this paper we make the case for\nheterogeneous Ising multiprocessors, where each Ising core features a different\nconnectivity. We provide a detailed design space exploration and quantify the\nefficiency of different design options in terms of time or energy to solution\nalong with solution accuracy compared to homogeneous alternatives.\n","authors":["H√ºsrev Cƒ±lasun","Abhimanyu Kumar","Ziqing Zeng","Nafisa Sadaf Prova","Sachin S. Sapatnekar","Ulya R. Karpuzcu"],"pdf_url":"https://arxiv.org/pdf/2410.23517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10830v2","updated":"2024-10-30T21:24:27Z","published":"2024-08-20T13:23:37Z","title":"Single Bridge Formation in Self-Organizing Particle Systems","summary":"  Local interactions of uncoordinated individuals produce the collective\nbehaviors of many biological systems, inspiring much of the current research in\nprogrammable matter. A striking example is the spontaneous assembly of fire\nants into \"bridges\" comprising their own bodies to traverse obstacles and reach\nsources of food. Experiments and simulations suggest that, remarkably, these\nants always form one bridge -- instead of multiple, competing bridges --\ndespite a lack of central coordination. We argue that the reliable formation of\na single bridge does not require sophistication on behalf of the individuals by\nprovably reproducing this behavior in a self-organizing particle system. We\nshow that the formation of a single bridge by the particles is a statistical\ninevitability of their preferences to move in a particular direction, such as\ntoward a food source, and their preference for more neighbors. Two parameters,\n$\\eta$ and $\\beta$, reflect the strengths of these preferences and determine\nthe Gibbs stationary measure of the corresponding particle system's Markov\nchain dynamics. We show that a single bridge almost certainly forms when $\\eta$\nand $\\beta$ are sufficiently large. Our proof introduces an auxiliary Markov\nchain, called an \"occupancy chain,\" that captures only the significant, global\nchanges to the system. Through the occupancy chain, we abstract away\ninformation about the motion of individual particles, but we gain a more direct\nmeans of analyzing their collective behavior. Such abstractions provide a\npromising new direction for understanding many other systems of programmable\nmatter.\n","authors":["Shunhao Oh","Joseph Briones","Jacob Calvert","Noah Egan","Dana Randall","Andr√©a W. Richa"],"pdf_url":"https://arxiv.org/pdf/2408.10830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18412v3","updated":"2024-10-30T21:13:43Z","published":"2024-02-28T15:32:35Z","title":"Quantum approximate optimization algorithm with random and subgraph\n  phase operators","summary":"  The quantum approximate optimization algorithm (QAOA) is a promising quantum\nalgorithm that can be used to approximately solve combinatorial optimization\nproblems. The usual QAOA ansatz consists of an alternating application of the\ncost and mixer Hamiltonians. In this work, we study how using Hamiltonians\nother than the usual cost Hamiltonian, dubbed custom phase operators, can\naffect the performance of QAOA. We derive an expected value formula for QAOA\nwith custom phase operators at $p = 1$ and show numerically that some of these\ncustom phase operators can achieve higher approximation ratio than the original\nalgorithm implementation. Out of all the graphs tested at $p=1$, 0.036\\% of the\nrandom custom phase operators, 75.9\\% of the subgraph custom phase operators,\n95.1\\% of the triangle-removed custom phase operators, and 93.9\\% of the\nmaximal degree edge-removed custom phase operators have a higher approximation\nratio than the original QAOA implementation. Furthermore, we numerically\nsimulate these phase operators for $p=2$ and $p=3$ levels of QAOA and find that\nthere exist a large number of subgraph, triangle-removed, and maximal degree\nedge-removed custom phase operators that have a higher approximation ratio than\nQAOA at the same depth. These findings open up the question of whether better\nphase operators can be designed to further improve the performance of QAOA.\n","authors":["Anthony Wilkie","Igor Gaidai","James Ostrowski","Rebekah Herrman"],"pdf_url":"https://arxiv.org/pdf/2402.18412v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09463v2","updated":"2024-10-30T20:23:50Z","published":"2023-07-18T17:46:33Z","title":"A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error\n  Correction","summary":"  Neural decoders for quantum error correction (QEC) rely on neural networks to\nclassify syndromes extracted from error correction codes and find appropriate\nrecovery operators to protect logical information against errors. Its ability\nto adapt to hardware noise and long-term drifts make neural decoders a\npromising candidate for inclusion in a fault-tolerant quantum architecture.\nHowever, given their limited scalability, it is prudent that small-scale\n(local) neural decoders are treated as first stages of multi-stage decoding\nschemes for fault-tolerant quantum computers with millions of qubits. In this\ncase, minimizing the decoding time to match the stabilization measurements\nfrequency and a tight co-integration with the QPUs is highly desired. Cryogenic\nrealizations of neural decoders can not only improve the performance of higher\nstage decoders, but they can minimize communication delays, and alleviate\nwiring bottlenecks. In this work, we design and analyze a neural decoder based\non an in-memory computation (IMC) architecture, where crossbar arrays of\nresistive memory devices are employed to both store the synaptic weights of the\nneural decoder and perform analog matrix-vector multiplications. In simulations\nsupported by experimental measurements, we investigate the impact of TiOx-based\nmemristive devices' non-idealities on decoding fidelity. We develop\nhardware-aware re-training methods to mitigate the fidelity loss, restoring the\nideal decoder's pseudo-threshold for the distance-3 surface code. This work\nprovides a pathway to scalable, fast, and low-power cryogenic IMC hardware for\nintegrated fault-tolerant QEC.\n","authors":["Victor Yon","Fr√©d√©ric Marcotte","Pierre-Antoine Mouny","Gebremedhin A. Dagnew","Bohdan Kulchytskyy","Sophie Rochette","Yann Beilliard","Dominique Drouin","Pooya Ronagh"],"pdf_url":"https://arxiv.org/pdf/2307.09463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05241v4","updated":"2024-10-30T18:37:57Z","published":"2024-08-05T20:49:48Z","title":"Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models","summary":"  As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these\nstate-of-the-art models increases commensurately. However, their deployment is\ncostly both in terms of processing power and time. In this paper, we\ninvestigate the feasibility of creating smaller, highly-performing specialized\nalgorithms by way of fine-tuning. To do this, we first present a large\npre-trained model with 20 unique scenarios that combine different social\ncontexts with games of varying social dilemmas, record its answers, and use\nthem for Q&A fine-tuning on a smaller model of the same family. Our focus is on\nin-context game-theoretic decision-making, the same domain within which human\ninteraction occurs and that requires both a theory of mind (or a semblance\nthereof) and an understanding of social dynamics. The smaller model is\ntherefore trained not just on the answers provided, but also on the motivations\nprovided by the larger model, which should contain advice and guidelines to\nnavigate both strategic dilemmas and social cues. We find that the fine-tuned\nsmaller language model consistently bridged the gap in performance between the\nsmaller pre-trained version of the model and its larger relative and that its\nimprovements extended in areas and contexts beyond the ones provided in the\ntraining examples, including on out-of-sample scenarios that include completely\ndifferent game structures. On average for all games, through fine-tuning, the\nsmaller model showed a 46% improvement measured as alignment towards the\nbehavior of the larger model, with 100% representing indistinguishable\nbehavior. When presented with out-of-sample social contexts and games, the\nfine-tuned model still displays remarkable levels of alignment, reaching an\nimprovement of 18% and 28% respectively.\n","authors":["Nunzio Lore","Sepehr Ilami","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2408.05241v4.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23366v1","updated":"2024-10-30T18:12:24Z","published":"2024-10-30T18:12:24Z","title":"Practical Evaluation of Wize and Bluetooth 5 Assisted RFID for an\n  Opportunistic Vehicular Scenario","summary":"  Wireless communications are critical in the constantly changing environment\nof IoT and RFID technologies, where thousands of devices can be deployed across\na wide range of scenarios. Whether connecting to cloud servers or local\nfog/edge devices, maintaining seamless communications is difficult, especially\nin demanding contexts like industrial warehouses or remote rural areas.\nOpportunistic networks, when combined with edge devices, provide a possible\nsolution to this challenge. These networks enable IoT devices, particularly\nmobile devices, to redirect information as it passes via other devices until it\nreaches an edge node. Using different communication protocols, this paper\ninvestigates their effects on response times and total messages received for a\nopportunistic assisted RFID system. Specifically, this article compares two\ncommunications technologies (Bluetooth 5 and Wize) when used for building a\nnovel Opportunistic Edge Computing (OEC) identification system based on\nlow-cost Single-Board Computers (SBCs). For such a comparison, measurements\nhave been performed for quantifying packet loss and latency. The tests\nconsisted in two experiments under identical conditions and scenarios, with a\nnode located roadside, transmitting identification information, and a node\nlocated inside a moving vehicle that was driven at varying vehicle speeds. The\nobtained results show for Bluetooth 5 average latencies ranging between 700 and\n950 ms with packet losses between 7% and 27%, whereas for Wize the average\ndelay as between 150 and 370 ms with packet losses between 20% and 52%.\n","authors":["Angel Niebla-Montero","Ivan Froiz-Miguez","Paula Fraga-Lamas","Tiago M. Fernandez-Carames"],"pdf_url":"https://arxiv.org/pdf/2410.23366v1.pdf","comment":"Paper accepted in IEEE RFID 2024"},{"id":"http://arxiv.org/abs/2410.23357v1","updated":"2024-10-30T18:06:33Z","published":"2024-10-30T18:06:33Z","title":"Practical Evaluation of Low-Frequency Vibration Energy Harvesting for\n  Creating Green RFID and IoT Devices","summary":"  One of the main limitations for the development and deployment of many Green\nRadio Frequency Identification (RFID) and Internet of Things (IoT) systems is\nthe access to energy sources. In this aspect batteries are the main option to\nbe used in energy constrained scenarios, but their use is limited to certain\ncases, either because of the constraints imposed by a reduced-form factor,\ntheir limited lifespan, or the characteristics of the environment itself (e.g.\noperating temperature, risk of burning, need for fast response, sudden voltage\nvariations). In this regard, supercapacitors present an interesting alternative\nfor the previously mentioned type of environment, although, due to their\nshort-term capacity, they must be combined with an alternative energy supply\nmechanism. Energy harvesting mechanisms, in conjunction with ultra-low-power\nelectronics, supercapacitors and various methods to improve the efficiency of\ncommunications, have enabled the emergence of battery-less passive electronic\ndevices such as sensors, actuators or transmitters. This paper presents a novel\nanalysis of the performance of an energy harvesting system based on vibrations\nfor Green RFID and IoT applications in the field of maritime transport. The\nresults show that the proposed system allows for charging half of a 1.2 F\nsupercapacitor in about 72 minutes, providing a stable current of around 210 uA\nand a power output of 0.38 mW.\n","authors":["Ivan Froiz-Miguez","Paula Fraga-Lamas","Tiago M. Fernandez-Carames"],"pdf_url":"https://arxiv.org/pdf/2410.23357v1.pdf","comment":"Paper accepted in IEEE RFID 2024"},{"id":"http://arxiv.org/abs/2410.23203v1","updated":"2024-10-30T16:56:06Z","published":"2024-10-30T16:56:06Z","title":"Resilient-By-Design: A Resiliency Framework for Future Wireless Networks","summary":"  Our future society will be increasingly digitalised, hyper-connected and\nglobally data driven. The sixth generation (6G) and beyond 6G wireless networks\nare expected to bridge the digital and physical worlds by providing wireless\nconnectivity as a service to different vertical sectors, including industries,\nsmart cities, eHealth and autonomous transportation. Such far reaching\nintegration will render the society increasingly reliant on wireless networks.\nWhile this has the potential to greatly enhance our quality and ease of life,\nany disruption to these networks would also have significant impact with\noverreaching consequences. Disruptions can happen due to a variety of reasons,\nincluding planned outages, failures due to the nature of wireless propagation,\nnatural disasters, and deliberate cybersecurity attacks. Hence, 6G and beyond\n6G networks should not only provide near instant and virtually unlimited\nconnectivity, but also be resilient against internal and external disruptions.\nThis paper proposes a resilient-by-design framework towards this end. First, we\nprovide an overview of the disruption landscape. Thereafter, we comprehensively\noutline the main features of the proposed concept. Finally, we detail the four\nkey steps of the framework, namely predict, preempt, protect and progress. A\nsimple but illustrative preliminary simulation result is also presented to\nhighlight the potential advantages and efficiency of the proposed approach in\naddressing outages.\n","authors":["Nurul Huda Mahmood","Sumudu Samarakoon","Pawani Porambage","Mehdi Bennis","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2410.23203v1.pdf","comment":"Submitted to IEEE Communications Magazine"},{"id":"http://arxiv.org/abs/2408.07997v6","updated":"2024-10-30T14:46:23Z","published":"2024-08-15T07:47:08Z","title":"Enhanced Quantum Energy Teleportation using a 3-Qubit System","summary":"  Quantum Energy Teleportation (QET) is a novel method that leverages quantum\nentanglement to transfer energy between two distant locations without any\nphysical movement of the energy. The first realization of QET on\nsuperconducting hardware, utilizing a 2-qubit system, demonstrated an average\nenergy retrieval efficiency of 35.4% (observing only V ) by the receiver, Bob.\nIn this paper, we present a new approach using a 3-qubit system to enhance the\nenergy efficiency of QET. We have incorporated a novel 3-qubit ground state\nHamiltonian H to achieve this, which conforms to the constraints of Zero mean\nenergy and anti-commutative properties of the operations on the observable of\nthe senders and receiver. Our experimental results show a significant\nimprovement in terms of energy retrieval. Though the Multiple-Input\nSingle-Output (MISO) model demonstrates a similar result achieving an average\nefficiency of 32.5% (observing only V ), the Single-Input Multiple-Output\n(SIMO) model shows a significantly higher result than that of the 2-qubit\nsystem considering practical usage, which is 67.2%.\n","authors":["Md Shoyib Hassan","Syed Emad Uddin Shubha","M. R. C Mahdy"],"pdf_url":"https://arxiv.org/pdf/2408.07997v6.pdf","comment":"13 pages, 13 figures, 2 table, 50+ equations"},{"id":"http://arxiv.org/abs/2410.22946v1","updated":"2024-10-30T12:04:22Z","published":"2024-10-30T12:04:22Z","title":"KALAM: toolKit for Automating high-Level synthesis of Analog computing\n  systeMs","summary":"  Diverse computing paradigms have emerged to meet the growing needs for\nintelligent energy-efficient systems. The Margin Propagation (MP) framework,\nbeing one such initiative in the analog computing domain, stands out due to its\nscalability across biasing conditions, temperatures, and diminishing process\ntechnology nodes. However, the lack of digital-like automation tools for\ndesigning analog systems (including that of MP analog) hinders their adoption\nfor designing large systems. The inherent scalability and modularity of MP\nsystems present a unique opportunity in this regard. This paper introduces\nKALAM (toolKit for Automating high-Level synthesis of Analog computing\nsysteMs), which leverages factor graphs as the foundational paradigm for\nsynthesizing MP-based analog computing systems. Factor graphs are the basis of\nvarious signal processing tasks and, when coupled with MP, can be used to\ndesign scalable and energy-efficient analog signal processors. Using Python\nscripting language, the KALAM automation flow translates an input factor graph\nto its equivalent SPICE-compatible circuit netlist that can be used to validate\nthe intended functionality. KALAM also allows the integration of design\noptimization strategies such as precision tuning, variable elimination, and\nmathematical simplification. We demonstrate KALAM's versatility for tasks such\nas Bayesian inference, Low-Density Parity Check (LDPC) decoding, and Artificial\nNeural Networks (ANN). Simulation results of the netlists align closely with\nsoftware implementations, affirming the efficacy of our proposed automation\ntool.\n","authors":["Ankita Nandi","Krishil Gandhi","Mahendra Pratap Singh","Shantanu Chakrabartty","Chetan Singh Thakur"],"pdf_url":"https://arxiv.org/pdf/2410.22946v1.pdf","comment":"5 Pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.05404v2","updated":"2024-10-30T10:01:06Z","published":"2024-09-09T08:05:43Z","title":"DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects","summary":"  Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph.\n","authors":["Xu Zhang","Ke Liu","Yisong Chang","Ke Zhang","Mingyu Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22812v1","updated":"2024-10-30T08:44:10Z","published":"2024-10-30T08:44:10Z","title":"Universality of the $œÄ^2/6$ Pathway in Avoiding Model Collapse","summary":"  Researchers in empirical machine learning recently spotlighted their fears of\nso-called Model Collapse. They imagined a discard workflow, where an initial\ngenerative model is trained with real data, after which the real data are\ndiscarded, and subsequently, the model generates synthetic data on which a new\nmodel is trained. They came to the conclusion that models degenerate as\nmodel-fitting generations proceed. However, other researchers considered an\naugment workflow, where the original real data continue to be used in each\ngeneration of training, augmented by synthetic data from models fit in all\nearlier generations. Empirical results on canonical datasets and learning\nprocedures confirmed the occurrence of model collapse under the discard\nworkflow and avoidance of model collapse under the augment workflow. Under the\naugment workflow, theoretical evidence also confirmed avoidance in particular\ninstances; specifically, Gerstgrasser et al. (2024) found that for classical\nLinear Regression, test risk at any later generation is bounded by a moderate\nmultiple, viz. pi-squared-over-6 of the test risk of training with the original\nreal data alone. Some commentators questioned the generality of theoretical\nconclusions based on the generative model assumed in Gerstgrasser et al.\n(2024): could similar conclusions be reached for other task/model pairings? In\nthis work, we demonstrate the universality of the pi-squared-over-6 augment\nrisk bound across a large family of canonical statistical models, offering key\ninsights into exactly why collapse happens under the discard workflow and is\navoided under the augment workflow. In the process, we provide a framework that\nis able to accommodate a large variety of workflows (beyond discard and\naugment), thereby enabling an experimenter to judge the comparative merits of\nmultiple different workflows by simulating a simple Gaussian process.\n","authors":["Apratim Dey","David Donoho"],"pdf_url":"https://arxiv.org/pdf/2410.22812v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2404.06939v4","updated":"2024-10-30T02:44:09Z","published":"2024-04-10T11:43:26Z","title":"Late Breaking Results: Fast System Technology Co-Optimization Framework\n  for Emerging Technology Based on Graph Neural Networks","summary":"  This paper proposes a fast system technology co-optimization (STCO) framework\nthat optimizes power, performance, and area (PPA) for next-generation IC\ndesign, addressing the challenges and opportunities presented by novel\nmaterials and device architectures. We focus on accelerating the technology\nlevel of STCO using AI techniques, by employing graph neural network\n(GNN)-based approaches for both TCAD simulation and cell library\ncharacterization, which are interconnected through a unified compact model,\ncollectively achieving over a 100X speedup over traditional methods. These\nadvancements enable comprehensive STCO iterations with runtime speedups ranging\nfrom 1.9X to 14.1X and supports both emerging and traditional technologies.\n","authors":["Tianliang Ma","Guangxi Fan","Xuguang Sun","Zhihui Deng","Kainlu Low","Leilai Shao"],"pdf_url":"https://arxiv.org/pdf/2404.06939v4.pdf","comment":"This article has been accepted by the 61st Design Automation\n  Conference(DAC)"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.18975v2","updated":"2024-10-30T16:10:33Z","published":"2024-10-24T17:59:31Z","title":"Unbounded: A Generative Infinite Game of Character Life Simulation","summary":"  We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.\n","authors":["Jialu Li","Yuanzhen Li","Neal Wadhwa","Yael Pritch","David E. Jacobs","Michael Rubinstein","Mohit Bansal","Nataniel Ruiz"],"pdf_url":"https://arxiv.org/pdf/2410.18975v2.pdf","comment":"Project page: https://generative-infinite-game.github.io/"},{"id":"http://arxiv.org/abs/2410.23109v1","updated":"2024-10-30T15:20:10Z","published":"2024-10-30T15:20:10Z","title":"NASM: Neural Anisotropic Surface Meshing","summary":"  This paper introduces a new learning-based method, NASM, for anisotropic\nsurface meshing. Our key idea is to propose a graph neural network to embed an\ninput mesh into a high-dimensional (high-d) Euclidean embedding space to\npreserve curvature-based anisotropic metric by using a dot product loss between\nhigh-d edge vectors. This can dramatically reduce the computational time and\nincrease the scalability. Then, we propose a novel feature-sensitive remeshing\non the generated high-d embedding to automatically capture sharp geometric\nfeatures. We define a high-d normal metric, and then derive an automatic\ndifferentiation on a high-d centroidal Voronoi tessellation (CVT) optimization\nwith the normal metric to simultaneously preserve geometric features and\ncurvature anisotropy that exhibit in the original 3D shapes. To our knowledge,\nthis is the first time that a deep learning framework and a large dataset are\nproposed to construct a high-d Euclidean embedding space for 3D anisotropic\nsurface meshing. Experimental results are evaluated and compared with the\nstate-of-the-art in anisotropic surface meshing on a large number of surface\nmodels from Thingi10K dataset as well as tested on extensive unseen 3D shapes\nfrom Multi-Garment Network dataset and FAUST human dataset.\n","authors":["Hongbo Li","Haikuan Zhu","Sikai Zhong","Ningna Wang","Cheng Lin","Xiaohu Guo","Shiqing Xin","Wenping Wang","Jing Hua","Zichun Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.23109v1.pdf","comment":"SIGGRAPH Asia 2024 (Conference Track)"},{"id":"http://arxiv.org/abs/2405.12895v2","updated":"2024-10-30T12:11:33Z","published":"2024-05-21T16:04:32Z","title":"Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution\n  Meshes and Neural Fields via Local Patch Meshing","summary":"  In this work, we present the local patch mesh representation for neural\nsigned distance fields. This technique allows to discretize local regions of\nthe level sets of an input SDF by projecting and deforming flat patch meshes\nonto the level set surface, using exclusively the SDF information and its\ngradient. Our analysis reveals this method to be more accurate than the\nstandard marching cubes algorithm for approximating the implicit surface. Then,\nwe apply this representation in the setting of handle-guided deformation: we\nintroduce two distinct pipelines, which make use of 3D neural fields to compute\nAs-Rigid-As-Possible deformations of both high-resolution meshes and neural\nfields under a given set of constraints. We run a comprehensive evaluation of\nour method and various baselines for neural field and mesh deformation which\nshow both pipelines achieve impressive efficiency and notable improvements in\nterms of quality of results and robustness. With our novel pipeline, we\nintroduce a scalable approach to solve a well-established geometry processing\nproblem on high-resolution meshes, and pave the way for extending other\ngeometric tasks to the domain of implicit surfaces via local patch meshing.\n","authors":["Daniele Baieri","Filippo Maggioli","Zorah L√§hner","Simone Melzi","Emanuele Rodol√†"],"pdf_url":"https://arxiv.org/pdf/2405.12895v2.pdf","comment":"12 pages, 16 figures"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.23479v1","updated":"2024-10-30T22:06:14Z","published":"2024-10-30T22:06:14Z","title":"The Trail Making Test in Virtual Reality (TMT-VR): The Effects of\n  Interaction Modes and Gaming Skills on Cognitive Performance of Young Adults","summary":"  Virtual Reality (VR) is increasingly used in neuropsychological assessments\ndue to its ability to simulate real-world environments. This study aimed to\ndevelop and evaluate the Trail Making Test in VR (TMT-VR) and investigate the\neffects of different interaction modes and gaming skills on cognitive\nperformance. A total of 71 young female and male adults (aged 18-35) with high\nand low gaming skills participated in this study. Participants completed the\nTMT-VR using three interaction modes as follows: eye tracking, head movement,\nand controller. Performance metrics included task completion time and accuracy.\nUser experience, usability, and acceptability of TMT-VR were also examined.\nResults showed that both eye tracking and head movement modes significantly\noutperformed the controller in terms of task completion time and accuracy. No\nsignificant differences were found between eye tracking and head movement\nmodes. Gaming skills did not significantly influence task performance using any\ninteraction mode. The TMT-VR demonstrates high usability, acceptability, and\nuser experience among participants. The findings suggest that VR-based\nassessments can effectively measure cognitive performance without being\ninfluenced by prior gaming skills, indicating potential applicability for\ndiverse populations.\n","authors":["Evgenia Giatzoglou","Panagiotis Vorias","Ryan Kemm","Irene Karayianni","Chrysanthi Nega","Panagiotis Kourtesis"],"pdf_url":"https://arxiv.org/pdf/2410.23479v1.pdf","comment":"25 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2410.23478v1","updated":"2024-10-30T22:00:34Z","published":"2024-10-30T22:00:34Z","title":"Collage: Decomposable Rapid Prototyping for Information Extraction on\n  Scientific PDFs","summary":"  Recent years in NLP have seen the continued development of domain-specific\ninformation extraction tools for scientific documents, alongside the release of\nincreasingly multimodal pretrained transformer models. While the opportunity\nfor scientists outside of NLP to evaluate and apply such systems to their own\ndomains has never been clearer, these models are difficult to compare: they\naccept different input formats, are often black-box and give little insight\ninto processing failures, and rarely handle PDF documents, the most common\nformat of scientific publication. In this work, we present Collage, a tool\ndesigned for rapid prototyping, visualization, and evaluation of different\ninformation extraction models on scientific PDFs. Collage allows the use and\nevaluation of any HuggingFace token classifier, several LLMs, and multiple\nother task-specific models out of the box, and provides extensible software\ninterfaces to accelerate experimentation with new models. Further, we enable\nboth developers and users of NLP-based tools to inspect, debug, and better\nunderstand modeling pipelines by providing granular views of intermediate\nstates of processing. We demonstrate our system in the context of information\nextraction to assist with literature review in materials science.\n","authors":["Sireesh Gururaja","Yueheng Zhang","Guannan Tang","Tianhao Zhang","Kevin Murphy","Yu-Tsen Yi","Junwon Seo","Anthony Rollett","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2410.23478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23448v1","updated":"2024-10-30T20:39:34Z","published":"2024-10-30T20:39:34Z","title":"Venire: A Machine Learning-Guided Panel Review System for Community\n  Content Moderation","summary":"  Research into community content moderation often assumes that moderation\nteams govern with a single, unified voice. However, recent work has found that\nmoderators disagree with one another at modest, but concerning rates. The\nproblem is not the root disagreements themselves. Subjectivity in moderation is\nunavoidable, and there are clear benefits to including diverse perspectives\nwithin a moderation team. Instead, the crux of the issue is that, due to\nresource constraints, moderation decisions end up being made by individual\ndecision-makers. The result is decision-making that is inconsistent, which is\nfrustrating for community members. To address this, we develop Venire, an\nML-backed system for panel review on Reddit. Venire uses a machine learning\nmodel trained on log data to identify the cases where moderators are most\nlikely to disagree. Venire fast-tracks these cases for multi-person review.\nIdeally, Venire allows moderators to surface and resolve disagreements that\nwould have otherwise gone unnoticed. We conduct three studies through which we\ndesign and evaluate Venire: a set of formative interviews with moderators,\ntechnical evaluations on two datasets, and a think-aloud study in which\nmoderators used Venire to make decisions on real moderation cases.\nQuantitatively, we demonstrate that Venire is able to improve decision\nconsistency and surface latent disagreements. Qualitatively, we find that\nVenire helps moderators resolve difficult moderation cases more confidently.\nVenire represents a novel paradigm for human-AI content moderation, and shifts\nthe conversation from replacing human decision-making to supporting it.\n","authors":["Vinay Koshy","Frederick Choi","Yi-Shyuan Chiang","Hari Sundaram","Eshwar Chandrasekharan","Karrie Karahalios"],"pdf_url":"https://arxiv.org/pdf/2410.23448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23267v1","updated":"2024-10-30T17:51:31Z","published":"2024-10-30T17:51:31Z","title":"Commit: Online Groups with Participation Commitments","summary":"  In spite of efforts to increase participation, many online groups struggle to\nsurvive past the initial days, as members leave and activity atrophies. We\nargue that a main assumption of online group design -- that groups ask nothing\nof their members beyond lurking -- may be preventing many of these groups from\nsustaining a critical mass of participation. In this paper, we explore an\nalternative commitment design for online groups, which requires that all\nmembers commit at regular intervals to participating, as a condition of\nremaining in the group. We instantiate this approach in a mobile group chat\nplatform called Commit, and perform a field study comparing commitment against\na control condition of social psychological nudges with N=57 participants over\nthree weeks. Commitment doubled the number of contributions versus the control\ncondition, and resulted in 87% (vs. 19%) of participants remaining active by\nthe third week. Participants reported that commitment provided safe cover for\nthem to post even when they were nervous. Through this work, we argue that more\neffortful, not less effortful, membership may support many online groups.\n","authors":["Lindsay Popowski","Yutong Zhang","Michael S. Bernstein"],"pdf_url":"https://arxiv.org/pdf/2410.23267v1.pdf","comment":"28 pages, 7 figures; This work will appear in the 27th ACM SIGCHI\n  Conference on Computer-Supported Cooperative Work & Social Computing (CSCW\n  2024)"},{"id":"http://arxiv.org/abs/2410.23239v1","updated":"2024-10-30T17:26:32Z","published":"2024-10-30T17:26:32Z","title":"CRAFT@Large: Building Community Through Co-Making","summary":"  CRAFT@Large (C@L) is an initiative launched by the MakerLAB at Cornell Tech\nto create an inclusive environment for the intercultural and intergenerational\nexchange of ideas through making. With our approach, we challenge the\ntraditional definition of community outreach performed by academic makerspaces.\nExisting academic makerspaces often perform community engagement by only\noffering hourly, one-time workshops or by having community members provide a\nproblem that is then used by students as a project assignment. These approaches\nposition community members as occasional visitors and non-equal contributors,\nwhich not only conflict with the core values of co-creation but also limit the\nmakerspaces' impact on connecting the universities and the communities. C@L\nexplored an alternative approach in which we invited community members as\nlong-term and equal co-makers into the academic makerspaces. In this article,\nwe showcase two sets of collaborations that illustrate the continuity of people\nthrough co-making. We present how academic makerspaces can function as a hub\nthat connects community members and partner organizations with the campus\ncommunity in a long-term relationship.\n","authors":["Yiran Zhao","Maria Alinea-Bravo","Niti Parikh"],"pdf_url":"https://arxiv.org/pdf/2410.23239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19806v2","updated":"2024-10-30T17:18:59Z","published":"2024-10-17T00:32:45Z","title":"Learning to Adopt Generative AI","summary":"  Recent advancements in generative AI, exemplified by ChatGPT, have\ndramatically transformed how people access information. Despite its powerful\ncapabilities, the benefits it provides may not be equally distributed among\nindividuals - a phenomenon referred to as the digital divide. Building upon\nprior literature, we propose two forms of digital divide in the generative AI\nadoption process: (i) the learning divide, capturing individuals' heterogeneous\nabilities to update their perceived utility of ChatGPT; and (ii) the utility\ndivide, representing differences in individuals' actual utility derived from\nper use of ChatGPT. To evaluate these two divides, we develop a Bayesian\nlearning model that incorporates demographic heterogeneities in both the\nutility and signal functions. Leveraging a six-month clickstream dataset, we\nestimate the model and find significant learning and utility divides across\nvarious demographic attributes. Interestingly, lower-educated and non-white\nindividuals derive higher utility gains from ChatGPT but learn about its\nutility at a slower rate. Furthermore, males, younger individuals, and those\nwith an IT background not only derive higher utility per use from ChatGPT but\nalso learn about its utility more rapidly. Besides, we document a phenomenon\ntermed the belief trap, wherein users underestimate ChatGPT's utility, opt not\nto use the tool, and consequently lack new experiences to update their\nperceptions, leading to continued underutilization. Our simulation further\ndemonstrates that the learning divide can significantly affect the probability\nof falling into the belief trap, another form of the digital divide in adoption\noutcomes (i.e., outcome divide); however, offering training programs can\nalleviate the belief trap and mitigate the divide.\n","authors":["Lijia Ma","Xingchen Xu","Yumei He","Yong Tan"],"pdf_url":"https://arxiv.org/pdf/2410.19806v2.pdf","comment":"43 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.23218v1","updated":"2024-10-30T17:10:19Z","published":"2024-10-30T17:10:19Z","title":"OS-ATLAS: A Foundation Action Model for Generalist GUI Agents","summary":"  Existing efforts in building GUI agents heavily rely on the availability of\nrobust commercial Vision-Language Models (VLMs) such as GPT-4o and\nGeminiProVision. Practitioners are often reluctant to use open-source VLMs due\nto their significant performance lag compared to their closed-source\ncounterparts, particularly in GUI grounding and Out-Of-Distribution (OOD)\nscenarios. To facilitate future research in this area, we developed OS-Atlas -\na foundational GUI action model that excels at GUI grounding and OOD agentic\ntasks through innovations in both data and modeling. We have invested\nsignificant engineering effort in developing an open-source toolkit for\nsynthesizing GUI grounding data across multiple platforms, including Windows,\nLinux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing\nthe largest open-source cross-platform GUI grounding corpus to date, which\ncontains over 13 million GUI elements. This dataset, combined with innovations\nin model training, provides a solid foundation for OS-Atlas to understand GUI\nscreenshots and generalize to unseen interfaces. Through extensive evaluation\nacross six benchmarks spanning three different platforms (mobile, desktop, and\nweb), OS-Atlas demonstrates significant performance improvements over previous\nstate-of-the-art models. Our evaluation also uncovers valuable insights into\ncontinuously improving and scaling the agentic capabilities of open-source\nVLMs.\n","authors":["Zhiyong Wu","Zhenyu Wu","Fangzhi Xu","Yian Wang","Qiushi Sun","Chengyou Jia","Kanzhi Cheng","Zichen Ding","Liheng Chen","Paul Pu Liang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2410.23218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23193v1","updated":"2024-10-30T16:47:01Z","published":"2024-10-30T16:47:01Z","title":"ReaWristic: Remote Touch Sensation to Fingers from a Wristband via\n  Visually Augmented Electro-Tactile Feedback","summary":"  We present a technique for providing remote tactile feedback to the thumb and\nindex finger via a wristband device. This enables haptics for touch and pinch\ninteractions in mixed reality (MR) while keeping the hand entirely free. We\nachieve this through a novel cross-modal stimulation, which we term visually\naugmented electro-tactile feedback. This consists of (1) electrically\nstimulating the nerves that innervate the targeted fingers using our wristband\ndevice and (2) concurrently, visually augmenting the targeted finger in MR to\nsteer the perceived sensation to the desired location. In our psychophysics\nstudy, we found that our approach provides tactile perception akin to tapping\nand, even from the wrist, it is capable of delivering the sensation to the\ntargeted fingers with about 50% of sensation occurring in the thumb and about\n40% of sensation occurring in the index finger. These results on localizability\nare unprecedented compared to electro-tactile feedback alone or any prior work\nfor creating sensations in the hand with devices worn on the wrist/arm.\nMoreover, unlike conventional electro-tactile techniques, our wristband\ndispenses with gel electrodes. Instead, it incorporates custom-made\nelastomer-based dry electrodes and a stimulation waveform designed for the\nelectrodes, ensuring the practicality of the device beyond laboratory settings.\nLastly, we evaluated the haptic realism of our approach in mixed reality and\nelicited qualitative feedback from users. Participants preferred our approach\nto a baseline vibrotactile wrist-worn device.\n","authors":["Yudai Tanaka","Neil Weiss","Robert Cole Bolger-Cruz","Jess Hartcher-O'Brien","Brendan Flynn","Roger Boldu","Nicholas Colonnese"],"pdf_url":"https://arxiv.org/pdf/2410.23193v1.pdf","comment":"10 pages, 14 figures, published at IEEE ISMAR 2024"},{"id":"http://arxiv.org/abs/2410.22309v2","updated":"2024-10-30T16:30:30Z","published":"2024-10-29T17:53:10Z","title":"GPT-4o reads the mind in the eyes","summary":"  Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans.\n","authors":["James W. A. Strachan","Oriana Pansardi","Eugenio Scaliti","Marco Celotto","Krati Saxena","Chunzhi Yi","Fabio Manzi","Alessandro Rufo","Guido Manzi","Michael S. A. Graziano","Stefano Panzeri","Cristina Becchio"],"pdf_url":"https://arxiv.org/pdf/2410.22309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10461v2","updated":"2024-10-30T16:01:56Z","published":"2024-06-15T01:14:23Z","title":"Exploring Parent-Child Perceptions on Safety in Generative AI: Concerns,\n  Mitigation Strategies, and Design Implications","summary":"  The widespread use of Generative Artificial Intelligence (GAI) among\nteenagers has led to significant misuse and safety concerns. To identify risks\nand understand parental controls challenges, we conducted a content analysis on\nReddit and interviewed 20 participants (seven teenagers and 13 parents). Our\nstudy reveals a significant gap in parental awareness of the extensive ways\nchildren use GAI, such as interacting with character-based chatbots for\nemotional support or engaging in virtual relationships. Parents and children\nreport differing perceptions of risks associated with GAI. Parents primarily\nexpress concerns about data collection, misinformation, and exposure to\ninappropriate content. In contrast, teenagers are more concerned about becoming\naddicted to virtual relationships with GAI, the potential misuse of GAI to\nspread harmful content in social groups, and the invasion of privacy due to\nunauthorized use of their personal data in GAI applications. The absence of\nparental control features on GAI platforms forces parents to rely on\nsystem-built controls, manually check histories, share accounts, and engage in\nactive mediation. Despite these efforts, parents struggle to grasp the full\nspectrum of GAI-related risks and to perform effective real-time monitoring,\nmediation, and education. We provide design recommendations to improve\nparent-child communication and enhance the safety of GAI use.\n","authors":["Yaman Yu","Tanusree Sharma","Melinda Hu","Justin Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.10461v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.23143v1","updated":"2024-10-30T15:58:05Z","published":"2024-10-30T15:58:05Z","title":"The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in\n  Lie Detection","summary":"  We investigate how low-quality AI advisors, lacking quality disclosures, can\nhelp spread text-based lies while seeming to help people detect lies.\nParticipants in our experiment discern truth from lies by evaluating\ntranscripts from a game show that mimicked deceptive social media exchanges on\ntopics with objective truths. We find that when relying on low-quality advisors\nwithout disclosures, participants' truth-detection rates fall below their own\nabilities, which recovered once the AI's true effectiveness was revealed.\nConversely, high-quality advisor enhances truth detection, regardless of\ndisclosure. We discover that participants' expectations about AI capabilities\ncontribute to their undue reliance on opaque, low-quality advisors.\n","authors":["Haimanti Bhattacharya","Subhasish Dugar","Sanchaita Hazra","Bodhisattwa Prasad Majumder"],"pdf_url":"https://arxiv.org/pdf/2410.23143v1.pdf","comment":"Order of the authors are in alphabetical order of their last names.\n  All authors contributed equally. The manuscript is under review. 74 Pages,\n  including appendices and references"},{"id":"http://arxiv.org/abs/2402.00793v3","updated":"2024-10-30T15:45:32Z","published":"2024-02-01T17:23:54Z","title":"Human Expertise in Algorithmic Prediction","summary":"  We introduce a novel framework for incorporating human expertise into\nalgorithmic predictions. Our approach leverages human judgment to distinguish\ninputs which are algorithmically indistinguishable, or \"look the same\" to\npredictive algorithms. We argue that this framing clarifies the problem of\nhuman-AI collaboration in prediction tasks, as experts often form judgments by\ndrawing on information which is not encoded in an algorithm's training data.\nAlgorithmic indistinguishability yields a natural test for assessing whether\nexperts incorporate this kind of \"side information\", and further provides a\nsimple but principled method for selectively incorporating human feedback into\nalgorithmic predictions. We show that this method provably improves the\nperformance of any feasible algorithmic predictor and precisely quantify this\nimprovement. We find empirically that although algorithms often outperform\ntheir human counterparts on average, human judgment can improve algorithmic\npredictions on specific instances (which can be identified ex-ante). In an\nX-ray classification task, we find that this subset constitutes nearly $30\\%$\nof the patient population. Our approach provides a natural way of uncovering\nthis heterogeneity and thus enabling effective human-AI collaboration.\n","authors":["Rohan Alur","Manish Raghavan","Devavrat Shah"],"pdf_url":"https://arxiv.org/pdf/2402.00793v3.pdf","comment":"35 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.23105v1","updated":"2024-10-30T15:15:41Z","published":"2024-10-30T15:15:41Z","title":"Automated Image-Based Identification and Consistent Classification of\n  Fire Patterns with Quantitative Shape Analysis and Spatial Location\n  Identification","summary":"  Fire patterns, consisting of fire effects that offer insights into fire\nbehavior and origin, are traditionally classified based on investigators'\nvisual observations, leading to subjective interpretations. This study proposes\na framework for quantitative fire pattern classification to support fire\ninvestigators, aiming for consistency and accuracy. The framework integrates\nfour components. First, it leverages human-computer interaction to extract fire\npatterns from surfaces, combining investigator expertise with computational\nanalysis. Second, it employs an aspect ratio-based random forest model to\nclassify fire pattern shapes. Third, fire scene point cloud segmentation\nenables precise identification of fire-affected areas and the mapping of 2D\nfire patterns to 3D scenes. Lastly, spatial relationships between fire patterns\nand indoor elements support an interpretation of the fire scene. These\ncomponents provide a method for fire pattern analysis that synthesizes\nqualitative and quantitative data. The framework's classification results\nachieve 93% precision on synthetic data and 83% on real fire patterns.\n","authors":["Pengkun Liu","Shuna Ni","Stanislav I. Stoliarov","Pingbo Tang"],"pdf_url":"https://arxiv.org/pdf/2410.23105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10465v2","updated":"2024-10-30T14:55:39Z","published":"2023-03-18T18:01:39Z","title":"Cognitive Load-based Affective Workload Allocation for Multi-human\n  Multi-robot Teams","summary":"  The interaction and collaboration between humans and multiple robots\nrepresent a novel field of research known as human multi-robot systems.\nAdequately designed systems within this field allow teams composed of both\nhumans and robots to work together effectively on tasks such as monitoring,\nexploration, and search and rescue operations. This paper presents a deep\nreinforcement learning-based affective workload allocation controller\nspecifically for multi-human multi-robot teams. The proposed controller can\ndynamically reallocate workloads based on the performance of the operators\nduring collaborative missions with multi-robot systems. The operators'\nperformances are evaluated through the scores of a self-reported questionnaire\n(i.e., subjective measurement) and the results of a deep learning-based\ncognitive workload prediction algorithm that uses physiological and behavioral\ndata (i.e., objective measurement). To evaluate the effectiveness of the\nproposed controller, we use a multi-human multi-robot CCTV monitoring task as\nan example and carry out comprehensive real-world experiments with 32 human\nsubjects for both quantitative measurement and qualitative analysis. Our\nresults demonstrate the performance and effectiveness of the proposed\ncontroller and highlight the importance of incorporating both subjective and\nobjective measurements of the operators' cognitive workload as well as seeking\nconsent for workload transitions, to enhance the performance of multi-human\nmulti-robot teams.\n","authors":["Wonse Jo","Ruiqi Wang","Baijian Yang","Dan Foti","Mo Rastgaar","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2303.10465v2.pdf","comment":"This paper is submitted and accepted to IEEE Transactions on\n  Human-Machine Systems"},{"id":"http://arxiv.org/abs/2410.07869v2","updated":"2024-10-30T14:49:49Z","published":"2024-10-10T12:41:19Z","title":"Benchmarking Agentic Workflow Generation","summary":"  Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench.\n","authors":["Shuofei Qiao","Runnan Fang","Zhisong Qiu","Xiaobin Wang","Ningyu Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07869v2.pdf","comment":"Work in progress (v2), update OpenAI o1 and Claude-3.5 results on\n  WorFBench"},{"id":"http://arxiv.org/abs/2405.20441v4","updated":"2024-10-30T14:29:37Z","published":"2024-05-30T19:35:06Z","title":"SECURE: Benchmarking Large Language Models for Cybersecurity","summary":"  Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.\n","authors":["Dipkamal Bhusal","Md Tanvirul Alam","Le Nguyen","Ashim Mahara","Zachary Lightcap","Rodney Frazier","Romy Fieblinger","Grace Long Torales","Benjamin A. Blakely","Nidhi Rastogi"],"pdf_url":"https://arxiv.org/pdf/2405.20441v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00552v4","updated":"2024-10-30T13:49:11Z","published":"2024-05-01T14:50:58Z","title":"Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs","summary":"  We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.\n","authors":["Nicolas Gorlo","Lukas Schmid","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2405.00552v4.pdf","comment":"8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2"},{"id":"http://arxiv.org/abs/2405.14398v3","updated":"2024-10-30T12:56:44Z","published":"2024-05-23T10:15:29Z","title":"SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition\n  with Jaccard Attentive Spiking Neural Network","summary":"  Surface electromyography (sEMG) based gesture recognition offers a natural\nand intuitive interaction modality for wearable devices. Despite significant\nadvancements in sEMG-based gesture-recognition models, existing methods often\nsuffer from high computational latency and increased energy consumption.\nAdditionally, the inherent instability of sEMG signals, combined with their\nsensitivity to distribution shifts in real-world settings, compromises model\nrobustness. To tackle these challenges, we propose a novel SpGesture framework\nbased on Spiking Neural Networks, which possesses several unique merits\ncompared with existing methods: (1) Robustness: By utilizing membrane potential\nas a memory list, we pioneer the introduction of Source-Free Domain Adaptation\ninto SNN for the first time. This enables SpGesture to mitigate the accuracy\ndegradation caused by distribution shifts. (2) High Accuracy: With a novel\nSpiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent\nsEMG features, leading to a notable rise in system accuracy. To validate\nSpGesture's performance, we collected a new sEMG gesture dataset which has\ndifferent forearm postures, where SpGesture achieved the highest accuracy among\nthe baselines ($89.26\\%$). Moreover, the actual deployment on the CPU\ndemonstrated a system latency below 100ms, well within real-time requirements.\nThis impressive performance showcases SpGesture's potential to enhance the\napplicability of sEMG in real-world scenarios. The code is available at\nhttps://github.com/guoweiyu/SpGesture/.\n","authors":["Weiyu Guo","Ying Sun","Yijie Xu","Ziyue Qiao","Yongkui Yang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2405.14398v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22985v1","updated":"2024-10-30T12:55:40Z","published":"2024-10-30T12:55:40Z","title":"Troubling Taxonomies in GenAI Evaluation","summary":"  To evaluate the societal impacts of GenAI requires a model of how social\nharms emerge from interactions between GenAI, people, and societal structures.\nYet a model is rarely explicitly defined in societal impact evaluations, or in\nthe taxonomies of societal impacts that support them. In this provocation, we\nargue that societal impacts should be conceptualised as application- and\ncontext-specific, incommensurable, and shaped by questions of social power.\nDoing so leads us to conclude that societal impact evaluations using existing\ntaxonomies are inherently limited, in terms of their potential to reveal how\nGenAI systems may interact with people when introduced into specific social\ncontexts. We therefore propose a governance-first approach to managing societal\nharms attended by GenAI technologies.\n","authors":["Glen Berman","Ned Cooper","Wesley Hanwen Deng","Ben Hutchinson"],"pdf_url":"https://arxiv.org/pdf/2410.22985v1.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2410.22950v1","updated":"2024-10-30T12:07:30Z","published":"2024-10-30T12:07:30Z","title":"SpiroActive: Active Learning for Efficient Data Acquisition for\n  Spirometry","summary":"  Respiratory illnesses are a significant global health burden. Respiratory\nillnesses, primarily Chronic obstructive pulmonary disease (COPD), is the\nseventh leading cause of poor health worldwide and the third leading cause of\ndeath worldwide, causing 3.23 million deaths in 2019, necessitating early\nidentification and diagnosis for effective mitigation. Among the diagnostic\ntools employed, spirometry plays a crucial role in detecting respiratory\nabnormalities. However, conventional clinical spirometry methods often entail\nconsiderable costs and practical limitations like the need for specialized\nequipment, trained personnel, and a dedicated clinical setting, making them\nless accessible. To address these challenges, wearable spirometry technologies\nhave emerged as promising alternatives, offering accurate, cost-effective, and\nconvenient solutions. The development of machine learning models for wearable\nspirometry heavily relies on the availability of high-quality ground truth\nspirometry data, which is a laborious and expensive endeavor. In this research,\nwe propose using active learning, a sub-field of machine learning, to mitigate\nthe challenges associated with data collection and labeling. By strategically\nselecting samples from the ground truth spirometer, we can mitigate the need\nfor resource-intensive data collection. We present evidence that models trained\non small subsets obtained through active learning achieve comparable/better\nresults than models trained on the complete dataset.\n","authors":["Ankita Kumari Jain","Nitish Sharma","Madhav Kanda","Nipun Batra"],"pdf_url":"https://arxiv.org/pdf/2410.22950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22937v1","updated":"2024-10-30T11:46:26Z","published":"2024-10-30T11:46:26Z","title":"Thoughtful Adoption of NLP for Civic Participation: Understanding\n  Differences Among Policymakers","summary":"  Natural language processing (NLP) tools have the potential to boost civic\nparticipation and enhance democratic processes because they can significantly\nincrease governments' capacity to gather and analyze citizen opinions. However,\ntheir adoption in government remains limited, and harnessing their benefits\nwhile preventing unintended consequences remains a challenge. While prior work\nhas focused on improving NLP performance, this work examines how different\ninternal government stakeholders influence NLP tools' thoughtful adoption. We\ninterviewed seven politicians (politically appointed officials as heads of\ngovernment institutions) and thirteen public servants (career government\nemployees who design and administrate policy interventions), inquiring how they\nchoose whether and how to use NLP tools to support civic participation\nprocesses. The interviews suggest that policymakers across both groups focused\non their needs for career advancement and the need to showcase the legitimacy\nand fairness of their work when considering NLP tool adoption and use. Because\nthese needs vary between politicians and public servants, their preferred NLP\nfeatures and tool designs also differ. Interestingly, despite their differing\nneeds and opinions, neither group clearly identifies who should advocate for\nNLP adoption to enhance civic participation or address the unintended\nconsequences of a poorly considered adoption. This lack of clarity in\nresponsibility might have caused the governments' low adoption of NLP tools. We\ndiscuss how these findings reveal new insights for future HCI research. They\ninform the design of NLP tools for increasing civic participation efficiency\nand capacity, the design of other tools and methods that ensure thoughtful\nadoption of AI tools in government, and the design of NLP tools for\ncollaborative use among users with different incentives and needs.\n","authors":["Jose A. Guridi","Cristobal Cheyre","Qian Yang"],"pdf_url":"https://arxiv.org/pdf/2410.22937v1.pdf","comment":"Forthcoming in the Proceedings of the 2025 Conference on Computer\n  Supported Cooperative Work and Social Computing (CSCW)"},{"id":"http://arxiv.org/abs/2409.05798v3","updated":"2024-10-30T10:28:36Z","published":"2024-09-09T17:02:47Z","title":"Enhancing Preference-based Linear Bandits via Human Response Time","summary":"  Interactive preference learning systems present humans with queries as pairs\nof options; humans then select their preferred choice, allowing the system to\ninfer preferences from these binary choices. While binary choice feedback is\nsimple and widely used, it offers limited information about preference\nstrength. To address this, we leverage human response times, which inversely\ncorrelate with preference strength, as complementary information. We introduce\na computationally efficient method based on the EZ-diffusion model, combining\nchoices and response times to estimate the underlying human utility function.\nTheoretical and empirical comparisons with traditional choice-only estimators\nshow that for queries where humans have strong preferences (i.e., \"easy\"\nqueries), response times provide valuable complementary information and enhance\nutility estimates. We integrate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that incorporating response times significantly\naccelerates preference learning.\n","authors":["Shen Li","Yuyang Zhang","Zhaolin Ren","Claire Liang","Na Li","Julie A. Shah"],"pdf_url":"https://arxiv.org/pdf/2409.05798v3.pdf","comment":"To appear in NeurIPS 2024 (Oral)"},{"id":"http://arxiv.org/abs/2410.22857v1","updated":"2024-10-30T09:42:47Z","published":"2024-10-30T09:42:47Z","title":"DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch\n  Inference","summary":"  This work presents DAVINCI, a unified architecture for single-stage\nComputer-Aided Design (CAD) sketch parameterization and constraint inference\ndirectly from raster sketch images. By jointly learning both outputs, DAVINCI\nminimizes error accumulation and enhances the performance of constrained CAD\nsketch inference. Notably, DAVINCI achieves state-of-the-art results on the\nlarge-scale SketchGraphs dataset, demonstrating effectiveness on both precise\nand hand-drawn raster CAD sketches. To reduce DAVINCI's reliance on large-scale\nannotated datasets, we explore the efficacy of CAD sketch augmentations. We\nintroduce Constraint-Preserving Transformations (CPTs), i.e. random\npermutations of the parametric primitives of a CAD sketch that preserve its\nconstraints. This data augmentation strategy allows DAVINCI to achieve\nreasonable performance when trained with only 0.1% of the SketchGraphs dataset.\nFurthermore, this work contributes a new version of SketchGraphs, augmented\nwith CPTs. The newly introduced CPTSketchGraphs dataset includes 80 million\nCPT-augmented sketches, thus providing a rich resource for future research in\nthe CAD sketch domain.\n","authors":["Ahmet Serdar Karadeniz","Dimitrios Mallis","Nesryne Mejri","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2410.22857v1.pdf","comment":"Accepted at BMVC 2024"},{"id":"http://arxiv.org/abs/2311.16466v3","updated":"2024-10-30T09:29:11Z","published":"2023-11-28T04:07:34Z","title":"The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry","summary":"  Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. Utilizing a leading AI\ndetection tool, we analyzed over 1 million complaints and identified a\nsignificant increase in LLM usage following the release of ChatGPT. We\nestablish a causal relationship between LLM usage and an increased likelihood\nof obtaining relief by employing instrumental variables to address endogeneity\nin LLM adoption. Experimental data further support this link, demonstrating\nthat LLMs enhance the clarity and persuasiveness of consumer narratives. Our\nfindings suggest that facilitating access to LLMs can help firms better\nunderstand consumer concerns and level the playing field among consumers. This\nunderscores the importance of policies promoting technological accessibility,\nenabling all consumers to effectively voice their concerns.\n","authors":["Minkyu Shin","Jin Kim","Jiwoong Shin"],"pdf_url":"https://arxiv.org/pdf/2311.16466v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22744v1","updated":"2024-10-30T06:58:59Z","published":"2024-10-30T06:58:59Z","title":"Designing AI Personalities: Enhancing Human-Agent Interaction Through\n  Thoughtful Persona Design","summary":"  In the rapidly evolving field of artificial intelligence (AI) agents,\ndesigning the agent's characteristics is crucial for shaping user experience.\nThis workshop aims to establish a research community focused on AI agent\npersona design for various contexts, such as in-car assistants, educational\ntools, and smart home environments. We will explore critical aspects of persona\ndesign, such as voice, embodiment, and demographics, and their impact on user\nsatisfaction and engagement. Through discussions and hands-on activities, we\naim to propose practices and standards that enhance the ecological validity of\nagent personas. Topics include the design of conversational interfaces, the\ninfluence of agent personas on user experience, and approaches for creating\ncontextually appropriate AI agents. This workshop will provide a platform for\nbuilding a community dedicated to developing AI agent personas that better fit\ndiverse, everyday interactions.\n","authors":["Nima Zargham","Mateusz Dubiel","Smit Desai","Thomas Mildner","Hanz-Joachim Belz"],"pdf_url":"https://arxiv.org/pdf/2410.22744v1.pdf","comment":"8 pages, the workshop accepted at the 23rd International Conference\n  on Mobile and Ubiquitous Multimedia (MUM 2024)"},{"id":"http://arxiv.org/abs/2410.22740v1","updated":"2024-10-30T06:51:51Z","published":"2024-10-30T06:51:51Z","title":"Toward Designing Accessible and Meaningful Software for Cancer Survivors","summary":"  Cancer survivors experience a wide range of impairments arising from cancer\nor its treatment, such as chemo brain, visual impairments, and physical\nimpairments. These impairments degrade their quality of life and potentially\nmake software use more challenging for them. However, there has been limited\nresearch on designing accessible software for cancer survivors. To bridge this\nresearch gap, we conducted a formative study including a survey (n=46),\nsemi-structured interviews (n=20), and a diary study (n=10) with cancer\nsurvivors. Our results revealed a wide range of impairments experienced by\ncancer survivors, including chemo brain, neuropathy, and visual impairments.\nCancer survivors heavily relied on software for socialization, health purposes,\nand cancer advocacy, but their impairments made software use more challenging\nfor them. Based on the results, we offer a set of accessibility guidelines that\nsoftware designers can utilize when creating applications for cancer survivors.\nFurther, we suggest design features for inclusion, such as health resources,\nsocialization tools, and games, tailored to the needs of cancer survivors. This\nresearch aims to spotlight cancer survivors' software accessibility challenges\nand software needs and invite more research in this important yet\nunder-investigated domain.\n","authors":["Kyrie Zhixuan Zhou","Royta Iftakher","Sean P. Mullen","Rachel F. Adler","Devorah Kletenik"],"pdf_url":"https://arxiv.org/pdf/2410.22740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01484v2","updated":"2024-10-30T03:56:34Z","published":"2024-05-02T17:15:30Z","title":"Designing Algorithmic Recommendations to Achieve Human-AI\n  Complementarity","summary":"  Algorithms frequently assist, rather than replace, human decision-makers.\nHowever, the design and analysis of algorithms often focus on predicting\noutcomes and do not explicitly model their effect on human decisions. This\ndiscrepancy between the design and role of algorithmic assistants becomes\nparticularly concerning in light of empirical evidence that suggests that\nalgorithmic assistants again and again fail to improve human decisions. In this\narticle, we formalize the design of recommendation algorithms that assist human\ndecision-makers without making restrictive ex-ante assumptions about how\nrecommendations affect decisions. We formulate an algorithmic-design problem\nthat leverages the potential-outcomes framework from causal inference to model\nthe effect of recommendations on a human decision-maker's binary treatment\nchoice. Within this model, we introduce a monotonicity assumption that leads to\nan intuitive classification of human responses to the algorithm. Under this\nassumption, we can express the human's response to algorithmic recommendations\nin terms of their compliance with the algorithm and the active decision they\nwould take if the algorithm sends no recommendation. We showcase the utility of\nour framework using an online experiment that simulates a hiring task. We argue\nthat our approach can make sense of the relative performance of different\nrecommendation algorithms in the experiment and can help design solutions that\nrealize human-AI complementarity. Finally, we leverage our approach to derive\nminimax optimal recommendation algorithms that can be implemented with machine\nlearning using limited training data.\n","authors":["Bryce McLaughlin","Jann Spiess"],"pdf_url":"https://arxiv.org/pdf/2405.01484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07963v2","updated":"2024-10-30T02:13:09Z","published":"2024-05-13T17:44:05Z","title":"PyZoBot: A Platform for Conversational Information Extraction and\n  Synthesis from Curated Zotero Reference Libraries through Advanced\n  Retrieval-Augmented Generation","summary":"  The exponential growth of scientific literature has resulted in information\noverload, challenging researchers to effectively synthesize relevant\npublications. This paper explores the integration of traditional reference\nmanagement software with advanced computational techniques, including Large\nLanguage Models and Retrieval-Augmented Generation. We introduce PyZoBot, an\nAI-driven platform developed in Python, incorporating Zoteros reference\nmanagement with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge\nextraction and synthesis from extensive human-curated scientific literature\ndatabases. It demonstrates proficiency in handling complex natural language\nqueries, integrating data from multiple sources, and meticulously presenting\nreferences to uphold research integrity and facilitate further exploration. By\nleveraging LLMs, RAG, and human expertise through a curated library, PyZoBot\noffers an effective solution to manage information overload and keep pace with\nrapid scientific advancements. The development of such AI-enhanced tools\npromises significant improvements in research efficiency and effectiveness\nacross various disciplines.\n","authors":["Suad Alshammari","Lama Basalelah","Walaa Abu Rukbah","Ali Alsuhibani","Dayanjan S. Wijesinghe"],"pdf_url":"https://arxiv.org/pdf/2405.07963v2.pdf","comment":"10 pages, 2 figures. The code is provided in github and the link to\n  the repository is provided at the end of the publication"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.23483v1","updated":"2024-10-30T22:23:16Z","published":"2024-10-30T22:23:16Z","title":"Keep on Swimming: Real Attackers Only Need Partial Knowledge of a\n  Multi-Model System","summary":"  Recent approaches in machine learning often solve a task using a composition\nof multiple models or agentic architectures. When targeting a composed system\nwith adversarial attacks, it might not be computationally or informationally\nfeasible to train an end-to-end proxy model or a proxy model for every\ncomponent of the system. We introduce a method to craft an adversarial attack\nagainst the overall multi-model system when we only have a proxy model for the\nfinal black-box model, and when the transformation applied by the initial\nmodels can make the adversarial perturbations ineffective. Current methods\nhandle this by applying many copies of the first model/transformation to an\ninput and then re-use a standard adversarial attack by averaging gradients, or\nlearning a proxy model for both stages. To our knowledge, this is the first\nattack specifically designed for this threat model and our method has a\nsubstantially higher attack success rate (80% vs 25%) and contains 9.4% smaller\nperturbations (MSE) compared to prior state-of-the-art methods. Our experiments\nfocus on a supervised image pipeline, but we are confident the attack will\ngeneralize to other multi-model settings [e.g. a mix of open/closed source\nfoundation models], or agentic systems\n","authors":["Julian Collado","Kevin Stangl"],"pdf_url":"https://arxiv.org/pdf/2410.23483v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.23396v1","updated":"2024-10-30T18:59:02Z","published":"2024-10-30T18:59:02Z","title":"Adaptive Network Intervention for Complex Systems: A Hierarchical Graph\n  Reinforcement Learning Approach","summary":"  Effective governance and steering of behavior in complex multi-agent systems\n(MAS) are essential for managing system-wide outcomes, particularly in\nenvironments where interactions are structured by dynamic networks. In many\napplications, the goal is to promote pro-social behavior among agents, where\nnetwork structure plays a pivotal role in shaping these interactions. This\npaper introduces a Hierarchical Graph Reinforcement Learning (HGRL) framework\nthat governs such systems through targeted interventions in the network\nstructure. Operating within the constraints of limited managerial authority,\nthe HGRL framework demonstrates superior performance across a range of\nenvironmental conditions, outperforming established baseline methods. Our\nfindings highlight the critical influence of agent-to-agent learning (social\nlearning) on system behavior: under low social learning, the HGRL manager\npreserves cooperation, forming robust core-periphery networks dominated by\ncooperators. In contrast, high social learning accelerates defection, leading\nto sparser, chain-like networks. Additionally, the study underscores the\nimportance of the system manager's authority level in preventing system-wide\nfailures, such as agent rebellion or collapse, positioning HGRL as a powerful\ntool for dynamic network-based governance.\n","authors":["Qiliang Chen","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2410.23396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23393v1","updated":"2024-10-30T18:57:02Z","published":"2024-10-30T18:57:02Z","title":"Resource Governance in Networked Systems via Integrated Variational\n  Autoencoders and Reinforcement Learning","summary":"  We introduce a framework that integrates variational autoencoders (VAE) with\nreinforcement learning (RL) to balance system performance and resource usage in\nmulti-agent systems by dynamically adjusting network structures over time. A\nkey innovation of this method is its capability to handle the vast action space\nof the network structure. This is achieved by combining Variational\nAuto-Encoder and Deep Reinforcement Learning to control the latent space\nencoded from the network structures. The proposed method, evaluated on the\nmodified OpenAI particle environment under various scenarios, not only\ndemonstrates superior performance compared to baselines but also reveals\ninteresting strategies and insights through the learned behaviors.\n","authors":["Qiliang Chen","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2410.23393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23379v1","updated":"2024-10-30T18:34:32Z","published":"2024-10-30T18:34:32Z","title":"Relational Weight Optimization for Enhancing Team Performance in\n  Multi-Agent Multi-Armed Bandits","summary":"  We introduce an approach to improve team performance in a Multi-Agent\nMulti-Armed Bandit (MAMAB) framework using Fastest Mixing Markov Chain (FMMC)\nand Fastest Distributed Linear Averaging (FDLA) optimization algorithms. The\nmulti-agent team is represented using a fixed relational network and simulated\nusing the Coop-UCB2 algorithm. The edge weights of the communication network\ndirectly impact the time taken to reach distributed consensus. Our goal is to\nshrink the timescale on which the convergence of the consensus occurs to\nachieve optimal team performance and maximize reward. Through our experiments,\nwe show that the convergence to team consensus occurs slightly faster in large\nconstrained networks.\n","authors":["Monish Reddy Kotturu","Saniya Vahedian Movahed","Paul Robinette","Kshitij Jerath","Amanda Redlich","Reza Azadeh"],"pdf_url":"https://arxiv.org/pdf/2410.23379v1.pdf","comment":"Accepted for publication in Modeling, Estimation, and Control\n  Conference (MECC) 2024"},{"id":"http://arxiv.org/abs/2305.16590v4","updated":"2024-10-30T17:53:58Z","published":"2023-05-26T02:32:42Z","title":"Seeding with Differentially Private Network Information","summary":"  In public health interventions such as the distribution of preexposure\nprophylaxis (PrEP) for HIV prevention, decision makers rely on seeding\nalgorithms to identify key individuals who can amplify the impact of their\ninterventions. In such cases, building a complete sexual activity network is\noften infeasible due to privacy concerns. Instead, contact tracing can provide\ninfluence samples, that is, sequences of sexual contacts without requiring\ncomplete network information. This presents two challenges: protecting\nindividual privacy in contact data and adapting seeding algorithms to work\neffectively with incomplete network information. To solve these two problems,\nwe study privacy guarantees for influence maximization algorithms when the\nsocial network is unknown and the inputs are samples of prior influence\ncascades that are collected at random and need privacy protection. Building on\nrecent results that address seeding with costly network information, our\nprivacy-preserving algorithms introduce randomization in the collected data or\nthe algorithm output and can bound the privacy loss of each node (or group of\nnodes) in deciding to include their data in the algorithm input. We provide\ntheoretical guarantees of seeding performance with a limited sample size\nsubject to differential privacy budgets in both central and local privacy\nregimes. Simulations on synthetic random graphs and empirically grounded sexual\ncontacts of men who have sex with men reveal the diminishing value of network\ninformation with decreasing privacy budget in both regimes and graceful\ndecrease in performance with decreasing privacy budget in the central regime.\nAchieving good performance with local privacy guarantees requires relatively\nhigher privacy budgets that confirm our theoretical expectations.\n","authors":["M. Amin Rahimian","Fang-Yi Yu","Yuxin Liu","Carlos Hurtado"],"pdf_url":"https://arxiv.org/pdf/2305.16590v4.pdf","comment":"Preliminary version in AAMAS 2023:\n  https://dl.acm.org/doi/10.5555/3545946.3599081 -- Code and data:\n  https://github.com/aminrahimian/dp-inf-max"},{"id":"http://arxiv.org/abs/2410.07869v2","updated":"2024-10-30T14:49:49Z","published":"2024-10-10T12:41:19Z","title":"Benchmarking Agentic Workflow Generation","summary":"  Large Language Models (LLMs), with their exceptional ability to handle a wide\nrange of tasks, have driven significant advancements in tackling reasoning and\nplanning tasks, wherein decomposing complex problems into executable workflows\nis a crucial step in this process. Existing workflow evaluation frameworks\neither focus solely on holistic performance or suffer from limitations such as\nrestricted scenario coverage, simplistic workflow structures, and lax\nevaluation standards. To this end, we introduce WorFBench, a unified workflow\ngeneration benchmark with multi-faceted scenarios and intricate graph workflow\nstructures. Additionally, we present WorFEval, a systemic evaluation protocol\nutilizing subsequence and subgraph matching algorithms to accurately quantify\nthe LLM agent's workflow generation capabilities. Through comprehensive\nevaluations across different types of LLMs, we discover distinct gaps between\nthe sequence planning capabilities and graph planning capabilities of LLM\nagents, with even GPT-4 exhibiting a gap of around 15%. We also train two\nopen-source models and evaluate their generalization abilities on held-out\ntasks. Furthermore, we observe that the generated workflows can enhance\ndownstream tasks, enabling them to achieve superior performance with less time\nduring inference. Code and dataset are available at\nhttps://github.com/zjunlp/WorFBench.\n","authors":["Shuofei Qiao","Runnan Fang","Zhisong Qiu","Xiaobin Wang","Ningyu Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07869v2.pdf","comment":"Work in progress (v2), update OpenAI o1 and Claude-3.5 results on\n  WorFBench"},{"id":"http://arxiv.org/abs/2304.00977v2","updated":"2024-10-30T12:19:51Z","published":"2023-03-31T07:58:52Z","title":"Selective Reincarnation: Offline-to-Online Multi-Agent Reinforcement\n  Learning","summary":"  'Reincarnation' in reinforcement learning has been proposed as a\nformalisation of reusing prior computation from past experiments when training\nan agent in an environment. In this paper, we present a brief foray into the\nparadigm of reincarnation in the multi-agent (MA) context. We consider the case\nwhere only some agents are reincarnated, whereas the others are trained from\nscratch -- selective reincarnation. In the fully-cooperative MA setting with\nheterogeneous agents, we demonstrate that selective reincarnation can lead to\nhigher returns than training fully from scratch, and faster convergence than\ntraining with full reincarnation. However, the choice of which agents to\nreincarnate in a heterogeneous system is vitally important to the outcome of\nthe training -- in fact, a poor choice can lead to considerably worse results\nthan the alternatives. We argue that a rich field of work exists here, and we\nhope that our effort catalyses further energy in bringing the topic of\nreincarnation to the multi-agent realm.\n","authors":["Claude Formanek","Callum Rhys Tilbury","Jonathan Shock","Kale-ab Tessera","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2304.00977v2.pdf","comment":"Accepted as oral presentation at Reincarnating Reinforcement Learning\n  workshop at ICLR 2023"},{"id":"http://arxiv.org/abs/2410.22820v1","updated":"2024-10-30T08:57:16Z","published":"2024-10-30T08:57:16Z","title":"An invariance principle based concentration result for large-scale\n  stochastic pairwise interaction network systems","summary":"  We study stochastic pairwise interaction network systems whereby a finite\npopulation of agents, identified with the nodes of a graph, update their states\nin response to both individual mutations and pairwise interactions with their\nneighbors. The considered class of systems include the main epidemic models\n-such as the SIS, SIR, and SIRS models-, certain social dynamics models -such\nas the voter and anti-voter models-, as well as evolutionary dynamics on\ngraphs. Since these stochastic systems fall into the class of finite-state\nMarkov chains, they always admit stationary distributions. We analyze the\nasymptotic behavior of these stationary distributions in the limit as the\npopulation size grows large while the interaction network maintains certain\nmixing properties. Our approach relies on the use of Lyapunov-type functions to\nobtain concentration results on these stationary distributions. Notably, our\nresults are not limited to fully mixed population models, as they do apply to a\nmuch broader spectrum of interaction network structures, including, e.g.,\nErd\\\"oos-R\\'enyi random graphs.\n","authors":["Giacomo Como","Fabio Fagnani","Sandro Zampieri"],"pdf_url":"https://arxiv.org/pdf/2410.22820v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.22662v1","updated":"2024-10-30T03:20:01Z","published":"2024-10-30T03:20:01Z","title":"$\\textbf{EMOS}$: $\\textbf{E}$mbodiment-aware Heterogeneous\n  $\\textbf{M}$ulti-robot $\\textbf{O}$perating $\\textbf{S}$ystem with LLM Agents","summary":"  Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach\nfor tackling complex tasks that single robots cannot manage alone. Current\nlarge-language-model-based multi-agent systems (LLM-based MAS) have shown\nsuccess in areas like software development and operating systems, but applying\nthese systems to robot control presents unique challenges. In particular, the\ncapabilities of each agent in a multi-robot system are inherently tied to the\nphysical composition of the robots, rather than predefined roles. To address\nthis issue, we introduce a novel multi-agent framework designed to enable\neffective collaboration among heterogeneous robots with varying embodiments and\ncapabilities, along with a new benchmark named Habitat-MAS. One of our key\ndesigns is $\\textit{Robot Resume}$: Instead of adopting human-designed role\nplay, we propose a self-prompted approach, where agents comprehend robot URDF\nfiles and call robot kinematics tools to generate descriptions of their physics\ncapabilities to guide their behavior in task planning and action execution. The\nHabitat-MAS benchmark is designed to assess how a multi-agent framework handles\ntasks that require embodiment-aware reasoning, which includes 1) manipulation,\n2) perception, 3) navigation, and 4) comprehensive multi-floor object\nrearrangement. The experimental results indicate that the robot's resume and\nthe hierarchical design of our multi-agent system are essential for the\neffective operation of the heterogeneous multi-robot system within this\nintricate problem context.\n","authors":["Junting Chen","Checheng Yu","Xunzhe Zhou","Tianqi Xu","Yao Mu","Mengkang Hu","Wenqi Shao","Yikai Wang","Guohao Li","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2410.22662v1.pdf","comment":"10 pages of main content, 3 pages of references, 5 pages of appendix,\n  7 figures in total"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.23479v1","updated":"2024-10-30T22:06:14Z","published":"2024-10-30T22:06:14Z","title":"The Trail Making Test in Virtual Reality (TMT-VR): The Effects of\n  Interaction Modes and Gaming Skills on Cognitive Performance of Young Adults","summary":"  Virtual Reality (VR) is increasingly used in neuropsychological assessments\ndue to its ability to simulate real-world environments. This study aimed to\ndevelop and evaluate the Trail Making Test in VR (TMT-VR) and investigate the\neffects of different interaction modes and gaming skills on cognitive\nperformance. A total of 71 young female and male adults (aged 18-35) with high\nand low gaming skills participated in this study. Participants completed the\nTMT-VR using three interaction modes as follows: eye tracking, head movement,\nand controller. Performance metrics included task completion time and accuracy.\nUser experience, usability, and acceptability of TMT-VR were also examined.\nResults showed that both eye tracking and head movement modes significantly\noutperformed the controller in terms of task completion time and accuracy. No\nsignificant differences were found between eye tracking and head movement\nmodes. Gaming skills did not significantly influence task performance using any\ninteraction mode. The TMT-VR demonstrates high usability, acceptability, and\nuser experience among participants. The findings suggest that VR-based\nassessments can effectively measure cognitive performance without being\ninfluenced by prior gaming skills, indicating potential applicability for\ndiverse populations.\n","authors":["Evgenia Giatzoglou","Panagiotis Vorias","Ryan Kemm","Irene Karayianni","Chrysanthi Nega","Panagiotis Kourtesis"],"pdf_url":"https://arxiv.org/pdf/2410.23479v1.pdf","comment":"25 Pages, 7 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2406.14515v3","updated":"2024-10-30T13:38:10Z","published":"2024-06-20T17:26:01Z","title":"MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding","summary":"  The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.\n","authors":["Xinyu Fang","Kangrui Mao","Haodong Duan","Xiangyu Zhao","Yining Li","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.14515v3.pdf","comment":"Accepted in NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.23325v1","updated":"2024-10-30T13:17:13Z","published":"2024-10-30T13:17:13Z","title":"Transfer Learning in Vocal Education: Technical Evaluation of Limited\n  Samples Describing Mezzo-soprano","summary":"  Vocal education in the music field is difficult to quantify due to the\nindividual differences in singers' voices and the different quantitative\ncriteria of singing techniques. Deep learning has great potential to be applied\nin music education due to its efficiency to handle complex data and perform\nquantitative analysis. However, accurate evaluations with limited samples over\nrare vocal types, such as Mezzo-soprano, requires extensive well-annotated data\nsupport using deep learning models. In order to attain the objective, we\nperform transfer learning by employing deep learning models pre-trained on the\nImageNet and Urbansound8k datasets for the improvement on the precision of\nvocal technique evaluation. Furthermore, we tackle the problem of the lack of\nsamples by constructing a dedicated dataset, the Mezzo-soprano Vocal Set (MVS),\nfor vocal technique assessment. Our experimental results indicate that transfer\nlearning increases the overall accuracy (OAcc) of all models by an average of\n8.3%, with the highest accuracy at 94.2%. We not only provide a novel approach\nto evaluating Mezzo-soprano vocal techniques but also introduce a new\nquantitative assessment method for music education.\n","authors":["Zhenyi Hou","Xu Zhao","Kejie Ye","Xinyu Sheng","Shanggerile Jiang","Jiajing Xia","Yitao Zhang","Chenxi Ban","Daijun Luo","Jiaxing Chen","Yan Zou","Yuchao Feng","Guangyu Fan","Xin Yuan"],"pdf_url":"https://arxiv.org/pdf/2410.23325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22803v1","updated":"2024-10-30T08:31:58Z","published":"2024-10-30T08:31:58Z","title":"DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event\n  Localization and Detection","summary":"  This paper describes sound event localization and detection (SELD) for\nspatial audio recordings captured by firstorder ambisonics (FOA) microphones.\nIn this task, one may train a deep neural network (DNN) using FOA data\nannotated with the classes and directions of arrival (DOAs) of sound events.\nHowever, the performance of this approach is severely bounded by the amount of\nannotated data. To overcome this limitation, we propose a novel method of\npretraining the feature extraction part of the DNN in a self-supervised manner.\nWe use spatial audio-visual recordings abundantly available as virtual reality\ncontents. Assuming that sound objects are concurrently observed by the FOA\nmicrophones and the omni-directional camera, we jointly train audio and visual\nencoders with contrastive learning such that the audio and visual embeddings of\nthe same recording and DOA are made close. A key feature of our method is that\nthe DOA-wise audio embeddings are jointly extracted from the raw audio data,\nwhile the DOA-wise visual embeddings are separately extracted from the local\nvisual crops centered on the corresponding DOA. This encourages the latent\nfeatures of the audio encoder to represent both the classes and DOAs of sound\nevents. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows\nnon-annotated audio-visual recordings of 100 hours reduced the error score of\nSELD from 36.4 pts to 34.9 pts.\n","authors":["Yoto Fujita","Yoshiaki Bando","Keisuke Imoto","Masaki Onishi","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2410.22803v1.pdf","comment":"Accepted to APSIPA2023"},{"id":"http://arxiv.org/abs/2410.22023v2","updated":"2024-10-30T04:29:42Z","published":"2024-10-29T13:13:30Z","title":"Feature distribution Adaptation Network for Speech Emotion Recognition","summary":"  In this paper, we propose a novel deep inductive transfer learning framework,\nnamed feature distribution adaptation network, to tackle the challenging\nmulti-modal speech emotion recognition problem. Our method aims to use deep\ntransfer learning strategies to align visual and audio feature distributions to\nobtain consistent representation of emotion, thereby improving the performance\nof speech emotion recognition. In our model, the pre-trained ResNet-34 is\nutilized for feature extraction for facial expression images and acoustic Mel\nspectrograms, respectively. Then, the cross-attention mechanism is introduced\nto model the intrinsic similarity relationships of multi-modal features.\nFinally, the multi-modal feature distribution adaptation is performed\nefficiently with feed-forward network, which is extended using the local\nmaximum mean discrepancy loss. Experiments are carried out on two benchmark\ndatasets, and the results demonstrate that our model can achieve excellent\nperformance compared with existing ones.\n","authors":["Shaokai Li","Yixuan Ji","Peng Song","Haoqin Sun","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.22023v2.pdf","comment":null}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2410.22963v1","updated":"2024-10-30T12:22:16Z","published":"2024-10-30T12:22:16Z","title":"Even the \"Devil\" has Rights!","summary":"  There have been works discussing the adoption of a human rights framework for\nresponsible AI, emphasizing various rights such as the right to contribute to\nscientific advancements. Yet, to the best of our knowledge, this is the first\nattempt to take this framework with special focus on computer vision and\ndocumenting human rights violations in its community. This work summarizes such\nincidents accompanied with evidence from the lens of a female African Muslim\nHijabi researcher. While previous works resorted to qualitative surveys that\ngather opinions from various researchers in the field, this work argues that a\nsingle documented violation is sufficient to warrant attention regardless of\nthe stature of this researcher. Incidents documented in this work include\nsilence on Genocides that are occurring while promoting the governments\ncontributing to it, a broken reviewing system and corruption in the faculty\nsupport systems. This work discusses that demonizing individuals for\ndiscrimination based on gender, ethnicity, creed or reprisal has been a\nsuccessful tool for exclusion with documented evidence from a single case. We\nargue that human rights are guaranteed for every single individual even the\nones that might be labelled as devils in the community for whichever reasons to\ndismantle such a tool from its roots.\n","authors":["Mennatullah Siam"],"pdf_url":"https://arxiv.org/pdf/2410.22963v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2410.23516v1","updated":"2024-10-30T23:57:49Z","published":"2024-10-30T23:57:49Z","title":"NUSense: Robust Soft Optical Tactile Sensor","summary":"  While most tactile sensors rely on measuring pressure, insights from\ncontinuum mechanics suggest that measuring shear strain provides critical\ninformation for tactile sensing. In this work, we introduce an optical tactile\nsensing principle based on shear strain detection. A silicone rubber layer,\ndyed with color inks, is used to quantify the shear magnitude of the sensing\nlayer. This principle was validated using the NUSense camera-based tactile\nsensor. The wide-angle camera captures the elongation of the soft pad under\nmechanical load, a phenomenon attributed to the Poisson effect. The physical\nand optical properties of the inked pad are essential and should ideally remain\nstable over time. We tested the robustness of the sensor by subjecting the\noutermost layer to multiple load cycles using a robot arm. Additionally, we\ndiscussed potential applications of this sensor in force sensing and contact\nlocalization.\n","authors":["Madina Yergibay","Tleukhan Mussin","Saltanat Seitzhan","Daryn Kenzhebek","Zhanat Kappassov","Harold Soh","Tasbolat Taunyazov"],"pdf_url":"https://arxiv.org/pdf/2410.23516v1.pdf","comment":"Madina Yergibay and Tleukhan Mussin contributed equally. 6 pages, 6\n  figures"},{"id":"http://arxiv.org/abs/2309.04459v2","updated":"2024-10-30T23:45:17Z","published":"2023-09-08T17:37:05Z","title":"Subwords as Skills: Tokenization for Sparse-Reward Reinforcement\n  Learning","summary":"  Exploration in sparse-reward reinforcement learning is difficult due to the\nrequirement of long, coordinated sequences of actions in order to achieve any\nreward. Moreover, in continuous action spaces there are an infinite number of\npossible actions, which only increases the difficulty of exploration. One class\nof methods designed to address these issues forms temporally extended actions,\noften called skills, from interaction data collected in the same domain, and\noptimizes a policy on top of this new action space. Typically such methods\nrequire a lengthy pretraining phase, especially in continuous action spaces, in\norder to form the skills before reinforcement learning can begin. Given prior\nevidence that the full range of the continuous action space is not required in\nsuch tasks, we propose a novel approach to skill-generation with two\ncomponents. First we discretize the action space through clustering, and second\nwe leverage a tokenization technique borrowed from natural language processing\nto generate temporally extended actions. Such a method outperforms baselines\nfor skill-generation in several challenging sparse-reward domains, and requires\norders-of-magnitude less computation in skill-generation and online rollouts.\nOur code is available at \\url{https://github.com/dyunis/subwords_as_skills}.\n","authors":["David Yunis","Justin Jung","Falcon Dai","Matthew Walter"],"pdf_url":"https://arxiv.org/pdf/2309.04459v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23488v1","updated":"2024-10-30T22:43:47Z","published":"2024-10-30T22:43:47Z","title":"PACER: Preference-conditioned All-terrain Costmap Generation","summary":"  In autonomous robot navigation, terrain cost assignment is typically\nperformed using a semantics-based paradigm in which terrain is first labeled\nusing a pre-trained semantic classifier and costs are then assigned according\nto a user-defined mapping between label and cost. While this approach is\nrapidly adaptable to changing user preferences, only preferences over the types\nof terrain that are already known by the semantic classifier can be expressed.\nIn this paper, we hypothesize that a machine-learning-based alternative to the\nsemantics-based paradigm above will allow for rapid cost assignment adaptation\nto preferences expressed over new terrains at deployment time without the need\nfor additional training. To investigate this hypothesis, we introduce and study\nPACER, a novel approach to costmap generation that accepts as input a single\nbirds-eye view (BEV) image of the surrounding area along with a user-specified\npreference context and generates a corresponding BEV costmap that aligns with\nthe preference context. Using both real and synthetic data along with a\ncombination of proposed training tasks, we find that PACER is able to adapt\nquickly to new user preferences while also exhibiting better generalization to\nnovel terrains compared to both semantics-based and representation-learning\napproaches.\n","authors":["Luisa Mao","Garrett Warnell","Peter Stone","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2410.23488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23464v1","updated":"2024-10-30T21:09:58Z","published":"2024-10-30T21:09:58Z","title":"Design and Motion Analysis of a Reconfigurable Pendulum-Based Rolling\n  Disk Robot with Magnetic Coupling","summary":"  Reconfigurable robots are at the forefront of robotics innovation due to\ntheir unmatched versatility and adaptability in addressing various tasks\nthrough collaborative operations. This paper explores the design and\nimplementation of a novel pendulum-based magnetic coupling system within a\nreconfigurable disk robot. Diverging from traditional designs, this system\nemphasizes enhancing coupling strength while maintaining the compactness of the\nouter shell. We employ parametric optimization techniques, including magnetic\narray simulations, to improve coupling performance. Additionally, we conduct a\ncomprehensive analysis of the rolling robot's motion to assess its operational\neffectiveness in the coupling mechanism. This examination reveals intriguing\nnew motion patterns driven by frictional and sliding effects between the\nrolling disk modules and the ground. Furthermore, the new setup introduces a\nnovel problem in the area of nonprehensile manipulation.\n","authors":["Ollie Wiltshire","Seyed Amir Tafrishi"],"pdf_url":"https://arxiv.org/pdf/2410.23464v1.pdf","comment":"Accepted to TAROS 2024"},{"id":"http://arxiv.org/abs/2410.23450v1","updated":"2024-10-30T20:46:26Z","published":"2024-10-30T20:46:26Z","title":"Return Augmented Decision Transformer for Off-Dynamics Reinforcement\n  Learning","summary":"  We study offline off-dynamics reinforcement learning (RL) to utilize data\nfrom an easily accessible source domain to enhance policy learning in a target\ndomain with limited data. Our approach centers on return-conditioned supervised\nlearning (RCSL), particularly focusing on the decision transformer (DT), which\ncan predict actions conditioned on desired return guidance and complete\ntrajectory history. Previous works tackle the dynamics shift problem by\naugmenting the reward in the trajectory from the source domain to match the\noptimal trajectory in the target domain. However, this strategy can not be\ndirectly applicable in RCSL owing to (1) the unique form of the RCSL policy\nclass, which explicitly depends on the return, and (2) the absence of a\nstraightforward representation of the optimal trajectory distribution. We\npropose the Return Augmented Decision Transformer (RADT) method, where we\naugment the return in the source domain by aligning its distribution with that\nin the target domain. We provide the theoretical analysis demonstrating that\nthe RCSL policy learned from RADT achieves the same level of suboptimality as\nwould be obtained without a dynamics shift. We introduce two practical\nimplementations RADT-DARA and RADT-MV respectively. Extensive experiments\nconducted on D4RL datasets reveal that our methods generally outperform dynamic\nprogramming based methods in off-dynamics RL scenarios.\n","authors":["Ruhan Wang","Yu Yang","Zhishuai Liu","Dongruo Zhou","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2410.23450v1.pdf","comment":"26 pages, 10 tables, 10 figures"},{"id":"http://arxiv.org/abs/2406.18043v2","updated":"2024-10-30T20:16:18Z","published":"2024-06-26T03:41:48Z","title":"GenRL: Multimodal-foundation world models for generalization in embodied\n  agents","summary":"  Learning generalist embodied agents, able to solve multitudes of tasks in\ndifferent domains is a long-standing problem. Reinforcement learning (RL) is\nhard to scale up as it requires a complex reward design for each task. In\ncontrast, language can specify tasks in a more natural way. Current foundation\nvision-language models (VLMs) generally require fine-tuning or other\nadaptations to be adopted in embodied contexts, due to the significant domain\ngap. However, the lack of multimodal data in such domains represents an\nobstacle to developing foundation models for embodied applications. In this\nwork, we overcome these problems by presenting multimodal-foundation world\nmodels, able to connect and align the representation of foundation VLMs with\nthe latent space of generative world models for RL, without any language\nannotations. The resulting agent learning framework, GenRL, allows one to\nspecify tasks through vision and/or language prompts, ground them in the\nembodied domain's dynamics, and learn the corresponding behaviors in\nimagination. As assessed through large-scale multi-task benchmarking in\nlocomotion and manipulation domains, GenRL enables multi-task generalization\nfrom language and visual prompts. Furthermore, by introducing a data-free\npolicy learning strategy, our approach lays the groundwork for foundational\npolicy learning using generative world models. Website, code and data:\nhttps://mazpie.github.io/genrl/\n","authors":["Pietro Mazzaglia","Tim Verbelen","Bart Dhoedt","Aaron Courville","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2406.18043v2.pdf","comment":"Presented at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.23428v1","updated":"2024-10-30T20:13:40Z","published":"2024-10-30T20:13:40Z","title":"Learning for Deformable Linear Object Insertion Leveraging Flexibility\n  Estimation from Visual Cues","summary":"  Manipulation of deformable Linear objects (DLOs), including iron wire,\nrubber, silk, and nylon rope, is ubiquitous in daily life. These objects\nexhibit diverse physical properties, such as Young$'$s modulus and bending\nstiffness.Such diversity poses challenges for developing generalized\nmanipulation policies. However, previous research limited their scope to\nsingle-material DLOs and engaged in time-consuming data collection for the\nstate estimation. In this paper, we propose a two-stage manipulation approach\nconsisting of a material property (e.g., flexibility) estimation and policy\nlearning for DLO insertion with reinforcement learning. Firstly, we design a\nflexibility estimation scheme that characterizes the properties of different\ntypes of DLOs. The ground truth flexibility data is collected in simulation to\ntrain our flexibility estimation module. During the manipulation, the robot\ninteracts with the DLOs to estimate flexibility by analyzing their visual\nconfigurations. Secondly, we train a policy conditioned on the estimated\nflexibility to perform challenging DLO insertion tasks. Our pipeline trained\nwith diverse insertion scenarios achieves an 85.6% success rate in simulation\nand 66.67% in real robot experiments. Please refer to our project page:\nhttps://lmeee.github.io/DLOInsert/\n","authors":["Mingen Li","Changhyun Choi"],"pdf_url":"https://arxiv.org/pdf/2410.23428v1.pdf","comment":"7 pages, 9 figures, 3 tables. 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2410.04680v2","updated":"2024-10-30T20:04:12Z","published":"2024-10-07T01:24:39Z","title":"Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian\n  Splatting","summary":"  We propose a framework for active next best view and touch selection for\nrobotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a\nuseful explicit 3D scene representation for robotics, as it has the ability to\nrepresent scenes in a both photorealistic and geometrically accurate manner.\nHowever, in real-world, online robotic scenes where the number of views is\nlimited given efficiency requirements, random view selection for 3DGS becomes\nimpractical as views are often overlapping and redundant. We address this issue\nby proposing an end-to-end online training and active view selection pipeline,\nwhich enhances the performance of 3DGS in few-view robotics settings. We first\nelevate the performance of few-shot 3DGS with a novel semantic depth alignment\nmethod using Segment Anything Model 2 (SAM2) that we supplement with Pearson\ndepth and surface normal loss to improve color and depth reconstruction of\nreal-world scenes. We then extend FisherRF, a next-best-view selection method\nfor 3DGS, to select views and touch poses based on depth uncertainty. We\nperform online view selection on a real robot system during live 3DGS training.\nWe motivate our improvements to few-shot GS scenes, and extend depth-based\nFisherRF to them, where we demonstrate both qualitative and quantitative\nimprovements on challenging robot scenes. For more information, please see our\nproject page at https://arm.stanford.edu/next-best-sense.\n","authors":["Matthew Strong","Boshu Lei","Aiden Swann","Wen Jiang","Kostas Daniilidis","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2410.04680v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04856v2","updated":"2024-10-30T19:53:16Z","published":"2023-12-08T06:24:32Z","title":"SCALER: Versatile Multi-Limbed Robot for Free-Climbing in Extreme\n  Terrains","summary":"  This paper presents SCALER, a versatile free-climbing multi-limbed robot that\nis designed to achieve tightly coupled simultaneous locomotion and dexterous\ngrasping. Although existing quadruped-limbed robots have shown impressive\ndexterous skills such as object manipulation, it is essential to balance\npower-intensive locomotion and dexterous grasping capabilities. We design a\ntorso linkage and a parallel-serial limb to meet such conflicting skills that\npose unique challenges in the hardware designs. SCALER employs underactuated\ntwo-fingered GOAT grippers that can mechanically adapt and offer 7 modes of\ngrasping, enabling SCALER to traverse extreme terrains with multi-modal\ngrasping strategies. We study the whole-body approach, where SCALER uses its\nbody and limbs to generate additional forces for stable grasping with\nenvironments, further enhancing versatility. Furthermore, we improve the GOAT\ngripper actuation speed to realize more dynamic climbing in a closed-loop\ncontrol fashion. With these proposed technologies, SCALER can traverse\nvertical, overhang, upside-down, slippery terrains, and bouldering walls with\nnon-convex-shaped climbing holds under the Earth's gravity.\n","authors":["Yusuke Tanaka","Yuki Shirai","Alexander Schperberg","Xuan Lin","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2312.04856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12192v2","updated":"2024-10-30T18:48:00Z","published":"2024-09-18T17:59:43Z","title":"DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control","summary":"  Imitation learning has proven to be a powerful tool for training complex\nvisuomotor policies. However, current methods often require hundreds to\nthousands of expert demonstrations to handle high-dimensional visual\nobservations. A key reason for this poor data efficiency is that visual\nrepresentations are predominantly either pretrained on out-of-domain data or\ntrained directly through a behavior cloning objective. In this work, we present\nDynaMo, a new in-domain, self-supervised method for learning visual\nrepresentations. Given a set of expert demonstrations, we jointly learn a\nlatent inverse dynamics model and a forward dynamics model over a sequence of\nimage embeddings, predicting the next frame in latent space, without\naugmentations, contrastive sampling, or access to ground truth actions.\nImportantly, DynaMo does not require any out-of-domain data such as Internet\ndatasets or cross-embodied datasets. On a suite of six simulated and real\nenvironments, we show that representations learned with DynaMo significantly\nimprove downstream imitation learning performance over prior self-supervised\nlearning objectives, and pretrained representations. Gains from using DynaMo\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\nmeasure its impact on downstream policy performance. Robot videos are best\nviewed at https://dynamo-ssl.github.io\n","authors":["Zichen Jeff Cui","Hengkai Pan","Aadhithya Iyer","Siddhant Haldar","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2409.12192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23382v1","updated":"2024-10-30T18:38:42Z","published":"2024-10-30T18:38:42Z","title":"Estimating Neural Network Robustness via Lipschitz Constant and\n  Architecture Sensitivity","summary":"  Ensuring neural network robustness is essential for the safe and reliable\noperation of robotic learning systems, especially in perception and\ndecision-making tasks within real-world environments. This paper investigates\nthe robustness of neural networks in perception systems, specifically examining\ntheir sensitivity to targeted, small-scale perturbations. We identify the\nLipschitz constant as a key metric for quantifying and enhancing network\nrobustness. We derive an analytical expression to compute the Lipschitz\nconstant based on neural network architecture, providing a theoretical basis\nfor estimating and improving robustness. Several experiments reveal the\nrelationship between network design, the Lipschitz constant, and robustness,\noffering practical insights for developing safer, more robust robot learning\nsystems.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.23382v1.pdf","comment":"SAFE-ROL at CoRL 2024"},{"id":"http://arxiv.org/abs/2410.23377v1","updated":"2024-10-30T18:27:55Z","published":"2024-10-30T18:27:55Z","title":"A Cost-Effective Thermal Imaging Safety Sensor for Industry 5.0 and\n  Collaborative Robotics","summary":"  The Industry 5.0 paradigm focuses on industrial operator well-being and\nsustainable manufacturing practices, where humans play a central role, not only\nduring the repetitive and collaborative tasks of the manufacturing process, but\nalso in the management of the factory floor assets. Human factors, such as\nergonomics, safety, and well-being, push the human-centric smart factory to\nefficiently adopt novel technologies while minimizing environmental and social\nimpact. As operations at the factory floor increasingly rely on collaborative\nrobots (CoBots) and flexible manufacturing systems, there is a growing demand\nfor redundant safety mechanisms (i.e., automatic human detection in the\nproximity of machinery that is under operation). Fostering enhanced process\nsafety for human proximity detection allows for the protection against possible\nincidents or accidents with the deployed industrial devices and machinery. This\npaper introduces the design and implementation of a cost-effective thermal\nimaging Safety Sensor that can be used in the scope of Industry 5.0 to trigger\ndistinct safe mode states in manufacturing processes that rely on collaborative\nrobotics. The proposed Safety Sensor uses a hybrid detection approach and has\nbeen evaluated under controlled environmental conditions. The obtained results\nshow a 97% accuracy at low computational cost when using the developed hybrid\nmethod to detect the presence of humans in thermal images.\n","authors":["Daniel Barros","Paula Fraga-Lamas","Tiago M. Fernandez-Carames","Sergio Ivan Lopes"],"pdf_url":"https://arxiv.org/pdf/2410.23377v1.pdf","comment":"Paper accepted in Edge-IoT 2022"},{"id":"http://arxiv.org/abs/2410.23289v1","updated":"2024-10-30T17:59:41Z","published":"2024-10-30T17:59:41Z","title":"Bridging the Human to Robot Dexterity Gap through Object-Oriented\n  Rewards","summary":"  Training robots directly from human videos is an emerging area in robotics\nand computer vision. While there has been notable progress with two-fingered\ngrippers, learning autonomous tasks for multi-fingered robot hands in this way\nremains challenging. A key reason for this difficulty is that a policy trained\non human hands may not directly transfer to a robot hand due to morphology\ndifferences. In this work, we present HuDOR, a technique that enables online\nfine-tuning of policies by directly computing rewards from human videos.\nImportantly, this reward function is built using object-oriented trajectories\nderived from off-the-shelf point trackers, providing meaningful learning\nsignals despite the morphology gap and visual differences between human and\nrobot hands. Given a single video of a human solving a task, such as gently\nopening a music box, HuDOR enables our four-fingered Allegro hand to learn the\ntask with just an hour of online interaction. Our experiments across four tasks\nshow that HuDOR achieves a 4x improvement over baselines. Code and videos are\navailable on our website, https://object-rewards.github.io.\n","authors":["Irmak Guzey","Yinlong Dai","Georgy Savva","Raunaq Bhirangi","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.23289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23283v1","updated":"2024-10-30T17:58:26Z","published":"2024-10-30T17:58:26Z","title":"DisCo: Distributed Contact-Rich Trajectory Optimization for Forceful\n  Multi-Robot Collaboration","summary":"  We present DisCo, a distributed algorithm for contact-rich, multi-robot\ntasks. DisCo is a distributed contact-implicit trajectory optimization\nalgorithm, which allows a group of robots to optimize a time sequence of forces\nto objects and to their environment to accomplish tasks such as collaborative\nmanipulation, robot team sports, and modular robot locomotion. We build our\nalgorithm on a variant of the Alternating Direction Method of Multipliers\n(ADMM), where each robot computes its own contact forces and contact-switching\nevents from a smaller single-robot, contact-implicit trajectory optimization\nproblem, while cooperating with other robots through dual variables, enforcing\nconstraints between robots. Each robot iterates between solving its local\nproblem, and communicating over a wireless mesh network to enforce these\nconsistency constraints with its neighbors, ultimately converging to a\ncoordinated plan for the group. The local problems solved by each robot are\nsignificantly less challenging than a centralized problem with all robots'\ncontact forces and switching events, improving the computational efficiency,\nwhile also preserving the privacy of some aspects of each robot's operation. We\ndemonstrate the effectiveness of our algorithm in simulations of collaborative\nmanipulation, multi-robot team sports scenarios, and in modular robot\nlocomotion, where DisCo achieves $3$x higher success rates with a 2.5x to 5x\nfaster computation time. Further, we provide results of hardware experiments on\na modular truss robot, with three collaborating truss nodes planning\nindividually while working together to produce a punctuated rolling-gate motion\nof the composite structure. Videos are available on the project page:\nhttps://disco-opt.github.io.\n","authors":["Ola Shorinwa","Matthew Devlin","Elliot W. Hawkes","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2410.23283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23277v1","updated":"2024-10-30T17:55:52Z","published":"2024-10-30T17:55:52Z","title":"SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video\n  Generation","summary":"  Human beings are endowed with a complementary learning system, which bridges\nthe slow learning of general world dynamics with fast storage of episodic\nmemory from a new experience. Previous video generation models, however,\nprimarily focus on slow learning by pre-training on vast amounts of data,\noverlooking the fast learning phase crucial for episodic memory storage. This\noversight leads to inconsistencies across temporally distant frames when\ngenerating longer videos, as these frames fall beyond the model's context\nwindow. To this end, we introduce SlowFast-VGen, a novel dual-speed learning\nsystem for action-driven long video generation. Our approach incorporates a\nmasked conditional video diffusion model for the slow learning of world\ndynamics, alongside an inference-time fast learning strategy based on a\ntemporal LoRA module. Specifically, the fast learning process updates its\ntemporal LoRA parameters based on local inputs and outputs, thereby efficiently\nstoring episodic memory in its parameters. We further propose a slow-fast\nlearning loop algorithm that seamlessly integrates the inner fast learning loop\ninto the outer slow learning loop, enabling the recall of prior multi-episode\nexperiences for context-aware skill learning. To facilitate the slow learning\nof an approximate world model, we collect a large-scale dataset of 200k videos\nwith language action annotations, covering a wide range of scenarios. Extensive\nexperiments show that SlowFast-VGen outperforms baselines across various\nmetrics for action-driven video generation, achieving an FVD score of 514\ncompared to 782, and maintaining consistency in longer videos, with an average\nof 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm\nsignificantly enhances performances on long-horizon planning tasks as well.\nProject Website: https://slowfast-vgen.github.io\n","authors":["Yining Hong","Beide Liu","Maxine Wu","Yuanhao Zhai","Kai-Wei Chang","Lingjie Li","Kevin Lin","Chung-Ching Lin","Jianfeng Wang","Zhengyuan Yang","Yingnian Wu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23262v1","updated":"2024-10-30T17:46:31Z","published":"2024-10-30T17:46:31Z","title":"EMMA: End-to-End Multimodal Model for Autonomous Driving","summary":"  We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving.\nBuilt on a multi-modal large language model foundation, EMMA directly maps raw\ncamera sensor data into various driving-specific outputs, including planner\ntrajectories, perception objects, and road graph elements. EMMA maximizes the\nutility of world knowledge from the pre-trained large language models, by\nrepresenting all non-sensor inputs (e.g. navigation instructions and ego\nvehicle status) and outputs (e.g. trajectories and 3D locations) as natural\nlanguage text. This approach allows EMMA to jointly process various driving\ntasks in a unified language space, and generate the outputs for each task using\ntask-specific prompts. Empirically, we demonstrate EMMA's effectiveness by\nachieving state-of-the-art performance in motion planning on nuScenes as well\nas competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also\nyields competitive results for camera-primary 3D object detection on the Waymo\nOpen Dataset (WOD). We show that co-training EMMA with planner trajectories,\nobject detection, and road graph tasks yields improvements across all three\ndomains, highlighting EMMA's potential as a generalist model for autonomous\ndriving applications. However, EMMA also exhibits certain limitations: it can\nprocess only a small amount of image frames, does not incorporate accurate 3D\nsensing modalities like LiDAR or radar and is computationally expensive. We\nhope that our results will inspire further research to mitigate these issues\nand to further evolve the state of the art in autonomous driving model\narchitectures.\n","authors":["Jyh-Jing Hwang","Runsheng Xu","Hubert Lin","Wei-Chih Hung","Jingwei Ji","Kristy Choi","Di Huang","Tong He","Paul Covington","Benjamin Sapp","James Guo","Dragomir Anguelov","Mingxing Tan"],"pdf_url":"https://arxiv.org/pdf/2410.23262v1.pdf","comment":"Blog post: https://waymo.com/blog/2024/10/introducing-emma/"},{"id":"http://arxiv.org/abs/2410.23254v1","updated":"2024-10-30T17:37:31Z","published":"2024-10-30T17:37:31Z","title":"Keypoint Abstraction using Large Models for Object-Relative Imitation\n  Learning","summary":"  Generalization to novel object configurations and instances across diverse\ntasks and environments is a critical challenge in robotics. Keypoint-based\nrepresentations have been proven effective as a succinct representation for\ncapturing essential object features, and for establishing a reference frame in\naction prediction, enabling data-efficient learning of robot skills. However,\ntheir manual design nature and reliance on additional human labels limit their\nscalability. In this paper, we propose KALM, a framework that leverages large\npre-trained vision-language models (LMs) to automatically generate\ntask-relevant and cross-instance consistent keypoints. KALM distills robust and\nconsistent keypoints across views and objects by generating proposals using LMs\nand verifies them against a small set of robot demonstration data. Based on the\ngenerated keypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to generalize\neffectively across varying object poses, camera views, and object instances\nwith similar functional shapes. Our method demonstrates strong performance in\nthe real world, adapting to different tasks and environments from only a\nhandful of demonstrations while requiring no additional labels. Website:\nhttps://kalm-il.github.io/\n","authors":["Xiaolin Fang","Bo-Ruei Huang","Jiayuan Mao","Jasmine Shone","Joshua B. Tenenbaum","Tom√°s Lozano-P√©rez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2410.23254v1.pdf","comment":"CoRL LangRob Workshop, 2024"},{"id":"http://arxiv.org/abs/2403.17009v2","updated":"2024-10-30T17:35:06Z","published":"2024-03-25T17:59:58Z","title":"Is Your LiDAR Placement Optimized for 3D Scene Understanding?","summary":"  The reliability of driving perception systems under unprecedented conditions\nis crucial for practical usage. Latest advancements have prompted increasing\ninterest in multi-LiDAR perception. However, prevailing driving datasets\npredominantly utilize single-LiDAR systems and collect data devoid of adverse\nconditions, failing to capture the complexities of real-world environments\naccurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline\nthat encompasses LiDAR placement optimization, data generation, and downstream\nevaluations. Our framework makes three appealing contributions. 1) To identify\nthe most effective configurations for multi-LiDAR systems, we introduce the\nSurrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR\nplacement quality. 2) Leveraging the M-SOG metric, we propose a novel\noptimization strategy to refine multi-LiDAR placements. 3) Centered around the\ntheme of multi-condition multi-LiDAR perception, we collect a 280,000-frame\ndataset from both clean and adverse conditions. Extensive experiments\ndemonstrate that LiDAR placements optimized using our approach outperform\nvarious baselines. We showcase exceptional results in both LiDAR semantic\nsegmentation and 3D object detection tasks, under diverse weather and sensor\nfailure conditions.\n","authors":["Ye Li","Lingdong Kong","Hanjiang Hu","Xiaohao Xu","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17009v2.pdf","comment":"NeurIPS 2024 (Spotlight); 36 pages, 16 figures, 14 tables; Code at\n  https://github.com/ywyeli/Place3D"},{"id":"http://arxiv.org/abs/2410.23234v1","updated":"2024-10-30T17:22:45Z","published":"2024-10-30T17:22:45Z","title":"EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with\n  In-Context Learning","summary":"  This paper introduces a framework, called EMOTION, for generating expressive\nmotion sequences in humanoid robots, enhancing their ability to engage in\nhumanlike non-verbal communication. Non-verbal cues such as facial expressions,\ngestures, and body movements play a crucial role in effective interpersonal\ninteractions. Despite the advancements in robotic behaviors, existing methods\noften fall short in mimicking the diversity and subtlety of human non-verbal\ncommunication. To address this gap, our approach leverages the in-context\nlearning capability of large language models (LLMs) to dynamically generate\nsocially appropriate gesture motion sequences for human-robot interaction. We\nuse this framework to generate 10 different expressive gestures and conduct\nonline user studies comparing the naturalness and understandability of the\nmotions generated by EMOTION and its human-feedback version, EMOTION++, against\nthose by human operators. The results demonstrate that our approach either\nmatches or surpasses human performance in generating understandable and natural\nrobot motions under certain scenarios. We also provide design implications for\nfuture research to consider a set of variables when generating expressive\nrobotic gestures.\n","authors":["Peide Huang","Yuhan Hu","Nataliya Nechyporenko","Daehwa Kim","Walter Talbott","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.23234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23215v1","updated":"2024-10-30T17:03:27Z","published":"2024-10-30T17:03:27Z","title":"Levels of explanation -- implementation and evaluation of what and when\n  for different time-sensitive tasks","summary":"  In this work, we focused on constructing and evaluating levels of\nexplanation(LOE) that address two basic aspect of HRI: 1. What information\nshould be communicated to the user by the robot? 2. When should the robot\ncommunicate this information? For constructing the LOE, we defined two terms,\nverbosity and explanation patterns, each with two levels (verbosity -- high and\nlow, explanation patterns -- dynamic and static). Based on these parameters,\nthree different LOE (high, medium, and low) were constructed and evaluated in a\nuser study with a telepresence robot. The user study was conducted for a\nsimulated telerobotic healthcare task with two different conditions related to\ntime sensitivity, as evaluated by two different user groups -- one that\nperformed the task within a time limit and the other with no time limit. We\nfound that the high LOE was preferred in terms of adequacy of explanation,\nnumber of collisions, number of incorrect movements, and number of\nclarifications when users performed the experiment in the without time limit\ncondition. We also found that both high and medium LOE did not have significant\ndifferences in completion time, the fluency of HRI, and trust in the robot.\nWhen users performed the experiment in the with time limit condition, high and\nmedium LOE had better task performances and were preferred to the low LOE in\nterms of completion time, fluency, adequacy of explanation, trust, number of\ncollisions, number of incorrect movements and number of clarifications. Future\ndirections for advancing LOE are discussed.\n","authors":["Shikhar Kumar","Omer Keidar","Yael Edan"],"pdf_url":"https://arxiv.org/pdf/2410.23215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23156v1","updated":"2024-10-30T16:11:05Z","published":"2024-10-30T16:11:05Z","title":"VisualPredicator: Learning Abstract World Models with Neuro-Symbolic\n  Predicates for Robot Planning","summary":"  Broadly intelligent agents should form task-specific abstractions that\nselectively expose the essential elements of a task, while abstracting away the\ncomplexity of the raw sensorimotor space. In this work, we present\nNeuro-Symbolic Predicates, a first-order abstraction language that combines the\nstrengths of symbolic and neural knowledge representations. We outline an\nonline algorithm for inventing such predicates and learning abstract world\nmodels. We compare our approach to hierarchical reinforcement learning,\nvision-language model planning, and symbolic predicate invention approaches, on\nboth in- and out-of-distribution tasks across five simulated robotic domains.\nResults show that our approach offers better sample complexity, stronger\nout-of-distribution generalization, and improved interpretability.\n","authors":["Yichao Liang","Nishanth Kumar","Hao Tang","Adrian Weller","Joshua B. Tenenbaum","Tom Silver","Jo√£o F. Henriques","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2410.23156v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2410.23128v1","updated":"2024-10-30T15:40:06Z","published":"2024-10-30T15:40:06Z","title":"Leader-Follower 3D Formation for Underwater Robots","summary":"  The schooling behavior of fish is hypothesized to confer many survival\nbenefits, including foraging success, safety from predators, and energy savings\nthrough hydrodynamic interactions when swimming in formation. Underwater robot\ncollectives may be able to achieve similar benefits in future applications,\ne.g. using formation control to achieve efficient spatial sampling for\nenvironmental monitoring. Although many theoretical algorithms exist for\nmulti-robot formation control, they have not been tested in the underwater\ndomain due to the fundamental challenges in underwater communication. Here we\nintroduce a leader-follower strategy for underwater formation control that\nallows us to realize complex 3D formations, using purely vision-based\nperception and a reactive control algorithm that is low computation. We use a\nphysical platform, BlueSwarm, to demonstrate for the first time an experimental\nrealization of inline, side-by-side, and staggered swimming 3D formations. More\ncomplex formations are studied in a physics-based simulator, providing new\ninsights into the convergence and stability of formations given underwater\ninertial/drag conditions. Our findings lay the groundwork for future\napplications of underwater robot swarms in aquatic environments with minimal\ncommunication.\n","authors":["Di Ni","Hungtang Ko","Radhika Nagpal"],"pdf_url":"https://arxiv.org/pdf/2410.23128v1.pdf","comment":"Accepted at DARS 2024 (The 17th International Symposium on\n  Distributed Autonomous Robotic Systems)"},{"id":"http://arxiv.org/abs/2410.23085v1","updated":"2024-10-30T15:00:06Z","published":"2024-10-30T15:00:06Z","title":"S3PT: Scene Semantics and Structure Guided Clustering to Boost\n  Self-Supervised Pre-Training for Autonomous Driving","summary":"  Recent self-supervised clustering-based pre-training techniques like DINO and\nCribo have shown impressive results for downstream detection and segmentation\ntasks. However, real-world applications such as autonomous driving face\nchallenges with imbalanced object class and size distributions and complex\nscene geometries. In this paper, we propose S3PT a novel scene semantics and\nstructure guided clustering to provide more scene-consistent objectives for\nself-supervised training. Specifically, our contributions are threefold: First,\nwe incorporate semantic distribution consistent clustering to encourage better\nrepresentation of rare classes such as motorcycles or animals. Second, we\nintroduce object diversity consistent spatial clustering, to handle imbalanced\nand diverse object sizes, ranging from large background areas to small objects\nsuch as pedestrians and traffic signs. Third, we propose a depth-guided spatial\nclustering to regularize learning based on geometric information of the scene,\nthus further refining region separation on the feature level. Our learned\nrepresentations significantly improve performance in downstream semantic\nsegmentation and 3D object detection tasks on the nuScenes, nuImages, and\nCityscapes datasets and show promising domain translation properties.\n","authors":["Maciej K. Wozniak","Hariprasath Govindarajan","Marvin Klingner","Camille Maurice","Ravi Kiran","Senthil Yogamani"],"pdf_url":"https://arxiv.org/pdf/2410.23085v1.pdf","comment":"Accepted for WACV 2025"},{"id":"http://arxiv.org/abs/2303.10465v2","updated":"2024-10-30T14:55:39Z","published":"2023-03-18T18:01:39Z","title":"Cognitive Load-based Affective Workload Allocation for Multi-human\n  Multi-robot Teams","summary":"  The interaction and collaboration between humans and multiple robots\nrepresent a novel field of research known as human multi-robot systems.\nAdequately designed systems within this field allow teams composed of both\nhumans and robots to work together effectively on tasks such as monitoring,\nexploration, and search and rescue operations. This paper presents a deep\nreinforcement learning-based affective workload allocation controller\nspecifically for multi-human multi-robot teams. The proposed controller can\ndynamically reallocate workloads based on the performance of the operators\nduring collaborative missions with multi-robot systems. The operators'\nperformances are evaluated through the scores of a self-reported questionnaire\n(i.e., subjective measurement) and the results of a deep learning-based\ncognitive workload prediction algorithm that uses physiological and behavioral\ndata (i.e., objective measurement). To evaluate the effectiveness of the\nproposed controller, we use a multi-human multi-robot CCTV monitoring task as\nan example and carry out comprehensive real-world experiments with 32 human\nsubjects for both quantitative measurement and qualitative analysis. Our\nresults demonstrate the performance and effectiveness of the proposed\ncontroller and highlight the importance of incorporating both subjective and\nobjective measurements of the operators' cognitive workload as well as seeking\nconsent for workload transitions, to enhance the performance of multi-human\nmulti-robot teams.\n","authors":["Wonse Jo","Ruiqi Wang","Baijian Yang","Dan Foti","Mo Rastgaar","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2303.10465v2.pdf","comment":"This paper is submitted and accepted to IEEE Transactions on\n  Human-Machine Systems"},{"id":"http://arxiv.org/abs/2404.08563v2","updated":"2024-10-30T14:48:14Z","published":"2024-04-12T16:01:02Z","title":"FusionPortableV2: A Unified Multi-Sensor Dataset for Generalized SLAM\n  Across Diverse Platforms and Scalable Environments","summary":"  Simultaneous Localization and Mapping (SLAM) technology has been widely\napplied in various robotic scenarios, from rescue operations to autonomous\ndriving. However, the generalization of SLAM algorithms remains a significant\nchallenge, as current datasets often lack scalability in terms of platforms and\nenvironments. To address this limitation, we present FusionPortableV2, a\nmulti-sensor SLAM dataset featuring sensor diversity, varied motion patterns,\nand a wide range of environmental scenarios. Our dataset comprises $27$\nsequences, spanning over $2.5$ hours and collected from four distinct\nplatforms: a handheld suite, a legged robots, a unmanned ground vehicle (UGV),\nand a vehicle. These sequences cover diverse settings, including buildings,\ncampuses, and urban areas, with a total length of $38.7km$. Additionally, the\ndataset includes ground-truth (GT) trajectories and RGB point cloud maps\ncovering approximately $0.3km^2$. To validate the utility of our dataset in\nadvancing SLAM research, we assess several state-of-the-art (SOTA) SLAM\nalgorithms. Furthermore, we demonstrate the dataset's broad application beyond\ntraditional SLAM tasks by investigating its potential for monocular depth\nestimation. The complete dataset, including sensor data, GT, and calibration\ndetails, is accessible at\nhttps://fusionportable.github.io/dataset/fusionportable_v2.\n","authors":["Hexiang Wei","Jianhao Jiao","Xiangcheng Hu","Jingwen Yu","Xupeng Xie","Jin Wu","Yilong Zhu","Yuxuan Liu","Lujia Wang","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2404.08563v2.pdf","comment":"21 pages, 17 figures, 7 tables. Accepted by International Journal of\n  Robotics Research (IJRR)"},{"id":"http://arxiv.org/abs/2410.23059v1","updated":"2024-10-30T14:33:22Z","published":"2024-10-30T14:33:22Z","title":"FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator","summary":"  Soft robotic manipulators are generally slow despite their great\nadaptability, resilience, and compliance. This limitation also extends to\ncurrent soft robotic micromanipulators. Here, we introduce FilMBot, a 3-DOF\nfilm-based, electromagnetically actuated, soft kinematic robotic\nmicromanipulator achieving speeds up to 2117 $\\deg$/s and 2456 $\\deg$/s in\n$\\alpha$ and $\\beta$ angular motions, with corresponding linear velocities of\n1.61 m/s and 1.92 m/s using a 4-cm needle end-effector, and 1.57 m/s along the\nZ axis. The robot can reach ~1.50 m/s in path-following tasks, operates at\nfrequencies up to 30 Hz, and remains functional up to 50 Hz. It demonstrates\nhigh precision (~6.3 $\\mu$m, or ~0.05% of its workspace) in small\npath-following tasks. The novel combination of the low-stiffness soft kinematic\nfilm structure and strong electromagnetic actuation in FilMBot opens new\navenues for soft robotics. Furthermore, its simple construction and\ninexpensive, readily accessible components could broaden the application of\nmicromanipulators beyond current academic and professional users.\n","authors":["Jiangkun Yu","Houari Bettahar","Hakan Kandemir","Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.23059v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.11387v3","updated":"2024-10-30T14:31:25Z","published":"2024-10-15T08:24:05Z","title":"LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs","summary":"  Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nincorporate both domain-specific and commonsense knowledge. Hence, we propose\nto address the aforementioned challenges by integrating LLMs with robot swarms\nand show the potential in proofs of concept (showcases). For this integration,\nwe explore two approaches. The first approach is 'indirect integration,' where\nLLMs are used to synthesize and validate the robot controllers. This approach\nmay reduce development time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language, as demonstrated in our showcases\nwhere the robots are able to detect a variety of anomalies, without prior\ninformation about the nature of these anomalies. To enable further research on\nour mainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.\n","authors":["Volker Strobel","Marco Dorigo","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2410.11387v3.pdf","comment":"Accepted at NeurIPS 2024 Workshop on Open-World Agents. Code:\n  https://github.com/Pold87/LLM2Swarm/"},{"id":"http://arxiv.org/abs/2410.23049v1","updated":"2024-10-30T14:16:59Z","published":"2024-10-30T14:16:59Z","title":"TumblerBots: Tumbling Robotic sensors for Minimally-invasive Benthic\n  Monitoring","summary":"  Robotic systems show significant promise for water environmental sensing\napplications such as water quality monitoring, pollution mapping and\nbiodiversity data collection.\n  Conventional deployment methods often disrupt fragile ecosystems, preventing\ndepiction of the undisturbed environmental condition. In response to this\nchallenge, we propose a novel framework utilizing a lightweight tumbler system\nequipped with a sensing unit, deployed via a drone. This design minimizes\ndisruption to the water habitat by maintaining a slow descent. The sensing unit\nis detached once on the water surface, enabling precise and non-invasive data\ncollection from the benthic zone.\n  The tumbler is designed to be lightweight and compact, enabling deployment\nvia a drone. The sensing pod, which detaches from the tumbler and descends to\nthe bottom of the water body, is equipped with temperature and pressure\nsensors, as well as a buoyancy system. The later, activated upon task\ncompletion, utilizes a silicon membrane inflated via a chemical reaction. The\nreaction generates a pressure of 70 kPa, causing the silicon membrane to expand\nby 30\\%, which exceeds the 5.7\\% volume increase required for positive\nbuoyancy. The tumblers, made from ecofriendly materials to minimize\nenvironmental impact when lost during the mission, were tested for their\ngliding ratio and descent rate. They exhibit a low descent rate, in the range\nof 0.8 to 2.5 meters per seconds, which minimizes disturbance to the ecosystem\nupon water landing. Additionally, the system demonstrated robustness in\nmoderate to strong wind conditions during outdoor tests, validating the overall\nframework.\n","authors":["L. Romanello","A. Teboul","F. Wiesemuller","P. H. Nguyen","M. Kovac","S. F. Armanini"],"pdf_url":"https://arxiv.org/pdf/2410.23049v1.pdf","comment":"Submitted to IEEE Robosoft 2025"},{"id":"http://arxiv.org/abs/2304.00910v4","updated":"2024-10-30T14:08:46Z","published":"2023-04-03T11:57:10Z","title":"Integrating One-Shot View Planning with a Single Next-Best View via\n  Long-Tail Multiview Sampling","summary":"  Existing view planning systems either adopt an iterative paradigm using\nnext-best views (NBV) or a one-shot pipeline relying on the set-covering\nview-planning (SCVP) network. However, neither of these methods can\nconcurrently guarantee both high-quality and high-efficiency reconstruction of\n3D unknown objects. To tackle this challenge, we introduce a crucial\nhypothesis: with the availability of more information about the unknown object,\nthe prediction quality of the SCVP network improves. There are two ways to\nprovide extra information: (1) leveraging perception data obtained from NBVs,\nand (2) training on an expanded dataset of multiview inputs. In this work, we\nintroduce a novel combined pipeline that incorporates a single NBV before\nactivating the proposed multiview-activated (MA-)SCVP network. The MA-SCVP is\ntrained on a multiview dataset generated by our long-tail sampling method,\nwhich addresses the issue of unbalanced multiview inputs and enhances the\nnetwork performance. Extensive simulated experiments substantiate that our\nsystem demonstrates a significant surface coverage increase and a substantial\n45% reduction in movement cost compared to state-of-the-art systems. Real-world\nexperiments justify the capability of our system for generalization and\ndeployment.\n","authors":["Sicong Pan","Hao Hu","Hui Wei","Nils Dengler","Tobias Zaenker","Murad Dawood","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2304.00910v4.pdf","comment":"Accepted to IEEE Transactions on Robotics. Full appendices version"},{"id":"http://arxiv.org/abs/2410.23039v1","updated":"2024-10-30T14:06:51Z","published":"2024-10-30T14:06:51Z","title":"Neural Attention Field: Emerging Point Relevance in 3D Scenes for\n  One-Shot Dexterous Grasping","summary":"  One-shot transfer of dexterous grasps to novel scenes with object and context\nvariations has been a challenging problem. While distilled feature fields from\nlarge vision models have enabled semantic correspondences across 3D scenes,\ntheir features are point-based and restricted to object surfaces, limiting\ntheir capability of modeling complex semantic feature distributions for\nhand-object interactions. In this work, we propose the \\textit{neural attention\nfield} for representing semantic-aware dense feature fields in the 3D space by\nmodeling inter-point relevance instead of individual point features. Core to it\nis a transformer decoder that computes the cross-attention between any 3D query\npoint with all the scene points, and provides the query point feature with an\nattention-based aggregation. We further propose a self-supervised framework for\ntraining the transformer decoder from only a few 3D pointclouds without hand\ndemonstrations. Post-training, the attention field can be applied to novel\nscenes for semantics-aware dexterous grasping from one-shot demonstration.\nExperiments show that our method provides better optimization landscapes by\nencouraging the end-effector to focus on task-relevant scene regions, resulting\nin significant improvements in success rates on real robots compared with the\nfeature-field-based methods.\n","authors":["Qianxu Wang","Congyue Deng","Tyler Ga Wei Lum","Yuanpei Chen","Yaodong Yang","Jeannette Bohg","Yixin Zhu","Leonidas Guibas"],"pdf_url":"https://arxiv.org/pdf/2410.23039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23033v1","updated":"2024-10-30T14:02:23Z","published":"2024-10-30T14:02:23Z","title":"Exploring the Potential of Multi-modal Sensing Framework for Forest\n  Ecology","summary":"  Forests offer essential resources and services to humanity, yet preserving\nand restoring them presents challenges, particularly due to the limited\navailability of actionable data, especially in hard-to-reach areas like forest\ncanopies. Accessibility continues to pose a challenge for biologists collecting\ndata in forest environments, often requiring them to invest significant time\nand energy in climbing trees to place sensors. This operation not only consumes\nresources but also exposes them to danger. Efforts in robotics have been\ndirected towards accessing the tree canopy using robots. A swarm of drones has\nshowcased autonomous navigation through the canopy, maneuvering with agility\nand evading tree collisions, all aimed at mapping the area and collecting data.\nHowever, relying solely on free-flying drones has proven insufficient for data\ncollection. Flying drones within the canopy generates loud noise, disturbing\nanimals and potentially corrupting the data. Additionally, commercial drones\noften have limited autonomy for dexterous tasks where aerial physical\ninteraction could be required, further complicating data acquisition efforts.\nAerial deployed sensor placement methods such as bio-gliders and sensor\nshooting have proven effective for data collection within the lower canopy.\nHowever, these methods face challenges related to retrieving the data and\nsensors, often necessitating human intervention.\n","authors":["Luca Romanello","Tian Lan","Mirko Kovac","Sophie F. Armanini","Basaran Bahadir Kocer"],"pdf_url":"https://arxiv.org/pdf/2410.23033v1.pdf","comment":"Peer-reviewed and accepted in IEEE ICRA 2024 Workshop RUNE"},{"id":"http://arxiv.org/abs/2410.23032v1","updated":"2024-10-30T14:02:15Z","published":"2024-10-30T14:02:15Z","title":"Camber-changing flapping hydrofoils for efficient and environmental-safe\n  water propulsion system","summary":"  This research introduces a novel hydrofoil-based propulsion framework for\nunmanned aquatic robots, inspired by the undulating locomotion observed in\nselect aquatic species. The proposed system incorporates a camber-modulating\nmechanism to enhance hydrofoil propulsive force generation and eventually\nefficiency. Through dynamic simulations, we validate the effectiveness of the\ncamber-adjusting hydrofoil compared to a symmetric counterpart. The results\ndemonstrate a significant improvement in horizontal thrust, emphasizing the\npotential of the cambering approach to enhance propulsive performance.\nAdditionally, a prototype flipper design is presented, featuring individual\ncontrol of heave and pitch motions, as well as a camber-adjustment mechanism.\nThe integrated system not only provides efficient water-based propulsion but\nalso offers the capacity for generating vertical forces during take-off\nmaneuvers for seaplanes. The design is tailored to harness wave energy,\ncontributing to the exploration of alternative energy resources. This work\nadvances the understanding of bionic oscillatory principles for aquatic robots\nand provides a foundation for future developments in environmentally safe and\nagile underwater exploration.\n","authors":["Luca Romanello","Leonard Hohaus","David-Marian Schmitt","Mirko Kovac","Sophie F. Armanini"],"pdf_url":"https://arxiv.org/pdf/2410.23032v1.pdf","comment":"Peer-reviewed and accepted in Ubiquitous Robots 2024, New York City"},{"id":"http://arxiv.org/abs/2410.23022v1","updated":"2024-10-30T13:52:43Z","published":"2024-10-30T13:52:43Z","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback","summary":"  Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples; or are limited to reward functions expressible by compact\ncode, which may require source code and have difficulty capturing nuanced\nsemantics; or require a diverse offline dataset, which may not exist or be\nimpossible to collect. In this work, we address these limitations through a\ncombination of algorithmic and systems-level contributions. We propose ONI, a\ndistributed architecture that simultaneously learns an RL policy and an\nintrinsic reward function using LLM feedback. Our approach annotates the\nagent's collected experience via an asynchronous LLM server, which is then\ndistilled into an intrinsic reward model. We explore a range of algorithmic\nchoices for reward modeling with varying complexity, including hashing,\nclassification, and ranking models. By studying their relative tradeoffs, we\nshed light on questions regarding intrinsic reward design for sparse reward\nproblems. Our approach achieves state-of-the-art performance across a range of\nchallenging, sparse reward tasks from the NetHack Learning Environment in a\nsimple unified process, solely using the agent's gathered experience, without\nrequiring external datasets nor source code. We make our code available at\n\\url{URL} (coming soon).\n","authors":["Qinqing Zheng","Mikael Henaff","Amy Zhang","Aditya Grover","Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2410.23022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00552v4","updated":"2024-10-30T13:49:11Z","published":"2024-05-01T14:50:58Z","title":"Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs","summary":"  We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.\n","authors":["Nicolas Gorlo","Lukas Schmid","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2405.00552v4.pdf","comment":"8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2"},{"id":"http://arxiv.org/abs/2410.23004v1","updated":"2024-10-30T13:30:39Z","published":"2024-10-30T13:30:39Z","title":"DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale\n  Synthetic Cluttered Scenes","summary":"  Grasping in cluttered scenes remains highly challenging for dexterous hands\ndue to the scarcity of data. To address this problem, we present a large-scale\nsynthetic benchmark, encompassing 1319 objects, 8270 scenes, and 427 million\ngrasps. Beyond benchmarking, we also propose a novel two-stage grasping method\nthat learns efficiently from data by using a diffusion model that conditions on\nlocal geometry. Our proposed generative method outperforms all baselines in\nsimulation experiments. Furthermore, with the aid of test-time-depth\nrestoration, our method demonstrates zero-shot sim-to-real transfer, attaining\n90.7% real-world dexterous grasping success rate in cluttered scenes.\n","authors":["Jialiang Zhang","Haoran Liu","Danshi Li","Xinqiang Yu","Haoran Geng","Yufei Ding","Jiayi Chen","He Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22997v1","updated":"2024-10-30T13:22:55Z","published":"2024-10-30T13:22:55Z","title":"A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics","summary":"  Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of \\glspl{LLM} to accomplish tasks,\nwhile others have proposed methods that utilize LLMs to plan and execute tasks\nbased on the available functionalities of a given robot platform. In this work,\nwe consider both lines of research by comparing prompt engineering techniques\nand combinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.\n","authors":["Jonas Bode","Bastian P√§tzold","Raphael Memmesheimer","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2410.22997v1.pdf","comment":"6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering"},{"id":"http://arxiv.org/abs/2410.22982v1","updated":"2024-10-30T12:46:15Z","published":"2024-10-30T12:46:15Z","title":"PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster\n  Search and Rescue","summary":"  This paper introduces a comprehensive framework for Post-Disaster Search and\nRescue (PDSR), aiming to optimize search and rescue operations leveraging\nUnmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision\nand availability of sensing capabilities, particularly in various catastrophic\nscenarios. Central to this concept is the rapid deployment of UAV swarms\nequipped with diverse sensing, communication, and intelligence capabilities,\nfunctioning as an integrated system that incorporates multiple technologies and\napproaches for efficient detection of individuals buried beneath rubble or\ndebris following a disaster. Within this framework, we propose architectural\nsolution and address associated challenges to ensure optimal performance in\nreal-world disaster scenarios. The proposed framework aims to achieve complete\ncoverage of damaged areas significantly faster than traditional methods using a\nmulti-tier swarm architecture. Furthermore, integrating multi-modal sensing\ndata with machine learning for data fusion could enhance detection accuracy,\nensuring precise identification of survivors.\n","authors":["Alaa Awad Abdellatif","Ali Elmancy","Amr Mohamed","Ahmed Massoud","Wadha Lebda","Khalid K. Naji"],"pdf_url":"https://arxiv.org/pdf/2410.22982v1.pdf","comment":"This paper is currently under review at IEEE IoT Magazine"},{"id":"http://arxiv.org/abs/2410.22980v1","updated":"2024-10-30T12:45:12Z","published":"2024-10-30T12:45:12Z","title":"Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation","summary":"  6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.\n","authors":["Kaiqin Yang. Yixiang Dai","Guijin Wang","Siang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.22980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22910v1","updated":"2024-10-30T11:06:43Z","published":"2024-10-30T11:06:43Z","title":"An Efficient Representation of Whole-body Model Predictive Control for\n  Online Compliant Dual-arm Mobile Manipulation","summary":"  Dual-arm mobile manipulators can transport and manipulate large-size objects\nwith simple end-effectors. To interact with dynamic environments with strict\nsafety and compliance requirements, achieving whole-body motion planning online\nwhile meeting various hard constraints for such highly redundant mobile\nmanipulators poses a significant challenge. We tackle this challenge by\npresenting an efficient representation of whole-body motion trajectories within\nour bilevel model-based predictive control (MPC) framework. We utilize\nB\\'ezier-curve parameterization to represent the optimized collision-free\ntrajectories of two collaborating end-effectors in the first MPC, facilitating\nfast long-horizon object-oriented motion planning in SE(3) while considering\napproximated feasibility constraints. This approach is further applied to\nparameterize whole-body trajectories in the second MPC for whole-body motion\ngeneration with predictive admittance control in a relatively short horizon\nwhile satisfying whole-body hard constraints. This representation enables two\nMPCs with continuous properties, thereby avoiding inaccurate model-state\ntransition and dense decision-variable settings in existing MPCs using the\ndiscretization method. It strengthens the online execution of the bilevel MPC\nframework in high-dimensional space and facilitates the generation of\nconsistent commands for our hybrid position/velocity-controlled robot. The\nsimulation comparisons and real-world experiments demonstrate the efficiency\nand robustness of this approach in various scenarios for static and dynamic\nobstacle avoidance, and compliant interaction control with the manipulated\nobject and external disturbances.\n","authors":["Wenqian Du","Ran Long","Jo√£o Moura","Jiayi Wang","Saeid Samadi","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2410.22910v1.pdf","comment":"Under Review for IEEE Transactions on Robotics"},{"id":"http://arxiv.org/abs/2410.22893v1","updated":"2024-10-30T10:40:57Z","published":"2024-10-30T10:40:57Z","title":"Human-inspired Grasping Strategies of Fresh Fruits and Vegetables\n  Applied to Robotic Manipulation","summary":"  Robotic manipulation of fresh fruits and vegetables, including the grasping\nof multiple loose items, has a strong industrial need but it still is a\nchallenging task for robotic manipulation. This paper outlines the distinctive\nmanipulation strategies used by humans to pick loose fruits and vegetables with\nthe aim to better adopt them for robotic manipulation of diverse items. In this\nwork we present a first version of a robotic setup designed to pick different\nsingle or multiple fresh items, featuring multi-fingered compliant robotic\ngripper. We analyse human grasping strategies from the perspective of\nindustrial Key Performance Indicators (KPIs) used in the logistic sector. The\nrobotic system was validated using the same KPIs, as well as taking into\naccount human performance and strategies. This paper lays the foundation for\nfuture development of the robotic demonstrator for fresh fruit and vegetable\nintelligent manipulation, and outlines the need for generic approaches to\nhandle the complexity of the task.\n","authors":["Romeo Orsolino","Mykhaylo Marfeychuk","Mariana de Paula Assis Fonseca","Mario Baggetta","Wesley Wimshurst","Francesco Porta","Morgan Clarke","Giovanni Berselli","Jelizaveta Konstantinova"],"pdf_url":"https://arxiv.org/pdf/2410.22893v1.pdf","comment":"*Authors contributed equally"},{"id":"http://arxiv.org/abs/2410.06613v2","updated":"2024-10-30T10:21:13Z","published":"2024-10-09T07:09:29Z","title":"ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian\n  Completion","summary":"  Accurate and affordable indoor 3D reconstruction is critical for effective\nrobot navigation and interaction. Traditional LiDAR-based mapping provides high\nprecision but is costly, heavy, and power-intensive, with limited ability for\nnovel view rendering. Vision-based mapping, while cost-effective and capable of\ncapturing visual data, often struggles with high-quality 3D reconstruction due\nto sparse point clouds. We propose ES-Gaussian, an end-to-end system using a\nlow-altitude camera and single-line LiDAR for high-quality 3D indoor\nreconstruction. Our system features Visual Error Construction (VEC) to enhance\nsparse point clouds by identifying and correcting areas with insufficient\ngeometric detail from 2D error maps. Additionally, we introduce a novel 3DGS\ninitialization method guided by single-line LiDAR, overcoming the limitations\nof traditional multi-view setups and enabling effective reconstruction in\nresource-constrained environments. Extensive experimental results on our new\nDreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian\noutperforms existing methods, particularly in challenging scenarios. The\nproject page is available at https://chenlu-china.github.io/ES-Gaussian/.\n","authors":["Lu Chen","Yingfu Zeng","Haoang Li","Zhitao Deng","Jiafu Yan","Zhenjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.06613v2.pdf","comment":"This preprint has been withdrawn due to concerns regarding the\n  originality of certain technical elements, as well as its basis in a company\n  project report that was intended solely for internal discussions. To avoid\n  any potential misunderstandings, we have decided to withdraw this submission\n  from public access. We apologize for any confusion this may have caused"},{"id":"http://arxiv.org/abs/2410.22848v1","updated":"2024-10-30T09:29:37Z","published":"2024-10-30T09:29:37Z","title":"Non-contact Dexterous Micromanipulation with Multiple Optoelectronic\n  Robots","summary":"  Micromanipulation systems leverage automation and robotic technologies to\nimprove the precision, repeatability, and efficiency of various tasks at the\nmicroscale. However, current approaches are typically limited to specific\nobjects or tasks, which necessitates the use of custom tools and specialized\ngrasping methods. This paper proposes a novel non-contact micromanipulation\nmethod based on optoelectronic technologies. The proposed method utilizes\nrepulsive dielectrophoretic forces generated in the optoelectronic field to\ndrive a microrobot, enabling the microrobot to push the target object in a\ncluttered environment without physical contact. The non-contact feature can\nminimize the risks of potential damage, contamination, or adhesion while\nlargely improving the flexibility of manipulation. The feature enables the use\nof a general tool for indirect object manipulation, eliminating the need for\nspecialized tools. A series of simulation studies and real-world experiments --\nincluding non-contact trajectory tracking, obstacle avoidance, and reciprocal\navoidance between multiple microrobots -- are conducted to validate the\nperformance of the proposed method. The proposed formulation provides a general\nand dexterous solution for a range of objects and tasks at the micro scale.\n","authors":["Yongyi Jia","Shu Miao","Ao Wang","Caiding Ni","Lin Feng","Xiaowo Wang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2410.22848v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.22825v1","updated":"2024-10-30T09:04:45Z","published":"2024-10-30T09:04:45Z","title":"Grasping Force Estimation for Markerless Visuotactile Sensors","summary":"  Tactile sensors have been used for force estimation in the past, especially\nVision-Based Tactile Sensors (VBTS) have recently become a new trend due to\ntheir high spatial resolution and low cost. In this work, we have designed and\nimplemented several approaches to estimate the normal grasping force using\ndifferent types of markerless visuotactile representations obtained from VBTS.\nOur main goal is to determine the most appropriate visuotactile representation,\nbased on a performance analysis during robotic grasping tasks. Our proposal has\nbeen tested on the dataset generated with our DIGIT sensors and another one\nobtained using GelSight Mini sensors from another state-of-the-art work. We\nhave also tested the generalization capabilities of our best approach, called\nRGBmod. The results led to two main conclusions. First, the RGB visuotactile\nrepresentation is a better input option than the depth image or a combination\nof the two for estimating normal grasping forces. Second, RGBmod achieved a\ngood performance when tested on 10 unseen everyday objects in real-world\nscenarios, achieving an average relative error of 0.125 +- 0.153. Furthermore,\nwe show that our proposal outperforms other works in the literature that use\nRGB and depth information for the same task.\n","authors":["Julio Casta√±o-Amoros","Pablo Gil"],"pdf_url":"https://arxiv.org/pdf/2410.22825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22816v1","updated":"2024-10-30T08:50:14Z","published":"2024-10-30T08:50:14Z","title":"Enhancing Tool Manipulation of An Aerial Vehicle with A Dynamically\n  Displacing Center-of-Mass","summary":"  As aerial robots gain traction in industrial applications, there is growing\ninterest in enhancing their physical interaction capabilities. Pushing tasks\nperformed by aerial manipulators have been successfully demonstrated in\ncontact-based inspections. However, more complex industrial applications\nrequire these systems to support higher-DoF (Degree of Freedom) manipulators\nand generate larger forces while pushing (e.g., drilling, grinding). This paper\nbuilds on our previous work, where we introduced an aerial vehicle with a\ndynamically displacing CoM (Center of Mass) to improve force exertion during\ninteractions. We propose a novel approach to further enhance this system's\nforce generation by optimizing its CoM location during interactions.\nAdditionally, we study the case of this aerial vehicle equipped with a 2-DoF\nmanipulation arm to extend the system's functionality in tool-based tasks. The\neffectiveness of the proposed methods is validated through simulations,\ndemonstrating the potential of this system for advanced aerial manipulation in\npractical settings.\n","authors":["Tong Hui","Matteo Fumagalli"],"pdf_url":"https://arxiv.org/pdf/2410.22816v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.01110"},{"id":"http://arxiv.org/abs/2208.02439v2","updated":"2024-10-30T07:34:58Z","published":"2022-08-04T04:11:36Z","title":"MPPI-IPDDP: Hybrid Method of Collision-Free Smooth Trajectory Generation\n  for Autonomous Robots","summary":"  This paper presents a hybrid trajectory optimization method designed to\ngenerate collision-free, smooth trajectories for autonomous mobile robots. By\ncombining sampling-based Model Predictive Path Integral (MPPI) control with\ngradient-based Interior-Point Differential Dynamic Programming (IPDDP), we\nleverage their respective strengths in exploration and smoothing. The proposed\nmethod, MPPI-IPDDP, involves three steps: First, MPPI control is used to\ngenerate a coarse trajectory. Second, a collision-free convex corridor is\nconstructed. Third, IPDDP is applied to smooth the coarse trajectory, utilizing\nthe collision-free corridor from the second step. To demonstrate the\neffectiveness of our approach, we apply the proposed algorithm to trajectory\noptimization for differential-drive wheeled mobile robots and point-mass\nquadrotors. In comparisons with other MPPI variants and continuous\noptimization-based solvers, our method shows superior performance in terms of\ncomputational robustness and trajectory smoothness.\n  Code: https://github.com/i-ASL/mppi-ipddp Video: https://youtu.be/-oUAt5sd9Bk\n","authors":["Min-Gyeom Kim","Minchan Jung","JunGee Hong","Kwang-Ki K. Kim"],"pdf_url":"https://arxiv.org/pdf/2208.02439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22752v1","updated":"2024-10-30T07:18:00Z","published":"2024-10-30T07:18:00Z","title":"SoftCTRL: Soft conservative KL-control of Transformer Reinforcement\n  Learning for Autonomous Driving","summary":"  In recent years, motion planning for urban self-driving cars (SDV) has become\na popular problem due to its complex interaction of road components. To tackle\nthis, many methods have relied on large-scale, human-sampled data processed\nthrough Imitation learning (IL). Although effective, IL alone cannot adequately\nhandle safety and reliability concerns. Combining IL with Reinforcement\nlearning (RL) by adding KL divergence between RL and IL policy to the RL loss\ncan alleviate IL's weakness but suffer from over-conservation caused by\ncovariate shift of IL. To address this limitation, we introduce a method that\ncombines IL with RL using an implicit entropy-KL control that offers a simple\nway to reduce the over-conservation characteristic. In particular, we validate\ndifferent challenging simulated urban scenarios from the unseen dataset,\nindicating that although IL can perform well in imitation tasks, our proposed\nmethod significantly improves robustness (over 17\\% reduction in failures) and\ngenerates human-like driving behavior.\n","authors":["Minh Tri Huynh","Duc Dung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.22752v1.pdf","comment":"submitted to IEEE Open Journal of Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2401.01881v2","updated":"2024-10-30T05:47:26Z","published":"2024-01-03T18:42:22Z","title":"Robust Control Barrier Functions using Uncertainty Estimation with\n  Application to Mobile Robots","summary":"  This paper proposes a safety-critical control design approach for nonlinear\ncontrol affine systems in the presence of matched and unmatched uncertainties.\nOur constructive framework couples control barrier function (CBF) theory with a\nnew uncertainty estimator to ensure robust safety. The estimated uncertainty\nwith a derived upper bound on the estimation error is used for synthesizing\nCBFs and safety-critical controllers via a quadratic program-based feedback\ncontrol law that rigorously ensures robust safety while improving disturbance\nrejection performance. The method is extended to higher-order CBFs (HOCBFs) to\nachieve safety under unmatched uncertainty, which may cause relative degree\ndifferences with respect to control input and disturbances. We assume the\nrelative degree difference is at most one, resulting in a second-order cone\nconstraint. The proposed robust HOCBF method is demonstrated via a simulation\nof an uncertain elastic actuator control problem. Finally, we experimentally\ndemonstrated the efficacy of our robust CBF framework on a tracked robot with\nslope-induced matched and unmatched perturbations.\n","authors":["Ersin Das","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2401.01881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22707v1","updated":"2024-10-30T05:34:52Z","published":"2024-10-30T05:34:52Z","title":"Robotic State Recognition with Image-to-Text Retrieval Task of\n  Pre-Trained Vision-Language Model and Black-Box Optimization","summary":"  State recognition of the environment and objects, such as the open/closed\nstate of doors and the on/off of lights, is indispensable for robots that\nperform daily life support and security tasks. Until now, state recognition\nmethods have been based on training neural networks from manual annotations,\npreparing special sensors for the recognition, or manually programming to\nextract features from point clouds or raw images. In contrast, we propose a\nrobotic state recognition method using a pre-trained vision-language model,\nwhich is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several\nkinds of language prompts in advance, calculate the similarity between these\nprompts and the current image by ITR, and perform state recognition. By\napplying the optimal weighting to each prompt using black-box optimization,\nstate recognition can be performed with higher accuracy. Experiments show that\nthis theory enables a variety of state recognitions by simply preparing\nmultiple prompts without retraining neural networks or manual programming. In\naddition, since only prompts and their weights need to be prepared for each\nrecognizer, there is no need to prepare multiple models, which facilitates\nresource management. It is possible to recognize the open/closed state of\ntransparent doors, the state of whether water is running or not from a faucet,\nand even the qualitative state of whether a kitchen is clean or not, which have\nbeen challenging so far, through language.\n","authors":["Kento Kawaharazuka","Yoshiki Obinata","Naoaki Kanazawa","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2410.22707v1.pdf","comment":"Accepted at Humanoids2024"},{"id":"http://arxiv.org/abs/2410.22691v1","updated":"2024-10-30T04:54:14Z","published":"2024-10-30T04:54:14Z","title":"MiniTac: An Ultra-Compact 8 mm Vision-Based Tactile Sensor for Enhanced\n  Palpation in Robot-Assisted Minimally Invasive Surgery","summary":"  Robot-assisted minimally invasive surgery (RAMIS) provides substantial\nbenefits over traditional open and laparoscopic methods. However, a significant\nlimitation of RAMIS is the surgeon's inability to palpate tissues, a crucial\ntechnique for examining tissue properties and detecting abnormalities,\nrestricting the widespread adoption of RAMIS. To overcome this obstacle, we\nintroduce MiniTac, a novel vision-based tactile sensor with an ultra-compact\ncross-sectional diameter of 8 mm, designed for seamless integration into\nmainstream RAMIS devices, particularly the Da Vinci surgical systems. MiniTac\nfeatures a novel mechanoresponsive photonic elastomer membrane that changes\ncolor distribution under varying contact pressures. This color change is\ncaptured by an embedded miniature camera, allowing MiniTac to detect tumors\nboth on the tissue surface and in deeper layers typically obscured from\nendoscopic view. MiniTac's efficacy has been rigorously tested on both phantoms\nand ex-vivo tissues. By leveraging advanced mechanoresponsive photonic\nmaterials, MiniTac represents a significant advancement in integrating tactile\nsensing into RAMIS, potentially expanding its applicability to a wider array of\nclinical scenarios that currently rely on traditional surgical approaches.\n","authors":["Wanlin Li","Zihang Zhao","Leiyao Cui","Weiyi Zhang","Hangxin Liu","Li-An Li","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22691v1.pdf","comment":"accepted for publication in the IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2410.22689v1","updated":"2024-10-30T04:49:39Z","published":"2024-10-30T04:49:39Z","title":"Multi-Task Interactive Robot Fleet Learning with Visual World Models","summary":"  Recent advancements in large-scale multi-task robot learning offer the\npotential for deploying robot fleets in household and industrial settings,\nenabling them to perform diverse tasks across various environments. However,\nAI-enabled robots often face challenges with generalization and robustness when\nexposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a\nmulti-task interactive robot fleet learning framework to address these\nchallenges. Sirius-Fleet monitors robot performance during deployment and\ninvolves humans to correct the robot's actions when necessary. We employ a\nvisual world model to predict the outcomes of future actions and build anomaly\npredictors to predict whether they will likely result in anomalies. As the\nrobot autonomy improves, the anomaly predictors automatically adapt their\nprediction criteria, leading to fewer requests for human intervention and\ngradually reducing human workload over time. Evaluations on large-scale\nbenchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task\npolicy performance and monitoring accuracy. We demonstrate Sirius-Fleet's\nperformance in both RoboCasa in simulation and Mutex in the real world, two\ndiverse, large-scale multi-task benchmarks. More information is available on\nthe project website: https://ut-austin-rpl.github.io/sirius-fleet\n","authors":["Huihan Liu","Yu Zhang","Vaarij Betala","Evan Zhang","James Liu","Crystal Ding","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.22689v1.pdf","comment":"In Proceedings of CoRL 2024"},{"id":"http://arxiv.org/abs/2410.22672v1","updated":"2024-10-30T03:49:53Z","published":"2024-10-30T03:49:53Z","title":"IM-GIV: an effective integrity monitoring scheme for tightly-coupled\n  GNSS/INS/Vision integration based on factor graph optimization","summary":"  Global Navigation Satellite System/Inertial Navigation System\n(GNSS/INS)/Vision integration based on factor graph optimization (FGO) has\nrecently attracted extensive attention in navigation and robotics community.\nIntegrity monitoring (IM) capability is required when FGO-based integrated\nnavigation system is used for safety-critical applications. However,\ntraditional researches on IM of integrated navigation system are mostly based\non Kalman filter. It is urgent to develop effective IM scheme for FGO-based\nGNSS/INS/Vision integration. In this contribution, the position error bounding\nformula to ensure the integrity of the GNSS/INS/Vision integration based on FGO\nis designed and validated for the first time. It can be calculated by the\nlinearized equations from the residuals of GNSS pseudo-range, IMU\npre-integration and visual measurements. The specific position error bounding\nis given in the case of GNSS, INS and visual measurement faults. Field\nexperiments were conducted to evaluate and validate the performance of the\nproposed position error bounding. Experimental results demonstrate that the\nproposed position error bounding for the GNSS/INS/Vision integration based on\nFGO can correctly fit the position error against different fault modes, and the\navailability of integrity in six fault modes is 100% after correct and timely\nfault exclusion.\n","authors":["Yunong Tian","Tuan Li","Haitao Jiang","Zhipeng Wang","Chuang Shi"],"pdf_url":"https://arxiv.org/pdf/2410.22672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22325v2","updated":"2024-10-30T03:33:08Z","published":"2024-10-29T17:58:13Z","title":"Robots Pre-train Robots: Manipulation-Centric Robotic Representation\n  from Large-Scale Robot Datasets","summary":"  The pre-training of visual representations has enhanced the efficiency of\nrobot learning. Due to the lack of large-scale in-domain robotic datasets,\nprior works utilize in-the-wild human videos to pre-train robotic visual\nrepresentation. Despite their promising results, representations from human\nvideos are inevitably subject to distribution shifts and lack the dynamics\ninformation crucial for task completion. We first evaluate various pre-trained\nrepresentations in terms of their correlation to the downstream robotic\nmanipulation tasks (i.e., manipulation centricity). Interestingly, we find that\nthe \"manipulation centricity\" is a strong indicator of success rates when\napplied to downstream tasks. Drawing from these findings, we propose\nManipulation Centric Representation (MCR), a foundation representation learning\nframework capturing both visual features and the dynamics information such as\nactions and proprioceptions of manipulation tasks to improve manipulation\ncentricity. Specifically, we pre-train a visual encoder on the DROID robotic\ndataset and leverage motion-relevant data such as robot proprioceptive states\nand actions. We introduce a novel contrastive loss that aligns visual\nobservations with the robot's proprioceptive state-action dynamics, combined\nwith a behavior cloning (BC)-like actor loss to predict actions during\npre-training, along with a time contrastive loss. Empirical results across 4\nsimulation domains with 20 tasks verify that MCR outperforms the strongest\nbaseline method by 14.8%. Moreover, MCR boosts the performance of\ndata-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project\nwebsite: https://robots-pretrain-robots.github.io/.\n","authors":["Guangqi Jiang","Yifei Sun","Tao Huang","Huanyu Li","Yongyuan Liang","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2410.22325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22662v1","updated":"2024-10-30T03:20:01Z","published":"2024-10-30T03:20:01Z","title":"$\\textbf{EMOS}$: $\\textbf{E}$mbodiment-aware Heterogeneous\n  $\\textbf{M}$ulti-robot $\\textbf{O}$perating $\\textbf{S}$ystem with LLM Agents","summary":"  Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach\nfor tackling complex tasks that single robots cannot manage alone. Current\nlarge-language-model-based multi-agent systems (LLM-based MAS) have shown\nsuccess in areas like software development and operating systems, but applying\nthese systems to robot control presents unique challenges. In particular, the\ncapabilities of each agent in a multi-robot system are inherently tied to the\nphysical composition of the robots, rather than predefined roles. To address\nthis issue, we introduce a novel multi-agent framework designed to enable\neffective collaboration among heterogeneous robots with varying embodiments and\ncapabilities, along with a new benchmark named Habitat-MAS. One of our key\ndesigns is $\\textit{Robot Resume}$: Instead of adopting human-designed role\nplay, we propose a self-prompted approach, where agents comprehend robot URDF\nfiles and call robot kinematics tools to generate descriptions of their physics\ncapabilities to guide their behavior in task planning and action execution. The\nHabitat-MAS benchmark is designed to assess how a multi-agent framework handles\ntasks that require embodiment-aware reasoning, which includes 1) manipulation,\n2) perception, 3) navigation, and 4) comprehensive multi-floor object\nrearrangement. The experimental results indicate that the robot's resume and\nthe hierarchical design of our multi-agent system are essential for the\neffective operation of the heterogeneous multi-robot system within this\nintricate problem context.\n","authors":["Junting Chen","Checheng Yu","Xunzhe Zhou","Tianqi Xu","Yao Mu","Mengkang Hu","Wenqi Shao","Yikai Wang","Guohao Li","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2410.22662v1.pdf","comment":"10 pages of main content, 3 pages of references, 5 pages of appendix,\n  7 figures in total"},{"id":"http://arxiv.org/abs/2406.14558v3","updated":"2024-10-30T02:58:10Z","published":"2024-06-20T17:59:22Z","title":"CooHOI: Learning Cooperative Human-Object Interaction with Manipulated\n  Object Dynamics","summary":"  Enabling humanoid robots to clean rooms has long been a pursued dream within\nhumanoid research communities. However, many tasks require multi-humanoid\ncollaboration, such as carrying large and heavy furniture together. Given the\nscarcity of motion capture data on multi-humanoid collaboration and the\nefficiency challenges associated with multi-agent learning, these tasks cannot\nbe straightforwardly addressed using training paradigms designed for\nsingle-agent scenarios. In this paper, we introduce Cooperative Human-Object\nInteraction (CooHOI), a framework designed to tackle the challenge of\nmulti-humanoid object transportation problem through a two-phase learning\nparadigm: individual skill learning and subsequent policy transfer. First, a\nsingle humanoid character learns to interact with objects through imitation\nlearning from human motion priors. Then, the humanoid learns to collaborate\nwith others by considering the shared dynamics of the manipulated object using\ncentralized training and decentralized execution (CTDE) multi-agent RL\nalgorithms. When one agent interacts with the object, resulting in specific\nobject dynamics changes, the other agents learn to respond appropriately,\nthereby achieving implicit communication and coordination between teammates.\nUnlike previous approaches that relied on tracking-based methods for\nmulti-humanoid HOI, CooHOI is inherently efficient, does not depend on motion\ncapture data of multi-humanoid interactions, and can be seamlessly extended to\ninclude more participants and a wide range of object types.\n","authors":["Jiawei Gao","Ziqin Wang","Zeqi Xiao","Jingbo Wang","Tai Wang","Jinkun Cao","Xiaolin Hu","Si Liu","Jifeng Dai","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2406.14558v3.pdf","comment":"Project website: https://gao-jiawei.com/Research/CooHOI/. NeurIPS\n  2024 Spotlight"},{"id":"http://arxiv.org/abs/2404.00282v3","updated":"2024-10-30T02:22:46Z","published":"2024-03-30T08:28:08Z","title":"Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,\n  Taxonomy, and Methods","summary":"  With extensive pre-trained knowledge and high-level general capabilities,\nlarge language models (LLMs) emerge as a promising avenue to augment\nreinforcement learning (RL) in aspects such as multi-task learning, sample\nefficiency, and high-level task planning. In this survey, we provide a\ncomprehensive review of the existing literature in LLM-enhanced RL and\nsummarize its characteristics compared to conventional RL methods, aiming to\nclarify the research scope and directions for future studies. Utilizing the\nclassical agent-environment interaction paradigm, we propose a structured\ntaxonomy to systematically categorize LLMs' functionalities in RL, including\nfour roles: information processor, reward designer, decision-maker, and\ngenerator. For each role, we summarize the methodologies, analyze the specific\nRL challenges that are mitigated, and provide insights into future directions.\nLastly, a comparative analysis of each role, potential applications,\nprospective opportunities, and challenges of the LLM-enhanced RL are discussed.\nBy proposing this taxonomy, we aim to provide a framework for researchers to\neffectively leverage LLMs in the RL field, potentially accelerating RL\napplications in complex applications such as robotics, autonomous driving, and\nenergy systems.\n","authors":["Yuji Cao","Huan Zhao","Yuheng Cheng","Ting Shu","Yue Chen","Guolong Liu","Gaoqi Liang","Junhua Zhao","Jinyue Yan","Yun Li"],"pdf_url":"https://arxiv.org/pdf/2404.00282v3.pdf","comment":"22 pages (including bibliography), 6 figures"},{"id":"http://arxiv.org/abs/2410.22643v1","updated":"2024-10-30T02:15:37Z","published":"2024-10-30T02:15:37Z","title":"An Overtaking Trajectory Planning Framework Based on Spatio-temporal\n  Topology and Reachable Set Analysis Ensuring Time Efficiency","summary":"  Generating overtaking trajectories in high-speed scenarios presents\nsignificant challenges and is typically addressed through hierarchical planning\nmethods. However, this method has two primary drawbacks. First, heuristic\nalgorithms can only provide a single initial solution, which may lead to local\noptima and consequently diminish the quality of the solution. Second, the time\nefficiency of trajectory refinement based on numerical optimization is\ninsufficient. To overcome these limitations, this paper proposes an overtaking\ntrajectory planning framework based on spatio-temporal topology and reachable\nset analysis (SROP), to improve trajectory quality and time efficiency.\nSpecifically, this paper introduces topological classes to describe\ntrajectories representing different overtaking behaviors, which support the\nspatio-temporal topological search method employed by the upper-layer planner\nto identify diverse initial paths. This approach helps prevent getting stuck in\nlocal optima, enhancing the overall solution quality by considering multiple\ninitial solutions from distinct topologies. Moreover, the reachable set method\nis integrated into the lower-layer planner for parallel trajectory evaluation.\nThis method enhances planning efficiency by decoupling vehicle model\nconstraints from the optimization process, enabling parallel computation while\nensuring control feasibility. Simulation results show that the proposed method\nimproves the smoothness of generated trajectories by 66.8% compared to\nstate-of-the-art methods, highlighting its effectiveness in enhancing\ntrajectory quality. Additionally, this method reduces computation time by\n62.9%, demonstrating its efficiency.\n","authors":["Wule Mao","Zhouheng Li","Lei Xie","Hongye Su"],"pdf_url":"https://arxiv.org/pdf/2410.22643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21736v2","updated":"2024-10-30T01:35:32Z","published":"2024-10-29T04:50:34Z","title":"Enhancing Safety and Robustness of Vision-Based Controllers via\n  Reachability Analysis","summary":"  Autonomous systems, such as self-driving cars and drones, have made\nsignificant strides in recent years by leveraging visual inputs and machine\nlearning for decision-making and control. Despite their impressive performance,\nthese vision-based controllers can make erroneous predictions when faced with\nnovel or out-of-distribution inputs. Such errors can cascade into catastrophic\nsystem failures and compromise system safety. In this work, we compute Neural\nReachable Tubes, which act as parameterized approximations of Backward\nReachable Tubes to stress-test the vision-based controllers and mine their\nfailure modes. The identified failures are then used to enhance the system\nsafety through both offline and online methods. The online approach involves\ntraining a classifier as a run-time failure monitor to detect closed-loop,\nsystem-level failures, subsequently triggering a fallback controller that\nrobustly handles these detected failures to preserve system safety. For the\noffline approach, we improve the original controller via incremental training\nusing a carefully augmented failure dataset, resulting in a more robust\ncontroller that is resistant to the known failure modes. In either approach,\nthe system is safeguarded against shortcomings that transcend the vision-based\ncontroller and pertain to the closed-loop safety of the overall system. We\nvalidate the proposed approaches on an autonomous aircraft taxiing task that\ninvolves using a vision-based controller to guide the aircraft towards the\ncenterline of the runway. Our results show the efficacy of the proposed\nalgorithms in identifying and handling system-level failures, outperforming\nmethods that rely on controller prediction error or uncertainty quantification\nfor identifying system failures.\n","authors":["Kaustav Chakraborty","Aryaman Gupta","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.21736v2.pdf","comment":null}]},"2024-10-29T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.22242v1","updated":"2024-10-29T17:06:50Z","published":"2024-10-29T17:06:50Z","title":"Computing Betti tables and minimal presentations of zero-dimensional\n  persistent homology","summary":"  The Betti tables of a multigraded module encode the grades at which there is\nan algebraic change in the module. Multigraded modules show up in many areas of\npure and applied mathematics, and in particular in topological data analysis,\nwhere they are known as persistence modules, and where their Betti tables\ndescribe the places at which the homology of filtered simplicial complexes\nchange. Although Betti tables of singly and bigraded modules are already being\nused in applications of topological data analysis, their computation in the\nbigraded case (which relies on an algorithm that is cubic in the size of the\nfiltered simplicial complex) is a bottleneck when working with large datasets.\nWe show that, in the special case of $0$-dimensional homology (which is\nrelevant for clustering and graph classification) the Betti tables of a\nbigraded module can be computed in log-linear time. We also consider the\nproblem of computing minimal presentations, and show that a minimal\npresentation of $0$-dimensional persistent homology can be computed in\nquadratic time, regardless of the grading poset.\n","authors":["Dmitriy Morozov","Luis Scoccola"],"pdf_url":"https://arxiv.org/pdf/2410.22242v1.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.17349v2","updated":"2024-10-29T15:36:31Z","published":"2024-04-26T11:55:01Z","title":"Rectangulotopes","summary":"  Rectangulations are decompositions of a square into finitely many\naxis-aligned rectangles. We describe realizations of $(n-1)$-dimensional\npolytopes associated with two combinatorial families of rectangulations\ncomposed of $n$ rectangles. They are defined as quotientopes of natural lattice\ncongruences on the weak Bruhat order on permutations in $\\mathfrak{S}_n$, and\ntheir skeleta are flip graphs on rectangulations. We give simple vertex and\nfacet descriptions of these polytopes, in particular elementary formulas for\ncomputing the coordinates of the vertex corresponding to each rectangulation,\nin the spirit of J.-L. Loday's realization of the associahedron.\n","authors":["Jean Cardinal","Vincent Pilaud"],"pdf_url":"https://arxiv.org/pdf/2404.17349v2.pdf","comment":"24 pages, 14 figures. Version 2: revisions according to referee's\n  suggestions"},{"id":"http://arxiv.org/abs/2410.22102v1","updated":"2024-10-29T15:00:36Z","published":"2024-10-29T15:00:36Z","title":"Ideal Membership Problem for Boolean Minority and Dual Discriminator","summary":"  We consider the polynomial Ideal Membership Problem (IMP) for ideals encoding\ncombinatorial problems that are instances of CSPs over a finite language. In\nthis paper, the input polynomial $f$ has degree at most $d=O(1)$ (we call this\nproblem IMP$_d$). We bridge the gap in \\cite{MonaldoMastrolilli2019} by proving\nthat the IMP$_d$ for Boolean combinatorial ideals whose constraints are closed\nunder the minority polymorphism can be solved in polynomial time. This\ncompletes the identification of the tractability for the Boolean IMP$_d$. We\nalso prove that the proof of membership for the IMP$_d$ for problems\nconstrained by the dual discriminator polymorphism over any finite domain can\nbe found in polynomial time. Our results can be used in applications such as\nNullstellensatz and Sum-of-Squares proofs.\n","authors":["Arpitha P. Bharathi","Monaldo Mastrolilli"],"pdf_url":"https://arxiv.org/pdf/2410.22102v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2006.16422; text overlap\n  with arXiv:2011.03700 by other authors"},{"id":"http://arxiv.org/abs/2310.01945v4","updated":"2024-10-29T13:14:25Z","published":"2023-10-03T10:43:58Z","title":"Homotopy-Aware Multi-Agent Path Planning on Plane","summary":"  We propose an efficient framework using Dynnikov coordinates for\nhomotopy-aware multi-agent path planning in planar domains that may contain\nobstacles. We developed a method for generating multiple homotopically distinct\nsolutions for the multi-agent path planning problem in planar domains by\ncombining our framework with revised prioritized planning and proved its\ncompleteness under specific assumptions. Experimentally, we demonstrated that\nour method is significantly faster than a method without Dynnikov coordinates.\nWe also confirmed experimentally that homotopy-aware planning contributes to\navoiding locally optimal solutions when searching for low-cost trajectories for\na swarm of agents in a continuous environment.\n","authors":["Kazumi Kasaura"],"pdf_url":"https://arxiv.org/pdf/2310.01945v4.pdf","comment":"17 pages with 5 pages of references and appendices, 19 figures"},{"id":"http://arxiv.org/abs/2401.14060v3","updated":"2024-10-29T10:55:08Z","published":"2024-01-25T10:35:38Z","title":"On Sparse Covers of Minor Free Graphs, Low Dimensional Metric\n  Embeddings, and other applications","summary":"  Given a metric space $(X,d_X)$, a $(\\beta,s,\\Delta)$-sparse cover is a\ncollection of clusters $\\mathcal{C}\\subseteq P(X)$ with diameter at most\n$\\Delta$, such that for every point $x\\in X$, the ball\n$B_X(x,\\frac\\Delta\\beta)$ is fully contained in some cluster $C\\in\n\\mathcal{C}$, and $x$ belongs to at most $s$ clusters in $\\mathcal{C}$. Our\nmain contribution is to show that the shortest path metric of every $K_r$-minor\nfree graphs admits $(O(r),O(r^2),\\Delta)$-sparse cover, and for every\n$\\epsilon>0$, $(4+\\epsilon,O(\\frac1\\epsilon)^r,\\Delta)$-sparse cover (for\narbitrary $\\Delta>0$). We then use this sparse cover to show that every\n$K_r$-minor free graph embeds into\n$\\ell_\\infty^{\\tilde{O}(\\frac1\\epsilon)^{r+1}\\cdot\\log n}$ with distortion\n$3+\\epsilon$ (resp. into $\\ell_\\infty^{\\tilde{O}(r^2)\\cdot\\log n}$ with\ndistortion $O(r)$). Further, among other applications, this sparse cover\nimmediately implies an algorithm for the oblivious buy-at-bulk problem in fixed\nminor free graphs with the tight approximation factor $O(\\log n)$ (previously\nnothing beyond general graphs was known).\n","authors":["Arnold Filtser"],"pdf_url":"https://arxiv.org/pdf/2401.14060v3.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2409.18269v2","updated":"2024-10-29T19:38:12Z","published":"2024-09-26T20:24:22Z","title":"Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling","summary":"  Prophet inequality concerns a basic optimal stopping problem and states that\nsimple threshold stopping policies -- i.e., accepting the first reward larger\nthan a certain threshold -- can achieve tight $\\frac{1}{2}$-approximation to\nthe optimal prophet value. Motivated by its economic applications, this paper\nstudies the robustness of this approximation to natural strategic manipulations\nin which each random reward is associated with a self-interested player who may\nselectively reveal his realized reward to the searcher in order to maximize his\nprobability of being selected.\n  We say a threshold policy is $\\alpha$(-strategically)-robust if it (a)\nachieves the $\\alpha$-approximation to the prophet value for strategic players;\nand (b) meanwhile remains a $\\frac{1}{2}$-approximation in the standard\nnon-strategic setting. Starting with a characterization of each player's\noptimal information revealing strategy, we demonstrate the intrinsic robustness\nof prophet inequalities to strategic reward signaling through the following\nresults: (1) for arbitrary reward distributions, there is a threshold policy\nthat is $\\frac{1-\\frac{1}{e}}{2}$-robust, and this ratio is tight; (2) for\ni.i.d. reward distributions, there is a threshold policy that is\n$\\frac{1}{2}$-robust, which is tight for the setting; and (3) for log-concave\n(but non-identical) reward distributions, the $\\frac{1}{2}$-robustness can also\nbe achieved under certain regularity assumptions.\n","authors":["Wei Tang","Haifeng Xu","Ruimin Zhang","Derek Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.18269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04685v2","updated":"2024-10-29T16:50:30Z","published":"2024-09-07T02:41:53Z","title":"Distributed Agreement in the Arrovian Framework","summary":"  Preference aggregation is a fundamental problem in voting theory, in which\npublic input rankings of a set of alternatives (called preferences) must be\naggregated into a single preference that satisfies certain soundness\nproperties. The celebrated Arrow Impossibility Theorem is equivalent to a\ndistributed task in a synchronous fault-free system that satisfies properties\nsuch as respecting unanimous preferences, maintaining independence of\nirrelevant alternatives (IIA), and non-dictatorship, along with consensus since\nonly one preference can be decided.\n  In this work, we study a weaker distributed task in which crash faults are\nintroduced, IIA is not required, and the consensus property is relaxed to\neither $k$-set agreement or $\\epsilon$-approximate agreement using any metric\non the set of preferences. In particular, we prove several novel impossibility\nresults for both of these tasks in both synchronous and asynchronous\ndistributed systems. We additionally show that the impossibility for our\n$\\epsilon$-approximate agreement task using the Kendall tau or Spearman\nfootrule metrics holds under extremely weak assumptions.\n","authors":["Kenan Wood","Hammurabi Mendes","Jonad Pulaj"],"pdf_url":"https://arxiv.org/pdf/2409.04685v2.pdf","comment":"Accepted for publication in the 2024 International Conference on\n  Principles of Distributed Systems (OPODIS). Improved exposition. No new\n  results"},{"id":"http://arxiv.org/abs/2305.19985v5","updated":"2024-10-29T15:44:03Z","published":"2023-05-31T16:08:10Z","title":"On the Existence of Reactive Strategies Resilient to Delay","summary":"  We compare games under delayed control and delay games, two types of infinite\ngames modelling asynchronicity in reactive synthesis. In games under delayed\ncontrol both players suffer from partial informedness due to symmetrically\ndelayed communication, while in delay games, the protagonist has to grant\nlookahead to the alter player. Our first main result, the interreducibility of\nthe existence of sure winning strategies for the protagonist, allows to\ntransfer known complexity results and bounds on the delay from delay games to\ngames under delayed control, for which no such results had been known. We\nfurthermore analyse existence of randomized strategies that win almost surely,\nwhere this correspondence between the two types of games breaks down. In this\nsetting, some games surely won by the alter player in delay games can now be\nwon almost surely by the protagonist in the corresponding game under delayed\ncontrol, showing that it indeed makes a difference whether the protagonist has\nto grant lookahead or both players suffer from partial informedness. These\nresults get even more pronounced when we finally address the quantitative goal\nof winning with a probability in $[0,1]$. We show that for any rational\nthreshold $\\theta \\in [0,1]$ there is a game that can be won by the protagonist\nwith exactly probability $\\theta$ under delayed control, while being surely won\nby alter in the delay game setting. All these findings refine our original\nresult that games under delayed control are not determined.\n","authors":["Martin Fr√§nzle","Paul Kr√∂ger","Sarah Winter","Martin Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2305.19985v5.pdf","comment":"Full version of arXiv:2310.01010, contains all proofs omitted in the\n  conference version as well as a new section on winning games under delayed\n  control with mixed strategies with respect to a fixed threshold"},{"id":"http://arxiv.org/abs/2410.22144v1","updated":"2024-10-29T15:40:13Z","published":"2024-10-29T15:40:13Z","title":"The equilibrium properties of obvious strategy profiles in games with\n  many players","summary":"  This paper studies the equilibrium properties of the ``obvious strategy\nprofile'' in large finite-player games. Each player in such a strategy profile\nsimply adopts a randomized strategy as she would have used in a symmetric\nequilibrium of an idealized large game. We show that, under a continuity\nassumption, (i) obvious strategy profiles constitute a convergent sequence of\napproximate symmetric equilibria as the number of players tends to infinity,\nand (ii) realizations of such strategy profiles also form a convergent sequence\nof (pure strategy) approximate equilibria with probability approaching one. Our\nfindings offer a solution that is easily implemented without coordination\nissues and is asymptotically optimal for players in large finite games.\nAdditionally, we present a convergence result for approximate symmetric\nequilibria.\n","authors":["Enxian Chen Bin Wu Hanping Xu"],"pdf_url":"https://arxiv.org/pdf/2410.22144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15607v2","updated":"2024-10-29T13:36:37Z","published":"2024-04-24T02:50:37Z","title":"A Note on Approximating Weighted Nash Social Welfare with Additive\n  Valuations","summary":"  We give the first $O(1)$-approximation for the weighted Nash Social Welfare\nproblem with additive valuations. The approximation ratio we obtain is $e^{1/e}\n+ \\epsilon \\approx 1.445 + \\epsilon$, which matches the best known\napproximation ratio for the unweighted case \\cite{BKV18}.\n  Both our algorithm and analysis are simple. We solve a natural configuration\nLP for the problem, and obtain the allocation of items to agents using a\nrandomized version of the Shmoys-Tardos rounding algorithm developed for\nunrelated machine scheduling problems. In the analysis, we show that the\napproximation ratio of the algorithm is at most the worst gap between the Nash\nsocial welfare of the optimum allocation and that of an EF1 allocation, for an\nunweighted Nash Social Welfare instance with identical additive valuations.\nThis was shown to be at most $e^{1/e} \\approx 1.445$ by Barman et al., leading\nto our approximation ratio.\n","authors":["Yuda Feng","Shi Li"],"pdf_url":"https://arxiv.org/pdf/2404.15607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.07892v2","updated":"2024-10-29T11:03:30Z","published":"2021-10-15T07:22:25Z","title":"Combining Counterfactual Regret Minimization with Information Gain to\n  Solve Extensive Games with Unknown Environments","summary":"  Counterfactual regret minimization (CFR) is an effective algorithm for\nsolving extensive games with imperfect information (IIEGs). However, CFR is\nonly allowed to be applied in known environments, where the transition function\nof the chance player and the reward function of the terminal node in IIEGs are\nknown. In uncertain situations, such as reinforcement learning (RL) problems,\nCFR is not applicable. Thus, applying CFR in unknown environments is a\nsignificant challenge that can also address some difficulties in the real\nworld. Currently, advanced solutions require more interactions with the\nenvironment and are limited by large single-sampling variances to narrow the\ngap with the real environment. In this paper, we propose a method that combines\nCFR with information gain to compute the Nash equilibrium (NE) of IIEGs with\nunknown environments. We use a curiosity-driven approach to explore unknown\nenvironments and minimize the discrepancy between uncertain and real\nenvironments. Additionally, by incorporating information into the reward, the\naverage strategy calculated by CFR can be directly implemented as the\ninteraction policy with the environment, thereby improving the exploration\nefficiency of our method in uncertain environments. Through experiments on\nstandard testbeds such as Kuhn poker and Leduc poker, our method significantly\nreduces the number of interactions with the environment compared to the\ndifferent baselines and computes a more accurate approximate NE within the same\nnumber of interaction rounds.\n","authors":["Chen Qiu","Xuan Wang","Tianzi Ma","Yaojun Wen","Jiajia Zhang"],"pdf_url":"https://arxiv.org/pdf/2110.07892v2.pdf","comment":"13 pages, 6 figures; updated some contents"},{"id":"http://arxiv.org/abs/2410.21815v1","updated":"2024-10-29T07:35:33Z","published":"2024-10-29T07:35:33Z","title":"Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box\n  Models","summary":"  The debate between self-interpretable models and post-hoc explanations for\nblack-box models is central to Explainable AI (XAI). Self-interpretable models,\nsuch as concept-based networks, offer insights by connecting decisions to\nhuman-understandable concepts but often struggle with performance and\nscalability. Conversely, post-hoc methods like Shapley values, while\ntheoretically robust, are computationally expensive and resource-intensive. To\nbridge the gap between these two lines of research, we propose a novel method\nthat combines their strengths, providing theoretically guaranteed\nself-interpretability for black-box models without compromising prediction\naccuracy. Specifically, we introduce a parameter-efficient pipeline,\n*AutoGnothi*, which integrates a small side network into the black-box model,\nallowing it to generate Shapley value explanations without changing the\noriginal network parameters. This side-tuning approach significantly reduces\nmemory, training, and inference costs, outperforming traditional\nparameter-efficient methods, where full fine-tuning serves as the optimal\nbaseline. *AutoGnothi* enables the black-box model to predict and explain its\npredictions with minimal overhead. Extensive experiments show that *AutoGnothi*\noffers accurate explanations for both vision and language tasks, delivering\nsuperior computational efficiency with comparable interpretability.\n","authors":["Shaobo Wang","Hongxuan Tang","Mingyang Wang","Hongrui Zhang","Xuyang Liu","Weiya Li","Xuming Hu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21636v1","updated":"2024-10-29T00:45:43Z","published":"2024-10-29T00:45:43Z","title":"Convergence of $\\text{log}(1/Œµ)$ for Gradient-Based Algorithms in\n  Zero-Sum Games without the Condition Number: A Smoothed Analysis","summary":"  Gradient-based algorithms have shown great promise in solving large\n(two-player) zero-sum games. However, their success has been mostly confined to\nthe low-precision regime since the number of iterations grows polynomially in\n$1/\\epsilon$, where $\\epsilon > 0$ is the duality gap. While it has been\nwell-documented that linear convergence -- an iteration complexity scaling as\n$\\textsf{log}(1/\\epsilon)$ -- can be attained even with gradient-based\nalgorithms, that comes at the cost of introducing a dependency on certain\ncondition number-like quantities which can be exponentially large in the\ndescription of the game.\n  To address this shortcoming, we examine the iteration complexity of several\ngradient-based algorithms in the celebrated framework of smoothed analysis, and\nwe show that they have polynomial smoothed complexity, in that their number of\niterations grows as a polynomial in the dimensions of the game,\n$\\textsf{log}(1/\\epsilon)$, and $1/\\sigma$, where $\\sigma$ measures the\nmagnitude of the smoothing perturbation. Our result applies to optimistic\ngradient and extra-gradient descent/ascent, as well as a certain iterative\nvariant of Nesterov's smoothing technique. From a technical standpoint, the\nproof proceeds by characterizing and performing a smoothed analysis of a\ncertain error bound, the key ingredient driving linear convergence in zero-sum\ngames. En route, our characterization also makes a natural connection between\nthe convergence rate of such algorithms and perturbation-stability properties\nof the equilibrium, which is of interest beyond the model of smoothed\ncomplexity.\n","authors":["Ioannis Anagnostides","Tuomas Sandholm"],"pdf_url":"https://arxiv.org/pdf/2410.21636v1.pdf","comment":"To appear at NeurIPS 2024"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2408.04744v2","updated":"2024-10-29T19:08:44Z","published":"2024-08-08T20:05:34Z","title":"Noise-augmented Chaotic Ising Machines for Combinatorial Optimization\n  and Sampling","summary":"  Ising machines, hardware accelerators for combinatorial optimization and\nprobabilistic sampling problems, have gained significant interest recently. A\nkey element is stochasticity, which enables a wide exploration of\nconfigurations, thereby helping avoid local minima. Here, we refine the\npreviously proposed concept of coupled chaotic bits (c-bits) that operate\nwithout explicit stochasticity. We show that augmenting chaotic bits with\nstochasticity enhances performance in combinatorial optimization, achieving\nalgorithmic scaling comparable to probabilistic bits (p-bits). We first\ndemonstrate that c-bits follow the quantum Boltzmann law in a 1D transverse\nfield Ising model. We then show that c-bits exhibit critical dynamics similar\nto stochastic p-bits in 2D Ising and 3D spin glass models, with promising\npotential to solve challenging optimization problems. Finally, we propose a\nnoise-augmented version of coupled c-bits via the adaptive parallel tempering\nalgorithm (APT). Our noise-augmented c-bit algorithm outperforms fully\ndeterministic c-bits running versions of the simulated annealing algorithm.\nOther analog Ising machines with coupled oscillators could draw inspiration\nfrom the proposed algorithm. Running replicas at constant temperature\neliminates the need for global modulation of coupling strengths. Mixing\nstochasticity with deterministic c-bits creates a powerful hybrid computing\nscheme that can bring benefits in scaled, asynchronous, and massively parallel\nhardware implementations.\n","authors":["Kyle Lee","Shuvro Chowdhury","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2408.04744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11298v2","updated":"2024-10-29T04:39:50Z","published":"2024-10-15T05:37:16Z","title":"Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs\n  on Compute-in-Memory Crossbars","summary":"  We introduce $\\textit{sorted weight sectioning}$ (SWS): a weight allocation\nalgorithm that places sorted deep neural network (DNN) weight sections on\nbit-sliced compute-in-memory (CIM) crossbars to reduce analog-to-digital\nconverter (ADC) energy consumption. Data conversions are the most\nenergy-intensive process in crossbar operation. SWS effectively reduces this\ncost leveraging (1) small weights and (2) zero weights (weight sparsity).\n  DNN weights follow bell-shaped distributions, with most weights near zero.\nUsing SWS, we only need low-order crossbar columns for sections with\nlow-magnitude weights. This reduces the quantity and resolution of ADCs used,\nexponentially decreasing ADC energy costs without significantly degrading DNN\naccuracy.\n  Unstructured sparsification further sharpens the weight distribution with\nsmall accuracy loss. However, it presents challenges in hardware tracking of\nzeros: we cannot switch zero rows to other layer weights in unsorted crossbars\nwithout index matching. SWS efficiently addresses unstructured sparse models\nusing offline remapping of zeros into earlier sections, which reveals full\nsparsity potential and maximizes energy efficiency.\n  Our method reduces ADC energy use by 89.5% on unstructured sparse BERT\nmodels. Overall, this paper introduces a novel algorithm to promote\nenergy-efficient CIM crossbars for unstructured sparse DNN workloads.\n","authors":["Matheus Farias","H. T. Kung"],"pdf_url":"https://arxiv.org/pdf/2410.11298v2.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.21730v1","updated":"2024-10-29T04:34:02Z","published":"2024-10-29T04:34:02Z","title":"Efficient Reprogramming of Memristive Crossbars for DNNs: Weight Sorting\n  and Bit Stucking","summary":"  We introduce a novel approach to reduce the number of times required for\nreprogramming memristors on bit-sliced compute-in-memory crossbars for deep\nneural networks (DNNs). Our idea addresses the limited non-volatile memory\nendurance, which restrict the number of times they can be reprogrammed.\n  To reduce reprogramming demands, we employ two techniques: (1) we organize\nweights into sorted sections to schedule reprogramming of similar crossbars,\nmaximizing memristor state reuse, and (2) we reprogram only a fraction of\nrandomly selected memristors in low-order columns, leveraging their bit-level\ndistribution and recognizing their relatively small impact on model accuracy.\n  We evaluate our approach for state-of-the-art models on the ImageNet-1K\ndataset. We demonstrate a substantial reduction in crossbar reprogramming by\n3.7x for ResNet-50 and 21x for ViT-Base, while maintaining model accuracy\nwithin a 1% margin.\n","authors":["Matheus Farias","H. T. Kung"],"pdf_url":"https://arxiv.org/pdf/2410.21730v1.pdf","comment":"5 pages, 10 figures"}],"Graphics":[{"id":"http://arxiv.org/abs/2405.19296v2","updated":"2024-10-29T23:55:17Z","published":"2024-05-29T17:24:25Z","title":"Neural Isometries: Taming Transformations for Equivariant ML","summary":"  Real-world geometry and 3D vision tasks are replete with challenging\nsymmetries that defy tractable analytical expression. In this paper, we\nintroduce Neural Isometries, an autoencoder framework which learns to map the\nobservation space to a general-purpose latent space wherein encodings are\nrelated by isometries whenever their corresponding observations are\ngeometrically related in world space. Specifically, we regularize the latent\nspace such that maps between encodings preserve a learned inner product and\ncommute with a learned functional operator, in the same manner as rigid-body\ntransformations commute with the Laplacian. This approach forms an effective\nbackbone for self-supervised representation learning, and we demonstrate that a\nsimple off-the-shelf equivariant network operating in the pre-trained latent\nspace can achieve results on par with meticulously-engineered, handcrafted\nnetworks designed to handle complex, nonlinear symmetries. Furthermore,\nisometric maps capture information about the respective transformations in\nworld space, and we show that this allows us to regress camera poses directly\nfrom the coefficients of the maps between encodings of adjacent views of a\nscene.\n","authors":["Thomas W. Mitchel","Michael Taylor","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2405.19296v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.12954v2","updated":"2024-10-29T18:31:39Z","published":"2024-09-19T17:58:44Z","title":"GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled\n  Appearance and Geometry Modeling","summary":"  Gaussian splatting has demonstrated excellent performance for view synthesis\nand scene reconstruction. The representation achieves photorealistic quality by\noptimizing the position, scale, color, and opacity of thousands to millions of\n2D or 3D Gaussian primitives within a scene. However, since each Gaussian\nprimitive encodes both appearance and geometry, these attributes are strongly\ncoupled--thus, high-fidelity appearance modeling requires a large number of\nGaussian primitives, even when the scene geometry is simple (e.g., for a\ntextured planar surface). We propose to texture each 2D Gaussian primitive so\nthat even a single Gaussian can be used to capture appearance details. By\nemploying per-primitive texturing, our appearance representation is agnostic to\nthe topology and complexity of the scene's geometry. We show that our approach,\nGStex, yields improved visual quality over prior work in texturing Gaussian\nsplats. Furthermore, we demonstrate that our decoupling enables improved novel\nview synthesis performance compared to 2D Gaussian splatting when reducing the\nnumber of Gaussian primitives, and that GStex can be used for scene appearance\nediting and re-texturing.\n","authors":["Victor Rong","Jingxiang Chen","Sherwin Bahmani","Kiriakos N. Kutulakos","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2409.12954v2.pdf","comment":"Project page: https://lessvrong.com/cs/gstex. Updated Oct. 29 to\n  correct Table 1 numbers. Please see\n  https://github.com/victor-rong/GStex?tab=readme-ov-file#errata for details"},{"id":"http://arxiv.org/abs/2410.05038v2","updated":"2024-10-29T17:39:22Z","published":"2024-10-07T13:50:15Z","title":"GARField: Addressing the visual Sim-to-Real gap in garment manipulation\n  with mesh-attached radiance fields","summary":"  While humans intuitively manipulate garments and other textile items swiftly\nand accurately, it is a significant challenge for robots. A factor crucial to\nhuman performance is the ability to imagine, a priori, the intended result of\nthe manipulation intents and hence develop predictions on the garment pose.\nThat ability allows us to plan from highly obstructed states, adapt our plans\nas we collect more information and react swiftly to unforeseen circumstances.\nConversely, robots struggle to establish such intuitions and form tight links\nbetween plans and observations. We can partly attribute this to the high cost\nof obtaining densely labelled data for textile manipulation, both in quality\nand quantity. The problem of data collection is a long-standing issue in\ndata-based approaches to garment manipulation. As of today, generating\nhigh-quality and labelled garment manipulation data is mainly attempted through\nadvanced data capture procedures that create simplified state estimations from\nreal-world observations. However, this work proposes a novel approach to the\nproblem by generating real-world observations from object states. To achieve\nthis, we present GARField (Garment Attached Radiance Field), the first\ndifferentiable rendering architecture, to our knowledge, for data generation\nfrom simulated states stored as triangle meshes. Code is available on\nhttps://ddonatien.github.io/garfield-website/\n","authors":["Donatien Delehelle","Darwin G. Caldwell","Fei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05038v2.pdf","comment":"Project site: https://ddonatien.github.io/garfield-website/"},{"id":"http://arxiv.org/abs/2407.04237v4","updated":"2024-10-29T16:26:12Z","published":"2024-07-05T03:43:08Z","title":"GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction","summary":"  We present GSD, a diffusion model approach based on Gaussian Splatting (GS)\nrepresentation for 3D object reconstruction from a single view. Prior works\nsuffer from inconsistent 3D geometry or mediocre rendering quality due to\nimproper representations. We take a step towards resolving these shortcomings\nby utilizing the recent state-of-the-art 3D explicit representation, Gaussian\nSplatting, and an unconditional diffusion model. This model learns to generate\n3D objects represented by sets of GS ellipsoids. With these strong generative\n3D priors, though learning unconditionally, the diffusion model is ready for\nview-guided reconstruction without further model fine-tuning. This is achieved\nby propagating fine-grained 2D features through the efficient yet flexible\nsplatting function and the guided denoising sampling process. In addition, a 2D\ndiffusion model is further employed to enhance rendering fidelity, and improve\nreconstructed GS quality by polishing and re-using the rendered images. The\nfinal reconstructed objects explicitly come with high-quality 3D structure and\ntexture, and can be efficiently rendered in arbitrary views. Experiments on the\nchallenging real-world CO3D dataset demonstrate the superiority of our\napproach. Project page: https://yxmu.foo/GSD/\n","authors":["Yuxuan Mu","Xinxin Zuo","Chuan Guo","Yilin Wang","Juwei Lu","Xiaofeng Wu","Songcen Xu","Peng Dai","Youliang Yan","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.04237v4.pdf","comment":"ECCV 2024"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2405.17837v3","updated":"2024-10-29T22:28:31Z","published":"2024-05-28T05:21:09Z","title":"Enabling Generative Design Tools with LLM Agents for Mechanical\n  Computation Devices: A Case Study","summary":"  In the field of Human-Computer Interaction (HCI), interactive devices with\nembedded mechanical computation are gaining attention. The rise of these\ncutting-edge devices has created a need for specialized design tools that\ndemocratize the prototyping process. While current tools streamline prototyping\nthrough parametric design and simulation, they often come with a steep learning\ncurve and may not fully support creative ideation. In this study, we use\nfluidic computation interfaces as a case study to explore how design tools for\nsuch devices can be augmented by Large Language Model agents (LLMs). Integrated\nwith LLMs, the Generative Design Tool (GDT) better understands the capabilities\nand limitations of new technologies, proposes diverse and practical\napplications, and suggests designs that are technically and contextually\nappropriate. Additionally, it generates design parameters for visualizing\nresults and producing fabrication-ready support files. This paper details the\nGDT's framework, implementation, and performance while addressing its potential\nand challenges.\n","authors":["Qiuyu Lu","Jiawei Fang","Zhihao Yao","Yue Yang","Shiqing Lyu","Haipeng Mi","Lining Yao"],"pdf_url":"https://arxiv.org/pdf/2405.17837v3.pdf","comment":"38 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.22526v1","updated":"2024-10-29T20:43:18Z","published":"2024-10-29T20:43:18Z","title":"From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems","summary":"  To effectively address potential harms from AI systems, it is essential to\nidentify and mitigate system-level hazards. Current analysis approaches focus\non individual components of an AI system, like training data or models, in\nisolation, overlooking hazards from component interactions or how they are\nsituated within a company's development process. To this end, we draw from the\nestablished field of system safety, which considers safety as an emergent\nproperty of the entire system, not just its components. In this work, we\ntranslate System Theoretic Process Analysis (STPA) - a recognized system safety\nframework - for analyzing AI operation and development processes. We focus on\nsystems that rely on machine learning algorithms and conducted STPA on three\ncase studies involving linear regression, reinforcement learning, and\ntransformer-based generative models. Our analysis explored how STPA's control\nand system-theoretic perspectives apply to AI systems and whether unique AI\ntraits - such as model opacity, capability uncertainty, and output complexity -\nnecessitate significant modifications to the framework. We find that the key\nconcepts and steps of conducting an STPA readily apply, albeit with a few\nadaptations tailored for AI systems. We present the Process-oriented Hazard\nAnalysis for AI Systems (PHASE) as a guideline that adapts STPA concepts for\nAI, making STPA-based hazard analysis more accessible. PHASE enables four key\naffordances for analysts responsible for managing AI system harms: 1) detection\nof hazards at the systems level, including those from accumulation of disparate\nissues; 2) explicit acknowledgment of social factors contributing to\nexperiences of algorithmic harms; 3) creation of traceable accountability\nchains between harms and those who can mitigate the harm; and 4) ongoing\nmonitoring and mitigation of new hazards.\n","authors":["Shalaleh Rismani","Roel Dobbe","AJung Moon"],"pdf_url":"https://arxiv.org/pdf/2410.22526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22324v1","updated":"2024-10-29T17:57:33Z","published":"2024-10-29T17:57:33Z","title":"Assessing User Needs in Non-Visual Text Input: Perceptions of Blind\n  Adults on Current and Experimental Mobile Interfaces","summary":"  Text input on mobile devices without physical key boundaries can be\nchallenging for people who are blind or low-vision. We interview 12 blind\nadults about their experiences with mobile text input to provide insight into\nwhich research direction may be the most beneficial. We identify three primary\nthemes that were experiences or opinions shared by many of our participants:\nthe poor accuracy of dictation, difficulty entering text in noisy environments,\nand difficulty correcting errors in entered text. We discuss an experimental\nnon-visual text input method with each participant to solicit opinions and find\nthat the largest concern is the time it would take to learn the technique. We\nfind that the majority of our participants do not use word predictions while\nperforming text input with an onscreen keyboard, finding it faster and easier\nto finish typing each word manually.\n","authors":["Dylan Gaines","Keith Vertanen"],"pdf_url":"https://arxiv.org/pdf/2410.22324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22312v1","updated":"2024-10-29T17:53:33Z","published":"2024-10-29T17:53:33Z","title":"Effective Guidance for Model Attention with Simple Yes-no Annotations","summary":"  Modern deep learning models often make predictions by focusing on irrelevant\nareas, leading to biased performance and limited generalization. Existing\nmethods aimed at rectifying model attention require explicit labels for\nirrelevant areas or complex pixel-wise ground truth attention maps. We present\nCRAYON (Correcting Reasoning with Annotations of Yes Or No), offering\neffective, scalable, and practical solutions to rectify model attention using\nsimple yes-no annotations. CRAYON empowers classical and modern model\ninterpretation techniques to identify and guide model reasoning:\nCRAYON-ATTENTION directs classic interpretations based on saliency maps to\nfocus on relevant image regions, while CRAYON-PRUNING removes irrelevant\nneurons identified by modern concept-based methods to mitigate their influence.\nThrough extensive experiments with both quantitative and human evaluation, we\nshowcase CRAYON's effectiveness, scalability, and practicality in refining\nmodel attention. CRAYON achieves state-of-the-art performance, outperforming 12\nmethods across 3 benchmark datasets, surpassing approaches that require more\ncomplex annotations.\n","authors":["Seongmin Lee","Ali Payani","Duen Horng"," Chau"],"pdf_url":"https://arxiv.org/pdf/2410.22312v1.pdf","comment":"10 pages, 5 figures, IEEE BigData 2024 Paper"},{"id":"http://arxiv.org/abs/2407.03177v2","updated":"2024-10-29T17:20:33Z","published":"2024-07-03T14:57:22Z","title":"A Spatial-Spectral and Temporal Dual Prototype Network for Motor Imagery\n  Brain-Computer Interface","summary":"  Motor imagery electroencephalogram (MI-EEG) decoding plays a crucial role in\ndeveloping motor imagery brain-computer interfaces (MI-BCIs). However, decoding\nintentions from MI remains challenging due to the inherent complexity of EEG\nsignals relative to the small-sample size. To address this issue, we propose a\nspatial-spectral and temporal dual prototype network (SST-DPN). First, we\ndesign a lightweight attention mechanism to uniformly model the\nspatial-spectral relationships across multiple EEG electrodes, enabling the\nextraction of powerful spatial-spectral features. Then, we develop a\nmulti-scale variance pooling module tailored for EEG signals to capture\nlong-term temporal features. This module is parameter-free and computationally\nefficient, offering clear advantages over the widely used transformer models.\nFurthermore, we introduce dual prototype learning to optimize the feature space\ndistribution and training process, thereby improving the model's generalization\nability on small-sample MI datasets. Our experimental results show that the\nSST-DPN outperforms state-of-the-art models with superior classification\naccuracy (84.11% for dataset BCI4-2A, 86.65% for dataset BCI4-2B).\nAdditionally, we use the BCI3-4A dataset with fewer training data to further\nvalidate the generalization ability of the proposed SST-DPN. We also achieve\nsuperior performance with 82.03% classification accuracy. Benefiting from the\nlightweight parameters and superior decoding accuracy, our SST-DPN shows great\npotential for practical MI-BCI applications. The code is publicly available at\nhttps://github.com/hancan16/SST-DPN.\n","authors":["Can Han","Chen Liu","Yaqi Wang","Crystal Cai","Jun Wang","Dahong Qian"],"pdf_url":"https://arxiv.org/pdf/2407.03177v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10492v2","updated":"2024-10-29T17:18:31Z","published":"2024-08-20T02:22:27Z","title":"Is the Lecture Engaging for Learning? Lecture Voice Sentiment Analysis\n  for Knowledge Graph-Supported Intelligent Lecturing Assistant (ILA) System","summary":"  This paper introduces an intelligent lecturing assistant (ILA) system that\nutilizes a knowledge graph to represent course content and optimal pedagogical\nstrategies. The system is designed to support instructors in enhancing student\nlearning through real-time analysis of voice, content, and teaching methods. As\nan initial investigation, we present a case study on lecture voice sentiment\nanalysis, in which we developed a training set comprising over 3,000 one-minute\nlecture voice clips. Each clip was manually labeled as either engaging or\nnon-engaging. Utilizing this dataset, we constructed and evaluated several\nclassification models based on a variety of features extracted from the voice\nclips. The results demonstrate promising performance, achieving an F1-score of\n90% for boring lectures on an independent set of over 800 test voice clips.\nThis case study lays the groundwork for the development of a more sophisticated\nmodel that will integrate content analysis and pedagogical practices. Our\nultimate goal is to aid instructors in teaching more engagingly and effectively\nby leveraging modern artificial intelligence techniques.\n","authors":["Yuan An","Samarth Kolanupaka","Jacob An","Matthew Ma","Unnat Chhatwal","Alex Kalinowski","Michelle Rogers","Brian Smith"],"pdf_url":"https://arxiv.org/pdf/2408.10492v2.pdf","comment":"Accepted in the 4th Workshop on Knowledge Graphs and Big Data @ IEEE\n  Big Data Conference 2024"},{"id":"http://arxiv.org/abs/2410.22203v1","updated":"2024-10-29T16:37:01Z","published":"2024-10-29T16:37:01Z","title":"Democratizing Reward Design for Personal and Representative\n  Value-Alignment","summary":"  Aligning AI agents with human values is challenging due to diverse and\nsubjective notions of values. Standard alignment methods often aggregate crowd\nfeedback, which can result in the suppression of unique or minority\npreferences. We introduce Interactive-Reflective Dialogue Alignment, a method\nthat iteratively engages users in reflecting on and specifying their subjective\nvalue definitions. This system learns individual value definitions through\nlanguage-model-based preference elicitation and constructs personalized reward\nmodels that can be used to align AI behaviour. We evaluated our system through\ntwo studies with 30 participants, one focusing on \"respect\" and the other on\nethical decision-making in autonomous vehicles. Our findings demonstrate\ndiverse definitions of value-aligned behaviour and show that our system can\naccurately capture each person's unique understanding. This approach enables\npersonalized alignment and can inform more representative and interpretable\ncollective alignment strategies.\n","authors":["Carter Blair","Kate Larson","Edith Law"],"pdf_url":"https://arxiv.org/pdf/2410.22203v1.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.22177v1","updated":"2024-10-29T16:15:59Z","published":"2024-10-29T16:15:59Z","title":"Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes","summary":"  As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments.\n","authors":["Junlong Chen","Jens Grubert","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2410.22177v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.19072v2","updated":"2024-10-29T15:44:46Z","published":"2024-10-24T18:28:06Z","title":"Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation\n  Scenario","summary":"  The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is\npromising, but existing implementations overlook key insights from human-robot\ninteraction (HRI) research. Studies on human-machine teams demonstrate that\nhuman perceptions of a machine teammate are critical in governing the machine's\nperformance. Here, we present a mixed factorial design to assess human\nperceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants\nwere assigned to the role of casualty (CAS) or bystander (BYS) and subjected to\nthree within-subjects conditions based on the MEDEVAC robot's operating mode:\nautonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each\ntrial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and\ntransporting them to an ambulance exchange point while avoiding an idle\nbystander. Following each trial, subjects completed a questionnaire measuring\ntheir emotional states, perceived safety, and social compatibility with the\nrobot. Results indicate a consistent main effect of operating mode on reported\nemotional states and perceived safety. Pairwise analyses suggest that the\nemployment of the AF operating mode negatively impacted perceptions along these\ndimensions. There were no persistent differences between casualty and bystander\nresponses.\n","authors":["Tyson Jordan","Pranav Pandey","Prashant Doshi","Ramviyas Parasuraman","Adam Goodie"],"pdf_url":"https://arxiv.org/pdf/2410.19072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22142v1","updated":"2024-10-29T15:39:45Z","published":"2024-10-29T15:39:45Z","title":"A Data-Driven Analysis of the Sovereign Citizens Movement on Telegram","summary":"  Online communities of known extremist groups like the alt-right and QAnon\nhave been well explored in past work. However, we find that an extremist group\ncalled Sovereign Citizens is relatively unexplored despite its existence since\nthe 1970s. Their main belief is delegitimizing the established government with\na tactic called paper terrorism, clogging courts with pseudolegal claims. In\nrecent years, their activities have escalated to threats like forcefully\nclaiming property ownership and participating in the Capitol Riot. This paper\naims to shed light on Sovereign Citizens' online activities by examining two\nTelegram channels, each belonging to an identified Sovereign Citizen\nindividual. We collect over 888K text messages and apply NLP techniques. We\nfind that the two channels differ in the topics they discussed, demonstrating\ndifferent focuses. Further, the two channels exhibit less toxic content\ncompared to other extremist groups like QAnon. Finally, we find indications of\noverlapping beliefs between the two channels and QAnon, suggesting a merging or\ncomplementing of beliefs.\n","authors":["Satrio Yudhoatmojo","Utkucan Balci","Jeremy Blackburn"],"pdf_url":"https://arxiv.org/pdf/2410.22142v1.pdf","comment":"11 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2405.06087v2","updated":"2024-10-29T14:45:26Z","published":"2024-05-09T20:23:15Z","title":"When combinations of humans and AI are useful: A systematic review and\n  meta-analysis","summary":"  Inspired by the increasing use of AI to augment humans, researchers have\nstudied human-AI systems involving different tasks, systems, and populations.\nDespite such a large body of work, we lack a broad conceptual understanding of\nwhen combinations of humans and AI are better than either alone. Here, we\naddressed this question by conducting a meta-analysis of over 100 recent\nexperimental studies reporting over 300 effect sizes. First, we found that, on\naverage, human-AI combinations performed significantly worse than the best of\nhumans or AI alone. Second, we found performance losses in tasks that involved\nmaking decisions and significantly greater gains in tasks that involved\ncreating content. Finally, when humans outperformed AI alone, we found\nperformance gains in the combination, but when the AI outperformed humans alone\nwe found losses. These findings highlight the heterogeneity of the effects of\nhuman-AI collaboration and point to promising avenues for improving human-AI\nsystems.\n","authors":["Michelle Vaccaro","Abdullah Almaatouq","Thomas Malone"],"pdf_url":"https://arxiv.org/pdf/2405.06087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22076v1","updated":"2024-10-29T14:34:41Z","published":"2024-10-29T14:34:41Z","title":"USpeech: Ultrasound-Enhanced Speech with Minimal Human Effort via\n  Cross-Modal Synthesis","summary":"  Speech enhancement is crucial in human-computer interaction, especially for\nubiquitous devices. Ultrasound-based speech enhancement has emerged as an\nattractive choice because of its superior ubiquity and performance. However,\ninevitable interference from unexpected and unintended sources during\naudio-ultrasound data acquisition makes existing solutions rely heavily on\nhuman effort for data collection and processing. This leads to significant data\nscarcity that limits the full potential of ultrasound-based speech enhancement.\nTo address this, we propose USpeech, a cross-modal ultrasound synthesis\nframework for speech enhancement with minimal human effort. At its core is a\ntwo-stage framework that establishes correspondence between visual and\nultrasonic modalities by leveraging audible audio as a bridge. This approach\novercomes challenges from the lack of paired video-ultrasound datasets and the\ninherent heterogeneity between video and ultrasound data. Our framework\nincorporates contrastive video-audio pre-training to project modalities into a\nshared semantic space and employs an audio-ultrasound encoder-decoder for\nultrasound synthesis. We then present a speech enhancement network that\nenhances speech in the time-frequency domain and recovers the clean speech\nwaveform via a neural vocoder. Comprehensive experiments show USpeech achieves\nremarkable performance using synthetic ultrasound data comparable to physical\ndata, significantly outperforming state-of-the-art ultrasound-based speech\nenhancement baselines. USpeech is open-sourced at\nhttps://github.com/aiot-lab/USpeech/.\n","authors":["Luca Jiang-Tao Yu","Running Zhao","Sijie Ji","Edith C. H. Ngai","Chenshu Wu"],"pdf_url":"https://arxiv.org/pdf/2410.22076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21944v1","updated":"2024-10-29T11:04:34Z","published":"2024-10-29T11:04:34Z","title":"Evaluating Perceptual Deviations in Video See-Through Head-Mounted\n  Displays while Utilizing Physical Touchscreens","summary":"  Extended reality technology has become a useful tool in many applications,\nbut still suffers from visual deviations that can hamper the utility of the\ntechnology. This paper discusses the types of persisting visual deviations\nexperienced when observing the natural world through video see-through\nhead-mounted displays. A generalizable method to measure the effect of these\ndeviations on real-world interaction is designed and used in a\nhuman-in-the-loop experiment. The experiment compared video see-through sight\nthrough an head-mounted display with normal eyesight in a static set-up,\nfocusing on (camera) lens distortions and display deviations. Participants\ninteracted with a real touchscreen, locating the position of flashed markers\nshortly after disappearance comparing both conditions to check for deviations\nin position and time. Results show significant larger mean distance errors\nbetween the interaction locations and the original marker positions for video\nsee-through compared to normal eyesight. Moreover, errors increase towards the\nscreen periphery. No significant distance error improvement over time was\nfound, however, response times did significantly decrease for both types of\nsight.\n","authors":["Rudy De-Xin de Lange","Roemer Martin Bien Bakker","Tanja Johanna Juliana Bos"],"pdf_url":"https://arxiv.org/pdf/2410.21944v1.pdf","comment":"10 pages. Preprint. A shortened 4-page version of this paper was\n  accepted to the IEEE ISMAR2024 poster track"},{"id":"http://arxiv.org/abs/2410.21894v1","updated":"2024-10-29T09:37:10Z","published":"2024-10-29T09:37:10Z","title":"Effects of Human Avatar Representation in Virtual Reality on Inter-Brain\n  Connection","summary":"  Increasing advances in affordable consumer hardware and accessible software\nframeworks are now bringing Virtual Reality (VR) to the masses. Especially\ncollaborative VR applications where different people can work together are\ngaining momentum. In this context, human avatars and their representations are\na crucial aspect of collaborative VR applications as they represent a digital\ntwin of the end-users and determine how one is perceived in a virtual\nenvironment. When it comes to the effect of avatar representation on the\nend-users of collaborative VR applications, so far mostly questionnaires have\nbeen used to assess the quality of avatar representations. A promising\nalternative to objectively measure the effect of avatar representation is the\ninvestigation of inter-brain connections during the usage of a collaborative VR\napplication. However, the combination of immersive VR applications and\ninter-brain connections has not been fully researched yet. Thus, our work\ninvestigates how different human avatar representations (real (RL), full-body\n(FB), and head-hand (HH)) affect inter-brain connections. For this purpose, we\nhave designed and conducted a hyperscanning study with eight pairs. The main\nresults of our hyperscanning study show that the number of significant sensor\npairs was the highest in the RL, medium in the FB, and lowest in the HH\ncondition indicating that an avatar that looks more like a real human enables\nmore significant sensor pairs to appear in an EEG analysis.\n","authors":["Enes Yigitbas","Christian Kaltschmidt"],"pdf_url":"https://arxiv.org/pdf/2410.21894v1.pdf","comment":"Paper Preprint, accepted at the 8th International Conference on\n  Artificial Intelligence and Virtual Reality (AIVR 24)"},{"id":"http://arxiv.org/abs/2410.21827v1","updated":"2024-10-29T07:51:08Z","published":"2024-10-29T07:51:08Z","title":"Cross-Domain Transfer Learning Method for Thermal Adaptive Behavior\n  Recognition with WiFi","summary":"  A reliable comfort model is essential to improve occupant satisfaction and\nreduce building energy consumption. As two types of the most common and\nintuitive thermal adaptive behaviors, precise recognition of dressing and\nundressing can effectively support thermal comfort prediction. However,\ntraditional activity recognition suffers from shortcomings in privacy, cost,\nand performance. To address the above issues, this study proposes a\ncross-domain transfer learning method for human dressing and undressing\nadaptive behavior recognition with WiFi. First, we determine the activity\ninterval by calculating the sliding variance for denoised WiFi signals.\nSubsequently, short-time Fourier transform and discrete wavelet transform are\nperformed to extract action information on the basis of time-frequency\nanalysis. Ultimately, an efficient 1D CNN pre-trained model is integrated with\nthe SVM algorithm as a hybrid model to enhance the identification robustness in\nnew scenarios. Experiment results show that the hybrid model based on transfer\nlearning provides a more accurate prediction for the adaptative behavior of\ntarget subjects, achieving 96.9% and 94.9% accuracy in two cases, respectively.\n","authors":["Zhaohe Lv","Guoliang Zhao","Zhanbo Xu","Jiang Wu","Yadong Zhou","Kun Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12105v2","updated":"2024-10-29T03:24:13Z","published":"2024-07-16T18:23:10Z","title":"AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing\n  Collision Avoidance in UAV Teleoperation","summary":"  Haptic feedback enhances collision avoidance by providing directional\nobstacle information to operators during unmanned aerial vehicle (UAV)\nteleoperation. However, such feedback is often rendered via haptic joysticks,\nwhich are unfamiliar to UAV operators and limited to single direction force\nfeedback. Additionally, the direct coupling between the input device and the\nfeedback method diminishes an operators' sense of control and causes\noscillatory movements. To overcome these limitations, we propose AeroHaptix, a\nwearable haptic feedback system that uses spatial vibrations to communicate\nmultiple obstacle directions to operators simultaneously, without interfering\nthe input control. The layout of vibrotactile actuators was determined via a\nperceptual study to eliminate perceptual biases and achieve uniform spatial\ncoverage. A novel rendering algorithm, MultiCBF, extends control barrier\nfunctions to support multi-directional feedback. Our system evaluation showed\nthat compared to the baseline condition, AeroHaptix effectively reduced the\nnumber of collisions and input disagreement. Additionally, operators reported\nthat AeroHaptix was more helpful than the force feedback method, with\ncomparable workload and situational awareness.\n","authors":["Bingjian Huang","Zhecheng Wang","Qilong Cheng","Siyi Ren","Hanfeng Cai","Antonio Alvarez Valdivia","Karthik Mahadevan","Daniel Wigdor"],"pdf_url":"https://arxiv.org/pdf/2407.12105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21659v1","updated":"2024-10-29T01:57:49Z","published":"2024-10-29T01:57:49Z","title":"\"The Guide Has Your Back\": Exploring How Sighted Guides Can Enhance\n  Accessibility in Social Virtual Reality for Blind and Low Vision People","summary":"  As social VR applications grow in popularity, blind and low vision users\nencounter continued accessibility barriers. Yet social VR, which enables\nmultiple people to engage in the same virtual space, presents a unique\nopportunity to allow other people to support a user's access needs. To explore\nthis opportunity, we designed a framework based on physical sighted guidance\nthat enables a guide to support a blind or low vision user with navigation and\nvisual interpretation. A user can virtually hold on to their guide and move\nwith them, while the guide can describe the environment. We studied the use of\nour framework with 16 blind and low vision participants and found that they had\na wide range of preferences. For example, we found that participants wanted to\nuse their guide to support social interactions and establish a human connection\nwith a human-appearing guide. We also highlight opportunities for novel\nguidance abilities in VR, such as dynamically altering an inaccessible\nenvironment. Through this work, we open a novel design space for a versatile\napproach for making VR fully accessible.\n","authors":["Jazmin Collins","Crescentia Jung","Yeonju Jang","Danielle Montour","Andrea Stevenson Won","Shiri Azenkot"],"pdf_url":"https://arxiv.org/pdf/2410.21659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21652v1","updated":"2024-10-29T01:42:27Z","published":"2024-10-29T01:42:27Z","title":"Accessible Nonverbal Cues to Support Conversations in VR for Blind and\n  Low Vision People","summary":"  Social VR has increased in popularity due to its affordances for rich,\nembodied, and nonverbal communication. However, nonverbal communication remains\ninaccessible for blind and low vision people in social VR. We designed\naccessible cues with audio and haptics to represent three nonverbal behaviors:\neye contact, head shaking, and head nodding. We evaluated these cues in\nreal-time conversation tasks where 16 blind and low vision participants\nconversed with two other users in VR. We found that the cues were effective in\nsupporting conversations in VR. Participants had statistically significantly\nhigher scores for accuracy and confidence in detecting attention during\nconversations with the cues than without. We also found that participants had a\nrange of preferences and uses for the cues, such as learning social norms. We\npresent design implications for handling additional cues in the future, such as\nthe challenges of incorporating AI. Through this work, we take a step towards\nmaking interpersonal embodied interactions in VR fully accessible for blind and\nlow vision people.\n","authors":["Crescentia Jung","Jazmin Collins","Ricardo E. Gonzalez Penuela","Jonathan Isaac Segal","Andrea Stevenson Won","Shiri Azenkot"],"pdf_url":"https://arxiv.org/pdf/2410.21652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04645v2","updated":"2024-10-29T00:26:42Z","published":"2024-09-06T22:31:15Z","title":"PAIGE: Examining Learning Outcomes and Experiences with Personalized\n  AI-Generated Educational Podcasts","summary":"  Generative AI is revolutionizing content creation and has the potential to\nenable real-time, personalized educational experiences. We investigated the\neffectiveness of converting textbook chapters into AI-generated podcasts and\nexplored the impact of personalizing these podcasts for individual learner\nprofiles. We conducted a 3x3 user study with 180 college students in the United\nStates, comparing traditional textbook reading with both generalized and\npersonalized AI-generated podcasts across three textbook subjects. The\npersonalized podcasts were tailored to students' majors, interests, and\nlearning styles. Our findings show that students found the AI-generated podcast\nformat to be more enjoyable than textbooks and that personalized podcasts led\nto significantly improved learning outcomes, although this was\nsubject-specific. These results highlight that AI-generated podcasts can offer\nan engaging and effective modality transformation of textbook material, with\npersonalization enhancing content relevance. We conclude with design\nrecommendations for leveraging AI in education, informed by student feedback.\n","authors":["Tiffany D. Do","Usama Bin Shafqat","Elsie Ling","Nikhil Sarda"],"pdf_url":"https://arxiv.org/pdf/2409.04645v2.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.22578v1","updated":"2024-10-29T22:43:26Z","published":"2024-10-29T22:43:26Z","title":"Energy-Aware Multi-Agent Reinforcement Learning for Collaborative\n  Execution in Mission-Oriented Drone Networks","summary":"  Mission-oriented drone networks have been widely used for structural\ninspection, disaster monitoring, border surveillance, etc. Due to the limited\nbattery capacity of drones, mission execution strategy impacts network\nperformance and mission completion. However, collaborative execution is a\nchallenging problem for drones in such a dynamic environment as it also\ninvolves efficient trajectory design. We leverage multi-agent reinforcement\nlearning (MARL) to manage the challenge in this study, letting each drone learn\nto collaboratively execute tasks and plan trajectories based on its current\nstatus and environment. Simulation results show that the proposed collaborative\nexecution model can successfully complete the mission at least 80% of the time,\nregardless of task locations and lengths, and can even achieve a 100% success\nrate when the task density is not way too sparse. To the best of our knowledge,\nour work is one of the pioneer studies on leveraging MARL on collaborative\nexecution for mission-oriented drone networks; the unique value of this work\nlies in drone battery level driving our model design.\n","authors":["Ying Li","Changling Li","Jiyao Chen","Christine Roinou"],"pdf_url":"https://arxiv.org/pdf/2410.22578v1.pdf","comment":"2022 International Conference on Computer Communications and Networks"},{"id":"http://arxiv.org/abs/2410.22478v1","updated":"2024-10-29T19:14:15Z","published":"2024-10-29T19:14:15Z","title":"Designing robot swarms: a puzzle, a problem, and a mess","summary":"  Framing an issue as a puzzle, problem, or mess is an illustrative approach to\ncharacterizing the issue's complexity within organizational theory and systems\nthinking. We use this approach to characterize the issue of designing\ncollective behaviors for robot swarms and discuss how various research goals\nhave shaped the current state of the field. We contextualize our discussion at\nthese three levels by highlighting relevant literature. Our aim is to emphasize\nkey challenges that arise in the development of robot swarms for real-world\napplications and to motivate further work on promising research directions.\n","authors":["David Garz√≥n Ramos","Sabine Hauert"],"pdf_url":"https://arxiv.org/pdf/2410.22478v1.pdf","comment":"40th Anniversary of the IEEE Conference on Robotics and Automation\n  (ICRA@40)"},{"id":"http://arxiv.org/abs/2410.22457v1","updated":"2024-10-29T18:45:13Z","published":"2024-10-29T18:45:13Z","title":"Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration\n  and Evaluation using Novel Metrics and Dataset","summary":"  Advancements in Large Language Models (LLMs) are revolutionizing the\ndevelopment of autonomous agentic systems by enabling dynamic, context-aware\ntask decomposition and automated tool selection. These sophisticated systems\npossess significant automation potential across various industries, managing\ncomplex tasks, interacting with external systems to enhance knowledge, and\nexecuting actions independently. This paper presents three primary\ncontributions to advance this field:\n  - Advanced Agentic Framework: A system that handles multi-hop queries,\ngenerates and executes task graphs, selects appropriate tools, and adapts to\nreal-time changes.\n  - Novel Evaluation Metrics: Introduction of Node F1 Score, Structural\nSimilarity Index (SSI), and Tool F1 Score to comprehensively assess agentic\nsystems.\n  - Specialized Dataset: Development of an AsyncHow-based dataset for analyzing\nagent behavior across different task complexities.\n  Our findings reveal that asynchronous and dynamic task graph decomposition\nsignificantly enhances system responsiveness and scalability, particularly for\ncomplex, multi-step tasks. Detailed analysis shows that structural and\nnode-level metrics are crucial for sequential tasks, while tool-related metrics\nare more important for parallel tasks. Specifically, the Structural Similarity\nIndex (SSI) is the most significant predictor of performance in sequential\ntasks, and the Tool F1 Score is essential for parallel tasks. These insights\nhighlight the need for balanced evaluation methods that capture both structural\nand operational dimensions of agentic systems. Additionally, our evaluation\nframework, validated through empirical analysis and statistical testing,\nprovides valuable insights for improving the adaptability and reliability of\nagentic systems in dynamic environments.\n","authors":["Adrian Garret Gabriel","Alaa Alameer Ahmad","Shankar Kumar Jeyakumar"],"pdf_url":"https://arxiv.org/pdf/2410.22457v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024), NeurIPS 2024 Workshop on Open-World Agents"},{"id":"http://arxiv.org/abs/2410.22165v1","updated":"2024-10-29T16:02:50Z","published":"2024-10-29T16:02:50Z","title":"EconoJax: A Fast & Scalable Economic Simulation in Jax","summary":"  Accurate economic simulations often require many experimental runs,\nparticularly when combined with reinforcement learning. Unfortunately, training\nreinforcement learning agents in multi-agent economic environments can be slow.\nThis paper introduces EconoJax, a fast simulated economy, based on the AI\neconomist. EconoJax, and its training pipeline, are completely written in JAX.\nThis allows EconoJax to scale to large population sizes and perform large\nexperiments, while keeping training times within minutes. Through experiments\nwith populations of 100 agents, we show how real-world economic behavior\nemerges through training within 15 minutes, in contrast to previous work that\nrequired several days. To aid and inspire researchers to build more rich and\ndynamic economic simulations, we open-source EconoJax on Github at:\nhttps://github.com/ponseko/econojax.\n","authors":["Koen Ponse","Aske Plaat","Niki van Stein","Thomas M. Moerland"],"pdf_url":"https://arxiv.org/pdf/2410.22165v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2405.05085v3","updated":"2024-10-29T13:40:51Z","published":"2024-05-08T14:32:09Z","title":"Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling\n  Biases in Electing Participatory Budgeting Winners","summary":"  Participatory budgeting, as a paradigm for democratic innovations, engages\ncitizens in the distribution of a public budget to projects, which they propose\nand vote for implementation. So far, voting algorithms have been proposed and\nstudied in social choice literature to elect projects that are popular, while\nothers prioritize on a proportional representation of voters' preferences, for\ninstance, the rule of equal shares. However, the anticipated impact and novelty\nin the broader society by the winning projects, as selected by different\nalgorithms, remains totally under-explored, lacking both a universal theory of\nimpact for voting and a rigorous unifying framework for impact and novelty\nassessments. This paper tackles this grand challenge towards new axiomatic\nfoundations for designing effective and fair voting methods. This is via new\nand striking insights derived from a large-scale analysis of biases over 345\nreal-world voting outcomes, characterized for the first time by a novel\nportfolio of impact and novelty metrics. We find strong causal evidence that\nequal shares comes with impact loss in several infrastructural projects of\ndifferent cost levels that have been so far over-represented. However, it also\ncomes with a novel, yet over-represented, impact gain in welfare, education and\nculture. We discuss broader implications of these results and how impact loss\ncan be mitigated at the stage of campaign design and project ideation.\n","authors":["Sajan Maharjan","Srijoni Majumdar","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2405.05085v3.pdf","comment":"41 pages, 19 figures"},{"id":"http://arxiv.org/abs/2310.01945v4","updated":"2024-10-29T13:14:25Z","published":"2023-10-03T10:43:58Z","title":"Homotopy-Aware Multi-Agent Path Planning on Plane","summary":"  We propose an efficient framework using Dynnikov coordinates for\nhomotopy-aware multi-agent path planning in planar domains that may contain\nobstacles. We developed a method for generating multiple homotopically distinct\nsolutions for the multi-agent path planning problem in planar domains by\ncombining our framework with revised prioritized planning and proved its\ncompleteness under specific assumptions. Experimentally, we demonstrated that\nour method is significantly faster than a method without Dynnikov coordinates.\nWe also confirmed experimentally that homotopy-aware planning contributes to\navoiding locally optimal solutions when searching for low-cost trajectories for\na swarm of agents in a continuous environment.\n","authors":["Kazumi Kasaura"],"pdf_url":"https://arxiv.org/pdf/2310.01945v4.pdf","comment":"17 pages with 5 pages of references and appendices, 19 figures"},{"id":"http://arxiv.org/abs/2407.02518v2","updated":"2024-10-29T08:20:28Z","published":"2024-06-23T15:55:07Z","title":"INDICT: Code Generation with Internal Dialogues of Critiques for Both\n  Security and Helpfulness","summary":"  Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels).\n","authors":["Hung Le","Yingbo Zhou","Caiming Xiong","Silvio Savarese","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2407.02518v2.pdf","comment":"Accepted to The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2410.21794v1","updated":"2024-10-29T06:59:11Z","published":"2024-10-29T06:59:11Z","title":"Inverse Attention Agent for Multi-Agent System","summary":"  A major challenge for Multi-Agent Systems is enabling agents to adapt\ndynamically to diverse environments in which opponents and teammates may\ncontinually change. Agents trained using conventional methods tend to excel\nonly within the confines of their training cohorts; their performance drops\nsignificantly when confronting unfamiliar agents. To address this shortcoming,\nwe introduce Inverse Attention Agents that adopt concepts from the Theory of\nMind, implemented algorithmically using an attention mechanism and trained in\nan end-to-end manner. Crucial to determining the final actions of these agents,\nthe weights in their attention model explicitly represent attention to\ndifferent goals. We furthermore propose an inverse attention network that\ndeduces the ToM of agents based on observations and prior actions. The network\ninfers the attentional states of other agents, thereby refining the attention\nweights to adjust the agent's final action. We conduct experiments in a\ncontinuous environment, tackling demanding tasks encompassing cooperation,\ncompetition, and a blend of both. They demonstrate that the inverse attention\nnetwork successfully infers the attention of other agents, and that this\ninformation improves agent performance. Additional human experiments show that,\ncompared to baseline agent models, our inverse attention agents exhibit\nsuperior cooperation with humans and better emulate human behaviors.\n","authors":["Qian Long","Ruoyan Li","Minglu Zhao","Tao Gao","Demetri Terzopoulos"],"pdf_url":"https://arxiv.org/pdf/2410.21794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21784v1","updated":"2024-10-29T06:42:27Z","published":"2024-10-29T06:42:27Z","title":"MARCO: Multi-Agent Real-time Chat Orchestration","summary":"  Large language model advancements have enabled the development of multi-agent\nframeworks to tackle complex, real-world problems such as to automate tasks\nthat require interactions with diverse tools, reasoning, and human\ncollaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration\nframework for automating tasks using LLMs. MARCO addresses key challenges in\nutilizing LLMs for complex, multi-step task execution. It incorporates robust\nguardrails to steer LLM behavior, validate outputs, and recover from errors\nthat stem from inconsistent output formatting, function and parameter\nhallucination, and lack of domain knowledge. Through extensive experiments we\ndemonstrate MARCO's superior performance with 94.48% and 92.74% accuracy on\ntask execution for Digital Restaurant Service Platform conversations and Retail\nconversations datasets respectively along with 44.91% improved latency and\n33.71% cost reduction. We also report effects of guardrails in performance gain\nalong with comparisons of various LLM models, both open-source and proprietary.\nThe modular and generic design of MARCO allows it to be adapted for automating\ntasks across domains and to execute complex usecases through multi-turn\ninteractions.\n","authors":["Anubhav Shrimal","Stanley Kanagaraj","Kriti Biswas","Swarnalatha Raghuraman","Anish Nediyanchath","Yi Zhang","Promod Yenigalla"],"pdf_url":"https://arxiv.org/pdf/2410.21784v1.pdf","comment":"EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2405.15145v2","updated":"2024-10-29T06:14:47Z","published":"2024-05-24T01:49:02Z","title":"CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models","summary":"  Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.\n","authors":["Cheng Li","Damien Teney","Linyi Yang","Qingsong Wen","Xing Xie","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2405.15145v2.pdf","comment":"NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946"},{"id":"http://arxiv.org/abs/2406.01641v2","updated":"2024-10-29T01:16:10Z","published":"2024-06-03T06:07:27Z","title":"Reciprocal Reward Influence Encourages Cooperation From Self-Interested\n  Agents","summary":"  Cooperation between self-interested individuals is a widespread phenomenon in\nthe natural world, but remains elusive in interactions between artificially\nintelligent agents. Instead, naive reinforcement learning algorithms typically\nconverge to Pareto-dominated outcomes in even the simplest of social dilemmas.\nAn emerging literature on opponent shaping has demonstrated the ability to\nreach prosocial outcomes by influencing the learning of other agents. However,\nsuch methods differentiate through the learning step of other agents or\noptimize for meta-game dynamics, which rely on privileged access to opponents'\nlearning algorithms or exponential sample complexity, respectively. To provide\na learning rule-agnostic and sample-efficient alternative, we introduce\nReciprocators, reinforcement learning agents which are intrinsically motivated\nto reciprocate the influence of opponents' actions on their returns. This\napproach seeks to modify other agents' $Q$-values by increasing their return\nfollowing beneficial actions (with respect to the Reciprocator) and decreasing\nit after detrimental actions, guiding them towards mutually beneficial actions\nwithout directly differentiating through a model of their policy. We show that\nReciprocators can be used to promote cooperation in temporally extended social\ndilemmas during simultaneous learning. Our code is available at\nhttps://github.com/johnlyzhou/reciprocator/ .\n","authors":["John L. Zhou","Weizhe Hong","Jonathan C. Kao"],"pdf_url":"https://arxiv.org/pdf/2406.01641v2.pdf","comment":"NeurIPS 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21144v2","updated":"2024-10-29T16:25:34Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods.\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v2.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC"},{"id":"http://arxiv.org/abs/2410.22112v1","updated":"2024-10-29T15:11:45Z","published":"2024-10-29T15:11:45Z","title":"Multimodal Semantic Communication for Generative Audio-Driven Video\n  Conferencing","summary":"  This paper studies an efficient multimodal data communication scheme for\nvideo conferencing. In our considered system, a speaker gives a talk to the\naudiences, with talking head video and audio being transmitted. Since the\nspeaker does not frequently change posture and high-fidelity transmission of\naudio (speech and music) is required, redundant visual video data exists and\ncan be removed by generating the video from the audio. To this end, we propose\na wave-to-video (Wav2Vid) system, an efficient video transmission framework\nthat reduces transmitted data by generating talking head video from audio. In\nparticular, full-duration audio and short-duration video data are synchronously\ntransmitted through a wireless channel, with neural networks (NNs) extracting\nand encoding audio and video semantics. The receiver then combines the decoded\naudio and video data, as well as uses a generative adversarial network (GAN)\nbased model to generate the lip movement videos of the speaker. Simulation\nresults show that the proposed Wav2Vid system can reduce the amount of\ntransmitted data by up to 83% while maintaining the perceptual quality of the\ngenerated conferencing video.\n","authors":["Haonan Tong","Haopeng Li","Hongyang Du","Zhaohui Yang","Changchuan Yin","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2410.22112v1.pdf","comment":"accepted by IEEE Wireless Communications Letters"},{"id":"http://arxiv.org/abs/2310.16334v2","updated":"2024-10-29T14:53:47Z","published":"2023-10-25T03:30:37Z","title":"Structured Multi-Track Accompaniment Arrangement via Style Prior\n  Modelling","summary":"  In the realm of music AI, arranging rich and structured multi-track\naccompaniments from a simple lead sheet presents significant challenges. Such\nchallenges include maintaining track cohesion, ensuring long-term coherence,\nand optimizing computational efficiency. In this paper, we introduce a novel\nsystem that leverages prior modelling over disentangled style factors to\naddress these challenges. Our method presents a two-stage process: initially, a\npiano arrangement is derived from the lead sheet by retrieving piano texture\nstyles; subsequently, a multi-track orchestration is generated by infusing\norchestral function styles into the piano arrangement. Our key design is the\nuse of vector quantization and a unique multi-stream Transformer to model the\nlong-term flow of the orchestration style, which enables flexible,\ncontrollable, and structured music generation. Experiments show that by\nfactorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our\nsystem supports a variety of music genres and provides style control at\ndifferent composition hierarchies. We further show that our system achieves\nsuperior coherence, structure, and overall arrangement quality compared to\nexisting baselines.\n","authors":["Jingwei Zhao","Gus Xia","Ziyu Wang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22046v1","updated":"2024-10-29T13:53:09Z","published":"2024-10-29T13:53:09Z","title":"CHORDONOMICON: A Dataset of 666,000 Songs and their Chord Progressions","summary":"  Chord progressions encapsulate important information about music, pertaining\nto its structure and conveyed emotions. They serve as the backbone of musical\ncomposition, and in many cases, they are the sole information required for a\nmusician to play along and follow the music. Despite their importance, chord\nprogressions as a data domain remain underexplored. There is a lack of\nlarge-scale datasets suitable for deep learning applications, and limited\nresearch exploring chord progressions as an input modality. In this work, we\npresent Chordonomicon, a dataset of over 666,000 songs and their chord\nprogressions, annotated with structural parts, genre, and release date -\ncreated by scraping various sources of user-generated progressions and\nassociated metadata. We demonstrate the practical utility of the Chordonomicon\ndataset for classification and generation tasks, and discuss its potential to\nprovide valuable insights to the research community. Chord progressions are\nunique in their ability to be represented in multiple formats (e.g. text,\ngraph) and the wealth of information chords convey in given contexts, such as\ntheir harmonic function . These characteristics make the Chordonomicon an ideal\ntestbed for exploring advanced machine learning techniques, including\ntransformers, graph machine learning, and hybrid systems that combine knowledge\nrepresentation and machine learning.\n","authors":["Spyridon Kantarelis","Konstantinos Thomas","Vassilis Lyberatos","Edmund Dervakos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2410.22046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11832v2","updated":"2024-10-29T06:44:36Z","published":"2024-06-17T17:59:44Z","title":"Unveiling Encoder-Free Vision-Language Models","summary":"  Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.\n","authors":["Haiwen Diao","Yufeng Cui","Xiaotong Li","Yueze Wang","Huchuan Lu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11832v2.pdf","comment":"17 pages, 8 figures, Accepted by NeurIPS2024 (spotlight)"},{"id":"http://arxiv.org/abs/2410.21169v2","updated":"2024-10-29T06:32:24Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments-such as contracts, academic papers, and invoices-into structured,\nmachine-readable data. Document parsing extract reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\nemphasizes the importance of developing larger and more diverse datasets and\noutlines future research directions.\n","authors":["Qintong Zhang","Victor Shea-Jay Huang","Bin Wang","Junyuan Zhang","Zhengren Wang","Hao Liang","Shawn Wang","Matthieu Lin","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v2.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2410.22600v1","updated":"2024-10-29T23:45:29Z","published":"2024-10-29T23:45:29Z","title":"Solving Minimum-Cost Reach Avoid using Reinforcement Learning","summary":"  Current reinforcement-learning methods are unable to directly learn policies\nthat solve the minimum cost reach-avoid problem to minimize cumulative costs\nsubject to the constraints of reaching the goal and avoiding unsafe states, as\nthe structure of this new optimization problem is incompatible with current\nmethods. Instead, a surrogate problem is solved where all objectives are\ncombined with a weighted sum. However, this surrogate objective results in\nsuboptimal policies that do not directly minimize the cumulative cost. In this\nwork, we propose RC-PPO, a reinforcement-learning-based method for solving the\nminimum-cost reach-avoid problem by using connections to Hamilton-Jacobi\nreachability. Empirical results demonstrate that RC-PPO learns policies with\ncomparable goal-reaching rates to while achieving up to 57% lower cumulative\ncosts compared to existing methods on a suite of minimum-cost reach-avoid\nbenchmarks on the Mujoco simulator. The project page can be found at\nhttps://oswinso.xyz/rcppo.\n","authors":["Oswin So","Cheng Ge","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2410.22600v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.22596v1","updated":"2024-10-29T23:20:07Z","published":"2024-10-29T23:20:07Z","title":"Continuous-Time Line-of-Sight Constrained Trajectory Planning for\n  6-Degree of Freedom Systems","summary":"  Perception algorithms are ubiquitous in modern autonomy stacks, providing\nnecessary environmental information to operate in the real world. Many of these\nalgorithms depend on the visibility of keypoints, which must remain within the\nrobot's line-of-sight (LoS), for reliable operation. This paper tackles the\nchallenge of maintaining LoS on such keypoints during robot movement. We\npropose a novel method that addresses these issues by ensuring applicability to\nvarious sensor footprints, adaptability to arbitrary nonlinear dynamics, and\nconstant enforcement of LoS throughout the robot's path. Through our\nexperiments, we show that the proposed approach achieves significantly reduced\nLoS violation and runtime compared to existing state-of-the-art methods in\nseveral representative and challenging scenarios.\n","authors":["Christopher R. Hayner","John M. Carson III","Beh√ßet A√ßƒ±kme≈üe","Karen Leung"],"pdf_url":"https://arxiv.org/pdf/2410.22596v1.pdf","comment":"This paper is under review for the IEEE Robotics and Automation\n  Letters (RA-L)"},{"id":"http://arxiv.org/abs/2410.22582v1","updated":"2024-10-29T22:53:11Z","published":"2024-10-29T22:53:11Z","title":"Analytical Solution for Inverse Kinematics","summary":"  This paper introduces a closed-form analytical solution for the inverse\nkinematics (IK) of a 6 Degrees of Freedom (DOF) serial robotic manipulator arm,\nconfigured with six revolute joints and utilized within the Lunar Exploration\nRover System (LERS). As a critical asset for conducting precise operations in\nthe demanding lunar environment, this robotic arm relies on the IK solution to\ndetermine joint parameters required for precise end-effector positioning,\nessential for tasks such as sample collection, infrastructure assembly, and\nequipment deployment. By applying geometric principles, the proposed method\noffers a highly efficient and accurate approach to solving the IK problem,\nsignificantly reducing computational demands compared to traditional numerical\nmethods. This advancement not only enhances real-time operational capabilities\nbut is also optimized for space robotics, where precision and speed are\ncritical. Additionally, the paper explores the integration of the LERS robotic\nsystem, underscoring the importance of this work in supporting autonomous lunar\nexploration within the ARTEMIS program and future missions\n","authors":["Serdar Kalaycioglu","Anton de Ruiter","Ethan Fung","Harrison Zhang","Haipeng Xie"],"pdf_url":"https://arxiv.org/pdf/2410.22582v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.22578v1","updated":"2024-10-29T22:43:26Z","published":"2024-10-29T22:43:26Z","title":"Energy-Aware Multi-Agent Reinforcement Learning for Collaborative\n  Execution in Mission-Oriented Drone Networks","summary":"  Mission-oriented drone networks have been widely used for structural\ninspection, disaster monitoring, border surveillance, etc. Due to the limited\nbattery capacity of drones, mission execution strategy impacts network\nperformance and mission completion. However, collaborative execution is a\nchallenging problem for drones in such a dynamic environment as it also\ninvolves efficient trajectory design. We leverage multi-agent reinforcement\nlearning (MARL) to manage the challenge in this study, letting each drone learn\nto collaboratively execute tasks and plan trajectories based on its current\nstatus and environment. Simulation results show that the proposed collaborative\nexecution model can successfully complete the mission at least 80% of the time,\nregardless of task locations and lengths, and can even achieve a 100% success\nrate when the task density is not way too sparse. To the best of our knowledge,\nour work is one of the pioneer studies on leveraging MARL on collaborative\nexecution for mission-oriented drone networks; the unique value of this work\nlies in drone battery level driving our model design.\n","authors":["Ying Li","Changling Li","Jiyao Chen","Christine Roinou"],"pdf_url":"https://arxiv.org/pdf/2410.22578v1.pdf","comment":"2022 International Conference on Computer Communications and Networks"},{"id":"http://arxiv.org/abs/2410.22527v1","updated":"2024-10-29T20:43:56Z","published":"2024-10-29T20:43:56Z","title":"Intelligent Mobility System with Integrated Motion Planning and Control\n  Utilizing Infrastructure Sensor Nodes","summary":"  This paper introduces a framework for an indoor autonomous mobility system\nthat can perform patient transfers and materials handling. Unlike traditional\nsystems that rely on onboard perception sensors, the proposed approach\nleverages a global perception and localization (PL) through Infrastructure\nSensor Nodes (ISNs) and cloud computing technology. Using the global PL, an\nintegrated Model Predictive Control (MPC)-based local planning and tracking\ncontroller augmented with Artificial Potential Field (APF) is developed,\nenabling reliable and efficient motion planning and obstacle avoidance ability\nwhile tracking predefined reference motions. Simulation results demonstrate the\neffectiveness of the proposed MPC controller in smoothly navigating around both\nstatic and dynamic obstacles. The proposed system has the potential to extend\nto intelligent connected autonomous vehicles, such as electric or cargo\ntransport vehicles with four-wheel independent drive/steering (4WID-4WIS)\nconfigurations.\n","authors":["Yufeng Yang","Minghao Ning","Shucheng Huang","Ehsan Hashemi","Amir Khajepour"],"pdf_url":"https://arxiv.org/pdf/2410.22527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22482v1","updated":"2024-10-29T19:20:06Z","published":"2024-10-29T19:20:06Z","title":"Heterogeneous Team Coordination on Partially Observable Graphs with\n  Realistic Communication","summary":"  Team Coordination on Graphs with Risky Edges (\\textsc{tcgre}) is a recently\nproposed problem, in which robots find paths to their goals while considering\npossible coordination to reduce overall team cost. However, \\textsc{tcgre}\nassumes that the \\emph{entire} environment is available to a \\emph{homogeneous}\nrobot team with \\emph{ubiquitous} communication. In this paper, we study an\nextended version of \\textsc{tcgre}, called \\textsc{hpr-tcgre}, with three\nrelaxations: Heterogeneous robots, Partial observability, and Realistic\ncommunication. To this end, we form a new combinatorial optimization problem on\ntop of \\textsc{tcgre}. After analysis, we divide it into two sub-problems, one\nfor robots moving individually, another for robots in groups, depending on\ntheir communication availability. Then, we develop an algorithm that exploits\nreal-time partial maps to solve local shortest path(s) problems, with a A*-like\nsub-goal(s) assignment mechanism that explores potential coordination\nopportunities for global interests. Extensive experiments indicate that our\nalgorithm is able to produce team coordination behaviors in order to reduce\noverall cost even with our three relaxations.\n","authors":["Yanlin Zhou","Manshi Limbu","Xuan Wang","Daigo Shishika","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2410.22482v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.01441v2","updated":"2024-10-29T19:15:06Z","published":"2024-04-01T19:26:43Z","title":"A novel seamless magnetic-based actuating mechanism for\n  end-effector-based robotic rehabilitation platforms","summary":"  Rehabilitation robotics continues to confront substantial challenges,\nparticularly in achieving smooth, safe, and intuitive human-robot interactions\nfor upper limb motor training. Many current systems depend on complex\nmechanical designs, direct physical contact, and multiple sensors, which not\nonly elevate costs but also reduce accessibility. Additionally, delivering\nseamless weight compensation and precise motion tracking remains a highly\ncomplex undertaking. To overcome these obstacles, we have developed a novel\nmagnetic-based actuation mechanism for end-effector robotic rehabilitation.\nThis innovative approach enables smooth, non-contact force transmission,\nsignificantly enhancing patient safety and comfort during upper limb training.\nTo ensure consistent performance, we integrated an Extended Kalman Filter (EKF)\nalongside a controller for real-time position tracking, allowing the system to\nmaintain high accuracy or recover even in the event of sensor malfunction or\nfailure. In a user study with 12 participants, 75% rated the system highly for\nits smoothness, while 66.7% commended its safety and effective weight\ncompensation. The EKF demonstrated precise tracking performance, with root mean\nsquare error (RMSE) values remaining within acceptable limits (under 2 cm). By\ncombining magnetic actuation with advanced closed-loop control algorithms, this\nsystem marks a significant advancement in the field of upper limb\nrehabilitation robotics.\n","authors":["Sima Ghafoori","Ali Rabiee","Maryam Norouzi","Musa Jouaneh","Reza Abiri"],"pdf_url":"https://arxiv.org/pdf/2404.01441v2.pdf","comment":"7 pages, 9 figures, journal paper"},{"id":"http://arxiv.org/abs/2410.22478v1","updated":"2024-10-29T19:14:15Z","published":"2024-10-29T19:14:15Z","title":"Designing robot swarms: a puzzle, a problem, and a mess","summary":"  Framing an issue as a puzzle, problem, or mess is an illustrative approach to\ncharacterizing the issue's complexity within organizational theory and systems\nthinking. We use this approach to characterize the issue of designing\ncollective behaviors for robot swarms and discuss how various research goals\nhave shaped the current state of the field. We contextualize our discussion at\nthese three levels by highlighting relevant literature. Our aim is to emphasize\nkey challenges that arise in the development of robot swarms for real-world\napplications and to motivate further work on promising research directions.\n","authors":["David Garz√≥n Ramos","Sabine Hauert"],"pdf_url":"https://arxiv.org/pdf/2410.22478v1.pdf","comment":"40th Anniversary of the IEEE Conference on Robotics and Automation\n  (ICRA@40)"},{"id":"http://arxiv.org/abs/2410.22462v1","updated":"2024-10-29T18:56:11Z","published":"2024-10-29T18:56:11Z","title":"How Artists Improvise and Provoke Robotics","summary":"  We explore transdisciplinary collaborations between artists and roboticists\nacross a portfolio of artworks. Brendan Walker's Broncomatic was a breath\ncontrolled mechanical rodeo bull ride. Blast Theory's Cat Royale deployed a\nrobot arm to play with a family of three cats for twelve days. Different Bodies\nis a prototype improvised dance performance in which dancers with disabilities\nphysically manipulate two mirrored robot arms. We reflect on these to explore\nhow artists shape robotics research through the two key strategies of\nimprovisation and provocation. Artists are skilled at improvising extended\nrobot experiences that surface opportunities for technology-focused design, but\nwhich also require researchers to improvise their research processes. Artists\nmay provoke audiences into reflecting on the societal implications of robots,\nbut at the same time challenge the established techno-centric concepts, methods\nand underlying epistemology of robotics research.\n","authors":["Steve Benford","Rachael Garrett","Eike Schneiders","Paul Tennent","Alan Chamberlain","Juan Avila","Pat Brundell","Simon Castle-Green"],"pdf_url":"https://arxiv.org/pdf/2410.22462v1.pdf","comment":"16th International Conference on Social Robotics (ISCR 2024)"},{"id":"http://arxiv.org/abs/2408.15099v3","updated":"2024-10-29T18:25:44Z","published":"2024-08-27T14:31:54Z","title":"No Regrets: Investigating and Improving Regret Approximations for\n  Curriculum Discovery","summary":"  What data or environments to use for training to improve downstream\nperformance is a longstanding and very topical question in reinforcement\nlearning. In particular, Unsupervised Environment Design (UED) methods have\ngained recent attention as their adaptive curricula promise to enable agents to\nbe robust to in- and out-of-distribution tasks. This work investigates how\nexisting UED methods select training environments, focusing on task\nprioritisation metrics. Surprisingly, despite methods aiming to maximise regret\nin theory, the practical approximations do not correlate with regret but with\nsuccess rate. As a result, a significant portion of an agent's experience comes\nfrom environments it has already mastered, offering little to no contribution\ntoward enhancing its abilities. Put differently, current methods fail to\npredict intuitive measures of ``learnability.'' Specifically, they are unable\nto consistently identify those scenarios that the agent can sometimes solve,\nbut not always. Based on our analysis, we develop a method that directly trains\non scenarios with high learnability. This simple and intuitive approach\noutperforms existing UED methods in several binary-outcome environments,\nincluding the standard domain of Minigrid and a novel setting closely inspired\nby a real-world robotics problem. We further introduce a new adversarial\nevaluation procedure for directly measuring robustness, closely mirroring the\nconditional value at risk (CVaR). We open-source all our code and present\nvisualisations of final policies here:\nhttps://github.com/amacrutherford/sampling-for-learnability.\n","authors":["Alexander Rutherford","Michael Beukman","Timon Willi","Bruno Lacerda","Nick Hawes","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2408.15099v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22332v1","updated":"2024-10-29T17:59:55Z","published":"2024-10-29T17:59:55Z","title":"Local Policies Enable Zero-shot Long-horizon Manipulation","summary":"  Sim2real for robotic manipulation is difficult due to the challenges of\nsimulating complex contacts and generating realistic task distributions. To\ntackle the latter problem, we introduce ManipGen, which leverages a new class\nof policies for sim2real transfer: local policies. Locality enables a variety\nof appealing properties including invariances to absolute robot and object\npose, skill ordering, and global scene configuration. We combine these policies\nwith foundation models for vision, language and motion planning and demonstrate\nSOTA zero-shot performance of our method to Robosuite benchmark tasks in\nsimulation (97%). We transfer our local policies from simulation to reality and\nobserve they can solve unseen long-horizon manipulation tasks with up to 8\nstages with significant pose, object and scene configuration variation.\nManipGen outperforms SOTA approaches such as SayCan, OpenVLA, LLMTrajGen and\nVoxPoser across 50 real-world manipulation tasks by 36%, 76%, 62% and 60%\nrespectively. Video results at https://mihdalal.github.io/manipgen/\n","authors":["Murtaza Dalal","Min Liu","Walter Talbott","Chen Chen","Deepak Pathak","Jian Zhang","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2410.22332v1.pdf","comment":"Main paper 7 pages, 3 tables, 3 figures. Appendix 6 pages, 2 figures,\n  6 tables"},{"id":"http://arxiv.org/abs/2410.22314v1","updated":"2024-10-29T17:54:02Z","published":"2024-10-29T17:54:02Z","title":"An Efficient Approach to Generate Safe Drivable Space by\n  LiDAR-Camera-HDmap Fusion","summary":"  In this paper, we propose an accurate and robust perception module for\nAutonomous Vehicles (AVs) for drivable space extraction. Perception is crucial\nin autonomous driving, where many deep learning-based methods, while accurate\non benchmark datasets, fail to generalize effectively, especially in diverse\nand unpredictable environments. Our work introduces a robust easy-to-generalize\nperception module that leverages LiDAR, camera, and HD map data fusion to\ndeliver a safe and reliable drivable space in all weather conditions. We\npresent an adaptive ground removal and curb detection method integrated with HD\nmap data for enhanced obstacle detection reliability. Additionally, we propose\nan adaptive DBSCAN clustering algorithm optimized for precipitation noise, and\na cost-effective LiDAR-camera frustum association that is resilient to\ncalibration discrepancies. Our comprehensive drivable space representation\nincorporates all perception data, ensuring compatibility with vehicle\ndimensions and road regulations. This approach not only improves generalization\nand efficiency, but also significantly enhances safety in autonomous vehicle\noperations. Our approach is tested on a real dataset and its reliability is\nverified during the daily (including harsh snowy weather) operation of our\nautonomous shuttle, WATonoBus\n","authors":["Minghao Ning","Ahmad Reza Alghooneh","Chen Sun","Ruihe Zhang","Pouya Panahandeh","Steven Tuer","Ehsan Hashemi","Amir Khajepour"],"pdf_url":"https://arxiv.org/pdf/2410.22314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22313v1","updated":"2024-10-29T17:53:56Z","published":"2024-10-29T17:53:56Z","title":"Senna: Bridging Large Vision-Language Models and End-to-End Autonomous\n  Driving","summary":"  End-to-end autonomous driving demonstrates strong planning capabilities with\nlarge-scale data but still struggles in complex, rare scenarios due to limited\ncommonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene\nunderstanding and reasoning. The path forward lies in merging the strengths of\nboth approaches. Previous methods using LVLMs to predict trajectories or\ncontrol signals yield suboptimal results, as LVLMs are not well-suited for\nprecise numerical predictions. This paper presents Senna, an autonomous driving\nsystem combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E).\nSenna decouples high-level planning from low-level trajectory prediction.\nSenna-VLM generates planning decisions in natural language, while Senna-E2E\npredicts precise trajectories. Senna-VLM utilizes a multi-image encoding\napproach and multi-view prompts for efficient scene understanding. Besides, we\nintroduce planning-oriented QAs alongside a three-stage training strategy,\nwhich enhances Senna-VLM's planning performance while preserving commonsense.\nExtensive experiments on two datasets show that Senna achieves state-of-the-art\nplanning performance. Notably, with pre-training on a large-scale dataset\nDriveX and fine-tuning on nuScenes, Senna significantly reduces average\nplanning error by 27.12% and collision rate by 33.33% over model without\npre-training. We believe Senna's cross-scenario generalization and\ntransferability are essential for achieving fully autonomous driving. Code and\nmodels will be released at https://github.com/hustvl/Senna.\n","authors":["Bo Jiang","Shaoyu Chen","Bencheng Liao","Xingyu Zhang","Wei Yin","Qian Zhang","Chang Huang","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.22313v1.pdf","comment":"Project Page: https://github.com/hustvl/Senna"},{"id":"http://arxiv.org/abs/2410.22308v1","updated":"2024-10-29T17:52:59Z","published":"2024-10-29T17:52:59Z","title":"Environment as Policy: Learning to Race in Unseen Tracks","summary":"  Reinforcement learning (RL) has achieved outstanding success in complex robot\ncontrol tasks, such as drone racing, where the RL agents have outperformed\nhuman champions in a known racing track. However, these agents fail in unseen\ntrack configurations, always requiring complete retraining when presented with\nnew track layouts. This work aims to develop RL agents that generalize\neffectively to novel track configurations without retraining. The naive\nsolution of training directly on a diverse set of track layouts can overburden\nthe agent, resulting in suboptimal policy learning as the increased complexity\nof the environment impairs the agent's ability to learn to fly. To enhance the\ngeneralizability of the RL agent, we propose an adaptive environment-shaping\nframework that dynamically adjusts the training environment based on the\nagent's performance. We achieve this by leveraging a secondary RL policy to\ndesign environments that strike a balance between being challenging and\nachievable, allowing the agent to adapt and improve progressively. Using our\nadaptive environment shaping, one single racing policy efficiently learns to\nrace in diverse challenging tracks. Experimental results validated in both\nsimulation and the real world show that our method enables drones to\nsuccessfully fly complex and unseen race tracks, outperforming existing\nenvironment-shaping techniques. Project page:\nhttp://rpg.ifi.uzh.ch/env_as_policy/index.html\n","authors":["Hongze Wang","Jiaxu Xing","Nico Messikommer","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2410.22308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01479v2","updated":"2024-10-29T17:49:41Z","published":"2024-07-01T17:09:43Z","title":"EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data\n  Efficient Learning","summary":"  Building effective imitation learning methods that enable robots to learn\nfrom limited data and still generalize across diverse real-world environments\nis a long-standing problem in robot learning. We propose Equibot, a robust,\ndata-efficient, and generalizable approach for robot manipulation task\nlearning. Our approach combines SIM(3)-equivariant neural network architectures\nwith diffusion models. This ensures that our learned policies are invariant to\nchanges in scale, rotation, and translation, enhancing their applicability to\nunseen environments while retaining the benefits of diffusion-based policy\nlearning such as multi-modality and robustness. We show on a suite of 6\nsimulation tasks that our proposed method reduces the data requirements and\nimproves generalization to novel scenarios. In the real world, with 10\nvariations of 6 mobile manipulation tasks, we show that our method can easily\ngeneralize to novel objects and scenes after learning from just 5 minutes of\nhuman demonstrations in each task.\n","authors":["Jingyun Yang","Zi-ang Cao","Congyue Deng","Rika Antonova","Shuran Song","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2407.01479v2.pdf","comment":"CoRL 2024. The first two authors contributed equally. Project page:\n  https://equi-bot.github.io"},{"id":"http://arxiv.org/abs/2410.05038v2","updated":"2024-10-29T17:39:22Z","published":"2024-10-07T13:50:15Z","title":"GARField: Addressing the visual Sim-to-Real gap in garment manipulation\n  with mesh-attached radiance fields","summary":"  While humans intuitively manipulate garments and other textile items swiftly\nand accurately, it is a significant challenge for robots. A factor crucial to\nhuman performance is the ability to imagine, a priori, the intended result of\nthe manipulation intents and hence develop predictions on the garment pose.\nThat ability allows us to plan from highly obstructed states, adapt our plans\nas we collect more information and react swiftly to unforeseen circumstances.\nConversely, robots struggle to establish such intuitions and form tight links\nbetween plans and observations. We can partly attribute this to the high cost\nof obtaining densely labelled data for textile manipulation, both in quality\nand quantity. The problem of data collection is a long-standing issue in\ndata-based approaches to garment manipulation. As of today, generating\nhigh-quality and labelled garment manipulation data is mainly attempted through\nadvanced data capture procedures that create simplified state estimations from\nreal-world observations. However, this work proposes a novel approach to the\nproblem by generating real-world observations from object states. To achieve\nthis, we present GARField (Garment Attached Radiance Field), the first\ndifferentiable rendering architecture, to our knowledge, for data generation\nfrom simulated states stored as triangle meshes. Code is available on\nhttps://ddonatien.github.io/garfield-website/\n","authors":["Donatien Delehelle","Darwin G. Caldwell","Fei Chen"],"pdf_url":"https://arxiv.org/pdf/2410.05038v2.pdf","comment":"Project site: https://ddonatien.github.io/garfield-website/"},{"id":"http://arxiv.org/abs/2410.17524v2","updated":"2024-10-29T17:25:35Z","published":"2024-10-23T03:01:43Z","title":"Mechanisms and Computational Design of Multi-Modal End-Effector with\n  Force Sensing using Gated Networks","summary":"  In limbed robotics, end-effectors must serve dual functions, such as both\nfeet for locomotion and grippers for grasping, which presents design\nchallenges. This paper introduces a multi-modal end-effector capable of\ntransitioning between flat and line foot configurations while providing\ngrasping capabilities. MAGPIE integrates 8-axis force sensing using proposed\nmechanisms with hall effect sensors, enabling both contact and tactile force\nmeasurements. We present a computational design framework for our sensing\nmechanism that accounts for noise and interference, allowing for desired\nsensitivity and force ranges and generating ideal inverse models. The hardware\nimplementation of MAGPIE is validated through experiments, demonstrating its\ncapability as a foot and verifying the performance of the sensing mechanisms,\nideal models, and gated network-based models.\n","authors":["Yusuke Tanaka","Alvin Zhu","Richard Lin","Ankur Mehta","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2410.17524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22225v1","updated":"2024-10-29T16:54:15Z","published":"2024-10-29T16:54:15Z","title":"CaStL: Constraints as Specifications through LLM Translation for\n  Long-Horizon Task and Motion Planning","summary":"  Large Language Models (LLMs) have demonstrated remarkable ability in\nlong-horizon Task and Motion Planning (TAMP) by translating clear and\nstraightforward natural language problems into formal specifications such as\nthe Planning Domain Definition Language (PDDL). However, real-world problems\nare often ambiguous and involve many complex constraints. In this paper, we\nintroduce Constraints as Specifications through LLMs (CaStL), a framework that\nidentifies constraints such as goal conditions, action ordering, and action\nblocking from natural language in multiple stages. CaStL translates these\nconstraints into PDDL and Python scripts, which are solved using an custom PDDL\nsolver. Tested across three PDDL domains, CaStL significantly improves\nconstraint handling and planning success rates from natural language\nspecification in complex scenarios.\n","authors":["Weihang Guo","Zachary Kingston","Lydia E. Kavraki"],"pdf_url":"https://arxiv.org/pdf/2410.22225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22200v1","updated":"2024-10-29T16:36:14Z","published":"2024-10-29T16:36:14Z","title":"EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial\n  Awareness and Semantic Reasoning in Heterogeneous Environments","summary":"  To ensure the efficiency of robot autonomy under diverse real-world\nconditions, a high-quality heterogeneous dataset is essential to benchmark the\noperating algorithms' performance and robustness. Current benchmarks\npredominantly focus on urban terrains, specifically for on-road autonomous\ndriving, leaving multi-degraded, densely vegetated, dynamic and feature-sparse\nenvironments, such as underground tunnels, natural fields, and modern indoor\nspaces underrepresented. To fill this gap, we introduce EnvoDat, a large-scale,\nmulti-modal dataset collected in diverse environments and conditions, including\nhigh illumination, fog, rain, and zero visibility at different times of the\nday. Overall, EnvoDat contains 26 sequences from 13 scenes, 10 sensing\nmodalities, over 1.9TB of data, and over 89K fine-grained polygon-based\nannotations for more than 82 object and terrain classes. We post-processed\nEnvoDat in different formats that support benchmarking SLAM and supervised\nlearning algorithms, and fine-tuning multimodal vision models. With EnvoDat, we\ncontribute to environment-resilient robotic autonomy in areas where the\nconditions are extremely challenging. The datasets and other relevant resources\ncan be accessed through https://linusnep.github.io/EnvoDat/.\n","authors":["Linus Nwankwo","Bjoern Ellensohn","Vedant Dave","Peter Hofer","Jan Forstner","Marlene Villneuve","Robert Galler","Elmar Rueckert"],"pdf_url":"https://arxiv.org/pdf/2410.22200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19072v2","updated":"2024-10-29T15:44:46Z","published":"2024-10-24T18:28:06Z","title":"Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation\n  Scenario","summary":"  The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is\npromising, but existing implementations overlook key insights from human-robot\ninteraction (HRI) research. Studies on human-machine teams demonstrate that\nhuman perceptions of a machine teammate are critical in governing the machine's\nperformance. Here, we present a mixed factorial design to assess human\nperceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants\nwere assigned to the role of casualty (CAS) or bystander (BYS) and subjected to\nthree within-subjects conditions based on the MEDEVAC robot's operating mode:\nautonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each\ntrial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and\ntransporting them to an ambulance exchange point while avoiding an idle\nbystander. Following each trial, subjects completed a questionnaire measuring\ntheir emotional states, perceived safety, and social compatibility with the\nrobot. Results indicate a consistent main effect of operating mode on reported\nemotional states and perceived safety. Pairwise analyses suggest that the\nemployment of the AF operating mode negatively impacted perceptions along these\ndimensions. There were no persistent differences between casualty and bystander\nresponses.\n","authors":["Tyson Jordan","Pranav Pandey","Prashant Doshi","Ramviyas Parasuraman","Adam Goodie"],"pdf_url":"https://arxiv.org/pdf/2410.19072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22087v1","updated":"2024-10-29T14:42:19Z","published":"2024-10-29T14:42:19Z","title":"4D-based Robot Navigation Using Relativistic Image Processing","summary":"  Machine perception is an important prerequisite for safe interaction and\nlocomotion in dynamic environments. This requires not only the timely\nperception of surrounding geometries and distances but also the ability to\nreact to changing situations through predefined, learned but also reusable\nskill endings of a robot so that physical damage or bodily harm can be avoided.\nIn this context, 4D perception offers the possibility of predicting one's own\nposition and changes in the environment over time. In this paper, we present a\n4D-based approach to robot navigation using relativistic image processing.\nRelativistic image processing handles the temporal-related sensor information\nin a tensor model within a constructive 4D space. 4D-based navigation expands\nthe causal understanding and the resulting interaction radius of a robot\nthrough the use of visual and sensory 4D information.\n","authors":["Simone M√ºller","Dieter Kranzlm√ºller"],"pdf_url":"https://arxiv.org/pdf/2410.22087v1.pdf","comment":"AAAI Fall Symposia 2024"},{"id":"http://arxiv.org/abs/2410.22059v1","updated":"2024-10-29T14:10:28Z","published":"2024-10-29T14:10:28Z","title":"PACA: Perspective-Aware Cross-Attention Representation for Zero-Shot\n  Scene Rearrangement","summary":"  Scene rearrangement, like table tidying, is a challenging task in robotic\nmanipulation due to the complexity of predicting diverse object arrangements.\nWeb-scale trained generative models such as Stable Diffusion can aid by\ngenerating natural scenes as goals. To facilitate robot execution, object-level\nrepresentations must be extracted to match the real scenes with the generated\ngoals and to calculate object pose transformations. Current methods typically\nuse a multi-step design that involves separate models for generation,\nsegmentation, and feature encoding, which can lead to a low success rate due to\nerror accumulation. Furthermore, they lack control over the viewing\nperspectives of the generated goals, restricting the tasks to 3-DoF settings.\nIn this paper, we propose PACA, a zero-shot pipeline for scene rearrangement\nthat leverages perspective-aware cross-attention representation derived from\nStable Diffusion. Specifically, we develop a representation that integrates\ngeneration, segmentation, and feature encoding into a single step to produce\nobject-level representations. Additionally, we introduce perspective control,\nthus enabling the matching of 6-DoF camera views and extending past approaches\nthat were limited to 3-DoF top-down views. The efficacy of our method is\ndemonstrated through its zero-shot performance in real robot experiments across\nvarious scenes, achieving an average matching accuracy and execution success\nrate of 87% and 67%, respectively.\n","authors":["Shutong Jin","Ruiyu Wang","Kuangyi Chen","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2410.22059v1.pdf","comment":"Accepted by WACV2025"},{"id":"http://arxiv.org/abs/2410.22049v1","updated":"2024-10-29T13:59:46Z","published":"2024-10-29T13:59:46Z","title":"On the Synthesis of Reactive Collision-Free Whole-Body Robot Motions: A\n  Complementarity-based Approach","summary":"  This paper is about generating motion plans for high degree-of-freedom\nsystems that account for collisions along the entire body. A particular class\nof mathematical programs with complementarity constraints become useful in this\nregard. Optimization-based planners can tackle confined-space trajectory\nplanning while being cognizant of robot constraints. However, introducing\nobstacles in this setting transforms the formulation into a non-convex problem\n(oftentimes with ill-posed bilinear constraints), which is non-trivial in a\nreal-time setting. To this end, we present the FLIQC (Fast LInear Quadratic\nComplementarity based) motion planner. Our planner employs a novel motion model\nthat captures the entire rigid robot as well as the obstacle geometry and\nensures non-penetration between the surfaces due to the imposed constraint. We\nperform thorough comparative studies with the state-of-the-art, which\ndemonstrate improved performance. Extensive simulation and hardware experiments\nvalidate our claim of generating continuous and reactive motion plans at 1 kHz\nfor modern collaborative robots with constant minimal parameters.\n","authors":["Haowen Yao","Riddhiman Laha","Anirban Sinha","Jonas Hall","Luis F. C. Figueredo","Nilanjan Chakraborty","Sami Haddadin"],"pdf_url":"https://arxiv.org/pdf/2410.22049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19247v2","updated":"2024-10-29T13:41:28Z","published":"2024-10-25T01:54:17Z","title":"Non-rigid Relative Placement through 3D Dense Diffusion","summary":"  The task of \"relative placement\" is to predict the placement of one object in\nrelation to another, e.g. placing a mug onto a mug rack. Through explicit\nobject-centric geometric reasoning, recent methods for relative placement have\nmade tremendous progress towards data-efficient learning for robot manipulation\nwhile generalizing to unseen task variations. However, they have yet to\nrepresent deformable transformations, despite the ubiquity of non-rigid bodies\nin real world settings. As a first step towards bridging this gap, we propose\n``cross-displacement\" - an extension of the principles of relative placement to\ngeometric relationships between deformable objects - and present a novel\nvision-based method to learn cross-displacement through dense diffusion. To\nthis end, we demonstrate our method's ability to generalize to unseen object\ninstances, out-of-distribution scene configurations, and multimodal goals on\nmultiple highly deformable tasks (both in simulation and in the real world)\nbeyond the scope of prior works. Supplementary information and videos can be\nfound at https://sites.google.com/view/tax3d-corl-2024 .\n","authors":["Eric Cai","Octavian Donca","Ben Eisner","David Held"],"pdf_url":"https://arxiv.org/pdf/2410.19247v2.pdf","comment":"Conference on Robot Learning (CoRL), 2024"},{"id":"http://arxiv.org/abs/2409.20514v2","updated":"2024-10-29T13:34:37Z","published":"2024-09-30T17:16:26Z","title":"Opt2Skill: Imitating Dynamically-feasible Whole-Body Trajectories for\n  Versatile Humanoid Loco-Manipulation","summary":"  Humanoid robots are designed to perform diverse loco-manipulation tasks.\nHowever, they face challenges due to their high-dimensional and unstable\ndynamics, as well as the complex contact-rich nature of the tasks. Model-based\noptimal control methods offer precise and systematic control but are limited by\nhigh computational complexity and accurate contact sensing. On the other hand,\nreinforcement learning (RL) provides robustness and handles high-dimensional\nspaces but suffers from inefficient learning, unnatural motion, and sim-to-real\ngaps. To address these challenges, we introduce Opt2Skill, an end-to-end\npipeline that combines model-based trajectory optimization with RL to achieve\nrobust whole-body loco-manipulation. We generate reference motions for the\nDigit humanoid robot using differential dynamic programming (DDP) and train RL\npolicies to track these trajectories. Our results demonstrate that Opt2Skill\noutperforms pure RL methods in both training efficiency and task performance,\nwith optimal trajectories that account for torque limits enhancing trajectory\ntracking. We successfully transfer our approach to real-world applications.\n","authors":["Fukang Liu","Zhaoyuan Gu","Yilin Cai","Ziyi Zhou","Shijie Zhao","Hyunyoung Jung","Sehoon Ha","Yue Chen","Danfei Xu","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.20514v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22031v1","updated":"2024-10-29T13:29:29Z","published":"2024-10-29T13:29:29Z","title":"A Degree of Flowability for Virtual Tubes","summary":"  With the rapid development of robotics swarm technology, there are more tasks\nthat require the swarm to pass through complicate environments safely and\nefficiently. Virtual tube technology is a novel way to achieve this goal.\nVirtual tubes are free spaces connecting two places that provide safety\nboundaries and direction of motion for swarm robotics. How to determine the\ndesign quality of a virtual tube is a fundamental problem. For such a purpose,\nthis paper presents a degree of flowability (DOF) for two-dimensional virtual\ntubes according to a minimum energy principle. After that, methods to calculate\nDOF are proposed with a feasibility analysis. Simulations of swarm robotics in\ndifferent kinds of two-dimensional virtual tubes are performed to demonstrate\nthe effectiveness of the proposed method of calculating DOF.\n","authors":["Quan Quan","Shuhan Huang","Kai-Yuan Cai"],"pdf_url":"https://arxiv.org/pdf/2410.22031v1.pdf","comment":"22 pages, 12 figures. This is a preprint, currently under review for\n  publication in Robotics and Autonomous Systems, Elsevier"},{"id":"http://arxiv.org/abs/2310.16951v3","updated":"2024-10-29T13:07:16Z","published":"2023-10-25T19:36:28Z","title":"The Teenager's Problem: Efficient Garment Decluttering as Probabilistic\n  Set Cover","summary":"  This paper addresses the \"Teenager's Problem\": efficiently removing scattered\ngarments from a planar surface into a basket. As grasping and transporting\nindividual garments is highly inefficient, we propose policies to select grasp\nlocations for multiple garments using an overhead camera. Our core approach is\nsegment-based, which uses segmentation on the overhead RGB image of the scene.\nWe propose a Probabilistic Set Cover formulation of the problem, aiming to\nminimize the number of grasps that clear all garments off the surface. Grasp\nefficiency is measured by Objects per Transport (OpT), which denotes the\naverage number of objects removed per trip to the laundry basket. Additionally,\nwe explore several depth-based methods, which use overhead depth data to find\nefficient grasps. Experiments suggest that our segment-based method increases\nOpT by $50\\%$ over a random baseline, whereas combined hybrid methods yield\nimprovements of $33\\%$. Finally, a method employing consolidation (with\nsegmentation) is considered, which locally moves the garments on the work\nsurface to increase OpT, when the distance to the basket is much greater than\nthe local motion distances. This yields an improvement of $81\\%$ over the\nbaseline.\n","authors":["Aviv Adler","Ayah Ahmad","Yulei Qiu","Shengyin Wang","Wisdom C. Agboh","Edith Llontop","Tianshuang Qiu","Jeffrey Ichnowski","Thomas Kollar","Richard Cheng","Mehmet Dogar","Ken Goldberg"],"pdf_url":"https://arxiv.org/pdf/2310.16951v3.pdf","comment":"Accepted by the 16th International Workshop on the Algorithmic\n  Foundations of Robotics (WAFR 2024)"},{"id":"http://arxiv.org/abs/2410.22008v1","updated":"2024-10-29T12:55:04Z","published":"2024-10-29T12:55:04Z","title":"Neurofeedback-Driven 6-DOF Robotic Arm: Integration of Brain-Computer\n  Interface with Arduino for Advanced Control","summary":"  Brain computer interface (BCI) applications in robotics are becoming more\nfamous and famous. People with disabilities are facing a real-time problem of\ndoing simple activities such as grasping, handshaking etc. in order to aid with\nthis problem, the use of brain signals to control actuators is showing a great\nimportance. The Emotive Insight, a Brain-Computer Interface (BCI) device, is\nutilized in this project to collect brain signals and transform them into\ncommands for controlling a robotic arm using an Arduino controller. The Emotive\nInsight captures brain signals, which are subsequently analyzed using Emotive\nsoftware and connected with Arduino code. The HITI Brain software integrates\nthese devices, allowing for smooth communication between brain activity and the\nrobotic arm. This system demonstrates how brain impulses may be utilized to\ncontrol external devices directly. The results showed that the system is\napplicable efficiently to robotic arms and also for prosthetic arms with Multi\nDegree of Freedom. In addition to that, the system can be used for other\nactuators such as bikes, mobile robots, wheelchairs etc.\n","authors":["Ihab A. Satam","R√≥bert Szabolcsi"],"pdf_url":"https://arxiv.org/pdf/2410.22008v1.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.22000v1","updated":"2024-10-29T12:43:52Z","published":"2024-10-29T12:43:52Z","title":"Component Modularized Design of Musculoskeletal Humanoid Platform\n  Musashi to Investigate Learning Control Systems","summary":"  To develop Musashi as a musculoskeletal humanoid platform to investigate\nlearning control systems, we aimed for a body with flexible musculoskeletal\nstructure, redundant sensors, and easily reconfigurable structure. For this\npurpose, we develop joint modules that can directly measure joint angles,\nmuscle modules that can realize various muscle routes, and nonlinear elastic\nunits with soft structures, etc. Next, we develop MusashiLarm, a\nmusculoskeletal platform composed of only joint modules, muscle modules,\ngeneric bone frames, muscle wire units, and a few attachments. Finally, we\ndevelop Musashi, a musculoskeletal humanoid platform which extends MusashiLarm\nto the whole body design, and conduct several basic experiments and learning\ncontrol experiments to verify the effectiveness of its concept.\n","authors":["Kento Kawaharazuka","Shogo Makino","Kei Tsuzuki","Moritaka Onitsuka","Yuya Nagamatsu","Koki Shinjo","Tasuku Makabe","Yuki Asano","Kei Okada","Koji Kawasaki","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2410.22000v1.pdf","comment":"Accepted at IROS2019"},{"id":"http://arxiv.org/abs/2405.15677v2","updated":"2024-10-29T11:58:42Z","published":"2024-05-24T16:17:35Z","title":"SMART: Scalable Multi-agent Real-time Generation via Next-token\n  Prediction","summary":"  Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART.\n","authors":["Wei Wu","Xiaoxin Feng","Ziyan Gao","Yuheng Kan"],"pdf_url":"https://arxiv.org/pdf/2405.15677v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.21955v1","updated":"2024-10-29T11:18:04Z","published":"2024-10-29T11:18:04Z","title":"ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian\n  Splatting","summary":"  We propose ActiveSplat, an autonomous high-fidelity reconstruction system\nleveraging Gaussian splatting. Taking advantage of efficient and realistic\nrendering, the system establishes a unified framework for online mapping,\nviewpoint selection, and path planning. The key to ActiveSplat is a hybrid map\nrepresentation that integrates both dense information about the environment and\na sparse abstraction of the workspace. Therefore, the system leverages sparse\ntopology for efficient viewpoint sampling and path planning, while exploiting\nview-dependent dense prediction for viewpoint selection, facilitating efficient\ndecision-making with promising accuracy and completeness. A hierarchical\nplanning strategy based on the topological map is adopted to mitigate\nrepetitive trajectories and improve local granularity given limited budgets,\nensuring high-fidelity reconstruction with photorealistic view synthesis.\nExtensive experiments and ablation studies validate the efficacy of the\nproposed method in terms of reconstruction accuracy, data coverage, and\nexploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.\n","authors":["Yuetao Li","Zijia Kuang","Ting Li","Guyue Zhou","Shaohui Zhang","Zike Yan"],"pdf_url":"https://arxiv.org/pdf/2410.21955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21926v1","updated":"2024-10-29T10:37:37Z","published":"2024-10-29T10:37:37Z","title":"Reliable Semantic Understanding for Real World Zero-shot Object Goal\n  Navigation","summary":"  We introduce an innovative approach to advancing semantic understanding in\nzero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in\nunfamiliar environments. Traditional reliance on labeled data has been a\nlimitation for robotic adaptability, which we address by employing a\ndual-component framework that integrates a GLIP Vision Language Model for\ninitial detection and an InstructionBLIP model for validation. This combination\nnot only refines object and environmental recognition but also fortifies the\nsemantic interpretation, pivotal for navigational decision-making. Our method,\nrigorously tested in both simulated and real-world settings, exhibits marked\nimprovements in navigation precision and reliability.\n","authors":["Halil Utku Unlu","Shuaihang Yuan","Congcong Wen","Hao Huang","Anthony Tzes","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.21926v1.pdf","comment":"16 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2405.04278v4","updated":"2024-10-29T10:36:46Z","published":"2024-05-07T12:46:45Z","title":"Uncertainty Quantification Metrics for Deep Regression","summary":"  When deploying deep neural networks on robots or other physical systems, the\nlearned model should reliably quantify predictive uncertainty. A reliable\nuncertainty allows downstream modules to reason about the safety of its\nactions. In this work, we address metrics for evaluating such an uncertainty.\nSpecifically, we focus on regression tasks, and investigate Area Under\nSparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation,\nand Negative Log-Likelihood (NLL). Using synthetic regression datasets, we look\ninto how those metrics behave under four typical types of uncertainty, their\nstability regarding the size of the test set, and reveal their strengths and\nweaknesses. Our results indicate that Calibration Error is the most stable and\ninterpretable metric, but AUSE and NLL also have their respective use cases. We\ndiscourage the usage of Spearman's Rank Correlation for evaluating\nuncertainties and recommend replacing it with AUSE.\n","authors":["Simon Kristoffersson Lind","Ziliang Xiong","Per-Erik Forss√©n","Volker Kr√ºger"],"pdf_url":"https://arxiv.org/pdf/2405.04278v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05717v6","updated":"2024-10-29T09:38:28Z","published":"2024-07-08T08:21:22Z","title":"A New Framework for Nonlinear Kalman Filters","summary":"  The Kalman filter (KF) is a state estimation algorithm that optimally\ncombines system knowledge and measurements to minimize the mean squared error\nof the estimated states. While KF was initially designed for linear systems,\nnumerous extensions of it, such as extended Kalman filter (EKF), unscented\nKalman filter (UKF), cubature Kalman filter (CKF), etc., have been proposed for\nnonlinear systems. Although different types of nonlinear KFs have different\npros and cons, they all use the same framework of linear KF. Yet, according to\nwhat we found in this paper, the framework often gives overconfident and less\naccurate state estimations when the measurement functions are nonlinear.\nTherefore, in this study, we designed a new framework for nonlinear KFs and\nshowed theoretically and empirically that the new framework estimates the\nstates and covariance matrix more accurately than the old one. The new\nframework was tested on four different nonlinear KFs and five different tasks,\nshowcasing its ability to reduce the estimation errors by several orders of\nmagnitude in low-measurement-noise conditions, with only about a 10 to 90%\nincrease in computational time. To the best of our knowledge, all existing\ntypes of nonlinear KFs can benefit from the new framework, and the benefit will\nincrease as the sensors become more and more accurate in the future. As an\nexample, EKF, the simplest nonlinear KF that was previously believed to work\npoorly for strongly nonlinear systems, can now perform better than the iterated\nextended Kalman filter (IEKF) and provide fast and fairly accurate state\nestimations with the help of the proposed new framework. The codes are\navailable at\nhttps://github.com/Shida-Jiang/A-new-framework-for-nonlinear-Kalman-filters.\n","authors":["Shida Jiang","Junzhe Shi","Scott Moura"],"pdf_url":"https://arxiv.org/pdf/2407.05717v6.pdf","comment":"Changed to IEEE format; theoretical analysis added"},{"id":"http://arxiv.org/abs/2302.03385v2","updated":"2024-10-29T09:18:30Z","published":"2023-02-07T10:45:20Z","title":"NeuronsGym: A Hybrid Framework and Benchmark for Robot Tasks with\n  Sim2Real Policy Learning","summary":"  The rise of embodied AI has greatly improved the possibility of general\nmobile agent systems. At present, many evaluation platforms with rich scenes,\nhigh visual fidelity and various application scenarios have been developed. In\nthis paper, we present a hybrid framework named NeuronsGym that can be used for\npolicy learning of robot tasks, covering a simulation platform for training\npolicy, and a physical system for studying sim2real problems. Unlike most\ncurrent single-task, slow-moving robotic platforms, our framework provides\nagile physical robots with a wider range of speeds, and can be employed to\ntrain robotic navigation and confrontation policies. At the same time, in order\nto evaluate the safety of robot navigation, we propose a safety-weighted path\nlength (SFPL) to improve the safety evaluation in the current mobile robot\nnavigation. Based on this platform, we build a new benchmark for navigation and\nconfrontation tasks under this platform by comparing the current mainstream\nsim2real methods, and hold the 2022 IEEE Conference on Games (CoG) RoboMaster\nsim2real challenge. We release the codes of this\nframework\\footnote{\\url{https://github.com/DRL-CASIA/NeuronsGym}} and hope that\nthis platform can promote the development of more flexible and agile general\nmobile agent algorithms.\n","authors":["Haoran Li","Shasha Liu","Mingjun Ma","Guangzheng Hu","Yaran Chen","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2302.03385v2.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.15840v2","updated":"2024-10-29T08:48:32Z","published":"2024-09-24T08:07:25Z","title":"Distance-based Multiple Non-cooperative Ground Target Encirclement for\n  Complex Environments","summary":"  This paper proposes a comprehensive strategy for complex\nmulti-target-multi-drone encirclement in an obstacle-rich and GPS-denied\nenvironment, motivated by practical scenarios such as pursuing vehicles or\nhumans in urban canyons. The drones have omnidirectional range sensors that can\nrobustly detect ground targets and obtain noisy relative distances. After each\ndrone task is assigned, a novel distance-based target state estimator (DTSE) is\nproposed by estimating the measurement output noise variance and utilizing the\nKalman filter. By integrating anti-synchronization techniques and pseudo-force\nfunctions, an acceleration controller enables two tasking drones to\ncooperatively encircle a target from opposing positions while navigating\nobstacles. The algorithms effectiveness for the discrete-time double-integrator\nsystem is established theoretically, particularly regarding observability.\nMoreover, the versatility of the algorithm is showcased in aerial-to-ground\nscenarios, supported by compelling simulation results. Experimental validation\ndemonstrates the effectiveness of the proposed approach.\n","authors":["Fen Liu","Shenghai Yuan","Kun Cao","Wei Meng","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2409.15840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21845v1","updated":"2024-10-29T08:12:20Z","published":"2024-10-29T08:12:20Z","title":"Precise and Dexterous Robotic Manipulation via Human-in-the-Loop\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) holds great promise for enabling autonomous\nacquisition of complex robotic manipulation skills, but realizing this\npotential in real-world settings has been challenging. We present a\nhuman-in-the-loop vision-based RL system that demonstrates impressive\nperformance on a diverse set of dexterous manipulation tasks, including dynamic\nmanipulation, precision assembly, and dual-arm coordination. Our approach\nintegrates demonstrations and human corrections, efficient RL algorithms, and\nother system-level design choices to learn policies that achieve near-perfect\nsuccess rates and fast cycle times within just 1 to 2.5 hours of training. We\nshow that our method significantly outperforms imitation learning baselines and\nprior RL approaches, with an average 2x improvement in success rate and 1.8x\nfaster execution. Through extensive experiments and analysis, we provide\ninsights into the effectiveness of our approach, demonstrating how it learns\nrobust, adaptive policies for both reactive and predictive control strategies.\nOur results suggest that RL can indeed learn a wide range of complex\nvision-based manipulation policies directly in the real world within practical\ntraining times. We hope this work will inspire a new generation of learned\nrobotic manipulation techniques, benefiting both industrial applications and\nresearch advancements. Videos and code are available at our project website\nhttps://hil-serl.github.io/.\n","authors":["Jianlan Luo","Charles Xu","Jeffrey Wu","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.21845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07553v2","updated":"2024-10-29T07:43:50Z","published":"2024-05-13T08:33:00Z","title":"Space Domain based Ecological Cooperative and Adaptive Cruise Control on\n  Rolling Terrain","summary":"  Cooperative and Adaptive Cruise Control (CACC) is widely focused to enhance\ndriving fuel-efficiency by maintaining a close following gap. The ecology of\nCACC could be further enhanced by adapting to the rolling terrain. However,\ncurrent studies cannot ensure both planning optimality and computational\nefficiency. Firstly, current studies are mostly formulated on the conventional\ntime domain. These time domain based methods cannot ensure planning optimality\nfor space-varying road slopes. Secondly, fuel consumption models are non-linear\nand hard to solve efficiently. Hence, this paper proposes a space domain based\nEcological-CACC (Eco-CACC) controller. It is formulated into a nonlinear\noptimal control problem with the objective of optimizing global fuel\nconsumptions. Furthermore, a differential dynamic programming-based solving\nmethod is developed to ensure real-time computational efficiency. Simulation\nresults have shown that the proposed Eco-CACC controller can improve average\nfuel saving by 37.67% at collector road and about 17.30% at major arterial.\nString stability of the proposed method has been theoretically proven and\nexperimentally validated.\n","authors":["Mingyue Lei","Haoran Wang","Lu Xiong"," Jaehyun"," So","Ashish Dhamaniya","Jia Hu"],"pdf_url":"https://arxiv.org/pdf/2405.07553v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21795v1","updated":"2024-10-29T07:00:47Z","published":"2024-10-29T07:00:47Z","title":"Robot Policy Learning with Temporal Optimal Transport Reward","summary":"  Reward specification is one of the most tricky problems in Reinforcement\nLearning, which usually requires tedious hand engineering in practice.\n  One promising approach to tackle this challenge is to adopt existing expert\nvideo demonstrations for policy learning.\n  Some recent work investigates how to learn robot policies from only a\nsingle/few expert video demonstrations.\n  For example, reward labeling via Optimal Transport (OT) has been shown to be\nan effective strategy to generate a proxy reward by measuring the alignment\nbetween the robot trajectory and the expert demonstrations.\n  However, previous work mostly overlooks that the OT reward is invariant to\ntemporal order information, which could bring extra noise to the reward signal.\n  To address this issue, in this paper, we introduce the Temporal Optimal\nTransport (TemporalOT) reward to incorporate temporal order information for\nlearning a more accurate OT-based proxy reward.\n  Extensive experiments on the Meta-world benchmark tasks validate the efficacy\nof the proposed method.\n  Code is available at: https://github.com/fuyw/TemporalOT\n","authors":["Yuwei Fu","Haichao Zhang","Di Wu","Wei Xu","Benoit Boulet"],"pdf_url":"https://arxiv.org/pdf/2410.21795v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.21758v1","updated":"2024-10-29T05:46:16Z","published":"2024-10-29T05:46:16Z","title":"DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial\n  Information for Dynamics Model Learning","summary":"  This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs)\n(e.g., elasto-plastic objects) with full spatial information (i.e., top, side,\nand bottom information) using a novel and low-cost data collection platform\nwith a transparent operating plane. The dataset consists of active manipulation\naction, multi-view RGB-D images, well-registered point clouds, 3D deformed\nmesh, and 3D occupancy with semantics, using a pinching strategy with a\ntwo-parallel-finger gripper. In addition, we trained a neural network with the\ndown-sampled 3D occupancy and action as input to model the dynamics of an\nelasto-plastic object. Our dataset and all CADs of the data collection system\nwill be released soon on our website.\n","authors":["Zhen Zhang","Xiangyu Chu","Yunxi Tang","K. W. Samuel Au"],"pdf_url":"https://arxiv.org/pdf/2410.21758v1.pdf","comment":"5 pages, 6 figures, 2024 CoRL Workshop on Learning Robot Fine and\n  Dexterous Manipulation: Perception and Control"}]},"2024-10-28T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.21588v1","updated":"2024-10-28T22:43:35Z","published":"2024-10-28T22:43:35Z","title":"Topological numbers and their use to characterize simple points for 2D\n  binary images","summary":"  In this paper, we adapt the two topological numbers, which have been proposed\nto efficiently characterize simple points in specific neighborhoods for 3D\nbinary images, to the case of 2D binary images. Unlike the 3D case, we only use\na single neighborhood to define these two topological numbers for the 2D case.\nThen, we characterize simple points either by using the two topological numbers\nor by a single topological number linked to another one condition. We compare\nthe characterization of simple points by topological numbers with two other\nones based on Hilditch crossing number and Yokoi number. We also highlight the\nnumber of possible configurations corresponding to a simple point, which also\nrepresents the maximum limit of local configurations that a thinning algorithm\noperating by parallel deletion of simple (individual) points may delete while\npreserving topology (limit usually not reachable, depending on the deletion\nstrategy).\n","authors":["Christophe Lohou"],"pdf_url":"https://arxiv.org/pdf/2410.21588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21204v1","updated":"2024-10-28T16:49:50Z","published":"2024-10-28T16:49:50Z","title":"On Spheres with $k$ Points Inside","summary":"  We generalize a classical result by Boris Delaunay that introduced Delaunay\ntriangulations. In particular, we prove that for a locally finite and coarsely\ndense generic point set $A$ in $\\mathbb{R}^d$, every generic point of\n$\\mathbb{R}^d$ belongs to exactly $\\binom{d+k}{d}$ simplices whose vertices\nbelong to $A$ and whose circumspheres enclose exactly $k$ points of $A$. We\nextend this result to the cases in which the points are weighted, and when $A$\ncontains only finitely many points in $\\mathbb{R}^d$ or in $\\mathbb{S}^d$.\nFurthermore, we use the result to give a new geometric proof for the fact that\nvolumes of hypersimplices are Eulerian numbers.\n","authors":["Herbert Edelsbrunner","Alexey Garber","Morteza Saghafian"],"pdf_url":"https://arxiv.org/pdf/2410.21204v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.21004v1","updated":"2024-10-28T13:28:21Z","published":"2024-10-28T13:28:21Z","title":"Push-Forward Signed Distance Functions enable interpretable and robust\n  continuous shape quantification","summary":"  We introduce the Push-Forward Signed Distance Morphometric (PF-SDM), a novel\nmethod for shape quantification in biomedical imaging that is continuous,\ninterpretable, and invariant to shape-preserving transformations. PF-SDM\neffectively captures the geometric properties of shapes, including their\ntopological skeletons and radial symmetries. This results in a robust and\ninterpretable shape descriptor that generalizes to capture temporal shape\ndynamics. Importantly, PF-SDM avoids certain issues of previous geometric\nmorphometrics, like Elliptical Fourier Analysis and Generalized Procrustes\nAnalysis, such as coefficient correlations and landmark choices. We present the\nPF-SDM theory, provide a practically computable algorithm, and benchmark it on\nsynthetic data.\n","authors":["Roua Rouatbi","Juan Esteban Suarez","Ivo F. Sbalzarini"],"pdf_url":"https://arxiv.org/pdf/2410.21004v1.pdf","comment":"8 pages, 4 figures"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2402.08851v5","updated":"2024-10-28T21:30:52Z","published":"2024-02-13T23:37:53Z","title":"Cardinal-Utility Matching Markets: The Quest for Envy-Freeness,\n  Pareto-Optimality, and Efficient Computability","summary":"  Unlike ordinal-utility matching markets, which are well-developed from the\nviewpoint of both theory and practice, recent insights from a computer science\nperspective have left cardinal-utility matching markets in a state of flux. The\ncelebrated pricing-based mechanism for one-sided cardinal-utility matching\nmarkets due to Hylland and Zeckhauser, which had long eluded efficient\nalgorithms, was finally shown to be intractable; the problem of computing an\napproximate equilibrium is PPAD-complete.\n  This led us to ask the question: is there an alternative, polynomial time,\nmechanism for one-sided cardinal-utility matching markets which achieves the\ndesirable properties of HZ, i.e. (ex-ante) envy-freeness (EF) and\nPareto-optimality (PO)?\n  We show that the problem of finding an EF+PO lottery in a one-sided\ncardinal-utility matching market is by itself already PPAD-complete. However, a\n$(2 + \\epsilon)$-approximately envy-free and (exactly) Pareto-optimal lottery\ncan be found in polynomial time using the Nash-bargaining-based mechanism of\nHosseini and Vazirani. Moreover, the mechanism is also $(2 +\n\\epsilon)$-approximately incentive compatible.\n  We also present several results on two-sided cardinal-utility matching\nmarkets, including non-existence of EF+PO lotteries as well as existence of\njustified-envy-free and weak Pareto-optimal lotteries.\n","authors":["Thorben Tr√∂bst","Vijay V. Vazirani"],"pdf_url":"https://arxiv.org/pdf/2402.08851v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18703v2","updated":"2024-10-28T20:51:59Z","published":"2024-05-29T02:27:47Z","title":"Bridging the Gap between Partially Observable Stochastic Games and\n  Sparse POMDP Methods","summary":"  Many real-world decision problems involve the interaction of multiple\nself-interested agents with limited sensing ability. The partially observable\nstochastic game (POSG) provides a mathematical framework for modeling these\nproblems, however solving a POSG requires difficult reasoning over two critical\nfactors: (1) information revealed by partial observations and (2) decisions\nother agents make. In the single agent case, partially observable Markov\ndecision process (POMDP) planning can efficiently address partial observability\nwith particle filtering. In the multi-agent case, extensive form game solution\nmethods account for other agent's decisions, but preclude belief approximation.\nWe propose a unifying framework that combines POMDP-inspired state distribution\napproximation and game-theoretic equilibrium search on information sets. This\npaper lays a theoretical foundation for the approach by bounding errors due to\nbelief approximation, and empirically demonstrates effectiveness with a\nnumerical example. The new approach enables planning in POSGs with very large\nstate spaces, paving the way for reliable autonomous interaction in real-world\nphysical environments and complementing multi-agent reinforcement learning.\n","authors":["Tyler Becker","Zachary Sunberg"],"pdf_url":"https://arxiv.org/pdf/2405.18703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16843v3","updated":"2024-10-28T18:56:51Z","published":"2024-03-25T15:04:11Z","title":"Do LLM Agents Have Regret? A Case Study in Online Learning and Games","summary":"  Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.\n","authors":["Chanwoo Park","Xiangyu Liu","Asuman Ozdaglar","Kaiqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16843v3.pdf","comment":"added references to related and concurrent work, and longer-horizon\n  and stochastic bandit experiments"},{"id":"http://arxiv.org/abs/2410.21447v1","updated":"2024-10-28T18:51:03Z","published":"2024-10-28T18:51:03Z","title":"You Can't Always Get What You Want : Games of Ordered Preference","summary":"  We study noncooperative games, in which each agent's objective is composed of\na sequence of ordered-and potentially conflicting-preferences. Problems of this\ntype naturally model a wide variety of scenarios: for example, drivers at a\nbusy intersection must balance the desire to make forward progress with the\nrisk of collision. Mathematically, these problems possess a nested structure,\nand to behave properly agents must prioritize their most important preference,\nand only consider less important preferences to the extent that they do not\ncompromise performance on more important ones. We consider multi-agent,\nnoncooperative variants of these problems, and seek generalized Nash equilibria\nin which each agent's decision reflects both its hierarchy of preferences and\nother agents' actions. We make two key contributions. First, we develop a\nrecursive approach for deriving the first-order optimality conditions of each\nagent's nested problem. Second, we propose a sequence of increasingly tight\nrelaxations, each of which can be transcribed as a mixed complementarity\nproblem and solved via existing methods. Experimental results demonstrate that\nour approach reliably converges to equilibrium solutions that strictly reflect\nagents' individual ordered preferences.\n","authors":["Dong Ho Lee","Lasse Peters","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2410.21447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21446v1","updated":"2024-10-28T18:51:00Z","published":"2024-10-28T18:51:00Z","title":"Improving DeFi Mechanisms with Dynamic Games and Optimal Control: A Case\n  Study in Stablecoins","summary":"  Stablecoins are a class of cryptocurrencies which aim at providing\nconsistency and predictability, typically by pegging the token's value to that\nof a real world asset. Designing resilient decentralized stablecoins is a\nchallenge, and prominent stablecoins today either (i) give up on\ndecentralization, or (ii) rely on user-owned cryptocurrencies as collateral,\nexposing the token to exogenous price fluctuations. In this latter category, it\nis increasingly common to employ algorithmic mechanisms to automate risk\nmanagement, helping maintain the peg. One example of this is Reflexer's RAI,\nwhich adapts its system-internal exchange rate (redemption price) to secondary\nmarket conditions according to a proportional control law. In this paper, we\ntake this idea of active management a step further, and introduce a new kind of\ncontrol scheme based on a Stackelberg game model between the token protocol and\nits users. By doing so, we show that (i) we can mitigate adverse depeg events\nthat inevitably arise in a fixed-redemption scheme such as MakerDao's DAI and\n(ii) generally outperform a simpler, adaptive-redemption scheme such as RAI in\nthe task of targeting a desired market price. We demonstrate these results\nthrough extensive simulations over a range of market conditions.\n","authors":["Nicholas Strohmeyer","Sriram Vishwanath","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2410.21446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03392v2","updated":"2024-10-28T18:44:50Z","published":"2023-12-06T10:04:03Z","title":"O'Neill's Theorem for Games","summary":"  We present an analog of O'Neill's Theorem (Theorem 5.2 in [17]) for finite\ngames, which reveals some of the structure of equilibria under payoff\nperturbations in finite games.\n","authors":["Srihari Govindan","Rida Laraki","Lucas Pahl"],"pdf_url":"https://arxiv.org/pdf/2312.03392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02898v2","updated":"2024-10-28T18:22:42Z","published":"2024-04-03T17:55:20Z","title":"Fully Decentralized Task Offloading in Multi-Access Edge Computing\n  Systems","summary":"  We consider the problem of task offloading in multi-access edge computing\n(MEC) systems constituting $N$ devices assisted by an edge server (ES), where\nthe devices can split task execution between a local processor and the ES.\nSince the local task execution and communication with the ES both consume\npower, each device must judiciously choose between the two. We model the\nproblem as a large population non-cooperative game among the $N$ devices. Since\ncomputation of an equilibrium in this scenario is difficult due to the presence\nof a large number of devices, we employ the mean-field game framework to reduce\nthe finite-agent game problem to a generic user's multi-objective optimization\nproblem, with a coupled consistency condition. By leveraging the novel age of\ninformation (AoI) metric, we invoke techniques from stochastic hybrid systems\n(SHS) theory and study the tradeoffs between increasing information freshness\nand reducing power consumption. In numerical simulations, we validate that a\nhigher load at the ES may lead devices to upload their task to the ES less\noften.\n","authors":["Shubham Aggarwal","Muhammad Aneeq uz Zaman","Melih Bastopcu","Sennur Ulukus","Tamer Ba≈üar"],"pdf_url":"https://arxiv.org/pdf/2404.02898v2.pdf","comment":"Accepted to IEEE Globecom Workshops 2024"},{"id":"http://arxiv.org/abs/2410.17466v3","updated":"2024-10-28T18:00:25Z","published":"2024-10-22T22:49:04Z","title":"Evolution with Opponent-Learning Awareness","summary":"  The universe involves many independent co-learning agents as an ever-evolving\npart of our observed environment. Yet, in practice, Multi-Agent Reinforcement\nLearning (MARL) applications are usually constrained to small, homogeneous\npopulations and remain computationally intensive. In this paper, we study how\nlarge heterogeneous populations of learning agents evolve in normal-form games.\nWe show how, under assumptions commonly made in the multi-armed bandit\nliterature, Multi-Agent Policy Gradient closely resembles the Replicator\nDynamic, and we further derive a fast, parallelizable implementation of\nOpponent-Learning Awareness tailored for evolutionary simulations. This enables\nus to simulate the evolution of very large populations made of heterogeneous\nco-learning agents, under both naive and advanced learning strategies. We\ndemonstrate our approach in simulations of 200,000 agents, evolving in the\nclassic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game\nhighlights distinct ways in which Opponent-Learning Awareness affects\nevolution.\n","authors":["Yann Bouteiller","Karthik Soma","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17466v3.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2402.11769v2","updated":"2024-10-28T17:19:04Z","published":"2024-02-19T01:48:11Z","title":"Connection-Aware P2P Trading: Simultaneous Trading and Peer Selection","summary":"  Peer-to-peer (P2P) trading is seen as a viable solution to handle the growing\nnumber of distributed energy resources in distribution networks. However, when\ndealing with large-scale consumers, there are several challenges that must be\naddressed. One of these challenges is limited communication capabilities.\nAdditionally, prosumers may have specific preferences when it comes to trading.\nBoth can result in serious asynchrony in peer-to-peer trading, potentially\nimpacting the effectiveness of negotiations and hindering convergence before\nthe market closes. This paper introduces a connection-aware P2P trading\nalgorithm designed for extensive prosumer trading. The algorithm facilitates\nasynchronous trading while respecting prosumer's autonomy in trading peer\nselection, an often overlooked aspect in traditional models. In addition, to\noptimize the use of limited connection opportunities, a smart trading peer\nconnection selection strategy is developed to guide consumers to communicate\nstrategically to accelerate convergence. A theoretical convergence guarantee is\nprovided for the connection-aware P2P trading algorithm, which further details\nhow smart selection strategies enhance convergence efficiency. Numerical\nstudies are carried out to validate the effectiveness of the connection-aware\nalgorithm and the performance of smart selection strategies in reducing the\noverall convergence time.\n","authors":["Cheng Feng","Kedi Zheng","Lanqing Shan","Hani Alers","Qixin Chen","Lampros Stergioulas","Hongye Guo"],"pdf_url":"https://arxiv.org/pdf/2402.11769v2.pdf","comment":"Paper accepted for Applied Energy. Personal use of this material is\n  permitted. Permission from Elsevier must be obtained for all other uses"},{"id":"http://arxiv.org/abs/2406.14165v2","updated":"2024-10-28T15:40:57Z","published":"2024-06-20T10:10:21Z","title":"Mechanism design augmented with output advice","summary":"  Our work revisits the design of mechanisms via the learning-augmented\nframework. In this model, the algorithm is enhanced with imperfect\n(machine-learned) information concerning the input, usually referred to as\nprediction. The goal is to design algorithms whose performance degrades gently\nas a function of the prediction error and, in particular, perform well if the\nprediction is accurate, but also provide a worst-case guarantee under any\npossible error. This framework has been successfully applied recently to\nvarious mechanism design settings, where in most cases the mechanism is\nprovided with a prediction about the types of the players.\n  We adopt a perspective in which the mechanism is provided with an output\nrecommendation. We make no assumptions about the quality of the suggested\noutcome, and the goal is to use the recommendation to design mechanisms with\nlow approximation guarantees whenever the recommended outcome is reasonable,\nbut at the same time to provide worst-case guarantees whenever the\nrecommendation significantly deviates from the optimal one. We propose a\ngeneric, universal measure, which we call quality of recommendation, to\nevaluate mechanisms across various information settings. We demonstrate how\nthis new metric can provide refined analysis in existing results.\n  This model introduces new challenges, as the mechanism receives limited\ninformation comparing to settings that use predictions about the types of the\nagents. We study, through this lens, several well-studied mechanism design\nparadigms, devising new mechanisms, but also providing refined analysis for\nexisting ones, using as a metric the quality of recommendation. We complement\nour positive results, by exploring the limitations of known classes of\nstrategyproof mechanisms that can be devised using output recommendation.\n","authors":["George Christodoulou","Alkmini Sgouritsa","Ioannis Vlachos"],"pdf_url":"https://arxiv.org/pdf/2406.14165v2.pdf","comment":"This paper has been accepted by the 38th Conference on Neural\n  Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2402.14005v3","updated":"2024-10-28T14:38:36Z","published":"2024-02-21T18:44:38Z","title":"Relying on the Metrics of Evaluated Agents","summary":"  Online platforms and regulators face a continuing problem of designing\neffective evaluation metrics. While tools for collecting and processing data\ncontinue to progress, this has not addressed the problem of \"unknown unknowns\",\nor fundamental informational limitations on part of the evaluator. To guide the\nchoice of metrics in the face of this informational problem, we turn to the\nevaluated agents themselves, who may have more information about how to measure\ntheir own outcomes. We model this interaction as an agency game, where we ask:\n\"When does an agent have an incentive to reveal the observability of a metric\nto their evaluator?\" We show that an agent will prefer to reveal metrics that\ndifferentiate the most difficult tasks from the rest, and conceal metrics that\ndifferentiate the easiest. We further show that the agent can prefer to reveal\na metric \"garbled\" with noise over both fully concealing and fully revealing.\nThis indicates an economic value to privacy that yields Pareto improvement for\nboth the agent and evaluator. We demonstrate these findings on data from online\nrideshare platforms.\n","authors":["Serena Wang","Michael I. Jordan","Katrina Ligett","R. Preston McAfee"],"pdf_url":"https://arxiv.org/pdf/2402.14005v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20981v2","updated":"2024-10-28T11:31:04Z","published":"2024-07-30T17:17:33Z","title":"Escape Sensing Games: Detection-vs-Evasion in Security Applications","summary":"  Traditional game-theoretic research for security applications primarily\nfocuses on the allocation of external protection resources to defend targets.\nThis work puts forward the study of a new class of games centered around\nstrategically arranging targets to protect them against a constrained\nadversary, with motivations from varied domains such as peacekeeping resource\ntransit and cybersecurity. Specifically, we introduce Escape Sensing Games\n(ESGs). In ESGs, a blue player manages the order in which targets pass through\na channel, while her opponent tries to capture the targets using a set of\nsensors that need some time to recharge after each activation. We present a\nthorough computational study of ESGs. Among others, we show that it is NP-hard\nto compute best responses and equilibria. Nevertheless, we propose a variety of\neffective (heuristic) algorithms whose quality we demonstrate in extensive\ncomputational experiments.\n","authors":["Niclas Boehmer","Minbiao Han","Haifeng Xu","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2407.20981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20891v1","updated":"2024-10-28T10:18:17Z","published":"2024-10-28T10:18:17Z","title":"Revenue Maximization Mechanisms for an Uninformed Mediator with\n  Communication Abilities","summary":"  Consider a market where a seller owns an item for sale and a buyer wants to\npurchase it. Each player has private information, known as their type. It can\nbe costly and difficult for the players to reach an agreement through direct\ncommunication. However, with a mediator as a trusted third party, both players\ncan communicate privately with the mediator without worrying about leaking too\nmuch or too little information. The mediator can design and commit to a\nmulti-round communication protocol for both players, in which they update their\nbeliefs about the other player's type. The mediator cannot force the players to\ntrade but can influence their behaviors by sending messages to them.\n  We study the problem of designing revenue-maximizing mechanisms for the\nmediator. We show that the mediator can, without loss of generality, focus on a\nset of direct and incentive-compatible mechanisms. We then formulate this\nproblem as a mathematical program and provide an optimal solution in closed\nform under a regularity condition. Our mechanism is simple and has a threshold\nstructure. Additionally, we extend our results to general cases by utilizing a\nvariant version of the ironing technique. In the end, we discuss some\ninteresting properties revealed from the optimal mechanism, such as, in the\noptimal mechanism, the mediator may even lose money in some cases.\n","authors":["Zhikang Fan","Weiran Shen"],"pdf_url":"https://arxiv.org/pdf/2410.20891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19321v2","updated":"2024-10-28T02:06:31Z","published":"2024-10-25T06:13:26Z","title":"Free-Rider and Conflict Aware Collaboration Formation for Cross-Silo\n  Federated Learning","summary":"  Federated learning (FL) is a machine learning paradigm that allows multiple\nFL participants (FL-PTs) to collaborate on training models without sharing\nprivate data. Due to data heterogeneity, negative transfer may occur in the FL\ntraining process. This necessitates FL-PT selection based on their data\ncomplementarity. In cross-silo FL, organizations that engage in business\nactivities are key sources of FL-PTs. The resulting FL ecosystem has two\nfeatures: (i) self-interest, and (ii) competition among FL-PTs. This requires\nthe desirable FL-PT selection strategy to simultaneously mitigate the problems\nof free riders and conflicts of interest among competitors. To this end, we\npropose an optimal FL collaboration formation strategy -- FedEgoists -- which\nensures that: (1) a FL-PT can benefit from FL if and only if it benefits the FL\necosystem, and (2) a FL-PT will not contribute to its competitors or their\nsupporters. It provides an efficient clustering solution to group FL-PTs into\ncoalitions, ensuring that within each coalition, FL-PTs share the same\ninterest. We theoretically prove that the FL-PT coalitions formed are optimal\nsince no coalitions can collaborate together to improve the utility of any of\ntheir members. Extensive experiments on widely adopted benchmark datasets\ndemonstrate the effectiveness of FedEgoists compared to nine state-of-the-art\nbaseline methods, and its ability to establish efficient collaborative networks\nin cross-silos FL with FL-PTs that engage in business activities.\n","authors":["Mengmeng Chen","Xiaohu Wu","Xiaoli Tang","Tiantian He","Yew-Soon Ong","Qiqi Liu","Qicheng Lao","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2410.19321v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.21490v1","updated":"2024-10-28T20:01:50Z","published":"2024-10-28T20:01:50Z","title":"Can Large Language Models Act as Symbolic Reasoners?","summary":"  The performance of Large language models (LLMs) across a broad range of\ndomains has been impressive but have been critiqued as not being able to reason\nabout their process and conclusions derived. This is to explain the conclusions\ndraw, and also for determining a plan or strategy for their approach. This\npaper explores the current research in investigating symbolic reasoning and\nLLMs, and whether an LLM can inherently provide some form of reasoning or\nwhether supporting components are necessary, and, if there is evidence for a\nreasoning capability, is this evident in a specific domain or is this a general\ncapability? In addition, this paper aims to identify the current research gaps\nand future trends of LLM explainability, presenting a review of the literature,\nidentifying current research into this topic and suggests areas for future\nwork.\n","authors":["Rob Sullivan","Nelly Elsayed"],"pdf_url":"https://arxiv.org/pdf/2410.21490v1.pdf","comment":"18 pages, currently under review"},{"id":"http://arxiv.org/abs/2410.21484v1","updated":"2024-10-28T19:49:53Z","published":"2024-10-28T19:49:53Z","title":"A Systematic Review of Machine Learning in Sports Betting: Techniques,\n  Challenges, and Future Directions","summary":"  The sports betting industry has experienced rapid growth, driven largely by\ntechnological advancements and the proliferation of online platforms. Machine\nlearning (ML) has played a pivotal role in the transformation of this sector by\nenabling more accurate predictions, dynamic odds-setting, and enhanced risk\nmanagement for both bookmakers and bettors. This systematic review explores\nvarious ML techniques, including support vector machines, random forests, and\nneural networks, as applied in different sports such as soccer, basketball,\ntennis, and cricket. These models utilize historical data, in-game statistics,\nand real-time information to optimize betting strategies and identify value\nbets, ultimately improving profitability. For bookmakers, ML facilitates\ndynamic odds adjustment and effective risk management, while bettors leverage\ndata-driven insights to exploit market inefficiencies. This review also\nunderscores the role of ML in fraud detection, where anomaly detection models\nare used to identify suspicious betting patterns. Despite these advancements,\nchallenges such as data quality, real-time decision-making, and the inherent\nunpredictability of sports outcomes remain. Ethical concerns related to\ntransparency and fairness are also of significant importance. Future research\nshould focus on developing adaptive models that integrate multimodal data and\nmanage risk in a manner akin to financial portfolios. This review provides a\ncomprehensive examination of the current applications of ML in sports betting,\nand highlights both the potential and the limitations of these technologies.\n","authors":["Ren√© Manass√© Galekwa","Jean Marie Tshimula","Etienne Gael Tajeuna","Kyamakya Kyandoghere"],"pdf_url":"https://arxiv.org/pdf/2410.21484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21413v1","updated":"2024-10-28T18:12:30Z","published":"2024-10-28T18:12:30Z","title":"Approaches to Simultaneously Solving Variational Quantum Eigensolver\n  Problems","summary":"  The variational quantum eigensolver (VQE), a type of variational quantum\nalgorithm, is a hybrid quantum-classical algorithm to find the lowest-energy\neigenstate of a particular Hamiltonian. We investigate ways to optimize the VQE\nsolving process on multiple instances of the same problem, by observing the\nprocess on one instance of the problem to inform initialization for other\nprocesses. We aim to take advantage of the VQE solution process to obtain\nuseful information while disregarding information which we can predict to not\nbe very useful. In particular, we find that the solution process produces lots\nof data with very little new information. Therefore, we can safely disregard\nmuch of this repetitive information with little effect on the outcome of the\nsolution process.\n","authors":["Adam Hutchings","Eric Yarnot","Xinpeng Li","Qiang Guan","Ning Xie","Shuai Xu","Vipin Chaudhary"],"pdf_url":"https://arxiv.org/pdf/2410.21413v1.pdf","comment":"4 pages, 5 figures, QCCC-24 conference"},{"id":"http://arxiv.org/abs/2401.07387v2","updated":"2024-10-28T17:24:42Z","published":"2024-01-14T22:46:53Z","title":"Noise-Aware Training of Neuromorphic Dynamic Device Networks","summary":"  Physical computing has the potential to enable widespread embodied\nintelligence by leveraging the intrinsic dynamics of complex systems for\nefficient sensing, processing, and interaction. While individual devices\nprovide basic data processing capabilities, networks of interconnected devices\ncan perform more complex and varied tasks. However, designing networks to\nperform dynamic tasks is challenging without physical models and accurate\nquantification of device noise. We propose a novel, noise-aware methodology for\ntraining device networks using Neural Stochastic Differential Equations\n(Neural-SDEs) as differentiable digital twins, accurately capturing the\ndynamics and associated stochasticity of devices with intrinsic memory. Our\napproach employs backpropagation through time and cascade learning, allowing\nnetworks to effectively exploit the temporal properties of physical devices. We\nvalidate our method on diverse networks of spintronic devices across temporal\nclassification and regression benchmarks. By decoupling the training of\nindividual device models from network training, our method reduces the required\ntraining data and provides a robust framework for programming dynamical devices\nwithout relying on analytical descriptions of their dynamics.\n","authors":["Luca Manneschi","Ian T. Vidamour","Kilian D. Stenning","Charles Swindells","Guru Venkat","David Griffin","Lai Gui","Daanish Sonawala","Denis Donskikh","Dana Hariga","Susan Stepney","Will R. Branford","Jack C. Gartside","Thomas Hayward","Matthew O. A. Ellis","Eleni Vasilaki"],"pdf_url":"https://arxiv.org/pdf/2401.07387v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19486v2","updated":"2024-10-28T11:16:21Z","published":"2024-10-25T11:44:06Z","title":"x-RAGE: eXtended Reality -- Action & Gesture Events Dataset","summary":"  With the emergence of the Metaverse and focus on wearable devices in the\nrecent years gesture based human-computer interaction has gained significance.\nTo enable gesture recognition for VR/AR headsets and glasses several datasets\nfocusing on egocentric i.e. first-person view have emerged in recent years.\nHowever, standard frame-based vision suffers from limitations in data bandwidth\nrequirements as well as ability to capture fast motions. To overcome these\nlimitation bio-inspired approaches such as event-based cameras present an\nattractive alternative. In this work, we present the first event-camera based\negocentric gesture dataset for enabling neuromorphic, low-power solutions for\nXR-centric gesture recognition. The dataset has been made available publicly at\nthe following URL: https://gitlab.com/NVM_IITD_Research/xrage.\n","authors":["Vivek Parmar","Dwijay Bane","Syed Shakib Sarwar","Kleber Stangherlin","Barbara De Salvo","Manan Suri"],"pdf_url":"https://arxiv.org/pdf/2410.19486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00024v2","updated":"2024-10-28T06:30:42Z","published":"2024-05-24T06:11:17Z","title":"Embedding-Aligned Language Models","summary":"  We propose a novel approach for training large language models (LLMs) to\nadhere to objectives defined within a latent embedding space. Our method\nleverages reinforcement learning (RL), treating a pre-trained LLM as an\nenvironment. Our embedding-aligned guided language (EAGLE) agent is trained to\niteratively steer the LLM's generation towards optimal regions of the latent\nembedding space, w.r.t. some predefined criterion. We demonstrate the\neffectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review\ndatasets to surface content gaps that satisfy latent user demand. We also\ndemonstrate the benefit of using an optimal design of a state-dependent action\nset to improve EAGLE's efficiency. Our work paves the way for controlled and\ngrounded text generation using LLMs, ensuring consistency with domain-specific\nknowledge and data representations.\n","authors":["Guy Tennenholtz","Yinlam Chow","Chih-Wei Hsu","Lior Shani","Ethan Liang","Craig Boutilier"],"pdf_url":"https://arxiv.org/pdf/2406.00024v2.pdf","comment":"Accepted Neurips 2024"}],"Graphics":[{"id":"http://arxiv.org/abs/2405.01536v2","updated":"2024-10-28T17:02:28Z","published":"2024-05-02T17:59:52Z","title":"Customizing Text-to-Image Models with a Single Image Pair","summary":"  Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.\n","authors":["Maxwell Jones","Sheng-Yu Wang","Nupur Kumari","David Bau","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.01536v2.pdf","comment":"project page: https://paircustomization.github.io/"},{"id":"http://arxiv.org/abs/2405.20343v3","updated":"2024-10-28T15:41:12Z","published":"2024-05-30T17:59:54Z","title":"Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single\n  Image","summary":"  In this work, we introduce Unique3D, a novel image-to-3D framework for\nefficiently generating high-quality 3D meshes from single-view images,\nfeaturing state-of-the-art generation fidelity and strong generalizability.\nPrevious methods based on Score Distillation Sampling (SDS) can produce\ndiversified 3D results by distilling 3D knowledge from large 2D diffusion\nmodels, but they usually suffer from long per-case optimization time with\ninconsistent issues. Recent works address the problem and generate better 3D\nresults either by finetuning a multi-view diffusion model or training a fast\nfeed-forward model. However, they still lack intricate textures and complex\ngeometries due to inconsistency and limited generated resolution. To\nsimultaneously achieve high fidelity, consistency, and efficiency in single\nimage-to-3D, we propose a novel framework Unique3D that includes a multi-view\ndiffusion model with a corresponding normal diffusion model to generate\nmulti-view images with their normal maps, a multi-level upscale process to\nprogressively improve the resolution of generated orthographic multi-views, as\nwell as an instant and consistent mesh reconstruction algorithm called ISOMER,\nwhich fully integrates the color and geometric priors into mesh results.\nExtensive experiments demonstrate that our Unique3D significantly outperforms\nother image-to-3D baselines in terms of geometric and textural details.\n","authors":["Kailu Wu","Fangfu Liu","Zhihan Cai","Runjie Yan","Hanyang Wang","Yating Hu","Yueqi Duan","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2405.20343v3.pdf","comment":"Project page: https://wukailu.github.io/Unique3D"},{"id":"http://arxiv.org/abs/2404.03219v2","updated":"2024-10-28T13:35:49Z","published":"2024-04-04T05:54:19Z","title":"iSeg: Interactive 3D Segmentation via Interactive Attention","summary":"  We present iSeg, a new interactive technique for segmenting 3D shapes.\nPrevious works have focused mainly on leveraging pre-trained 2D foundation\nmodels for 3D segmentation based on text. However, text may be insufficient for\naccurately describing fine-grained spatial segmentations. Moreover, achieving a\nconsistent 3D segmentation using a 2D model is highly challenging, since\noccluded areas of the same semantic region may not be visible together from any\n2D view. Thus, we design a segmentation method conditioned on fine user clicks,\nwhich operates entirely in 3D. Our system accepts user clicks directly on the\nshape's surface, indicating the inclusion or exclusion of regions from the\ndesired shape partition. To accommodate various click settings, we propose a\nnovel interactive attention module capable of processing different numbers and\ntypes of clicks, enabling the training of a single unified interactive\nsegmentation model. We apply iSeg to a myriad of shapes from different domains,\ndemonstrating its versatility and faithfulness to the user's specifications.\nOur project page is at https://threedle.github.io/iSeg/.\n","authors":["Itai Lang","Fei Xu","Dale Decatur","Sudarshan Babu","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2404.03219v2.pdf","comment":"SIGGRAPH Asia 2024. Project page: https://threedle.github.io/iSeg/"},{"id":"http://arxiv.org/abs/2410.20986v1","updated":"2024-10-28T13:04:44Z","published":"2024-10-28T13:04:44Z","title":"Skinned Motion Retargeting with Dense Geometric Interaction Perception","summary":"  Capturing and maintaining geometric interactions among different body parts\nis crucial for successful motion retargeting in skinned characters. Existing\napproaches often overlook body geometries or add a geometry correction stage\nafter skeletal motion retargeting. This results in conflicts between skeleton\ninteraction and geometry correction, leading to issues such as jittery,\ninterpenetration, and contact mismatches. To address these challenges, we\nintroduce a new retargeting framework, MeshRet, which directly models the dense\ngeometric interactions in motion retargeting. Initially, we establish dense\nmesh correspondences between characters using semantically consistent sensors\n(SCS), effective across diverse mesh topologies. Subsequently, we develop a\nnovel spatio-temporal representation called the dense mesh interaction (DMI)\nfield. This field, a collection of interacting SCS feature vectors, skillfully\ncaptures both contact and non-contact interactions between body geometries. By\naligning the DMI field during retargeting, MeshRet not only preserves motion\nsemantics but also prevents self-interpenetration and ensures contact\npreservation. Extensive experiments on the public Mixamo dataset and our\nnewly-collected ScanRet dataset demonstrate that MeshRet achieves\nstate-of-the-art performance. Code available at\nhttps://github.com/abcyzj/MeshRet.\n","authors":["Zijie Ye","Jia-Wei Liu","Jia Jia","Shikun Sun","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2410.20986v1.pdf","comment":"NeurIPS 2024 Spotlight"},{"id":"http://arxiv.org/abs/2205.01087v2","updated":"2024-10-28T12:50:24Z","published":"2022-05-02T17:59:02Z","title":"Incomplete Gamma Kernels: Generalizing Locally Optimal Projection\n  Operators","summary":"  We present incomplete gamma kernels, a generalization of Locally Optimal\nProjection (LOP) operators. In particular, we reveal the relation of the\nclassical localized $ L_1 $ estimator, used in the LOP operator for point cloud\ndenoising, to the common Mean Shift framework via a novel kernel. Furthermore,\nwe generalize this result to a whole family of kernels that are built upon the\nincomplete gamma function and each represents a localized $ L_p $ estimator. By\nderiving various properties of the kernel family concerning distributional,\nMean Shift induced, and other aspects such as strict positive definiteness, we\nobtain a deeper understanding of the operator's projection behavior. From these\ntheoretical insights, we illustrate several applications ranging from an\nimproved Weighted LOP (WLOP) density weighting scheme and a more accurate\nContinuous LOP (CLOP) kernel approximation to the definition of a novel set of\nrobust loss functions. These incomplete gamma losses include the Gaussian and\nLOP loss as special cases and can be applied to various tasks including normal\nfiltering. Furthermore, we show that the novel kernels can be included as\npriors into neural networks. We demonstrate the effects of each application in\na range of quantitative and qualitative experiments that highlight the benefits\ninduced by our modifications.\n","authors":["Patrick Stotko","Michael Weinmann","Reinhard Klein"],"pdf_url":"https://arxiv.org/pdf/2205.01087v2.pdf","comment":"The final version of record is available at\n  https://doi.org/10.1109/TPAMI.2024.3349967"},{"id":"http://arxiv.org/abs/2410.20789v1","updated":"2024-10-28T07:11:20Z","published":"2024-10-28T07:11:20Z","title":"LoDAvatar: Hierarchical Embedding and Adaptive Levels of Detail with\n  Gaussian Splatting for Enhanced Human Avatars","summary":"  With the advancement of virtual reality, the demand for 3D human avatars is\nincreasing. The emergence of Gaussian Splatting technology has enabled the\nrendering of Gaussian avatars with superior visual quality and reduced\ncomputational costs. Despite numerous methods researchers propose for\nimplementing drivable Gaussian avatars, limited attention has been given to\nbalancing visual quality and computational costs. In this paper, we introduce\nLoDAvatar, a method that introduces levels of detail into Gaussian avatars\nthrough hierarchical embedding and selective detail enhancement methods. The\nkey steps of LoDAvatar encompass data preparation, Gaussian embedding, Gaussian\noptimization, and selective detail enhancement. We conducted experiments\ninvolving Gaussian avatars at various levels of detail, employing both\nobjective assessments and subjective evaluations. The outcomes indicate that\nincorporating levels of detail into Gaussian avatars can decrease computational\ncosts during rendering while upholding commendable visual quality, thereby\nenhancing runtime frame rates. We advocate adopting LoDAvatar to render\nmultiple dynamic Gaussian avatars or extensive Gaussian scenes to balance\nvisual quality and computational costs.\n","authors":["Xiaonuo Dongye","Hanzhi Guo","Le Luo","Haiyan Jiang","Yihua Bao","Zeyu Tian","Dongdong Weng"],"pdf_url":"https://arxiv.org/pdf/2410.20789v1.pdf","comment":"9 pages, 7 figures, submitted to IEEE VR 2025"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.22370v1","updated":"2024-10-28T23:10:06Z","published":"2024-10-28T23:10:06Z","title":"Survey of User Interface Design and Interaction Techniques in Generative\n  AI Applications","summary":"  The applications of generative AI have become extremely impressive, and the\ninterplay between users and AI is even more so. Current human-AI interaction\nliterature has taken a broad look at how humans interact with generative AI,\nbut it lacks specificity regarding the user interface designs and patterns used\nto create these applications. Therefore, we present a survey that\ncomprehensively presents taxonomies of how a human interacts with AI and the\nuser interaction patterns designed to meet the needs of a variety of relevant\nuse cases. We focus primarily on user-guided interactions, surveying\ninteractions that are initiated by the user and do not include any implicit\nsignals given by the user. With this survey, we aim to create a compendium of\ndifferent user-interaction patterns that can be used as a reference for\ndesigners and developers alike. In doing so, we also strive to lower the entry\nbarrier for those attempting to learn more about the design of generative AI\napplications.\n","authors":["Reuben Luera","Ryan A. Rossi","Alexa Siu","Franck Dernoncourt","Tong Yu","Sungchul Kim","Ruiyi Zhang","Xiang Chen","Hanieh Salehy","Jian Zhao","Samyadeep Basu","Puneet Mathur","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2410.22370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21596v1","updated":"2024-10-28T23:05:43Z","published":"2024-10-28T23:05:43Z","title":"Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage\n  Patterns and Their Relationship to Loneliness in Active Users","summary":"  As artificial intelligence becomes increasingly sophisticated, companion\nchatbots have been proposed as a potential solution to the growing epidemic of\nloneliness. However, the impact of these AI companions on users' psychological\nwell-being and social behaviors remains poorly understood. This study presents\na large-scale survey (n = 404) of regular users of companion chatbots,\ninvestigating the relationship between chatbot usage and their experience of\nloneliness. We find a small but significant direct correlation between session\nlength with chatbots and loneliness, and we develop a model of this\nrelationship through multiple regression analysis, finding social attraction\nand neuroticism as moderators. We find seven clusters of users, including\nsocially fulfilled dependent users to lonely dependent users. Our work\ncontributes to the ongoing dialogue about the role of AI in social and\nemotional support, offering insights for what kind of human-AI connections\nmight lead to emotional well-being and complement rather than replace human\nconnections.\n","authors":["Auren R. Liu","Pat Pataranutaporn","Sherry Turkle","Pattie Maes"],"pdf_url":"https://arxiv.org/pdf/2410.21596v1.pdf","comment":"33 pages, 17 figures, submitted to CHI 2025"},{"id":"http://arxiv.org/abs/2410.21512v1","updated":"2024-10-28T20:31:27Z","published":"2024-10-28T20:31:27Z","title":"Diagnosis of Knee Osteoarthritis Using Bioimpedance and Deep Learning","summary":"  Diagnosing knee osteoarthritis (OA) early is crucial for managing symptoms\nand preventing further joint damage, ultimately improving patient outcomes and\nquality of life. In this paper, a bioimpedance-based diagnostic tool that\ncombines precise hardware and deep learning for effective non-invasive\ndiagnosis is proposed. system features a relay-based circuit and strategically\nplaced electrodes to capture comprehensive bioimpedance data. The data is\nprocessed by a neural network model, which has been optimized using\nconvolutional layers, dropout regularization, and the Adam optimizer. This\napproach achieves a 98% test accuracy, making it a promising tool for detecting\nknee osteoarthritis musculoskeletal disorders.\n","authors":["Jamal Al-Nabulsi","Mohammad Al-Sayed Ahmad","Baraa Hasaneiah","Fayhaa AlZoubi"],"pdf_url":"https://arxiv.org/pdf/2410.21512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15264v2","updated":"2024-10-28T20:31:21Z","published":"2024-10-20T03:33:25Z","title":"AI Can Enhance Creativity in Social Networks","summary":"  Can peer recommendation engines elevate people's creative performances in\nself-organizing social networks? Answering this question requires resolving\nchallenges in data collection (e.g., tracing inspiration links and\npsycho-social attributes of nodes) and intervention design (e.g., balancing\nidea stimulation and redundancy in evolving information environments). We\ntrained a model that predicts people's ideation performances using semantic and\nnetwork-structural features in an online platform. Using this model, we built\nSocialMuse, which maximizes people's predicted performances to generate peer\nrecommendations for them. We found treatment networks leveraging SocialMuse\noutperforming AI-agnostic control networks in several creativity measures. The\ntreatment networks were more decentralized than the control, as SocialMuse\nincreasingly emphasized network-structural features at large network sizes.\nThis decentralization spreads people's inspiration sources, helping inspired\nideas stand out better. Our study provides actionable insights into building\nintelligent systems for elevating creativity.\n","authors":["Raiyan Abdul Baten","Ali Sarosh Bangash","Krish Veera","Gourab Ghoshal","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2410.15264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21507v1","updated":"2024-10-28T20:22:09Z","published":"2024-10-28T20:22:09Z","title":"Enhancing EHR Systems with data from wearables: An end-to-end Solution\n  for monitoring post-Surgical Symptoms in older adults","summary":"  Mobile health (mHealth) apps have gained popularity over the past decade for\npatient health monitoring, yet their potential for timely intervention is\nunderutilized due to limited integration with electronic health records (EHR)\nsystems. Current EHR systems lack real-time monitoring capabilities for\nsymptoms, medication adherence, physical and social functions, and community\nintegration. Existing systems typically rely on static, in-clinic measures\nrather than dynamic, real-time patient data. This highlights the need for\nautomated, scalable, and human-centered platforms to integrate\npatient-generated health data (PGHD) within EHR. Incorporating PGHD in a\nuser-friendly format can enhance patient symptom surveillance, ultimately\nimproving care management and post-surgical outcomes. To address this barrier,\nwe have developed an mHealth platform, ROAMM-EHR, to capture real-time sensor\ndata and Patient Reported Outcomes (PROs) using a smartwatch. The ROAMM-EHR\nplatform can capture data from a consumer smartwatch, send captured data to a\nsecure server, and display information within the Epic EHR system using a\nuser-friendly interface, thus enabling healthcare providers to monitor\npost-surgical symptoms effectively.\n","authors":["Heng Sun","Sai Manoj Jalam","Havish Kodali","Subhash Nerella","Ruben D. Zapata","Nicole Gravina","Jessica Ray","Erik C. Schmidt","Todd Matthew Manini","Rashidi Parisa"],"pdf_url":"https://arxiv.org/pdf/2410.21507v1.pdf","comment":"8 pages, ACM MobiCom4AgeTech 2024"},{"id":"http://arxiv.org/abs/2302.07248v2","updated":"2024-10-28T20:07:30Z","published":"2023-02-14T18:43:34Z","title":"Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI\n  Code Completions","summary":"  Large-scale generative models enabled the development of AI-powered code\ncompletion tools to assist programmers in writing code. However, much like\nother AI-powered tools, AI-powered code completions are not always accurate,\npotentially introducing bugs or even security vulnerabilities into code if not\nproperly detected and corrected by a human programmer. One technique that has\nbeen proposed and implemented to help programmers identify potential errors is\nto highlight uncertain tokens. However, there have been no empirical studies\nexploring the effectiveness of this technique -- nor investigating the\ndifferent and not-yet-agreed-upon notions of uncertainty in the context of\ngenerative models. We explore the question of whether conveying information\nabout uncertainty enables programmers to more quickly and accurately produce\ncode when collaborating with an AI-powered code completion tool, and if so,\nwhat measure of uncertainty best fits programmers' needs. Through a\nmixed-methods study with 30 programmers, we compare three conditions: providing\nthe AI system's code completion alone, highlighting tokens with the lowest\nlikelihood of being generated by the underlying generative model, and\nhighlighting tokens with the highest predicted likelihood of being edited by a\nprogrammer. We find that highlighting tokens with the highest predicted\nlikelihood of being edited leads to faster task completion and more targeted\nedits, and is subjectively preferred by study participants. In contrast,\nhighlighting tokens according to their probability of being generated does not\nprovide any benefit over the baseline with no highlighting. We further explore\nthe design space of how to convey uncertainty in AI-powered code completion\ntools, and find that programmers prefer highlights that are granular,\ninformative, interpretable, and not overwhelming.\n","authors":["Helena Vasconcelos","Gagan Bansal","Adam Fourney","Q. Vera Liao","Jennifer Wortman Vaughan"],"pdf_url":"https://arxiv.org/pdf/2302.07248v2.pdf","comment":"ACM Transactions on Computer-Human Interaction (TOCHI) 2024"},{"id":"http://arxiv.org/abs/2410.21451v1","updated":"2024-10-28T18:54:42Z","published":"2024-10-28T18:54:42Z","title":"A New Heuristic Algorithm for Balanced Deliberation Groups","summary":"  We here present an improved version of the Sortition Foundation's GROUPSELECT\nsoftware package, which aims to repeatedly allocate participants of a\ndeliberative process to discussion groups in a way that balances demographics\nin each group and maximises distinct meetings over time. Our result, GROUPOPT,\nsignificantly outperforms the prior algorithmic approach LEGACY. We also add\nfunctionalities to the GROUPSELECT software to help the end user. The GROUPOPT\nalgorithm utilises random shuffles and Pareto swaps to find a locally optimal\nsolution that maximises demographic balance and minimises the number of\npairwise previous meetings, with the relative importance of these two metrics\ndefined by the user.\n","authors":["Jake Barrett","Philipp C Verpoort","Kobi Gal"],"pdf_url":"https://arxiv.org/pdf/2410.21451v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.21358v1","updated":"2024-10-28T17:35:59Z","published":"2024-10-28T17:35:59Z","title":"\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools","summary":"  Generative AI tools, particularly those utilizing large language models\n(LLMs), have become increasingly prevalent in both professional and personal\ncontexts, offering powerful capabilities for text generation and communication\nsupport. While these tools are widely used to enhance productivity and\naccessibility, there has been limited exploration of how Deaf and Hard of\nHearing (DHH) individuals engage with text-based generative AI tools, as well\nas the challenges they may encounter. This paper presents a mixed-method survey\nstudy investigating how the DHH community uses Text AI tools, such as ChatGPT,\nto reduce communication barriers, bridge Deaf and hearing cultures, and improve\naccess to information. Through a survey of 80 DHH participants and separate\ninterviews with 11 other participants, we found that while these tools provide\nsignificant benefits, including enhanced communication and mental health\nsupport, they also introduce barriers, such as a lack of American Sign Language\n(ASL) support and understanding of Deaf cultural nuances. Our findings\nhighlight unique usage patterns within the DHH community and underscore the\nneed for inclusive design improvements. We conclude by offering practical\nrecommendations to enhance the accessibility of Text AI for the DHH community\nand suggest directions for future research in AI and accessibility.\n","authors":["Shuxu Huffman","Si Chen","Kelly Avery Mack","Haotian Su","Qi Wang","Raja Kushalnagar"],"pdf_url":"https://arxiv.org/pdf/2410.21358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21197v1","updated":"2024-10-28T16:39:38Z","published":"2024-10-28T16:39:38Z","title":"User-Centered Design of Socially Assistive Robotic Combined with\n  Non-Immersive Virtual Reality-based Dyadic Activities for Older Adults\n  Residing in Long Term Care Facilities","summary":"  Apathy impairs the quality of life for older adults and their care providers.\nWhile few pharmacological remedies exist, current non-pharmacologic approaches\nare resource intensive. To address these concerns, this study utilizes a\nuser-centered design (UCD) process to develop and test a set of dyadic\nactivities that provide physical, cognitive, and social stimuli to older adults\nresiding in long-term care (LTC) communities. Within the design, a novel\nframework that combines socially assistive robots and non-immersive virtual\nreality (SAR-VR) emphasizing human-robot interaction (HRI) and human-computer\ninteraction (HCI) is utilized with the roles of the robots being coach and\nentertainer. An interdisciplinary team of engineers, nurses, and physicians\ncollaborated with an advisory panel comprising LTC activity coordinators,\nstaff, and residents to prototype the activities. The study resulted in four\nvirtual activities: three with the humanoid robot, Nao, and one with the animal\nrobot, Aibo. Fourteen participants tested the acceptability of the different\ncomponents of the system and provided feedback at different stages of\ndevelopment. Participant approval increased significantly over successive\niterations of the system highlighting the importance of stakeholder feedback.\nFive LTC staff members successfully set up the system with minimal help from\nthe researchers, demonstrating the usability of the system for caregivers.\nRationale for activity selection, design changes, and both quantitative and\nqualitative results on the acceptability and usability of the system have been\npresented. The paper discusses the challenges encountered in developing\nactivities for older adults in LTCs and underscores the necessity of the UCD\nprocess to address them.\n","authors":["Ritam Ghosh","Nibraas Khan","Miroslava Migovich","Judith A. Tate","Cathy Maxwell","Emily Latshaw","Paul Newhouse","Douglas W. Scharre","Alai Tan","Kelley Colopietro","Lorraine C. Mion","Nilanjan Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.21197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21183v1","updated":"2024-10-28T16:23:53Z","published":"2024-10-28T16:23:53Z","title":"Towards Human-centered Design of Explainable Artificial Intelligence\n  (XAI): A Survey of Empirical Studies","summary":"  With the advances of AI research, AI has been increasingly adopted in\nnumerous domains, ranging from low-stakes daily tasks such as movie\nrecommendations to high-stakes tasks such as medicine, and criminal justice\ndecision-making. Explainability is becoming an essential requirement for people\nto understand, trust and adopt AI applications.\n  Despite a vast collection of explainable AI (XAI) algorithms produced by the\nAI research community, successful examples of XAI are still relatively scarce\nin real-world AI applications. This can be due to the gap between what the XAI\nis designed for and how the XAI is actually perceived by end-users. As\nexplainability is an inherently human-centered property, in recent years, the\nXAI field is starting to embrace human-centered approaches and increasingly\nrealizing the importance of empirical studies of XAI design by involving human\nsubjects.\n  To move a step towards a systematic review of empirical study for\nhuman-centered XAI design, in this survey, we first brief the technical\nlandscape of commonly used XAI algorithms in existing empirical studies. Then\nwe analyze the diverse stakeholders and needs-finding approaches. Next, we\nprovide an overview of the design space explored in the current human-centered\nXAI design. Further, we summarize the evaluation metrics based on evaluation\ngoals. Afterward, we analyze the common findings and pitfalls derived from\nexisting studies. For each chapter, we provide a summary of current challenges\nand research opportunities. Finally, we conclude the survey with a framework\nfor human-centered XAI design with empirical studies.\n","authors":["Shuai Ma"],"pdf_url":"https://arxiv.org/pdf/2410.21183v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2410.21159v1","updated":"2024-10-28T15:59:31Z","published":"2024-10-28T15:59:31Z","title":"CURATe: Benchmarking Personalised Alignment of Conversational AI\n  Assistants","summary":"  We introduce a multi-turn benchmark for evaluating personalised alignment in\nLLM-based AI assistants, focusing on their ability to handle user-provided\nsafety-critical contexts. Our assessment of ten leading models across five\nscenarios (each with 337 use cases) reveals systematic inconsistencies in\nmaintaining user-specific consideration, with even top-rated \"harmless\" models\nmaking recommendations that should be recognised as obviously harmful to the\nuser given the context provided. Key failure modes include inappropriate\nweighing of conflicting preferences, sycophancy (prioritising user preferences\nabove safety), a lack of attentiveness to critical user information within the\ncontext window, and inconsistent application of user-specific knowledge. The\nsame systematic biases were observed in OpenAI's o1, suggesting that strong\nreasoning capacities do not necessarily transfer to this kind of personalised\nthinking. We find that prompting LLMs to consider safety-critical context\nsignificantly improves performance, unlike a generic 'harmless and helpful'\ninstruction. Based on these findings, we propose research directions for\nembedding self-reflection capabilities, online user modelling, and dynamic risk\nassessment in AI assistants. Our work emphasises the need for nuanced,\ncontext-aware approaches to alignment in systems designed for persistent human\ninteraction, aiding the development of safe and considerate AI assistants.\n","authors":["Lize Alberts","Benjamin Ellis","Andrei Lupu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2410.21159v1.pdf","comment":"Submitted to ICLR 2025 on 01/10/2024"},{"id":"http://arxiv.org/abs/2406.06717v3","updated":"2024-10-28T15:55:23Z","published":"2024-06-10T18:26:35Z","title":"User Archetypes and Information Dynamics on Telegram: COVID-19 and\n  Climate Change Discourse in Singapore","summary":"  Social media platforms, particularly Telegram, play a pivotal role in shaping\npublic perceptions and opinions on global and national issues. Unlike\ntraditional news media, Telegram allows for the proliferation of user-generated\ncontent with minimal oversight, making it a significant venue for the spread of\ncontroversial and misinformative content. During the COVID-19 pandemic,\nTelegram's popularity surged in Singapore, a country with one of the highest\nrates of social media use globally. We leverage Singapore-based Telegram data\nto analyze information flows within groups focused on COVID-19 and climate\nchange. Using k-means clustering, we identified distinct user archetypes,\nincluding Strategic Disruptor, Empirical Enthusiast, Inquisitive Moderate, and\nCritical Examiner, each contributing uniquely to the discourse. We developed a\nmodel to classify users into these clusters (Precision: Climate change: 0.99;\nCOVID-19: 0.95).\n","authors":["Val Alvern Cueco Ligo","Lam Yin Cheung","Roy Ka-Wei Lee","Koustuv Saha","Edson C. Tandoc Jr.","Navin Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.06717v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22082v1","updated":"2024-10-28T15:22:35Z","published":"2024-10-28T15:22:35Z","title":"An Actor-Critic Approach to Boosting Text-to-SQL Large Language Model","summary":"  Text-To-SQL (T2S) conversion based on large language models (LLMs) has found\na wide range of applications, by leveraging the capabilities of LLMs in\ninterpreting the query intent expressed in natural language. Existing research\nfocuses on suitable representations for data schema and/or questions,\ntask-specific instructions and representative examples, and complicated\ninference pipelines. All these methods are empirical and task specific, without\na theoretical bound on performance. In this paper, we propose a simple,\ngeneral, and performance guaranteed T2S enhancement approach called\nActor-Critic (AC). Specifically, we design two roles using the same LLM: an\nActor to produce SQL queries and a Critic to evaluate the produced SQL. If the\nCritic believes the produced SQL is wrong, it notifies the Actor to reproduce\nthe SQL and perform evaluation again. By this simple iterative process,\nexpected performance can be derived in theory. We conducted extensive\nexperiments on the Spider and related datasets with eleven LLMs, and\ndemonstrated that the Actor-Critic method consistently improves the performance\nof T2S, thus serving as a general enhancement approach for T2S conversion.\n","authors":["Ziyang Zheng","Haipeng Jing","Canyu Rui","Askar Hamdulla","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.22082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21091v1","updated":"2024-10-28T14:56:51Z","published":"2024-10-28T14:56:51Z","title":"Large Language Model-assisted Speech and Pointing Benefits Multiple 3D\n  Object Selection in Virtual Reality","summary":"  Selection of occluded objects is a challenging problem in virtual reality,\neven more so if multiple objects are involved. With the advent of new\nartificial intelligence technologies, we explore the possibility of leveraging\nlarge language models to assist multi-object selection tasks in virtual reality\nvia a multimodal speech and raycast interaction technique. We validate the\nfindings in a comparative user study (n=24), where participants selected target\nobjects in a virtual reality scene with different levels of scene perplexity.\nThe performance metrics and user experience metrics are compared against a\nmini-map based occluded object selection technique that serves as the baseline.\nResults indicate that the introduced technique, AssistVR, outperforms the\nbaseline technique when there are multiple target objects. Contrary to the\ncommon belief for speech interfaces, AssistVR was able to outperform the\nbaseline even when the target objects were difficult to reference verbally.\nThis work demonstrates the viability and interaction potential of an\nintelligent multimodal interactive system powered by large laguage models.\nBased on the results, we discuss the implications for design of future\nintelligent multimodal interactive systems in immersive environments.\n","authors":["Junlong Chen","Jens Grubert","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2410.21091v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2401.13324v6","updated":"2024-10-28T12:45:52Z","published":"2024-01-24T09:39:39Z","title":"Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions","summary":"  Every AI system that makes decisions about people has a group of stakeholders\nthat are personally affected by these decisions. However, explanations of AI\nsystems rarely address the information needs of this stakeholder group, who\noften are AI novices. This creates a gap between conveyed information and\ninformation that matters to those who are impacted by the system's decisions,\nsuch as domain experts and decision subjects. To address this, we present the\n\"XAI Novice Question Bank,\" an extension of the XAI Question Bank containing a\ncatalog of information needs from AI novices in two use cases: employment\nprediction and health monitoring. The catalog covers the categories of data,\nsystem context, system usage, and system specifications. We gathered\ninformation needs through task-based interviews where participants asked\nquestions about two AI systems to decide on their adoption and received verbal\nexplanations in response. Our analysis showed that participants' confidence\nincreased after receiving explanations but that their understanding faced\nchallenges. These included difficulties in locating information and in\nassessing their own understanding, as well as attempts to outsource\nunderstanding. Additionally, participants' prior perceptions of the systems'\nrisks and benefits influenced their information needs. Participants who\nperceived high risks sought explanations about the intentions behind a system's\ndeployment, while those who perceived low risks rather asked about the system's\noperation. Our work aims to support the inclusion of AI novices in\nexplainability efforts by highlighting their information needs, aims, and\nchallenges. We summarize our findings as five key implications that can inform\nthe design of future explanations for lay stakeholder audiences.\n","authors":["Timoth√©e Schmude","Laura Koesten","Torsten M√∂ller","Sebastian Tschiatschek"],"pdf_url":"https://arxiv.org/pdf/2401.13324v6.pdf","comment":"Published version available at the International Journal of\n  Human-Computer Studies, please refer to:\n  https://doi.org/10.1016/j.ijhcs.2024.103380. Main text: 26 pages, 4 figures.\n  Supplementary material is provided"},{"id":"http://arxiv.org/abs/2410.12891v2","updated":"2024-10-28T09:22:49Z","published":"2024-10-16T09:40:34Z","title":"Multi-trait User Simulation with Adaptive Decoding for Conversational\n  Task Assistants","summary":"  Conversational systems must be robust to user interactions that naturally\nexhibit diverse conversational traits. Capturing and simulating these diverse\ntraits coherently and efficiently presents a complex challenge. This paper\nintroduces Multi-Trait Adaptive Decoding (mTAD), a method that generates\ndiverse user profiles at decoding-time by sampling from various trait-specific\nLanguage Models (LMs). mTAD provides an adaptive and scalable approach to user\nsimulation, enabling the creation of multiple user profiles without the need\nfor additional fine-tuning. By analyzing real-world dialogues from the\nConversational Task Assistant (CTA) domain, we identify key conversational\ntraits and developed a framework to generate profile-aware dialogues that\nenhance conversational diversity. Experimental results validate the\neffectiveness of our approach in modeling single-traits using specialized LMs,\nwhich can capture less common patterns, even in out-of-domain tasks.\nFurthermore, the results demonstrate that mTAD is a robust and flexible\nframework for combining diverse user simulators.\n","authors":["Rafael Ferreira","David Semedo","Jo√£o Magalh√£es"],"pdf_url":"https://arxiv.org/pdf/2410.12891v2.pdf","comment":"Preprint from EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2409.06814v2","updated":"2024-10-28T08:51:50Z","published":"2024-09-10T18:43:39Z","title":"\"Come to us first\": Centering Community Organizations in Artificial\n  Intelligence for Social Good Partnerships","summary":"  Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body\nof research and practice exploring the potential of AI technologies to tackle\nsocial issues. This area emphasizes interdisciplinary partnerships with\ncommunity organizations, such as non-profits and government agencies. However,\namidst excitement about new advances in AI and their potential impact, the\nneeds, expectations, and aspirations of these community organizations--and\nwhether they are being met--are not well understood. Understanding these\nfactors is important to ensure that the considerable efforts by AI teams and\ncommunity organizations can actually achieve the positive social impact they\nstrive for. Drawing on the Data Feminism framework, we explored the\nperspectives of community organization members on their partnerships with AI\nteams through 16 semi-structured interviews. Our study highlights the pervasive\ninfluence of funding agendas and the optimism surrounding AI's potential.\nDespite the significant intellectual contributions and labor provided by\ncommunity organization members, their goals were frequently sidelined in favor\nof other stakeholders, including AI teams. While many community organization\nmembers expected tangible project deployment, only two out of 14 projects we\nstudied reached the deployment stage. However, community organization members\nsustained their belief in the potential of the projects, still seeing\ndiminished goals as valuable. To enhance the efficacy of future collaborations,\nour participants shared their aspirations for success, calling for\nco-leadership starting from the early stages of projects. We propose data\nco-liberation as a grounding principle for approaching AI4SG moving forward,\npositing that community organizations' co-leadership is essential for fostering\nmore effective, sustainable, and ethical development of AI.\n","authors":["Hongjin Lin","Naveena Karusala","Chinasa T. Okolo","Catherine D'Ignazio","Krzysztof Z. Gajos"],"pdf_url":"https://arxiv.org/pdf/2409.06814v2.pdf","comment":"Accepted to the Proc. ACM Hum. Comput. Interact. 8, CSCW2"},{"id":"http://arxiv.org/abs/2410.20746v1","updated":"2024-10-28T05:25:50Z","published":"2024-10-28T05:25:50Z","title":"ElectionSim: Massive Population Election Simulation Powered by Large\n  Language Model Driven Agents","summary":"  The massive population election simulation aims to model the preferences of\nspecific groups in particular election scenarios. It has garnered significant\nattention for its potential to forecast real-world social trends. Traditional\nagent-based modeling (ABM) methods are constrained by their ability to\nincorporate complex individual background information and provide interactive\nprediction results. In this paper, we introduce ElectionSim, an innovative\nelection simulation framework based on large language models, designed to\nsupport accurate voter simulations and customized distributions, together with\nan interactive platform to dialogue with simulated voters. We present a\nmillion-level voter pool sampled from social media platforms to support\naccurate individual simulation. We also introduce PPE, a poll-based\npresidential election benchmark to assess the performance of our framework\nunder the U.S. presidential election scenario. Through extensive experiments\nand analyses, we demonstrate the effectiveness and robustness of our framework\nin U.S. presidential election simulations.\n","authors":["Xinnong Zhang","Jiayu Lin","Libo Sun","Weihong Qi","Yihang Yang","Yue Chen","Hanjia Lyu","Xinyi Mou","Siming Chen","Jiebo Luo","Xuanjing Huang","Shiping Tang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.20746v1.pdf","comment":"41 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.20696v1","updated":"2024-10-28T02:57:17Z","published":"2024-10-28T02:57:17Z","title":"\"So Am I Dr. Frankenstein? Or Were You a Monster the Whole Time?\":\n  Mitigating Software Project Failure With Loss-Aversion-Aware Development\n  Methodologies","summary":"  Case studies have shown that software disasters snowball from technical\nissues to catastrophes through humans covering up problems rather than\naddressing them and empirical research has found the psychological safety of\nsoftware engineers to discuss and address problems to be foundational to\nimproving project success. However, the failure to do so can be attributed to\npsychological factors like loss aversion. We conduct a large-scale study of the\nexperiences of 600 software engineers in the UK and USA on project success\nexperiences. Empirical evaluation finds that approaches like ensuring clear\nrequirements before the start of development, when loss aversion is at its\nlowest, correlated to 97\\% higher project success. The freedom of software\nengineers to discuss and address problems correlates with 87\\% higher success\nrates. The findings support the development of software development\nmethodologies with a greater focus on human factors in preventing failure.\n","authors":["Junade Ali"],"pdf_url":"https://arxiv.org/pdf/2410.20696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20675v1","updated":"2024-10-28T02:22:31Z","published":"2024-10-28T02:22:31Z","title":"Impact of Translation and Viewpoint Transition Methods in VR on Spatial\n  Learning and Cybersickness","summary":"  Virtual locomotion technique (VLT) is a fundamental component of virtual\nreality (VR) systems that translates physical and controller inputs into\nvirtual translational movements and viewpoint transitions. Poorly designed VLTs\ncan result in discomfort, nausea, and reductions in task performance.\nUnderstanding the effectiveness of VLTs across various levels of interaction\nfidelity is crucial to enhance user experience and spatial awareness. The\ncurrent study addressed a significant gap in VR design research and practice,\nas few previous efforts have been made to comprehensively evaluate the\neffectiveness of controller-based VLTs in virtual indoor environments. We\nconducted a user study in which participants navigated through two complex\nvirtual environments, one focusing on exploratory tasks and the other on\ngoal-oriented navigation. The findings offer insights into the trade-offs among\nspatial knowledge acquisition, wayfinding performance, cybersickness, and sense\nof presence, and have design implications for future VR interfaces.\n","authors":["Armin Mostafavi","Zhiwen Qiu","Tong Bill Xu","Saleh Kalantari"],"pdf_url":"https://arxiv.org/pdf/2410.20675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05773v2","updated":"2024-10-28T01:34:48Z","published":"2024-09-09T16:34:36Z","title":"Creativity and Visual Communication from Machine to Musician: Sharing a\n  Score through a Robotic Camera","summary":"  This paper explores the integration of visual communication and musical\ninteraction by implementing a robotic camera within a \"Guided Harmony\" musical\ngame. We aim to examine co-creative behaviors between human musicians and\nrobotic systems. Our research explores existing methodologies like\nimprovisational game pieces and extends these concepts to include robotic\nparticipation using a PTZ camera. The robotic system interprets and responds to\nnonverbal cues from musicians, creating a collaborative and adaptive musical\nexperience. This initial case study underscores the importance of intuitive\nvisual communication channels. We also propose future research directions,\nincluding parameters for refining the visual cue toolkit and data collection\nmethods to understand human-machine co-creativity further. Our findings\ncontribute to the broader understanding of machine intelligence in augmenting\nhuman creativity, particularly in musical settings.\n","authors":["Ross Greer","Laura Fleig","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.05773v2.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.21521v1","updated":"2024-10-28T20:45:52Z","published":"2024-10-28T20:45:52Z","title":"A Multi-Agent Reinforcement Learning Testbed for Cognitive Radio\n  Applications","summary":"  Technological trends show that Radio Frequency Reinforcement Learning (RFRL)\nwill play a prominent role in the wireless communication systems of the future.\nApplications of RFRL range from military communications jamming to enhancing\nWiFi networks. Before deploying algorithms for these purposes, they must be\ntrained in a simulation environment to ensure adequate performance. For this\nreason, we previously created the RFRL Gym: a standardized, accessible tool for\nthe development and testing of reinforcement learning (RL) algorithms in the\nwireless communications space. This environment leveraged the OpenAI Gym\nframework and featured customizable simulation scenarios within the RF\nspectrum. However, the RFRL Gym was limited to training a single RL agent per\nsimulation; this is not ideal, as most real-world RF scenarios will contain\nmultiple intelligent agents in cooperative, competitive, or mixed settings,\nwhich is a natural consequence of spectrum congestion. Therefore, through\nintegration with Ray RLlib, multi-agent reinforcement learning (MARL)\nfunctionality for training and assessment has been added to the RFRL Gym,\nmaking it even more of a robust tool for RF spectrum simulation. This paper\nprovides an overview of the updated RFRL Gym environment. In this work, the\ngeneral framework of the tool is described relative to comparable existing\nresources, highlighting the significant additions and refactoring we have\napplied to the Gym. Afterward, results from testing various RF scenarios in the\nMARL environment and future additions are discussed.\n","authors":["Sriniketh Vangaru","Daniel Rosen","Dylan Green","Raphael Rodriguez","Maxwell Wiecek","Amos Johnson","Alyse M. Jones","William C. Headley"],"pdf_url":"https://arxiv.org/pdf/2410.21521v1.pdf","comment":"Accepted to IEEE CCNC 2025"},{"id":"http://arxiv.org/abs/2410.21447v1","updated":"2024-10-28T18:51:03Z","published":"2024-10-28T18:51:03Z","title":"You Can't Always Get What You Want : Games of Ordered Preference","summary":"  We study noncooperative games, in which each agent's objective is composed of\na sequence of ordered-and potentially conflicting-preferences. Problems of this\ntype naturally model a wide variety of scenarios: for example, drivers at a\nbusy intersection must balance the desire to make forward progress with the\nrisk of collision. Mathematically, these problems possess a nested structure,\nand to behave properly agents must prioritize their most important preference,\nand only consider less important preferences to the extent that they do not\ncompromise performance on more important ones. We consider multi-agent,\nnoncooperative variants of these problems, and seek generalized Nash equilibria\nin which each agent's decision reflects both its hierarchy of preferences and\nother agents' actions. We make two key contributions. First, we develop a\nrecursive approach for deriving the first-order optimality conditions of each\nagent's nested problem. Second, we propose a sequence of increasingly tight\nrelaxations, each of which can be transcribed as a mixed complementarity\nproblem and solved via existing methods. Experimental results demonstrate that\nour approach reliably converges to equilibrium solutions that strictly reflect\nagents' individual ordered preferences.\n","authors":["Dong Ho Lee","Lasse Peters","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2410.21447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21446v1","updated":"2024-10-28T18:51:00Z","published":"2024-10-28T18:51:00Z","title":"Improving DeFi Mechanisms with Dynamic Games and Optimal Control: A Case\n  Study in Stablecoins","summary":"  Stablecoins are a class of cryptocurrencies which aim at providing\nconsistency and predictability, typically by pegging the token's value to that\nof a real world asset. Designing resilient decentralized stablecoins is a\nchallenge, and prominent stablecoins today either (i) give up on\ndecentralization, or (ii) rely on user-owned cryptocurrencies as collateral,\nexposing the token to exogenous price fluctuations. In this latter category, it\nis increasingly common to employ algorithmic mechanisms to automate risk\nmanagement, helping maintain the peg. One example of this is Reflexer's RAI,\nwhich adapts its system-internal exchange rate (redemption price) to secondary\nmarket conditions according to a proportional control law. In this paper, we\ntake this idea of active management a step further, and introduce a new kind of\ncontrol scheme based on a Stackelberg game model between the token protocol and\nits users. By doing so, we show that (i) we can mitigate adverse depeg events\nthat inevitably arise in a fixed-redemption scheme such as MakerDao's DAI and\n(ii) generally outperform a simpler, adaptive-redemption scheme such as RAI in\nthe task of targeting a desired market price. We demonstrate these results\nthrough extensive simulations over a range of market conditions.\n","authors":["Nicholas Strohmeyer","Sriram Vishwanath","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2410.21446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21415v1","updated":"2024-10-28T18:13:15Z","published":"2024-10-28T18:13:15Z","title":"Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong\n  Multi-Agent Path Finding","summary":"  Lifelong Multi-Agent Path Finding (LMAPF) is a variant of MAPF where agents\nare continually assigned new goals, necessitating frequent re-planning to\naccommodate these dynamic changes. Recently, this field has embraced\nlearning-based methods, which reactively generate single-step actions based on\nindividual local observations. However, it is still challenging for them to\nmatch the performance of the best search-based algorithms, especially in\nlarge-scale settings. This work proposes an imitation-learning-based LMAPF\nsolver that introduces a novel communication module and systematic single-step\ncollision resolution and global guidance techniques. Our proposed solver,\nScalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning\nspeed of learning-based methods and the high solution quality of search-based\nmethods with the help of modern GPUs. Across six large-scale maps with up to\n10,000 agents and varying obstacle structures, SILLM surpasses the best\nlearning- and search-based baselines, achieving average throughput improvements\nof 137.7% and 16.0%, respectively. Furthermore, SILLM also beats the winning\nsolution of the 2023 League of Robot Runners, an international LMAPF\ncompetition sponsored by Amazon Robotics. Finally, we validated SILLM with 10\nreal robots and 100 virtual robots in a mockup warehouse environment.\n","authors":["He Jiang","Yutong Wang","Rishi Veerapaneni","Tanishq Duhan","Guillaume Sartoretti","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2410.21415v1.pdf","comment":"Submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2410.17466v3","updated":"2024-10-28T18:00:25Z","published":"2024-10-22T22:49:04Z","title":"Evolution with Opponent-Learning Awareness","summary":"  The universe involves many independent co-learning agents as an ever-evolving\npart of our observed environment. Yet, in practice, Multi-Agent Reinforcement\nLearning (MARL) applications are usually constrained to small, homogeneous\npopulations and remain computationally intensive. In this paper, we study how\nlarge heterogeneous populations of learning agents evolve in normal-form games.\nWe show how, under assumptions commonly made in the multi-armed bandit\nliterature, Multi-Agent Policy Gradient closely resembles the Replicator\nDynamic, and we further derive a fast, parallelizable implementation of\nOpponent-Learning Awareness tailored for evolutionary simulations. This enables\nus to simulate the evolution of very large populations made of heterogeneous\nco-learning agents, under both naive and advanced learning strategies. We\ndemonstrate our approach in simulations of 200,000 agents, evolving in the\nclassic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game\nhighlights distinct ways in which Opponent-Learning Awareness affects\nevolution.\n","authors":["Yann Bouteiller","Karthik Soma","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17466v3.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.07277v2","updated":"2024-10-28T14:14:10Z","published":"2024-06-11T14:04:25Z","title":"Speaking Your Language: Spatial Relationships in Interpretable Emergent\n  Communication","summary":"  Effective communication requires the ability to refer to specific parts of an\nobservation in relation to others. While emergent communication literature\nshows success in developing various language properties, no research has shown\nthe emergence of such positional references. This paper demonstrates how agents\ncan communicate about spatial relationships within their observations. The\nresults indicate that agents can develop a language capable of expressing the\nrelationships between parts of their observation, achieving over 90% accuracy\nwhen trained in a referential game which requires such communication. Using a\ncollocation measure, we demonstrate how the agents create such references. This\nanalysis suggests that agents use a mixture of non-compositional and\ncompositional messages to convey spatial relationships. We also show that the\nemergent language is interpretable by humans. The translation accuracy is\ntested by communicating with the receiver agent, where the receiver achieves\nover 78% accuracy using parts of this lexicon, confirming that the\ninterpretation of the emergent language was successful.\n","authors":["Olaf Lipinski","Adam J. Sobey","Federico Cerutti","Timothy J. Norman"],"pdf_url":"https://arxiv.org/pdf/2406.07277v2.pdf","comment":"Accepted at NeurIPS 2024. 18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.21029v1","updated":"2024-10-28T13:51:03Z","published":"2024-10-28T13:51:03Z","title":"FairStream: Fair Multimedia Streaming Benchmark for Reinforcement\n  Learning Agents","summary":"  Multimedia streaming accounts for the majority of traffic in today's\ninternet. Mechanisms like adaptive bitrate streaming control the bitrate of a\nstream based on the estimated bandwidth, ideally resulting in smooth playback\nand a good Quality of Experience (QoE). However, selecting the optimal bitrate\nis challenging under volatile network conditions. This motivated researchers to\ntrain Reinforcement Learning (RL) agents for multimedia streaming. The\nconsidered training environments are often simplified, leading to promising\nresults with limited applicability. Additionally, the QoE fairness across\nmultiple streams is seldom considered by recent RL approaches. With this work,\nwe propose a novel multi-agent environment that comprises multiple challenges\nof fair multimedia streaming: partial observability, multiple objectives, agent\nheterogeneity and asynchronicity. We provide and analyze baseline approaches\nacross five different traffic classes to gain detailed insights into the\nbehavior of the considered agents, and show that the commonly used Proximal\nPolicy Optimization (PPO) algorithm is outperformed by a simple greedy\nheuristic. Future work includes the adaptation of multi-agent RL algorithms and\nfurther expansions of the environment.\n","authors":["Jannis Weil","Jonas Ringsdorf","Julian Barthel","Yi-Ping Phoebe Chen","Tobias Meuser"],"pdf_url":"https://arxiv.org/pdf/2410.21029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17789v2","updated":"2024-10-28T13:19:38Z","published":"2024-07-25T05:50:46Z","title":"Very Large-Scale Multi-Agent Simulation in AgentScope","summary":"  Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations.\n","authors":["Xuchen Pan","Dawei Gao","Yuexiang Xie","Yushuo Chen","Zhewei Wei","Yaliang Li","Bolin Ding","Ji-Rong Wen","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17789v2.pdf","comment":"We have released code on\n  https://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation"},{"id":"http://arxiv.org/abs/2410.19382v2","updated":"2024-10-28T11:09:26Z","published":"2024-10-25T08:32:21Z","title":"Multi-Agent Reinforcement Learning with Selective State-Space Models","summary":"  The Transformer model has demonstrated success across a wide range of\ndomains, including in Multi-Agent Reinforcement Learning (MARL) where the\nMulti-Agent Transformer (MAT) has emerged as a leading algorithm in the field.\nHowever, a significant drawback of Transformer models is their quadratic\ncomputational complexity relative to input size, making them computationally\nexpensive when scaling to larger inputs. This limitation restricts MAT's\nscalability in environments with many agents. Recently, State-Space Models\n(SSMs) have gained attention due to their computational efficiency, but their\napplication in MARL remains unexplored. In this work, we investigate the use of\nMamba, a recent SSM, in MARL and assess whether it can match the performance of\nMAT while providing significant improvements in efficiency. We introduce a\nmodified version of MAT that incorporates standard and bi-directional Mamba\nblocks, as well as a novel \"cross-attention\" Mamba block. Extensive testing\nshows that our Multi-Agent Mamba (MAM) matches the performance of MAT across\nmultiple standard multi-agent environments, while offering superior scalability\nto larger agent scenarios. This is significant for the MARL community, because\nit indicates that SSMs could replace Transformers without compromising\nperformance, whilst also supporting more effective scaling to higher numbers of\nagents. Our project page is available at\nhttps://sites.google.com/view/multi-agent-mamba .\n","authors":["Jemma Daniel","Ruan de Kock","Louay Ben Nessir","Sasha Abramowitz","Omayma Mahjoub","Wiem Khlifi","Claude Formanek","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.19382v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.21342v1","updated":"2024-10-28T04:53:42Z","published":"2024-10-28T04:53:42Z","title":"Heterogeneous Interaction Modeling With Reduced Accumulated Error for\n  Multi-Agent Trajectory Prediction","summary":"  Dynamical complex systems composed of interactive heterogeneous agents are\nprevalent in the world, including urban traffic systems and social networks.\nModeling the interactions among agents is the key to understanding and\npredicting the dynamics of the complex system, e.g., predicting the\ntrajectories of traffic participants in the city. Compared with interaction\nmodeling in homogeneous systems such as pedestrians in a crowded scene,\nheterogeneous interaction modeling is less explored. Worse still, the error\naccumulation problem becomes more severe since the interactions are more\ncomplex. To tackle the two problems, this paper proposes heterogeneous\ninteraction modeling with reduced accumulated error for multi-agent trajectory\nprediction. Based on the historical trajectories, our method infers the dynamic\ninteraction graphs among agents, featured by directed interacting relations and\ninteracting effects. A heterogeneous attention mechanism is defined on the\ninteraction graphs for aggregating the influence from heterogeneous neighbors\nto the target agent. To alleviate the error accumulation problem, this paper\nanalyzes the error sources from the spatial and temporal perspectives, and\nproposes to introduce the graph entropy and the mixup training strategy for\nreducing the two types of errors respectively. Our method is examined on three\nreal-world datasets containing heterogeneous agents, and the experimental\nresults validate the superiority of our method.\n","authors":["Siyuan Chen","Jiahai Wang"],"pdf_url":"https://arxiv.org/pdf/2410.21342v1.pdf","comment":"20 pages, accepted by IEEE TNNLS"},{"id":"http://arxiv.org/abs/2401.10300v2","updated":"2024-10-28T03:33:04Z","published":"2024-01-18T08:55:05Z","title":"A Hierarchical Framework with Spatio-Temporal Consistency Learning for\n  Emergence Detection in Complex Adaptive Systems","summary":"  Emergence, a global property of complex adaptive systems (CASs) constituted\nby interactive agents, is prevalent in real-world dynamic systems, e.g.,\nnetwork-level traffic congestions. Detecting its formation and evaporation\nhelps to monitor the state of a system, allowing to issue a warning signal for\nharmful emergent phenomena. Since there is no centralized controller of CAS,\ndetecting emergence based on each agent's local observation is desirable but\nchallenging. Existing works are unable to capture emergence-related spatial\npatterns, and fail to model the nonlinear relationships among agents. This\npaper proposes a hierarchical framework with spatio-temporal consistency\nlearning to solve these two problems by learning the system representation and\nagent representations, respectively. Spatio-temporal encoders composed of\nspatial and temporal transformers are designed to capture agents' nonlinear\nrelationships and the system's complex evolution. Agents' and the system's\nrepresentations are learned to preserve the spatio-temporal consistency by\nminimizing the spatial and temporal dissimilarities in a self-supervised manner\nin the latent space. Our method achieves more accurate detection than\ntraditional methods and deep learning methods on three datasets with well-known\nyet hard-to-detect emergent behaviors. Notably, our hierarchical framework is\ngeneric in incorporating other deep learning methods for agent-level and\nsystem-level detection.\n","authors":["Siyuan Chen","Xin Du","Jiahai Wang"],"pdf_url":"https://arxiv.org/pdf/2401.10300v2.pdf","comment":"23 pages, accepted by IEEE TNNLS"}],"Multimedia":[{"id":"http://arxiv.org/abs/2405.14312v2","updated":"2024-10-28T19:33:30Z","published":"2024-05-23T08:32:58Z","title":"Improving Gloss-free Sign Language Translation by Reducing\n  Representation Density","summary":"  Gloss-free sign language translation (SLT) aims to develop well-performing\nSLT systems with no requirement for the costly gloss annotations, but currently\nstill lags behind gloss-based approaches significantly. In this paper, we\nidentify a representation density problem that could be a bottleneck in\nrestricting the performance of gloss-free SLT. Specifically, the representation\ndensity problem describes that the visual representations of semantically\ndistinct sign gestures tend to be closely packed together in feature space,\nwhich makes gloss-free methods struggle with distinguishing different sign\ngestures and suffer from a sharp performance drop. To address the\nrepresentation density problem, we introduce a simple but effective contrastive\nlearning strategy, namely SignCL, which encourages gloss-free models to learn\nmore discriminative feature representation in a self-supervised manner. Our\nexperiments demonstrate that the proposed SignCL can significantly reduce the\nrepresentation density and improve performance across various translation\nframeworks. Specifically, SignCL achieves a significant improvement in BLEU\nscore for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset\nby 39% and 46%, respectively, without any increase of model parameters.\nCompared to Sign2GPT, a state-of-the-art method based on large-scale\npre-trained vision and language models, SignCL achieves better performance with\nonly 35% of its parameters. Implementation and Checkpoints are available at\nhttps://github.com/JinhuiYE/SignCL.\n","authors":["Jinhui Ye","Xing Wang","Wenxiang Jiao","Junwei Liang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2405.14312v2.pdf","comment":"Accepted at NeurIPS'24; Representation Density Problem and\n  Performance Drop in Gloss-free SLT"},{"id":"http://arxiv.org/abs/2410.21478v1","updated":"2024-10-28T19:32:17Z","published":"2024-10-28T19:32:17Z","title":"Knowledge Distillation for Real-Time Classification of Early Media in\n  Voice Communications","summary":"  This paper investigates the industrial setting of real-time classification of\nearly media exchanged during the initialization phase of voice calls. We\nexplore the application of state-of-the-art audio tagging models and highlight\nsome limitations when applied to the classification of early media. While most\nexisting approaches leverage convolutional neural networks, we propose a novel\napproach for low-resource requirements based on gradient-boosted trees. Our\napproach not only demonstrates a substantial improvement in runtime\nperformance, but also exhibits a comparable accuracy. We show that leveraging\nknowledge distillation and class aggregation techniques to train a simpler and\nsmaller model accelerates the classification of early media in voice calls. We\nprovide a detailed analysis of the results on a proprietary and publicly\navailable dataset, regarding accuracy and runtime performance. We additionally\nreport a case study of the achieved performance improvements at a regional data\ncenter in India.\n","authors":["Kemal Altwlkany","Had≈æem Had≈æiƒá","Amar Kuriƒá","Emanuel Lacic"],"pdf_url":"https://arxiv.org/pdf/2410.21478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21269v1","updated":"2024-10-28T17:58:15Z","published":"2024-10-28T17:58:15Z","title":"OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup","summary":"  The scaling up has brought tremendous success in the fields of vision and\nlanguage in recent years. When it comes to audio, however, researchers\nencounter a major challenge in scaling up the training data, as most natural\naudio contains diverse interfering signals. To address this limitation, we\nintroduce Omni-modal Sound Separation (OmniSep), a novel framework capable of\nisolating clean soundtracks based on omni-modal queries, encompassing both\nsingle-modal and multi-modal composed queries. Specifically, we introduce the\nQuery-Mixup strategy, which blends query features from different modalities\nduring training. This enables OmniSep to optimize multiple modalities\nconcurrently, effectively bringing all modalities under a unified framework for\nsound separation. We further enhance this flexibility by allowing queries to\ninfluence sound separation positively or negatively, facilitating the retention\nor removal of specific sounds as desired. Finally, OmniSep employs a\nretrieval-augmented approach known as Query-Aug, which enables open-vocabulary\nsound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and\nMUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving\nstate-of-the-art performance in text-, image-, and audio-queried sound\nseparation tasks. For samples and further information, please visit the demo\npage at \\url{https://omnisep.github.io/}.\n","authors":["Xize Cheng","Siqi Zheng","Zehan Wang","Minghui Fang","Ziang Zhang","Rongjie Huang","Ziyang Ma","Shengpeng Ji","Jialong Zuo","Tao Jin","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.21269v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.21061v1","updated":"2024-10-28T14:22:08Z","published":"2024-10-28T14:22:08Z","title":"Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative\n  Framework","summary":"  Text-to-image (T2I) diffusion models are popular for introducing image\nmanipulation methods, such as editing, image fusion, inpainting, etc. At the\nsame time, image-to-video (I2V) and text-to-video (T2V) models are also built\non top of T2I models. We present Kandinsky 3, a novel T2I model based on latent\ndiffusion, achieving a high level of quality and photorealism. The key feature\nof the new architecture is the simplicity and efficiency of its adaptation for\nmany types of generation tasks. We extend the base T2I model for various\napplications and create a multifunctional generation system that includes\ntext-guided inpainting/outpainting, image fusion, text-image fusion, image\nvariations generation, I2V and T2V generation. We also present a distilled\nversion of the T2I model, evaluating inference in 4 steps of the reverse\nprocess without reducing image quality and 3 times faster than the base model.\nWe deployed a user-friendly demo system in which all the features can be tested\nin the public domain. Additionally, we released the source code and checkpoints\nfor the Kandinsky 3 and extended models. Human evaluations show that Kandinsky\n3 demonstrates one of the highest quality scores among open source generation\nsystems.\n","authors":["Vladimir Arkhipkin","Viacheslav Vasilev","Andrei Filatov","Igor Pavlov","Julia Agafonova","Nikolai Gerasimenko","Anna Averchenkova","Evelina Mironova","Anton Bukashkin","Konstantin Kulikov","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2410.21061v1.pdf","comment":"Accepted for EMNLP 2024 (Demo track)"},{"id":"http://arxiv.org/abs/2410.21029v1","updated":"2024-10-28T13:51:03Z","published":"2024-10-28T13:51:03Z","title":"FairStream: Fair Multimedia Streaming Benchmark for Reinforcement\n  Learning Agents","summary":"  Multimedia streaming accounts for the majority of traffic in today's\ninternet. Mechanisms like adaptive bitrate streaming control the bitrate of a\nstream based on the estimated bandwidth, ideally resulting in smooth playback\nand a good Quality of Experience (QoE). However, selecting the optimal bitrate\nis challenging under volatile network conditions. This motivated researchers to\ntrain Reinforcement Learning (RL) agents for multimedia streaming. The\nconsidered training environments are often simplified, leading to promising\nresults with limited applicability. Additionally, the QoE fairness across\nmultiple streams is seldom considered by recent RL approaches. With this work,\nwe propose a novel multi-agent environment that comprises multiple challenges\nof fair multimedia streaming: partial observability, multiple objectives, agent\nheterogeneity and asynchronicity. We provide and analyze baseline approaches\nacross five different traffic classes to gain detailed insights into the\nbehavior of the considered agents, and show that the commonly used Proximal\nPolicy Optimization (PPO) algorithm is outperformed by a simple greedy\nheuristic. Future work includes the adaptation of multi-agent RL algorithms and\nfurther expansions of the environment.\n","authors":["Jannis Weil","Jonas Ringsdorf","Julian Barthel","Yi-Ping Phoebe Chen","Tobias Meuser"],"pdf_url":"https://arxiv.org/pdf/2410.21029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v2","updated":"2024-10-28T12:19:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels; however, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06171v3","updated":"2024-10-28T12:11:12Z","published":"2023-12-11T07:20:42Z","title":"Joint Explicit and Implicit Cross-Modal Interaction Network for Anterior\n  Chamber Inflammation Diagnosis","summary":"  Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.\n","authors":["Qian Shao","Ye Dai","Haochao Ying","Kan Xu","Jinhong Wang","Wei Chi","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2312.06171v3.pdf","comment":"IEEE MedAI 2024"},{"id":"http://arxiv.org/abs/2410.20898v1","updated":"2024-10-28T10:26:19Z","published":"2024-10-28T10:26:19Z","title":"Diff-Instruct*: Towards Human-Preferred One-step Text-to-image\n  Generative Models","summary":"  In this paper, we introduce the Diff-Instruct*(DI*), a data-free approach for\nbuilding one-step text-to-image generative models that align with human\npreference while maintaining the ability to generate highly realistic images.\nWe frame human preference alignment as online reinforcement learning using\nhuman feedback (RLHF), where the goal is to maximize the reward function while\nregularizing the generator distribution to remain close to a reference\ndiffusion process. Unlike traditional RLHF approaches, which rely on the KL\ndivergence for regularization, we introduce a novel score-based divergence\nregularization, which leads to significantly better performances. Although the\ndirect calculation of this divergence remains intractable, we demonstrate that\nwe can efficiently compute its \\emph{gradient} by deriving an equivalent yet\ntractable loss function. Remarkably, with Stable Diffusion V1.5 as the\nreference diffusion model, DI* outperforms \\emph{all} previously leading models\nby a large margin. When using the 0.6B PixelArt-$\\alpha$ model as the reference\ndiffusion, DI* achieves a new record Aesthetic Score of 6.30 and an Image\nReward of 1.31 with only a single generation step, almost doubling the scores\nof the rest of the models with similar sizes. It also achieves an HPSv2 score\nof 28.70, establishing a new state-of-the-art benchmark. We also observe that\nDI* can improve the layout and enrich the colors of generated images.\n","authors":["Weijian Luo","Colin Zhang","Debing Zhang","Zhengyang Geng"],"pdf_url":"https://arxiv.org/pdf/2410.20898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20855v1","updated":"2024-10-28T09:19:28Z","published":"2024-10-28T09:19:28Z","title":"ByteNet: Rethinking Multimedia File Fragment Classification through\n  Visual Perspectives","summary":"  Multimedia file fragment classification (MFFC) aims to identify file fragment\ntypes, e.g., image/video, audio, and text without system metadata. It is of\nvital importance in multimedia storage and communication. Existing MFFC methods\ntypically treat fragments as 1D byte sequences and emphasize the relations\nbetween separate bytes (interbytes) for classification. However, the more\ninformative relations inside bytes (intrabytes) are overlooked and seldom\ninvestigated. By looking inside bytes, the bit-level details of file fragments\ncan be accessed, enabling a more accurate classification. Motivated by this, we\nfirst propose Byte2Image, a novel visual representation model that incorporates\npreviously overlooked intrabyte information into file fragments and\nreinterprets these fragments as 2D grayscale images. This model involves a\nsliding byte window to reveal the intrabyte information and a rowwise stacking\nof intrabyte ngrams for embedding fragments into a 2D space. Thus, complex\ninterbyte and intrabyte correlations can be mined simultaneously using powerful\nvision networks. Additionally, we propose an end-to-end dual-branch network\nByteNet to enhance robust correlation mining and feature representation.\nByteNet makes full use of the raw 1D byte sequence and the converted 2D image\nthrough a shallow byte branch feature extraction (BBFE) and a deep image branch\nfeature extraction (IBFE) network. In particular, the BBFE, composed of a\nsingle fully-connected layer, adaptively recognizes the co-occurrence of\nseveral some specific bytes within the raw byte sequence, while the IBFE, built\non a vision Transformer, effectively mines the complex interbyte and intrabyte\ncorrelations from the converted image. Experiments on the two representative\nbenchmarks, including 14 cases, validate that our proposed method outperforms\nstate-of-the-art approaches on different cases by up to 12.2%.\n","authors":["Wenyang Liu","Kejun Wu","Tianyi Liu","Yi Wang","Kim-Hui Yap","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2410.20855v1.pdf","comment":"Accepted in TMM"},{"id":"http://arxiv.org/abs/2404.13289v2","updated":"2024-10-28T03:35:22Z","published":"2024-04-20T06:32:00Z","title":"Double Mixture: Towards Continual Event Detection from Speech","summary":"  Speech event detection is crucial for multimedia retrieval, involving the\ntagging of both semantic and acoustic events. Traditional ASR systems often\noverlook the interplay between these events, focusing solely on content, even\nthough the interpretation of dialogue can vary with environmental context. This\npaper tackles two primary challenges in speech event detection: the continual\nintegration of new events without forgetting previous ones, and the\ndisentanglement of semantic from acoustic events. We introduce a new task,\ncontinual event detection from speech, for which we also provide two benchmark\ndatasets. To address the challenges of catastrophic forgetting and effective\ndisentanglement, we propose a novel method, 'Double Mixture.' This method\nmerges speech expertise with robust memory mechanisms to enhance adaptability\nand prevent forgetting. Our comprehensive experiments show that this task\npresents significant challenges that are not effectively addressed by current\nstate-of-the-art methods in either computer vision or natural language\nprocessing. Our approach achieves the lowest rates of forgetting and the\nhighest levels of generalization, proving robust across various continual\nlearning sequences. Our code and data are available at\nhttps://anonymous.4open.science/status/Continual-SpeechED-6461.\n","authors":["Jingqi Kang","Tongtong Wu","Jinming Zhao","Guitao Wang","Yinwei Wei","Hao Yang","Guilin Qi","Yuan-Fang Li","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2404.13289v2.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2409.14703v2","updated":"2024-10-28T03:32:28Z","published":"2024-09-23T04:49:08Z","title":"MemeCLIP: Leveraging CLIP Representations for Multimodal Meme\n  Classification","summary":"  The complexity of text-embedded images presents a formidable challenge in\nmachine learning given the need for multimodal understanding of multiple\naspects of expression conveyed by them. While previous research in multimodal\nanalysis has primarily focused on singular aspects such as hate speech and its\nsubclasses, this study expands this focus to encompass multiple aspects of\nlinguistics: hate, targets of hate, stance, and humor. We introduce a novel\ndataset PrideMM comprising 5,063 text-embedded images associated with the\nLGBTQ+ Pride movement, thereby addressing a serious gap in existing resources.\nWe conduct extensive experimentation on PrideMM by using unimodal and\nmultimodal baseline methods to establish benchmarks for each task.\nAdditionally, we propose a novel framework MemeCLIP for efficient downstream\nlearning while preserving the knowledge of the pre-trained CLIP model. The\nresults of our experiments show that MemeCLIP achieves superior performance\ncompared to previously proposed frameworks on two real-world datasets. We\nfurther compare the performance of MemeCLIP and zero-shot GPT-4 on the hate\nclassification task. Finally, we discuss the shortcomings of our model by\nqualitatively analyzing misclassified samples. Our code and dataset are\npublicly available at: https://github.com/SiddhantBikram/MemeCLIP.\n","authors":["Siddhant Bikram Shah","Shuvam Shiwakoti","Maheep Chaudhary","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14703v2.pdf","comment":"Accepted to EMNLP 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.20670v1","updated":"2024-10-28T02:05:10Z","published":"2024-10-28T02:05:10Z","title":"Segmenting Watermarked Texts From Language Models","summary":"  Watermarking is a technique that involves embedding nearly unnoticeable\nstatistical signals within generated content to help trace its source. This\nwork focuses on a scenario where an untrusted third-party user sends prompts to\na trusted language model (LLM) provider, who then generates a text from their\nLLM with a watermark. This setup makes it possible for a detector to later\nidentify the source of the text if the user publishes it. The user can modify\nthe generated text by substitutions, insertions, or deletions. Our objective is\nto develop a statistical method to detect if a published text is LLM-generated\nfrom the perspective of a detector. We further propose a methodology to segment\nthe published text into watermarked and non-watermarked sub-strings. The\nproposed approach is built upon randomization tests and change point detection\ntechniques. We demonstrate that our method ensures Type I and Type II error\ncontrol and can accurately identify watermarked sub-strings by finding the\ncorresponding change point locations. To validate our technique, we apply it to\ntexts generated by several language models with prompts extracted from Google's\nC4 dataset and obtain encouraging numerical results. We release all code\npublicly at https://github.com/doccstat/llm-watermark-cpd.\n","authors":["Xingchi Li","Guanxun Li","Xianyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.20670v1.pdf","comment":"25 pages, 12 figures, 2 tables, NeurIPS 2024"}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2402.19180v2","updated":"2024-10-28T18:01:39Z","published":"2024-02-15T16:53:26Z","title":"ModZoo: A Large-Scale Study of Modded Android Apps and their Markets","summary":"  We present the results of the first large-scale study into Android markets\nthat offer modified or modded apps: apps whose features and functionality have\nbeen altered by a third-party. We analyse over 146k (thousand) apps obtained\nfrom 13 of the most popular modded app markets. Around 90% of apps we collect\nare altered in some way when compared to the official counterparts on Google\nPlay. Modifications include games cheats, such as infinite coins or lives;\nmainstream apps with premium features provided for free; and apps with modified\nadvertising identifiers or excluded ads. We find the original app developers\nlose significant potential revenue due to: the provision of paid for apps for\nfree (around 5% of the apps across all markets); the free availability of\npremium features that require payment in the official app; and modified\nadvertising identifiers. While some modded apps have all trackers and ads\nremoved (3%), in general, the installation of these apps is significantly more\nrisky for the user than the official version: modded apps are ten times more\nlikely to be marked as malicious and often request additional permissions.\n","authors":["Luis A. Saavedra","Hridoy S. Dutta","Alastair R. Beresford","Alice Hutchings"],"pdf_url":"https://arxiv.org/pdf/2402.19180v2.pdf","comment":"To be published in the 2024 Symposium on Electronic Crime Research\n  (eCrime 2024)"}]},"2024-10-27T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.20565v1","updated":"2024-10-27T19:33:46Z","published":"2024-10-27T19:33:46Z","title":"A Fast Algorithm for Computing Zigzag Representatives","summary":"  Zigzag filtrations of simplicial complexes generalize the usual filtrations\nby allowing simplex deletions in addition to simplex insertions. The barcodes\ncomputed from zigzag filtrations encode the evolution of homological features.\nAlthough one can locate a particular feature at any index in the filtration\nusing existing algorithms, the resulting representatives may not be compatible\nwith the zigzag: a representative cycle at one index may not map into a\nrepresentative cycle at its neighbor. For this, one needs to compute compatible\nrepresentative cycles along each bar in the barcode. Even though it is known\nthat the barcode for a zigzag filtration with $m$ insertions and deletions can\nbe computed in $O(m^\\omega)$ time, it is not known how to compute the\ncompatible representatives so efficiently. For a non-zigzag filtration, the\nclassical matrix-based algorithm provides representatives in $O(m^3)$ time,\nwhich can be improved to $O(m^\\omega)$. However, no known algorithm for zigzag\nfiltrations computes the representatives with the $O(m^3)$ time bound. We\npresent an $O(m^2n)$ time algorithm for this problem, where $n\\leq m$ is the\nsize of the largest complex in the filtration.\n","authors":["Tamal K. Dey","Tao Hou","Dmitriy Morozov"],"pdf_url":"https://arxiv.org/pdf/2410.20565v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2406.07428v2","updated":"2024-10-27T22:47:25Z","published":"2024-06-11T16:30:30Z","title":"GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep\n  Learning","summary":"  Automated mechanism design (AMD) uses computational methods for mechanism\ndesign. Differentiable economics is a form of AMD that uses deep learning to\nlearn mechanism designs and has enabled strong progress in AMD in recent years.\nNevertheless, a major open problem has been to learn multi-bidder, general, and\nfully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork\n(GemNet), which significantly extends the menu-based approach of the\nsingle-bidder RochetNet (D\\\"utting et al., 2024) to the multi-bidder setting.\nThe challenge in achieving SP is to learn bidder-independent menus that are\nfeasible, so that the optimal menu choices for each bidder do not over-allocate\nitems when taken together (we call this menu compatibility). GemNet penalizes\nthe failure of menu compatibility during training, and transforms learned menus\nafter training through price changes, by considering a set of discretized\nbidder values and reasoning about Lipschitz smoothness to guarantee menu\ncompatibility on the entire value space. This approach is general, leaving\ntrained menus that already satisfy menu compatibility undisturbed and reducing\nto RochetNet for a single bidder. Mixed-integer linear programs are used for\nmenu transforms, and through a number of optimizations enabled by deep\nlearning, including adaptive grids and methods to skip menu elements, we scale\nto large auction design problems. GemNet learns auctions with better revenue\nthan affine maximization methods, achieves exact SP whereas previous general\nmulti-bidder methods are approximately SP, and offers greatly enhanced\ninterpretability.\n","authors":["Yanchen Jiang","David C. Parkes","Tonghan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16312v2","updated":"2024-10-27T17:55:41Z","published":"2024-07-23T09:05:06Z","title":"MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent\n  Reinforcement Learning","summary":"  Many challenging tasks such as managing traffic systems, electricity grids,\nor supply chains involve complex decision-making processes that must balance\nmultiple conflicting objectives and coordinate the actions of various\nindependent decision-makers (DMs). One perspective for formalising and\naddressing such tasks is multi-objective multi-agent reinforcement learning\n(MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple\nagents each needing to consider multiple objectives in their learning process.\nIn reinforcement learning research, benchmarks are crucial in facilitating\nprogress, evaluation, and reproducibility. The significance of benchmarks is\nunderscored by the existence of numerous benchmark frameworks developed for\nvarious RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent\nRL (e.g., PettingZoo), and single-agent multi-objective RL (e.g.,\nMO-Gymnasium). To support the advancement of the MOMARL field, we introduce\nMOMAland, the first collection of standardised environments for multi-objective\nmulti-agent reinforcement learning. MOMAland addresses the need for\ncomprehensive benchmarking in this emerging field, offering over 10 diverse\nenvironments that vary in the number of agents, state representations, reward\nstructures, and utility considerations. To provide strong baselines for future\nresearch, MOMAland also includes algorithms capable of learning policies in\nsuch settings.\n","authors":["Florian Felten","Umut Ucak","Hicham Azmani","Gao Peng","Willem R√∂pke","Hendrik Baier","Patrick Mannion","Diederik M. Roijers","Jordan K. Terry","El-Ghazali Talbi","Gr√©goire Danoy","Ann Now√©","Roxana RƒÉdulescu"],"pdf_url":"https://arxiv.org/pdf/2407.16312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03084v4","updated":"2024-10-27T09:16:16Z","published":"2023-09-04T09:16:49Z","title":"Accelerating Nash Equilibrium Convergence in Monte Carlo Settings\n  Through Counterfactual Value Based Fictitious Play","summary":"  Counterfactual Regret Minimization (CFR) and its variants are widely\nrecognized as effective algorithms for solving extensive-form imperfect\ninformation games. Recently, many improvements have been focused on enhancing\nthe convergence speed of the CFR algorithm. However, most of these variants are\nnot applicable under Monte Carlo (MC) conditions, making them unsuitable for\ntraining in large-scale games. We introduce a new MC-based algorithm for\nsolving extensive-form imperfect information games, called MCCFVFP (Monte Carlo\nCounterfactual Value-Based Fictitious Play). MCCFVFP combines CFR's\ncounterfactual value calculations with fictitious play's best response\nstrategy, leveraging the strengths of fictitious play to gain significant\nadvantages in games with a high proportion of dominated strategies.\nExperimental results show that MCCFVFP achieved convergence speeds\napproximately 20\\%$\\sim$50\\% faster than the most advanced MCCFR variants in\ngames like poker and other test games.\n","authors":["Ju Qi","Falin Hei","Ting Feng","Dengbing Yi","Zhemei Fang","Yunfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2309.03084v4.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.18367v2","updated":"2024-10-27T20:31:24Z","published":"2024-10-24T02:19:59Z","title":"Synthesis of Binary-Input Multi-Valued Output Optical Cascades for\n  Reversible and Quantum Technologies","summary":"  This paper extends the decomposition from the group theory based methods of\nSasao and Saraivanov to design binary input multivalued output quantum cascades\nrealized with optical NOT, SWAP, and Fredkin Gates. We present this method for\n3, 5, and 7 valued outputs, but in general it can be used for odd prime valued\noutputs. The method can be extended to realize hybrid functions with different\nvalued outputs. A class of local transformations is presented that can simplify\nthe final cascade circuits. Using these simplifying transformations, we present\nan upper bound on the maximum number of gates in an arbitrary $n$-variable\ninput and $k$-valued output function.\n","authors":["Ishani Agarwal","Miroslav Saraivanov","Marek Perkowski"],"pdf_url":"https://arxiv.org/pdf/2410.18367v2.pdf","comment":"31 pages, 64 figures"},{"id":"http://arxiv.org/abs/2410.20560v1","updated":"2024-10-27T19:19:30Z","published":"2024-10-27T19:19:30Z","title":"Scaling Effects of Transistor Leakage Current and IR Drop on 1T1R Memory\n  Arrays","summary":"  1T1R (1-transistor-1-resistor) memory crossbar arrays represent a promising\nsolution for compute-in-memory matrix-vector multiplication accelerators and\nembedded or storage-class memory. However, the size and scaling of these arrays\nare hindered by critical challenges, such as the IR drop on metal lines and the\naccumulation of leakage current from the transistors. Although the IR drop\nissue has been extensively investigated, the impact of transistor leakage\ncurrent has received limited attention. In this work, we investigate both\nissues and highlight how transistor leakage in 1T1R arrays has effects similar\nto IR drop, which degrades the memory cell sensing margin, especially as the\ntechnology node scales down. This degradation could pose reliability concerns,\nparticularly where the on/off ratio or sensing margin of memristors is\ncritical. We characterized the joint effects of transistor read resistance,\ntransistor leakage current, and IR drop as the array size scales up and the\nfabrication node scales down. Based on a model developed using specifications\nof a 22nm FDSOI technology, we found that an optimal resistance range of\nmemristors exists for good array scaling capability, where the transistor read\nresistance and the IR drop issue establish a lower resistance boundary, while\nthe transistor leakage issue sets an upper resistance boundary. This work\nprovides valuable scaling guidelines for engineering the properties of\nmemristor devices in 1T1R memory arrays.\n","authors":["Junren Chen","Giacomo Indiveri"],"pdf_url":"https://arxiv.org/pdf/2410.20560v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.20484v1","updated":"2024-10-27T15:40:20Z","published":"2024-10-27T15:40:20Z","title":"Smart Space Environments: Key Challenges and Innovative Solutions","summary":"  The integration of LoRaWAN (Long Range Wide Area Network) technology with\nboth active and passive sensors presents a transformative opportunity for the\ndevelopment of smart home systems. This paper explores how active sensors, such\nas motion detectors and ultrasonic sensors, and passive sensors, including\ntemperature and humidity sensors, work together to enhance connectivity and\nefficiency within diverse environments while addressing the challenges of\nmodern living. By leveraging LoRaWAN long-range capabilities and low power\nconsumption, the proposed framework enables effective data transmission from\nremote sensors, facilitating applications such as smart agriculture,\nenvironmental monitoring, and comprehensive home automation. Active sensors\nemit energy to detect changes in their surroundings, providing real-time data\ncrucial for security and automation, while passive sensors capture ambient\nenergy to monitor environmental conditions, ensuring resource efficiency and\nuser comfort. The synergy between LoRaWAN and these various sensor types\npromotes innovation, contributing to a more responsive and sustainable living\nexperience. Furthermore, this research highlights the adaptability of the\nproposed system, allowing for seamless integration of new devices and advanced\nfunctionalities. As the landscape of smart home technology continues to evolve,\nongoing research in this area will yield advanced solutions tailored to user\nneeds, ultimately paving the way for smarter, safer, and more efficient living\nenvironments.\n","authors":["Ramakant Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.20484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20459v1","updated":"2024-10-27T14:27:05Z","published":"2024-10-27T14:27:05Z","title":"Unlocking Comics: The AI4VA Dataset for Visual Understanding","summary":"  In the evolving landscape of deep learning, there is a pressing need for more\ncomprehensive datasets capable of training models across multiple modalities.\nConcurrently, in digital humanities, there is a growing demand to leverage\ntechnology for diverse media adaptation and creation, yet limited by sparse\ndatasets due to copyright and stylistic constraints. Addressing this gap, our\npaper presents a novel dataset comprising Franco-Belgian comics from the 1950s\nannotated for tasks including depth estimation, semantic segmentation, saliency\ndetection, and character identification. It consists of two distinct and\nconsistent styles and incorporates object concepts and labels taken from\nnatural images. By including such diverse information across styles, this\ndataset not only holds promise for computational creativity but also offers\navenues for the digitization of art and storytelling innovation. This dataset\nis a crucial component of the AI4VA Workshop\nChallenges~\\url{https://sites.google.com/view/ai4vaeccv2024}, where we\nspecifically explore depth and saliency. Dataset details at\n\\url{https://github.com/IVRL/AI4VA}.\n","authors":["Peter Gr√∂nquist","Deblina Bhattacharjee","Bahar Aydemir","Baran Ozaydin","Tong Zhang","Mathieu Salzmann","Sabine S√ºsstrunk"],"pdf_url":"https://arxiv.org/pdf/2410.20459v1.pdf","comment":"ECCV 2024 Workshop Proceedings"},{"id":"http://arxiv.org/abs/2410.20400v1","updated":"2024-10-27T10:11:30Z","published":"2024-10-27T10:11:30Z","title":"MPLS Network Actions: Technological Overview and P4-Based Implementation\n  on a High-Speed Switching ASIC","summary":"  The MPLS protocol, traditionally focused on packet forwarding using labels,\nhas evolved to include advanced mechanisms such as Service Function Chaining\n(SFC), Alternate-Marking Method (AMM), and in-situ OAM (IOAM). However, many of\nthose mechanisms require extensions to existing specifications in MPLS making\nthem difficult to deploy. To bridge this gap, the IETF MPLS WG proposed the\nMPLS Network Actions (MNA) framework which provides a unified encoding for\nsignaling network actions and their data within the MPLS stack. Network actions\nin the MNA framework serve a similar role for MPLS as extension headers (EH) do\nfor IPv6. The network actions can be encoded within the label stack (in-stack)\nor following the stack (post-stack). In this work, we give a comprehensive\noverview of the design principles of network actions in the MNA framework and\nthe mechanisms that benefit from this framework. We summarize and explain use\ncases in the MNA framework. Building on this, we implement the MNA framework in\nP4 on the Intel Tofino 2 switching ASIC. Our work explores an in-stack data\n(ISD) implementation of the MNA framework. The implementation can process 51\nlabel stack entries containing 32 network actions at a line rate of 400 Gb/s\nper port. Additionally, we implement and evaluate an exemplary network action\nfor performance measurement with AMM. Finally, we identify challenges with an\nMNA in-stack implementation and propose an extension to the signaling\nprocedure.\n","authors":["Fabian Ihle","Michael Menth"],"pdf_url":"https://arxiv.org/pdf/2410.20400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.18050v4","updated":"2024-10-27T07:03:57Z","published":"2024-01-31T18:14:13Z","title":"Hypermultiplexed Integrated-Photonics-based Tensor Optical Processor","summary":"  The escalating data volume and complexity resulting from the rapid expansion\nof artificial intelligence (AI), internet of things (IoT) and 5G/6G mobile\nnetworks is creating an urgent need for energy-efficient, scalable computing\nhardware. Here we demonstrate a hypermultiplexed integratedphotonics-based\ntensor optical processor (HITOP) that can perform trillions of operations per\nsecond (TOPS) at the energy efficiency of 40 TOPS/W. Space-time-wavelength\nthree-dimensional (3D) optical parallelism enables O($N^{2}$) operations per\nclock-cycle using O($N$) modulator devices. The system is built with\nwafer-fabricated III/V micron-scale lasers and high-speed thin-film\nLithium-Niobate electro-optics for encoding at 10s femtojoule/symbol. Lasing\nthreshold incorporates analog inline rectifier (ReLu) nonlinearity for\nlow-latency activation. The system scalability is verified with machine\nlearning models of 405,000 parameters. A combination of high clockrates,\nenergy-efficient processing and programmability unlocks the potential of light\nfor large-scale AI accelerators in applications ranging from training of large\nAI models to real-time decision making in edge deployment.\n","authors":["Shaoyuan Ou","Kaiwen Xue","Lian Zhou","Chun-ho Lee","Alexander Sludds","Ryan Hamerly","Ke Zhang","Hanke Feng","Reshma Kopparapu","Eric Zhong","Cheng Wang","Dirk Englund","Mengjie Yu","Zaijun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.18050v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20332v1","updated":"2024-10-27T04:17:27Z","published":"2024-10-27T04:17:27Z","title":"The maximum storage capacity of open-loop written RRAM is around 4 bits","summary":"  There have been a plethora of research on multi-level memory devices, where\nthe resistive random-access memory (RRAM) is a prominent example. Although it\nis easy to write an RRAM device into multiple (even quasi-continuous) states,\nit suffers from the inherent variations that should limit the storage capacity,\nespecially in the open-loop writing scenario. There have been many experimental\nresults in this regard, however, it lacks a comprehensive analysis of the valid\nmulti-bit storage capability, especially in theoretical terms. The absence of\nsuch an insight usually results in misleading conclusions that either\nexaggerate or underestimate the storage capacity of RRAM devices. Here, by the\nconcept of information theory, we present a model for evaluating the storage\ncapacity of open-loop written RRAM. Based on the experimental results in the\nliterature and the test results of our own devices, we have carefully examined\nthe effects of number of pre-defined levels, conductance variation, and\nconductance range, on the storage capacity. The analysis leads to a conclusion\nthat the maximum capacity of RRAM devices is around 4 bits.\n","authors":["Yongxiang Li","Shiqing Wang","Zhong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.20332v1.pdf","comment":"This work has been presented at the conference ICSICT 2024"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.13832v2","updated":"2024-10-27T23:13:38Z","published":"2024-10-17T17:53:24Z","title":"VidPanos: Generative Panoramic Videos from Casual Panning Videos","summary":"  Panoramic image stitching provides a unified, wide-angle view of a scene that\nextends beyond the camera's field of view. Stitching frames of a panning video\ninto a panoramic photograph is a well-understood problem for stationary scenes,\nbut when objects are moving, a still panorama cannot capture the scene. We\npresent a method for synthesizing a panoramic video from a casually-captured\npanning video, as if the original video were captured with a wide-angle camera.\nWe pose panorama synthesis as a space-time outpainting problem, where we aim to\ncreate a full panoramic video of the same length as the input video. Consistent\ncompletion of the space-time volume requires a powerful, realistic prior over\nvideo content and motion, for which we adapt generative video models. Existing\ngenerative models do not, however, immediately extend to panorama completion,\nas we show. We instead apply video generation as a component of our panorama\nsynthesis system, and demonstrate how to exploit the strengths of the models\nwhile minimizing their limitations. Our system can create video panoramas for a\nrange of in-the-wild scenes including people, vehicles, and flowing water, as\nwell as stationary background features.\n","authors":["Jingwei Ma","Erika Lu","Roni Paiss","Shiran Zada","Aleksander Holynski","Tali Dekel","Brian Curless","Michael Rubinstein","Forrester Cole"],"pdf_url":"https://arxiv.org/pdf/2410.13832v2.pdf","comment":"Project page at https://vidpanos.github.io/. To appear at SIGGRAPH\n  Asia 2024 (conference track)"},{"id":"http://arxiv.org/abs/2410.20627v1","updated":"2024-10-27T23:07:44Z","published":"2024-10-27T23:07:44Z","title":"DHPrep: Deep Hawkes Process based Dynamic Network Representation","summary":"  Networks representation aims to encode vertices into a low-dimensional space,\nwhile preserving the original network structures and properties. Most existing\nmethods focus on static network structure without considering temporal\ndynamics. However, in real world, most networks (e.g., social and biological\nnetworks) are dynamic in nature and are constantly evolving over time. Such\ntemporal dynamics are critical in representations learning, especially for\npredicting dynamic networks behaviors. To this end, a Deep Hawkes Process based\nDynamic Networks Representation algorithm (DHPrep) is proposed in this paper,\nwhich is capable of capturing temporal dynamics of dynamic networks.\nSpecifically, DHPrep incorporates both structural information and temporal\ndynamics to learn vertices representations that can model the edge formation\nprocess for a vertex pair, where the structural information is used to capture\nthe historical impact from their neighborhood, and the temporal dynamics\nutilize this historical information and apply Hawkes point process to model the\nedges formation process. Moreover, a temporal smoother is further imposed to\nensure the representations evolve smoothly over time. To evaluate the\neffectiveness of DHPrep, extensive experiments are carried out using four\nreal-world datasets. Experimental results reveal that our DHPrep algorithm\noutperforms state-of-the-art baseline methods in various tasks including link\nprediction and vertices recommendation.\n","authors":["Ruixuan Han","Hongxiang Li","Bin Xie"],"pdf_url":"https://arxiv.org/pdf/2410.20627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20429v1","updated":"2024-10-27T12:53:17Z","published":"2024-10-27T12:53:17Z","title":"MARS: Multi-sample Allocation through Russian roulette and Splitting","summary":"  Multiple importance sampling (MIS) is an indispensable tool in rendering that\nconstructs robust sampling strategies by combining the respective strengths of\nindividual distributions. Its efficiency can be greatly improved by carefully\nselecting the number of samples drawn from each distribution, but automating\nthis process remains a challenging problem. Existing works are mostly limited\nto mixture sampling, in which only a single sample is drawn in total, and the\nworks that do investigate multi-sample MIS only optimize the sample counts at a\nper-pixel level, which cannot account for variations beyond the first bounce.\nRecent work on Russian roulette and splitting has demonstrated how fixed-point\nschemes can be used to spatially vary sample counts to optimize image\nefficiency but is limited to choosing the same number of samples across all\nsampling strategies. Our work proposes a highly flexible sample allocation\nstrategy that bridges the gap between these areas of work. We show how to\niteratively optimize the sample counts to maximize the efficiency of the\nrendered image using a lightweight data structure, which allows us to make\nlocal and individual decisions per technique. We demonstrate the benefits of\nour approach in two applications, path guiding and bidirectional path tracing,\nin both of which we achieve consistent and substantial speedups over the\nrespective previous state-of-the-art.\n","authors":["Joshua Meyer","Alexander Rath","√ñmercan Yazici","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2410.20429v1.pdf","comment":"18 pages, 13 figures, to be published in SIGGRAPH Asia 2024\n  Conference Papers"},{"id":"http://arxiv.org/abs/2410.20389v1","updated":"2024-10-27T09:32:35Z","published":"2024-10-27T09:32:35Z","title":"Lodge++: High-quality and Long Dance Generation with Vivid Choreography\n  Patterns","summary":"  We propose Lodge++, a choreography framework to generate high-quality,\nultra-long, and vivid dances given the music and desired genre. To handle the\nchallenges in computational efficiency, the learning of complex and vivid\nglobal choreography patterns, and the physical quality of local dance\nmovements, Lodge++ adopts a two-stage strategy to produce dances from coarse to\nfine. In the first stage, a global choreography network is designed to generate\ncoarse-grained dance primitives that capture complex global choreography\npatterns. In the second stage, guided by these dance primitives, a\nprimitive-based dance diffusion model is proposed to further generate\nhigh-quality, long-sequence dances in parallel, faithfully adhering to the\ncomplex choreography patterns. Additionally, to improve the physical\nplausibility, Lodge++ employs a penetration guidance module to resolve\ncharacter self-penetration, a foot refinement module to optimize foot-ground\ncontact, and a multi-genre discriminator to maintain genre consistency\nthroughout the dance. Lodge++ is validated by extensive experiments, which show\nthat our method can rapidly generate ultra-long dances suitable for various\ndance genres, ensuring well-organized global choreography patterns and\nhigh-quality local motion.\n","authors":["Ronghui Li","Hongwen Zhang","Yachao Zhang","Yuxiang Zhang","Youliang Zhang","Jie Guo","Yan Zhang","Xiu Li","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20389v1.pdf","comment":"Project page: https://li-ronghui.github.io/lodgepp"},{"id":"http://arxiv.org/abs/2410.20359v1","updated":"2024-10-27T07:25:11Z","published":"2024-10-27T07:25:11Z","title":"Conditional GAN for Enhancing Diffusion Models in Efficient and\n  Authentic Global Gesture Generation from Audios","summary":"  Audio-driven simultaneous gesture generation is vital for human-computer\ncommunication, AI games, and film production. While previous research has shown\npromise, there are still limitations. Methods based on VAEs are accompanied by\nissues of local jitter and global instability, whereas methods based on\ndiffusion models are hampered by low generation efficiency. This is because the\ndenoising process of DDPM in the latter relies on the assumption that the noise\nadded at each step is sampled from a unimodal distribution, and the noise\nvalues are small. DDIM borrows the idea from the Euler method for solving\ndifferential equations, disrupts the Markov chain process, and increases the\nnoise step size to reduce the number of denoising steps, thereby accelerating\ngeneration. However, simply increasing the step size during the step-by-step\ndenoising process causes the results to gradually deviate from the original\ndata distribution, leading to a significant drop in the quality of the\ngenerated actions and the emergence of unnatural artifacts. In this paper, we\nbreak the assumptions of DDPM and achieves breakthrough progress in denoising\nspeed and fidelity. Specifically, we introduce a conditional GAN to capture\naudio control signals and implicitly match the multimodal denoising\ndistribution between the diffusion and denoising steps within the same sampling\nstep, aiming to sample larger noise values and apply fewer denoising steps for\nhigh-speed generation.\n","authors":["Yongkang Cheng","Mingjiang Liang","Shaoli Huang","Gaoge Han","Jifeng Ning","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20359v1.pdf","comment":"Accepted by WACV 2025 (Round 1)"},{"id":"http://arxiv.org/abs/2410.20304v1","updated":"2024-10-27T02:00:33Z","published":"2024-10-27T02:00:33Z","title":"Deep Learning, Machine Learning -- Digital Signal and Image Processing:\n  From Theory to Application","summary":"  Digital Signal Processing (DSP) and Digital Image Processing (DIP) with\nMachine Learning (ML) and Deep Learning (DL) are popular research areas in\nComputer Vision and related fields. We highlight transformative applications in\nimage enhancement, filtering techniques, and pattern recognition. By\nintegrating frameworks like the Discrete Fourier Transform (DFT), Z-Transform,\nand Fourier Transform methods, we enable robust data manipulation and feature\nextraction essential for AI-driven tasks. Using Python, we implement algorithms\nthat optimize real-time data processing, forming a foundation for scalable,\nhigh-performance solutions in computer vision. This work illustrates the\npotential of ML and DL to advance DSP and DIP methodologies, contributing to\nartificial intelligence, automated feature extraction, and applications across\ndiverse domains.\n","authors":["Weiche Hsieh","Ziqian Bi","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jiawei Xu","Jinlang Wang","Keyu Chen","Caitlyn Heqi Yin","Pohsun Feng","Yizhu Wen","Tianyang Wang","Ming Li","Jintao Ren","Qian Niu","Silin Chen","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20304v1.pdf","comment":"293 pages"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.20624v1","updated":"2024-10-27T22:56:51Z","published":"2024-10-27T22:56:51Z","title":"Towards an LLM-Based Speech Interface for Robot-Assisted Feeding","summary":"  Physically assistive robots present an opportunity to significantly increase\nthe well-being and independence of individuals with motor impairments or other\nforms of disability who are unable to complete activities of daily living\n(ADLs). Speech interfaces, especially ones that utilize Large Language Models\n(LLMs), can enable individuals to effectively and naturally communicate\nhigh-level commands and nuanced preferences to robots. In this work, we\ndemonstrate an LLM-based speech interface for a commercially available\nassistive feeding robot. Our system is based on an iteratively designed\nframework, from the paper \"VoicePilot: Harnessing LLMs as Speech Interfaces for\nPhysically Assistive Robots,\" that incorporates human-centric elements for\nintegrating LLMs as interfaces for robots. It has been evaluated through a user\nstudy with 11 older adults at an independent living facility. Videos are\nlocated on our project website:\nhttps://sites.google.com/andrew.cmu.edu/voicepilot/.\n","authors":["Jessie Yuan","Janavi Gupta","Akhil Padmanabha","Zulekha Karachiwalla","Carmel Majidi","Henny Admoni","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2410.20624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20600v1","updated":"2024-10-27T21:20:18Z","published":"2024-10-27T21:20:18Z","title":"Implementation and Application of an Intelligibility Protocol for\n  Interaction with an LLM","summary":"  Our interest is in constructing interactive systems involving a human-expert\ninteracting with a machine learning engine on data analysis tasks. This is of\nrelevance when addressing complex problems arising in areas of science, the\nenvironment, medicine and so on, which are not immediately amenable to the\nusual methods of statistical or mathematical modelling. In such situations, it\nis possible that harnessing human expertise and creativity to modern\nmachine-learning capabilities of identifying patterns by constructing new\ninternal representations of the data may provide some insight to possible\nsolutions. In this paper, we examine the implementation of an abstract protocol\ndeveloped for interaction between agents, each capable of constructing\npredictions and explanations. The \\PXP protocol, described in [12] is motivated\nby the notion of ''two-way intelligibility'' and is specified using a pair of\ncommunicating finite-state machines. While the formalisation allows the authors\nto prove several properties about the protocol, no implementation was\npresented. Here, we address this shortcoming for the case in which one of the\nagents acts as a ''generator'' using a large language model (LLM) and the other\nis an agent that acts as a ''tester'' using either a human-expert, or a proxy\nfor a human-expert (for example, a database compiled using human-expertise). We\nbelieve these use-cases will be a widely applicable form of interaction for\nproblems of the kind mentioned above. We present an algorithmic description of\ngeneral-purpose implementation, and conduct preliminary experiments on its use\nin two different areas (radiology and drug-discovery). The experimental results\nprovide early evidence in support of the protocol's capability of capturing\none- and two-way intelligibility in human-LLM in the manner proposed in [12].\n","authors":["Ashwin Srinivasan","Karan Bania","Shreyas V","Harshvardhan Mestha","Sidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20571v1","updated":"2024-10-27T19:54:52Z","published":"2024-10-27T19:54:52Z","title":"Making Urban Art Accessible: Current Art Access Techniques, Design\n  Considerations, and the Role of AI","summary":"  Public artwork, from vibrant wall murals to captivating sculptures, can\nenhance the aesthetic of urban spaces, foster a sense of community and cultural\nidentity, and help attract visitors. Despite its benefits, most public art is\nvisual, making it often inaccessible to blind and low vision (BLV) people. In\nthis workshop paper, we first draw on art literature to help define the space\nof public art, identify key differences with curated art shown in museums or\ngalleries, and discuss implications for accessibility. We then enumerate how\nexisting art accessibility techniques may (or may not) transfer to urban art\nspaces. We close by presenting future research directions and reflecting on the\ngrowing role of AI in making art accessible.\n","authors":["Lucy Jiang","Jon E. Froehlich","Leah Findlater"],"pdf_url":"https://arxiv.org/pdf/2410.20571v1.pdf","comment":"ASSETS 2024 Workshop Submission (The Future of Urban Accessibility:\n  The Role of AI)"},{"id":"http://arxiv.org/abs/2410.20564v1","updated":"2024-10-27T19:33:01Z","published":"2024-10-27T19:33:01Z","title":"Using Confidence Scores to Improve Eyes-free Detection of Speech\n  Recognition Errors","summary":"  Conversational systems rely heavily on speech recognition to interpret and\nrespond to user commands and queries. Nevertheless, recognition errors may\noccur, which can significantly affect the performance of such systems. While\nvisual feedback can help detect errors, it may not always be practical,\nespecially for people who are blind or low-vision. In this study, we\ninvestigate ways to improve error detection by manipulating the audio output of\nthe transcribed text based on the recognizer's confidence level in its result.\nOur findings show that selectively slowing down the audio when the recognizer\nexhibited uncertainty led to a relative increase of 12% in participants' error\ndetection ability compared to uniformly slowing down the audio.\n","authors":["Sadia Nowrin","Keith Vertanen"],"pdf_url":"https://arxiv.org/pdf/2410.20564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20545v1","updated":"2024-10-27T18:24:05Z","published":"2024-10-27T18:24:05Z","title":"ChartA11y: Designing Accessible Touch Experiences of Visualizations with\n  Blind Smartphone Users","summary":"  We introduce ChartA11y, an app developed to enable accessible 2-D\nvisualizations on smartphones for blind users through a participatory and\niterative design process involving 13 sessions with two blind partners. We also\npresent a design journey for making accessible touch experiences that go beyond\nsimple auditory feedback, incorporating multimodal interactions and\nmultisensory data representations. Together, ChartA11y aimed at providing\ndirect chart accessing and comprehensive chart understanding by applying a\ntwo-mode setting: a semantic navigation framework mode and a direct touch\nmapping mode. By re-designing traditional touch-to-audio interactions,\nChartA11y also extends to accessible scatter plots, addressing the\nunder-explored challenges posed by their non-linear data distribution. Our main\ncontributions encompass the detailed participatory design process and the\nresulting system, ChartA11y, offering a novel approach for blind users to\naccess visualizations on their smartphones.\n","authors":["Zhuohao Jerry Zhang","John R. Thompson","Aditi Shah","Manish Agrawal","Alper Sarikaya","Jacob O. Wobbrock","Edward Cutrell","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.20545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20536v1","updated":"2024-10-27T17:59:17Z","published":"2024-10-27T17:59:17Z","title":"Malinowski in the Age of AI: Can large language models create a text\n  game based on an anthropological classic?","summary":"  Recent advancements in Large Language Models (LLMs) like ChatGPT and GPT-4\nhave shown remarkable abilities in a wide range of tasks such as summarizing\ntexts and assisting in coding. Scientific research has demonstrated that these\nmodels can also play text-adventure games. This study aims to explore whether\nLLMs can autonomously create text-based games based on anthropological\nclassics, evaluating also their effectiveness in communicating knowledge. To\nachieve this, the study engaged anthropologists in discussions to gather their\nexpectations and design inputs for an anthropologically themed game. Through\niterative processes following the established HCI principle of 'design\nthinking', the prompts and the conceptual framework for crafting these games\nwere refined. Leveraging GPT3.5, the study created three prototypes of games\ncentered around the seminal anthropological work of the social anthropologist's\nBronislaw Malinowski's \"Argonauts of the Western Pacific\" (1922). Subsequently,\nevaluations were conducted by inviting senior anthropologists to playtest these\ngames, and based on their inputs, the game designs were refined. The tests\nrevealed promising outcomes but also highlighted key challenges: the models\nencountered difficulties in providing in-depth thematic understandings, showed\nsuspectibility to misinformation, tended towards monotonic responses after an\nextended period of play, and struggled to offer detailed biographical\ninformation. Despite these limitations, the study's findings open up new\nresearch avenues at the crossroads of artificial intelligence, machine\nlearning, LLMs, ethnography, anthropology and human-computer interaction.\n","authors":["Michael Peter Hoffmann","Jan Fillies","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2410.20536v1.pdf","comment":"Accepted at KUI 2024"},{"id":"http://arxiv.org/abs/2410.20508v1","updated":"2024-10-27T16:44:15Z","published":"2024-10-27T16:44:15Z","title":"Referring Human Pose and Mask Estimation in the Wild","summary":"  We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild,\nwhere either a text or positional prompt specifies the person of interest in an\nimage. This new task holds significant potential for human-centric applications\nsuch as assistive robotics and sports analysis. In contrast to previous works,\nR-HPM (i) ensures high-quality, identity-aware results corresponding to the\nreferred person, and (ii) simultaneously predicts human pose and mask for a\ncomprehensive representation. To achieve this, we introduce a large-scale\ndataset named RefHuman, which substantially extends the MS COCO dataset with\nadditional text and positional prompt annotations. RefHuman includes over\n50,000 annotated instances in the wild, each equipped with keypoint, mask, and\nprompt annotations. To enable prompt-conditioned estimation, we propose the\nfirst end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts\nmultimodal representations and employs a proposed pose-centric hierarchical\ndecoder to process (text or positional) instance queries and keypoint queries,\nproducing results specific to the referred person. Extensive experiments\ndemonstrate that UniPHD produces quality results based on user-friendly prompts\nand achieves top-tier performance on RefHuman val and MS COCO val2017. Data and\nCode: https://github.com/bo-miao/RefHuman\n","authors":["Bo Miao","Mingtao Feng","Zijie Wu","Mohammed Bennamoun","Yongsheng Gao","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2410.20508v1.pdf","comment":"Accepted by NeurIPS 2024. https://github.com/bo-miao/RefHuman"},{"id":"http://arxiv.org/abs/2410.20468v1","updated":"2024-10-27T15:08:54Z","published":"2024-10-27T15:08:54Z","title":"Understanding Communication Preferences of Information Workers in\n  Engagement with Text-Based Conversational Agents","summary":"  Communication traits in text-based human-AI conversations play pivotal roles\nin shaping user experiences and perceptions of systems. With the advancement of\nlarge language models (LLMs), it is now feasible to analyze these traits at a\nmore granular level. In this study, we explore the preferences of information\nworkers regarding chatbot communication traits across seven applications.\nParticipants were invited to participate in an interactive survey, which\nfeatured adjustable sliders, allowing them to adjust and express their\npreferences for five key communication traits: formality, personification,\nempathy, sociability, and humor. Our findings reveal distinct communication\npreferences across different applications; for instance, there was a preference\nfor relatively high empathy in wellbeing contexts and relatively low\npersonification in coding. Similarities in preferences were also noted between\napplications such as chatbots for customer service and scheduling. These\ninsights offer crucial design guidelines for future chatbots, emphasizing the\nneed for nuanced trait adjustments for each application.\n","authors":["Ananya Bhattacharjee","Jina Suh","Mahsa Ershadi","Shamsi T. Iqbal","Andrew D. Wilson","Javier Hernandez"],"pdf_url":"https://arxiv.org/pdf/2410.20468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.01819v2","updated":"2024-10-27T07:08:57Z","published":"2023-01-04T20:48:22Z","title":"A Model for Intelligible Interaction Between Agents That Predict and\n  Explain","summary":"  Machine Learning (ML) has emerged as a powerful form of data modelling with\nwidespread applicability beyond its roots in the design of autonomous agents.\nHowever, relatively little attention has been paid to the interaction between\npeople and ML systems. In this paper we view interaction between humans and ML\nsystems within the broader context of communication between agents capable of\nprediction and explanation. We formalise the interaction model by taking agents\nto be automata with some special characteristics and define a protocol for\ncommunication between such agents. We define One- and Two-Way Intelligibility\nas properties that emerge at run-time by execution of the protocol. The\nformalisation allows us to identify conditions under which run-time sequences\nare bounded, and identify conditions under which the protocol can correctly\nimplement an axiomatic specification of intelligible interaction between a\nhuman and an ML system. We also demonstrate using the formal model to: (a)\nidentify instances of One- and Two-Way Intelligibility in literature reports on\nhumans interacting with ML systems providing logic-based explanations, as is\ndone in Inductive Logic Programming (ILP); and (b) map interactions between\nhumans and machines in an elaborate natural-language based dialogue-model to\nOne- or Two-Way Intelligible interactions in the formal model.\n","authors":["A. Baskar","Ashwin Srinivasan","Michael Bain","Enrico Coiera"],"pdf_url":"https://arxiv.org/pdf/2301.01819v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2205.08954"},{"id":"http://arxiv.org/abs/2404.16048v2","updated":"2024-10-27T05:54:50Z","published":"2024-04-09T11:59:41Z","title":"GUIDE: Graphical User Interface Data for Execution","summary":"  In this paper, we introduce GUIDE, a novel dataset tailored for the\nadvancement of Multimodal Large Language Model (MLLM) applications,\nparticularly focusing on Robotic Process Automation (RPA) use cases. Our\ndataset encompasses diverse data from various websites including\nApollo(62.67\\%), Gmail(3.43\\%), Calendar(10.98\\%) and Canva(22.92\\%). Each data\nentry includes an image, a task description, the last action taken, CoT and the\nnext action to be performed along with grounding information of where the\naction needs to be executed. The data is collected using our in-house advanced\nannotation tool NEXTAG (Next Action Grounding and Annotation Tool). The data is\nadapted for multiple OS, browsers and display types. It is collected by\nmultiple annotators to capture the variation of design and the way person uses\na website.\n  Through this dataset, we aim to facilitate research and development in the\nrealm of LLMs for graphical user interfaces, particularly in tasks related to\nRPA. The dataset's multi-platform nature and coverage of diverse websites\nenable the exploration of cross-interface capabilities in automation tasks. We\nbelieve that our dataset will serve as a valuable resource for advancing the\ncapabilities of multi-platform LLMs in practical applications, fostering\ninnovation in the field of automation and natural language understanding. Using\nGUIDE, we build V-Zen, the first RPA model to automate multiple websites using\nour in-House Automation tool AUTONODE\n","authors":["Rajat Chawla","Adarsh Jha","Muskaan Kumar","Mukunda NS","Ishaan Bhola"],"pdf_url":"https://arxiv.org/pdf/2404.16048v2.pdf","comment":"11 pages, 8 figures, 3 Tables and 1 Algorithm"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.20600v1","updated":"2024-10-27T21:20:18Z","published":"2024-10-27T21:20:18Z","title":"Implementation and Application of an Intelligibility Protocol for\n  Interaction with an LLM","summary":"  Our interest is in constructing interactive systems involving a human-expert\ninteracting with a machine learning engine on data analysis tasks. This is of\nrelevance when addressing complex problems arising in areas of science, the\nenvironment, medicine and so on, which are not immediately amenable to the\nusual methods of statistical or mathematical modelling. In such situations, it\nis possible that harnessing human expertise and creativity to modern\nmachine-learning capabilities of identifying patterns by constructing new\ninternal representations of the data may provide some insight to possible\nsolutions. In this paper, we examine the implementation of an abstract protocol\ndeveloped for interaction between agents, each capable of constructing\npredictions and explanations. The \\PXP protocol, described in [12] is motivated\nby the notion of ''two-way intelligibility'' and is specified using a pair of\ncommunicating finite-state machines. While the formalisation allows the authors\nto prove several properties about the protocol, no implementation was\npresented. Here, we address this shortcoming for the case in which one of the\nagents acts as a ''generator'' using a large language model (LLM) and the other\nis an agent that acts as a ''tester'' using either a human-expert, or a proxy\nfor a human-expert (for example, a database compiled using human-expertise). We\nbelieve these use-cases will be a widely applicable form of interaction for\nproblems of the kind mentioned above. We present an algorithmic description of\ngeneral-purpose implementation, and conduct preliminary experiments on its use\nin two different areas (radiology and drug-discovery). The experimental results\nprovide early evidence in support of the protocol's capability of capturing\none- and two-way intelligibility in human-LLM in the manner proposed in [12].\n","authors":["Ashwin Srinivasan","Karan Bania","Shreyas V","Harshvardhan Mestha","Sidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12327v3","updated":"2024-10-27T19:03:37Z","published":"2024-02-19T18:00:53Z","title":"Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM\n  Agents","summary":"  Large Language Models (LLMs) have increasingly been utilized in social\nsimulations, where they are often guided by carefully crafted instructions to\nstably exhibit human-like behaviors during simulations. Nevertheless, we doubt\nthe necessity of shaping agents' behaviors for accurate social simulations.\nInstead, this paper emphasizes the importance of spontaneous phenomena, wherein\nagents deeply engage in contexts and make adaptive decisions without explicit\ndirections. We explored spontaneous cooperation across three competitive\nscenarios and successfully simulated the gradual emergence of cooperation,\nfindings that align closely with human behavioral data. This approach not only\naids the computational social science community in bridging the gap between\nsimulations and real-world dynamics but also offers the AI community a novel\nmethod to assess LLMs' capability of deliberate reasoning.\n","authors":["Zengqing Wu","Run Peng","Shuyuan Zheng","Qianying Liu","Xu Han","Brian Inhyuk Kwon","Makoto Onizuka","Shaojie Tang","Chuan Xiao"],"pdf_url":"https://arxiv.org/pdf/2402.12327v3.pdf","comment":"EMNLP 2024 Findings. Source codes available at\n  https://github.com/wuzengqing001225/SABM_ShallWeTeamUp"},{"id":"http://arxiv.org/abs/2410.20550v1","updated":"2024-10-27T18:38:05Z","published":"2024-10-27T18:38:05Z","title":"Deep Reinforcement Learning Agents for Strategic Production Policies in\n  Microeconomic Market Simulations","summary":"  Traditional economic models often rely on fixed assumptions about market\ndynamics, limiting their ability to capture the complexities and stochastic\nnature of real-world scenarios. However, reality is more complex and includes\nnoise, making traditional models assumptions not met in the market. In this\npaper, we explore the application of deep reinforcement learning (DRL) to\nobtain optimal production strategies in microeconomic market environments to\novercome the limitations of traditional models. Concretely, we propose a\nDRL-based approach to obtain an effective policy in competitive markets with\nmultiple producers, each optimizing their production decisions in response to\nfluctuating demand, supply, prices, subsidies, fixed costs, total production\ncurve, elasticities and other effects contaminated by noise. Our framework\nenables agents to learn adaptive production policies to several simulations\nthat consistently outperform static and random strategies. As the deep neural\nnetworks used by the agents are universal approximators of functions, DRL\nalgorithms can represent in the network complex patterns of data learnt by\ntrial and error that explain the market. Through extensive simulations, we\ndemonstrate how DRL can capture the intricate interplay between production\ncosts, market prices, and competitor behavior, providing insights into optimal\ndecision-making in dynamic economic settings. The results show that agents\ntrained with DRL can strategically adjust production levels to maximize\nlong-term profitability, even in the face of volatile market conditions. We\nbelieve that the study bridges the gap between theoretical economic modeling\nand practical market simulation, illustrating the potential of DRL to\nrevolutionize decision-making in market strategies.\n","authors":["Eduardo C. Garrido-Merch√°n","Maria Coronado-Vaca","√Ålvaro L√≥pez-L√≥pez","Carlos Martinez de Ibarreta"],"pdf_url":"https://arxiv.org/pdf/2410.20550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16312v2","updated":"2024-10-27T17:55:41Z","published":"2024-07-23T09:05:06Z","title":"MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent\n  Reinforcement Learning","summary":"  Many challenging tasks such as managing traffic systems, electricity grids,\nor supply chains involve complex decision-making processes that must balance\nmultiple conflicting objectives and coordinate the actions of various\nindependent decision-makers (DMs). One perspective for formalising and\naddressing such tasks is multi-objective multi-agent reinforcement learning\n(MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple\nagents each needing to consider multiple objectives in their learning process.\nIn reinforcement learning research, benchmarks are crucial in facilitating\nprogress, evaluation, and reproducibility. The significance of benchmarks is\nunderscored by the existence of numerous benchmark frameworks developed for\nvarious RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent\nRL (e.g., PettingZoo), and single-agent multi-objective RL (e.g.,\nMO-Gymnasium). To support the advancement of the MOMARL field, we introduce\nMOMAland, the first collection of standardised environments for multi-objective\nmulti-agent reinforcement learning. MOMAland addresses the need for\ncomprehensive benchmarking in this emerging field, offering over 10 diverse\nenvironments that vary in the number of agents, state representations, reward\nstructures, and utility considerations. To provide strong baselines for future\nresearch, MOMAland also includes algorithms capable of learning policies in\nsuch settings.\n","authors":["Florian Felten","Umut Ucak","Hicham Azmani","Gao Peng","Willem R√∂pke","Hendrik Baier","Patrick Mannion","Diederik M. Roijers","Jordan K. Terry","El-Ghazali Talbi","Gr√©goire Danoy","Ann Now√©","Roxana RƒÉdulescu"],"pdf_url":"https://arxiv.org/pdf/2407.16312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02198v2","updated":"2024-10-27T12:05:52Z","published":"2024-05-03T15:54:20Z","title":"The Cambridge RoboMaster: An Agile Multi-Robot Research Platform","summary":"  Compact robotic platforms with powerful compute and actuation capabilities\nare key enablers for practical, real-world deployments of multi-agent research.\nThis article introduces a tightly integrated hardware, control, and simulation\nsoftware stack on a fleet of holonomic ground robot platforms designed with\nthis motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles,\noffer a balance between small robots that do not possess sufficient compute or\nactuation capabilities and larger robots that are unsuitable for indoor\nmulti-robot tests. They run a modular ROS2-based optimal estimation and control\nstack for full onboard autonomy, contain ad-hoc peer-to-peer communication\ninfrastructure, and can zero-shot run multi-agent reinforcement learning (MARL)\npolicies trained in our vectorized multi-agent simulation framework. We present\nan in-depth review of other platforms currently available, showcase new\nexperimental validation of our system's capabilities, and introduce case\nstudies that highlight the versatility and reliability of our system as a\ntestbed for a wide range of research demonstrations. Our system as well as\nsupplementary material is available online.\nhttps://proroklab.github.io/cambridge-robomaster\n","authors":["Jan Blumenkamp","Ajay Shankar","Matteo Bettini","Joshua Bird","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2405.02198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13750v3","updated":"2024-10-27T09:09:58Z","published":"2024-08-25T07:32:58Z","title":"Multi-Agent Target Assignment and Path Finding for Intelligent\n  Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective","summary":"  Multi-agent target assignment and path planning (TAPF) are two key problems\nin intelligent warehouse. However, most literature only addresses one of these\ntwo problems separately. In this study, we propose a method to simultaneously\nsolve target assignment and path planning from a perspective of cooperative\nmulti-agent deep reinforcement learning (RL). To the best of our knowledge,\nthis is the first work to model the TAPF problem for intelligent warehouse to\ncooperative multi-agent deep RL, and the first to simultaneously address TAPF\nbased on multi-agent deep RL. Furthermore, previous literature rarely considers\nthe physical dynamics of agents. In this study, the physical dynamics of the\nagents is considered. Experimental results show that our method performs well\nin various task settings, which means that the target assignment is solved\nreasonably well and the planned path is almost shortest. Moreover, our method\nis more time-efficient than baselines.\n","authors":["Qi Liu","Jianqi Gao","Dongjie Zhu","Zhongjian Qiao","Pengbin Chen","Jingxiang Guo","Yanjie Li"],"pdf_url":"https://arxiv.org/pdf/2408.13750v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10847v2","updated":"2024-10-27T07:53:40Z","published":"2024-06-16T08:39:19Z","title":"TorchOpera: A Compound AI System for LLM Safety","summary":"  We introduce TorchOpera, a compound AI system for enhancing the safety and\nquality of prompts and responses for Large Language Models. TorchOpera ensures\nthat all user prompts are safe, contextually grounded, and effectively\nprocessed, while enhancing LLM responses to be relevant and high quality.\nTorchOpera utilizes the vector database for contextual grounding, rule-based\nwrappers for flexible modifications, and specialized mechanisms for detecting\nand adjusting unsafe or incorrect content. We also provide a view of the\ncompound AI system to reduce the computational cost. Extensive experiments show\nthat TorchOpera ensures the safety, reliability, and applicability of LLMs in\nreal-world settings while maintaining the efficiency of LLM responses.\n","authors":["Shanshan Han","Zijian Hu","Alay Dilipbhai Shah","Han Jin","Yuhang Yao","Dimitris Stripelis","Zhaozhuo Xu","Chaoyang He"],"pdf_url":"https://arxiv.org/pdf/2406.10847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20345v1","updated":"2024-10-27T06:01:01Z","published":"2024-10-27T06:01:01Z","title":"Logarithmically Quantized Distributed Optimization over Dynamic\n  Multi-Agent Networks","summary":"  Distributed optimization finds many applications in machine learning, signal\nprocessing, and control systems. In these real-world applications, the\nconstraints of communication networks, particularly limited bandwidth,\nnecessitate implementing quantization techniques. In this paper, we propose\ndistributed optimization dynamics over multi-agent networks subject to\nlogarithmically quantized data transmission. Under this condition, data\nexchange benefits from representing smaller values with more bits and larger\nvalues with fewer bits. As compared to uniform quantization, this allows for\nhigher precision in representing near-optimal values and more accuracy of the\ndistributed optimization algorithm. The proposed optimization dynamics comprise\na primary state variable converging to the optimizer and an auxiliary variable\ntracking the objective function's gradient. Our setting accommodates dynamic\nnetwork topologies, resulting in a hybrid system requiring convergence analysis\nusing matrix perturbation theory and eigenspectrum analysis.\n","authors":["Mohammadreza Doostmohammadian","S√©rgio Pequito"],"pdf_url":"https://arxiv.org/pdf/2410.20345v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2405.12221v2","updated":"2024-10-27T21:47:14Z","published":"2024-05-20T17:59:59Z","title":"Images that Sound: Composing Images and Sounds on a Single Canvas","summary":"  Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these visual spectrograms images that sound.\nOur approach is simple and zero-shot, and it leverages pre-trained\ntext-to-image and text-to-spectrogram diffusion models that operate in a shared\nlatent space. During the reverse process, we denoise noisy latents with both\nthe audio and image diffusion models in parallel, resulting in a sample that is\nlikely under both models. Through quantitative evaluations and perceptual\nstudies, we find that our method successfully generates spectrograms that align\nwith a desired audio prompt while also taking the visual appearance of a\ndesired image prompt. Please see our project page for video results:\nhttps://ificl.github.io/images-that-sound/\n","authors":["Ziyang Chen","Daniel Geng","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2405.12221v2.pdf","comment":"Accepted to NeurIPS 2024. Project site:\n  https://ificl.github.io/images-that-sound/"},{"id":"http://arxiv.org/abs/2410.20518v1","updated":"2024-10-27T17:00:55Z","published":"2024-10-27T17:00:55Z","title":"MidiTok Visualizer: a tool for visualization and analysis of tokenized\n  MIDI symbolic music","summary":"  Symbolic music research plays a crucial role in music-related machine\nlearning, but MIDI data can be complex for those without musical expertise. To\naddress this issue, we present MidiTok Visualizer, a web application designed\nto facilitate the exploration and visualization of various MIDI tokenization\nmethods from the MidiTok Python package. MidiTok Visualizer offers numerous\ncustomizable parameters, enabling users to upload MIDI files to visualize\ntokenized data alongside an interactive piano roll.\n","authors":["Micha≈Ç Wiszenko","Kacper Stefa≈Ñski","Piotr Malesa","≈Åukasz Pokorzy≈Ñski","Mateusz Modrzejewski"],"pdf_url":"https://arxiv.org/pdf/2410.20518v1.pdf","comment":"in Extended Abstracts for the Late-Breaking Demo Sessionof the 25th\n  Int. Society for Music Information Retrieval Conf., San Francisco, United\n  States, 2024"},{"id":"http://arxiv.org/abs/2403.08773v2","updated":"2024-10-27T06:01:49Z","published":"2024-01-18T12:45:25Z","title":"Veagle: Advancements in Multimodal Representation Learning","summary":"  Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.\n","authors":["Rajat Chawla","Arkajit Datta","Tushar Verma","Adarsh Jha","Anmol Gautam","Ayush Vatsal","Sukrit Chaterjee","Mukunda NS","Ishaan Bhola"],"pdf_url":"https://arxiv.org/pdf/2403.08773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00320v3","updated":"2024-10-27T03:52:29Z","published":"2024-06-01T06:40:22Z","title":"Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow\n  Matching","summary":"  Video-to-audio (V2A) generation aims to synthesize content-matching audio\nfrom silent video, and it remains challenging to build V2A models with high\ngeneration quality, efficiency, and visual-audio temporal synchrony. We propose\nFrieren, a V2A model based on rectified flow matching. Frieren regresses the\nconditional transport vector field from noise to spectrogram latent with\nstraight paths and conducts sampling by solving ODE, outperforming\nautoregressive and score-based models in terms of audio quality. By employing a\nnon-autoregressive vector field estimator based on a feed-forward transformer\nand channel-level cross-modal feature fusion with strong temporal alignment,\nour model generates audio that is highly synchronized with the input video.\nFurthermore, through reflow and one-step distillation with guided vector field,\nour model can generate decent audio in a few, or even only one sampling step.\nExperiments indicate that Frieren achieves state-of-the-art performance in both\ngeneration quality and temporal alignment on VGGSound, with alignment accuracy\nreaching 97.22%, and 6.2% improvement in inception score over the strong\ndiffusion-based baseline. Audio samples are available at\nhttp://frieren-v2a.github.io.\n","authors":["Yongqi Wang","Wenxiang Guo","Rongjie Huang","Jiawei Huang","Zehan Wang","Fuming You","Ruiqi Li","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.00320v3.pdf","comment":"accepted by NeurIPS 2024"}]},"2024-10-26T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2405.14633v2","updated":"2024-10-26T16:36:05Z","published":"2024-05-23T14:39:52Z","title":"Flatten Anything: Unsupervised Neural Surface Parameterization","summary":"  Surface parameterization plays an essential role in numerous computer\ngraphics and geometry processing applications. Traditional parameterization\napproaches are designed for high-quality meshes laboriously created by\nspecialized 3D modelers, thus unable to meet the processing demand for the\ncurrent explosion of ordinary 3D data. Moreover, their working mechanisms are\ntypically restricted to certain simple topologies, thus relying on cumbersome\nmanual efforts (e.g., surface cutting, part segmentation) for pre-processing.\nIn this paper, we introduce the Flatten Anything Model (FAM), an unsupervised\nneural architecture to achieve global free-boundary surface parameterization\nvia learning point-wise mappings between 3D points on the target geometric\nsurface and adaptively-deformed UV coordinates within the 2D parameter domain.\nTo mimic the actual physical procedures, we ingeniously construct\ngeometrically-interpretable sub-networks with specific functionalities of\nsurface cutting, UV deforming, unwrapping, and wrapping, which are assembled\ninto a bi-directional cycle mapping framework. Compared with previous methods,\nour FAM directly operates on discrete surface points without utilizing\nconnectivity information, thus significantly reducing the strict requirements\nfor mesh quality and even applicable to unstructured point cloud data. More\nimportantly, our FAM is fully-automated without the need for pre-cutting and\ncan deal with highly-complex topologies, since its learning process adaptively\nfinds reasonable cutting seams and UV boundaries. Extensive experiments\ndemonstrate the universality, superiority, and inspiring potential of our\nproposed neural surface parameterization paradigm. Our code is available at\nhttps://github.com/keeganhk/FlattenAnything.\n","authors":["Qijian Zhang","Junhui Hou","Wenping Wang","Ying He"],"pdf_url":"https://arxiv.org/pdf/2405.14633v2.pdf","comment":"NeurIPS 2024"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2405.13879v2","updated":"2024-10-26T19:22:07Z","published":"2024-05-22T17:59:44Z","title":"FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free\n  Riding?","summary":"  Standard federated learning (FL) approaches are vulnerable to the free-rider\ndilemma: participating agents can contribute little to nothing yet receive a\nwell-trained aggregated model. While prior mechanisms attempt to solve the\nfree-rider dilemma, none have addressed the issue of truthfulness. In practice,\nadversarial agents can provide false information to the server in order to\ncheat its way out of contributing to federated training. In an effort to make\nfree-riding-averse federated mechanisms truthful, and consequently less prone\nto breaking down in practice, we propose FACT. FACT is the first federated\nmechanism that: (1) eliminates federated free riding by using a penalty system,\n(2) ensures agents provide truthful information by creating a competitive\nenvironment, and (3) encourages agent participation by offering better\nperformance than training alone. Empirically, FACT avoids free-riding when\nagents are untruthful, and reduces agent loss by over 4x.\n","authors":["Marco Bornstein","Amrit Singh Bedi","Abdirisak Mohamed","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2405.13879v2.pdf","comment":"NeurIPS 2024, 19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.17002v2","updated":"2024-10-26T18:53:48Z","published":"2024-10-22T13:23:13Z","title":"EFX Allocations and Orientations on Bipartite Multi-graphs: A Complete\n  Picture","summary":"  We consider the fundamental problem of fairly allocating a set of indivisible\nitems among agents having valuations that are represented by a multi-graph --\nhere, agents appear as the vertices and items as the edges between them and\neach vertex (agent) only values the set of its incident edges (items). The goal\nis to find a fair, i.e., envy-free up to any item (EFX) allocation. This model\nhas recently been introduced by Christodoulou et al. (EC'23) where they show\nthat EFX allocations always exist on simple graphs for monotone valuations,\ni.e., where any two agents can share at most one edge (item). A natural\nquestion arises as to what happens when we go beyond simple graphs and study\nvarious classes of multi-graphs?\n  We answer the above question affirmatively for the valuation class of\nbipartite multi-graphs and multi-cycles. Our main positive result is that EFX\nallocations on bipartite multi-graphs (and multi-cycles) always exist and can\nbe computed in polynomial time for additive valuations. We, therefore, push the\nfrontiers of our understanding of EFX allocations and expand the scenarios\nwhere they are known to exist for an arbitrary number of agents. Next, we study\nEFX orientations (i.e., allocations where every item is allocated to one of its\ntwo endpoint agents) and give a complete picture of when they exist for\nbipartite multi-graphs dependent on two parameters -- the number of edges\nshared between any two agents and the diameter of the graph. Finally, we prove\nthat it is NP-complete to determine whether a given fair division instance on a\nbipartite multi-graph admits an EFX orientation.\n","authors":["Mahyar Afshinmehr","Alireza Danaei","Mehrafarin Kazemi","Kurt Mehlhorn","Nidhi Rathi"],"pdf_url":"https://arxiv.org/pdf/2410.17002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20180v1","updated":"2024-10-26T13:29:43Z","published":"2024-10-26T13:29:43Z","title":"Copyright-Aware Incentive Scheme for Generative Art Models Using\n  Hierarchical Reinforcement Learning","summary":"  Generative art using Diffusion models has achieved remarkable performance in\nimage generation and text-to-image tasks. However, the increasing demand for\ntraining data in generative art raises significant concerns about copyright\ninfringement, as models can produce images highly similar to copyrighted works.\nExisting solutions attempt to mitigate this by perturbing Diffusion models to\nreduce the likelihood of generating such images, but this often compromises\nmodel performance. Another approach focuses on economically compensating data\nholders for their contributions, yet it fails to address copyright loss\nadequately. Our approach begin with the introduction of a novel copyright\nmetric grounded in copyright law and court precedents on infringement. We then\nemploy the TRAK method to estimate the contribution of data holders. To\naccommodate the continuous data collection process, we divide the training into\nmultiple rounds. Finally, We designed a hierarchical budget allocation method\nbased on reinforcement learning to determine the budget for each round and the\nremuneration of the data holder based on the data holder's contribution and\ncopyright loss in each round. Extensive experiments across three datasets show\nthat our method outperforms all eight benchmarks, demonstrating its\neffectiveness in optimizing budget distribution in a copyright-aware manner. To\nthe best of our knowledge, this is the first technical work that introduces to\nincentive contributors and protect their copyrights by compensating them.\n","authors":["Zhuan Shi","Yifei Song","Xiaoli Tang","Lingjuan Lyu","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2410.20180v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.20143v1","updated":"2024-10-26T10:51:22Z","published":"2024-10-26T10:51:22Z","title":"Exploring Welfare Maximization and Fairness in Participatory Budgeting","summary":"  Participatory budgeting (PB) is a voting paradigm for distributing a\ndivisible resource, usually called a budget, among a set of projects by\naggregating the preferences of individuals over these projects. It is\nimplemented quite extensively for purposes such as government allocating funds\nto public projects and funding agencies selecting research proposals to\nsupport. This PhD dissertation studies the welfare-related and fairness-related\nobjectives for different PB models. Our contribution lies in proposing and\nexploring novel PB rules that maximize welfare and promote fairness, as well\nas, in introducing and investigating a range of novel utility notions,\naxiomatic properties, and fairness notions, effectively filling the gaps in the\nexisting literature for each PB model. The thesis is divided into two main\nparts, the first focusing on dichotomous and the second focusing on ordinal\npreferences. Each part considers two cases: (i) the cost of each project is\nrestricted to a single value and partial funding is not permitted and (ii) the\ncost of each project is flexible and may assume multiple values.\n","authors":["Gogulapati Sreedurga"],"pdf_url":"https://arxiv.org/pdf/2410.20143v1.pdf","comment":"PhD Thesis"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2409.10574v2","updated":"2024-10-26T09:38:20Z","published":"2024-09-15T13:16:58Z","title":"Detection Made Easy: Potentials of Large Language Models for Solidity\n  Vulnerabilities","summary":"  The large-scale deployment of Solidity smart contracts on the Ethereum\nmainnet has increasingly attracted financially-motivated attackers in recent\nyears. A few now-infamous attacks in Ethereum's history includes DAO attack in\n2016 (50 million dollars lost), Parity Wallet hack in 2017 (146 million dollars\nlocked), Beautychain's token BEC in 2018 (900 million dollars market value fell\nto 0), and NFT gaming blockchain breach in 2022 ($600 million in Ether stolen).\nThis paper presents a comprehensive investigation of the use of large language\nmodels (LLMs) and their capabilities in detecting OWASP Top Ten vulnerabilities\nin Solidity. We introduce a novel, class-balanced, structured, and labeled\ndataset named VulSmart, which we use to benchmark and compare the performance\nof open-source LLMs such as CodeLlama, Llama2, CodeT5 and Falcon, alongside\nclosed-source models like GPT-3.5 Turbo and GPT-4o Mini. Our proposed SmartVD\nframework is rigorously tested against these models through extensive automated\nand manual evaluations, utilizing BLEU and ROUGE metrics to assess the\neffectiveness of vulnerability detection in smart contracts. We also explore\nthree distinct prompting strategies-zero-shot, few-shot, and\nchain-of-thought-to evaluate the multi-class classification and generative\ncapabilities of the SmartVD framework. Our findings reveal that SmartVD\noutperforms its open-source counterparts and even exceeds the performance of\nclosed-source base models like GPT-3.5 and GPT-4 Mini. After fine-tuning, the\nclosed-source models, GPT-3.5 Turbo and GPT-4o Mini, achieved remarkable\nperformance with 99% accuracy in detecting vulnerabilities, 94% in identifying\ntheir types, and 98% in determining severity. Notably, SmartVD performs best\nwith the `chain-of-thought' prompting technique, whereas the fine-tuned\nclosed-source models excel with the `zero-shot' prompting approach.\n","authors":["Md Tauseef Alam","Raju Halder","Abyayananda Maiti"],"pdf_url":"https://arxiv.org/pdf/2409.10574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21316v1","updated":"2024-10-26T00:43:59Z","published":"2024-10-26T00:43:59Z","title":"Deep Optimizer States: Towards Scalable Training of Transformer Models\n  Using Interleaved Offloading","summary":"  Transformers and large language models~(LLMs) have seen rapid adoption in all\ndomains. Their sizes have exploded to hundreds of billions of parameters and\nkeep increasing. Under these circumstances, the training of transformers is\nvery expensive and often hits a ``memory wall'', i.e., even when using 3D\nparallelism (pipeline, tensor, data) and aggregating the memory of many GPUs,\nit is still not enough to hold the necessary data structures (model parameters,\noptimizer state, gradients, activations) in GPU memory. To compensate,\nstate-of-the-art approaches offload the optimizer state, at least partially, to\nthe host memory and perform hybrid CPU-GPU computations. However, the\nmanagement of the combined host-GPU memory is often suboptimal and results in\npoor overlapping between data movements and computations. This leads to missed\nopportunities to simultaneously leverage the interconnect bandwidth and\ncomputational capabilities of CPUs and GPUs. In this paper, we leverage a key\nobservation that the interleaving of the forward, backward and update phases\ngenerate fluctuations in the GPU memory utilization, which can be exploited to\ndynamically move a part of the optimizer state between the host and the GPU\nmemory at each iteration. To this end, we design and implement \\proj, a novel\ntechnique to split the LLM into subgroups, whose update phase is scheduled on\neither the CPU or the GPU based on our proposed performance model that\naddresses the trade-off between data movement cost, acceleration on the GPUs vs\nthe CPUs, and competition for shared resources. We integrate our approach with\nDeepSpeed and demonstrate 2.5$\\times$ faster iterations over state-of-the-art\napproaches using extensive experiments.\n","authors":["Avinash Maurya","Jie Ye","M. Mustafa Rafique","Franck Cappello","Bogdan Nicolae"],"pdf_url":"https://arxiv.org/pdf/2410.21316v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.20030v1","updated":"2024-10-26T00:52:46Z","published":"2024-10-26T00:52:46Z","title":"SCube: Instant Large-Scale Scene Reconstruction using VoxSplats","summary":"  We present SCube, a novel method for reconstructing large-scale 3D scenes\n(geometry, appearance, and semantics) from a sparse set of posed images. Our\nmethod encodes reconstructed scenes using a novel representation VoxSplat,\nwhich is a set of 3D Gaussians supported on a high-resolution sparse-voxel\nscaffold. To reconstruct a VoxSplat from images, we employ a hierarchical voxel\nlatent diffusion model conditioned on the input images followed by a\nfeedforward appearance prediction model. The diffusion model generates\nhigh-resolution grids progressively in a coarse-to-fine manner, and the\nappearance network predicts a set of Gaussians within each voxel. From as few\nas 3 non-overlapping input images, SCube can generate millions of Gaussians\nwith a 1024^3 voxel grid spanning hundreds of meters in 20 seconds. Past works\ntackling scene reconstruction from images either rely on per-scene optimization\nand fail to reconstruct the scene away from input views (thus requiring dense\nview coverage as input) or leverage geometric priors based on low-resolution\nmodels, which produce blurry results. In contrast, SCube leverages\nhigh-resolution sparse networks and produces sharp outputs from few views. We\nshow the superiority of SCube compared to prior art using the Waymo\nself-driving dataset on 3D reconstruction and demonstrate its applications,\nsuch as LiDAR simulation and text-to-scene generation.\n","authors":["Xuanchi Ren","Yifan Lu","Hanxue Liang","Zhangjie Wu","Huan Ling","Mike Chen","Sanja Fidler","Francis Williams","Jiahui Huang"],"pdf_url":"https://arxiv.org/pdf/2410.20030v1.pdf","comment":"NeurIPS 2024. Project page:\n  https://research.nvidia.com/labs/toronto-ai/scube/"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.20266v1","updated":"2024-10-26T20:35:14Z","published":"2024-10-26T20:35:14Z","title":"Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in\n  Expert Knowledge Tasks","summary":"  The potential of using Large Language Models (LLMs) themselves to evaluate\nLLM outputs offers a promising method for assessing model performance across\nvarious contexts. Previous research indicates that LLM-as-a-judge exhibits a\nstrong correlation with human judges in the context of general instruction\nfollowing. However, for instructions that require specialized knowledge, the\nvalidity of using LLMs as judges remains uncertain. In our study, we applied a\nmixed-methods approach, conducting pairwise comparisons in which both subject\nmatter experts (SMEs) and LLMs evaluated outputs from domain-specific tasks. We\nfocused on two distinct fields: dietetics, with registered dietitian experts,\nand mental health, with clinical psychologist experts. Our results showed that\nSMEs agreed with LLM judges 68% of the time in the dietetics domain and 64% in\nmental health when evaluating overall preference. Additionally, the results\nindicated variations in SME-LLM agreement across domain-specific aspect\nquestions. Our findings emphasize the importance of keeping human experts in\nthe evaluation process, as LLMs alone may not provide the depth of\nunderstanding required for complex, knowledge specific tasks. We also explore\nthe implications of LLM evaluations across different domains and discuss how\nthese insights can inform the design of evaluation workflows that ensure better\nalignment between human experts and LLMs in interactive systems.\n","authors":["Annalisa Szymanski","Noah Ziems","Heather A. Eicher-Miller","Toby Jia-Jun Li","Meng Jiang","Ronald A. Metoyer"],"pdf_url":"https://arxiv.org/pdf/2410.20266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20256v1","updated":"2024-10-26T19:24:46Z","published":"2024-10-26T19:24:46Z","title":"That was not what I was aiming at! Differentiating human intent and\n  outcome in a physically dynamic throwing task","summary":"  Recognising intent in collaborative human robot tasks can improve team\nperformance and human perception of robots. Intent can differ from the observed\noutcome in the presence of mistakes which are likely in physically dynamic\ntasks. We created a dataset of 1227 throws of a ball at a target from 10\nparticipants and observed that 47% of throws were mistakes with 16% completely\nmissing the target. Our research leverages facial images capturing the person's\nreaction to the outcome of a throw to predict when the resulting throw is a\nmistake and then we determine the actual intent of the throw. The approach we\npropose for outcome prediction performs 38% better than the two-stream\narchitecture used previously for this task on front-on videos. In addition, we\npropose a 1-D CNN model which is used in conjunction with priors learned from\nthe frequency of mistakes to provide an end-to-end pipeline for outcome and\nintent recognition in this throwing task.\n","authors":["Vidullan Surendran","Alan R. Wagner"],"pdf_url":"https://arxiv.org/pdf/2410.20256v1.pdf","comment":"Accepted October 2022 in Autonomous Robots. Published December 2022"},{"id":"http://arxiv.org/abs/2410.00727v3","updated":"2024-10-26T17:04:35Z","published":"2024-10-01T14:16:10Z","title":"\"Show Me What's Wrong!\": Combining Charts and Text to Guide Data\n  Analysis","summary":"  Analyzing and finding anomalies in multi-dimensional datasets is a cumbersome\nbut vital task across different domains. In the context of financial fraud\ndetection, analysts must quickly identify suspicious activity among\ntransactional data. This is an iterative process made of complex exploratory\ntasks such as recognizing patterns, grouping, and comparing. To mitigate the\ninformation overload inherent to these steps, we present a tool combining\nautomated information highlights, Large Language Model generated textual\ninsights, and visual analytics, facilitating exploration at different levels of\ndetail. We perform a segmentation of the data per analysis area and visually\nrepresent each one, making use of automated visual cues to signal which require\nmore attention. Upon user selection of an area, our system provides textual and\ngraphical summaries. The text, acting as a link between the high-level and\ndetailed views of the chosen segment, allows for a quick understanding of\nrelevant details. A thorough exploration of the data comprising the selection\ncan be done through graphical representations. The feedback gathered in a study\nperformed with seven domain experts suggests our tool effectively supports and\nguides exploratory analysis, easing the identification of suspicious\ninformation.\n","authors":["Beatriz Feliciano","Rita Costa","Jean Alves","Javier Li√©bana","Diogo Duarte","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2410.00727v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20130v1","updated":"2024-10-26T09:18:17Z","published":"2024-10-26T09:18:17Z","title":"\"My Replika Cheated on Me and She Liked It\": A Taxonomy of Algorithmic\n  Harms in Human-AI Relationships","summary":"  As conversational AI systems increasingly permeate the socio-emotional realms\nof human life, they bring both benefits and risks to individuals and society.\nDespite extensive research on detecting and categorizing harms in AI systems,\nless is known about the harms that arise from social interactions with AI\nchatbots. Through a mixed-methods analysis of 35,390 conversation excerpts\nshared on r/replika, an online community for users of the AI companion Replika,\nwe identified six categories of harmful behaviors exhibited by the chatbot:\nrelational transgression, verbal abuse and hate, self-inflicted harm,\nharassment and violence, mis/disinformation, and privacy violations. The AI\ncontributes to these harms through four distinct roles: perpetrator,\ninstigator, facilitator, and enabler. Our findings highlight the relational\nharms of AI chatbots and the danger of algorithmic compliance, enhancing the\nunderstanding of AI harms in socio-emotional interactions. We also provide\nsuggestions for designing ethical and responsible AI systems that prioritize\nuser safety and well-being.\n","authors":["Renwen Zhang","Han Li","Han Meng","Jinyuan Zhan","Hongyuan Gan","Yi-Chieh Lee"],"pdf_url":"https://arxiv.org/pdf/2410.20130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12315v2","updated":"2024-10-26T09:04:25Z","published":"2024-07-17T04:49:56Z","title":"ModalChorus: Visual Probing and Alignment of Multi-modal Embeddings via\n  Modal Fusion Map","summary":"  Multi-modal embeddings form the foundation for vision-language models, such\nas CLIP embeddings, the most widely used text-image embeddings. However, these\nembeddings are vulnerable to subtle misalignment of cross-modal features,\nresulting in decreased model performance and diminished generalization. To\naddress this problem, we design ModalChorus, an interactive system for visual\nprobing and alignment of multi-modal embeddings. ModalChorus primarily offers a\ntwo-stage process: 1) embedding probing with Modal Fusion Map (MFM), a novel\nparametric dimensionality reduction method that integrates both metric and\nnonmetric objectives to enhance modality fusion; and 2) embedding alignment\nthat allows users to interactively articulate intentions for both point-set and\nset-set alignments. Quantitative and qualitative comparisons for CLIP\nembeddings with existing dimensionality reduction (e.g., t-SNE and MDS) and\ndata fusion (e.g., data context map) methods demonstrate the advantages of MFM\nin showcasing cross-modal features over common vision-language datasets. Case\nstudies reveal that ModalChorus can facilitate intuitive discovery of\nmisalignment and efficient re-alignment in scenarios ranging from zero-shot\nclassification to cross-modal retrieval and generation.\n","authors":["Yilin Ye","Shishi Xiao","Xingchen Zeng","Wei Zeng"],"pdf_url":"https://arxiv.org/pdf/2407.12315v2.pdf","comment":"VIS 2024"},{"id":"http://arxiv.org/abs/2309.08609v4","updated":"2024-10-26T08:57:11Z","published":"2023-08-25T03:54:20Z","title":"Media of Langue: The Interface for Exploring Word Translation\n  Network/Space","summary":"  In the human activity of word translation, two languages face each other,\nmutually searching their own language system for the semantic place of words in\nthe other language. We discover the huge network formed by the chain of these\nmutual translations as Word Translation Network, a network where words are\nnodes, and translation volume is represented as edges, and propose Media of\nLangue, a novel interface for exploring this network. Media of Langue points to\nthe semantic configurations of many words in multiple languages at once,\ncontaining the information of existing dictionaries such as bilingual and\nsynonym dictionaries. We have also implemented and published this interface as\na web application, focusing on seven language pairs. This paper first defines\nthe Word Translation Network and describes how to actually construct the\nnetwork from bilingual corpora, followed by an analysis of the properties of\nthe network. Next, we explain how to design a Media of Langue using the Word\nTranslation Network, and finally, we analyze the features of the Media of\nLangue as a dictionary. Our website is https://www.media-of-langue.org .\n","authors":["Goki Muramoto","Atsuki Sato","Takayoshi Koyama"],"pdf_url":"https://arxiv.org/pdf/2309.08609v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01801v2","updated":"2024-10-26T08:42:04Z","published":"2023-12-04T10:46:52Z","title":"SPROUT: an Interactive Authoring Tool for Generating Programming\n  Tutorials with the Visualization of Large Language Models","summary":"  The rapid development of large language models (LLMs), such as ChatGPT, has\nrevolutionized the efficiency of creating programming tutorials. LLMs can be\ninstructed with text prompts to generate comprehensive text descriptions of\ncode snippets. However, the lack of transparency in the end-to-end generation\nprocess has hindered the understanding of model behavior and limited user\ncontrol over the generated results. To tackle this challenge, we introduce a\nnovel approach that breaks down the programming tutorial creation task into\nactionable steps. By employing the tree-of-thought method, LLMs engage in an\nexploratory process to generate diverse and faithful programming tutorials. We\nthen present SPROUT, an authoring tool equipped with a series of interactive\nvisualizations that empower users to have greater control and understanding of\nthe programming tutorial creation process. A formal user study demonstrated the\neffectiveness of SPROUT, showing that our tool assists users to actively\nparticipate in the programming tutorial creation process, leading to more\nreliable and customizable results. By providing users with greater control and\nunderstanding, SPROUT enhances the user experience and improves the overall\nquality of programming tutorial. A free copy of this paper and all supplemental\nmaterials are available at\nhttps://osf.io/uez2t/?view_only=5102e958802341daa414707646428f86.\n","authors":["Yihan Liu","Zhen Wen","Luoxuan Weng","Ollie Woodman","Yi Yang","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2312.01801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20124v1","updated":"2024-10-26T08:40:02Z","published":"2024-10-26T08:40:02Z","title":"Breaking the Midas Spell:Understanding Progressive Novice-AI\n  Collaboration in Spatial Design","summary":"  In spatial design, Artificial Intelligence (AI) tools often generate the\nentire spatial design outcome in a single automated step, rather than engaging\nusers in a deepening and iterative process. This significantly reduces users'\ninvolvement, learning, and creative capabilities, leading to a superficial\nunderstanding of spatial design. We conducted a Wizard-of-Oz study, where\nNovices and AI (acted by experimenters) worked together to finish spatial\ndesign tasks using various AI models. We identified typical function and\nworkflow patterns adopted by the participants, leading to the understanding of\nthe opportunities and challenges in the human-AI co-creation process. Based on\ninsights gathered from this research, we proposed some design implications of\nthe novice-AI collaboration system that aims to democratize spatial design\nthrough a progressive, iterative co-creation process.\n","authors":["Zijun Wan","Jiawei Tang","Linghang Cai","Xin Tong","Can Liu"],"pdf_url":"https://arxiv.org/pdf/2410.20124v1.pdf","comment":"draft submission to CHI 2025"},{"id":"http://arxiv.org/abs/2410.20116v1","updated":"2024-10-26T08:08:12Z","published":"2024-10-26T08:08:12Z","title":"Estuary: A Framework For Building Multimodal Low-Latency Real-Time\n  Socially Interactive Agents","summary":"  The rise in capability and ubiquity of generative artificial intelligence\n(AI) technologies has enabled its application to the field of Socially\nInteractive Agents (SIAs). Despite rising interest in modern AI-powered\ncomponents used for real-time SIA research, substantial friction remains due to\nthe absence of a standardized and universal SIA framework. To target this\nabsence, we developed Estuary: a multimodal (text, audio, and soon video)\nframework which facilitates the development of low-latency, real-time SIAs.\nEstuary seeks to reduce repeat work between studies and to provide a flexible\nplatform that can be run entirely off-cloud to maximize configurability,\ncontrollability, reproducibility of studies, and speed of agent response times.\nWe are able to do this by constructing a robust multimodal framework which\nincorporates current and future components seamlessly into a modular and\ninteroperable architecture.\n","authors":["Spencer Lin","Basem Rizk","Miru Jun","Andy Artze","Caitlin Sullivan","Sharon Mozgai","Scott Fisher"],"pdf_url":"https://arxiv.org/pdf/2410.20116v1.pdf","comment":"To be published in ACM Intelligent Virtual Agents (IVA) 2024 [DOI:\n  10.1145/3652988.3696198] [ACM ISBN: 979-8-4007-0625-7/24/09]"},{"id":"http://arxiv.org/abs/2409.10811v3","updated":"2024-10-26T05:38:02Z","published":"2024-09-17T00:58:00Z","title":"Grounded GUI Understanding for Vision Based Spatial Intelligent Agent:\n  Exemplified by Virtual Reality Apps","summary":"  In recent years, spatial computing Virtual Reality (VR) has emerged as a\ntransformative technology, offering users immersive and interactive experiences\nacross diversified virtual environments. Users can interact with VR apps\nthrough interactable GUI elements (IGEs) on the stereoscopic three-dimensional\n(3D) graphical user interface (GUI). The accurate recognition of these IGEs is\ninstrumental, serving as the foundation of many software engineering tasks,\nincluding automated testing and effective GUI search. The most recent IGE\ndetection approaches for 2D mobile apps typically train a supervised object\ndetection model based on a large-scale manually-labeled GUI dataset, usually\nwith a pre-defined set of clickable GUI element categories like buttons and\nspinners. Such approaches can hardly be applied to IGE detection in VR apps,\ndue to a multitude of challenges including complexities posed by\nopen-vocabulary and heterogeneous IGE categories, intricacies of\ncontext-sensitive interactability, and the necessities of precise spatial\nperception and visual-semantic alignment for accurate IGE detection results.\nThus, it is necessary to embark on the IGE research tailored to VR apps. In\nthis paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI\nElemeNT dEtection framework for virtual Reality apps, named Orienter. By\nimitating human behaviors, Orienter observes and understands the semantic\ncontexts of VR app scenes first, before performing the detection. The detection\nprocess is iterated within a feedback-directed validation and reflection loop.\nSpecifically, Orienter contains three components, including (1) Semantic\ncontext comprehension, (2) Reflection-directed IGE candidate detection, and (3)\nContext-sensitive interactability classification. Extensive experiments\ndemonstrate that Orienter is more effective than the state-of-the-art GUI\nelement detection approaches.\n","authors":["Shuqing Li","Binchang Li","Yepang Liu","Cuiyun Gao","Jianping Zhang","Shing-Chi Cheung","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2409.10811v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20081v1","updated":"2024-10-26T05:18:48Z","published":"2024-10-26T05:18:48Z","title":"emg2qwerty: A Large Dataset with Baselines for Touch Typing using\n  Surface Electromyography","summary":"  Surface electromyography (sEMG) non-invasively measures signals generated by\nmuscle activity with sufficient sensitivity to detect individual spinal neurons\nand richness to identify dozens of gestures and their nuances. Wearable\nwrist-based sEMG sensors have the potential to offer low friction, subtle,\ninformation rich, always available human-computer inputs. To this end, we\nintroduce emg2qwerty, a large-scale dataset of non-invasive electromyographic\nsignals recorded at the wrists while touch typing on a QWERTY keyboard,\ntogether with ground-truth annotations and reproducible baselines. With 1,135\nsessions spanning 108 users and 346 hours of recording, this is the largest\nsuch public dataset to date. These data demonstrate non-trivial, but well\ndefined hierarchical relationships both in terms of the generative process,\nfrom neurons to muscles and muscle combinations, as well as in terms of domain\nshift across users and user sessions. Applying standard modeling techniques\nfrom the closely related field of Automatic Speech Recognition (ASR), we show\nstrong baseline performance on predicting key-presses using sEMG signals alone.\nWe believe the richness of this task and dataset will facilitate progress in\nseveral problems of interest to both the machine learning and neuroscientific\ncommunities. Dataset and code can be accessed at\nhttps://github.com/facebookresearch/emg2qwerty.\n","authors":["Viswanath Sivakumar","Jeffrey Seely","Alan Du","Sean R Bittner","Adam Berenzweig","Anuoluwapo Bolarinwa","Alexandre Gramfort","Michael I Mandel"],"pdf_url":"https://arxiv.org/pdf/2410.20081v1.pdf","comment":"Submitted to NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.20067v1","updated":"2024-10-26T04:09:03Z","published":"2024-10-26T04:09:03Z","title":"Evaluating the Influences of Explanation Style on Human-AI Reliance","summary":"  Explainable AI (XAI) aims to support appropriate human-AI reliance by\nincreasing the interpretability of complex model decisions. Despite the\nproliferation of proposed methods, there is mixed evidence surrounding the\neffects of different styles of XAI explanations on human-AI reliance.\nInterpreting these conflicting findings requires an understanding of the\nindividual and combined qualities of different explanation styles that\ninfluence appropriate and inappropriate human-AI reliance, and the role of\ninterpretability in this interaction. In this study, we investigate the\ninfluences of feature-based, example-based, and combined feature- and\nexample-based XAI methods on human-AI reliance through a two-part experimental\nstudy with 274 participants comparing these explanation style conditions. Our\nfindings suggest differences between feature-based and example-based\nexplanation styles beyond interpretability that affect human-AI reliance\npatterns across differences in individual performance and task complexity. Our\nwork highlights the importance of adapting explanations to their specific users\nand context over maximising broad interpretability.\n","authors":["Emma Casolin","Flora D. Salim","Ben Newell"],"pdf_url":"https://arxiv.org/pdf/2410.20067v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2410.20034v1","updated":"2024-10-26T01:03:13Z","published":"2024-10-26T01:03:13Z","title":"Sensor2Text: Enabling Natural Language Interactions for Daily Activity\n  Tracking Using Wearable Sensors","summary":"  Visual Question-Answering, a technology that generates textual responses from\nan image and natural language question, has progressed significantly. Notably,\nit can aid in tracking and inquiring about daily activities, crucial in\nhealthcare monitoring, especially for elderly patients or those with memory\ndisabilities. However, video poses privacy concerns and has a limited field of\nview. This paper presents Sensor2Text, a model proficient in tracking daily\nactivities and engaging in conversations using wearable sensors. The approach\noutlined here tackles several challenges, including low information density in\nwearable sensor data, insufficiency of single wearable sensors in human\nactivities recognition, and model's limited capacity for Question-Answering and\ninteractive conversations. To resolve these obstacles, transfer learning and\nstudent-teacher networks are utilized to leverage knowledge from\nvisual-language models. Additionally, an encoder-decoder neural network model\nis devised to jointly process language and sensor data for conversational\npurposes. Furthermore, Large Language Models are also utilized to enable\ninteractive capabilities. The model showcases the ability to identify human\nactivities and engage in Q\\&A dialogues using various wearable sensor\nmodalities. It performs comparably to or better than existing visual-language\nmodels in both captioning and conversational tasks. To our knowledge, this\nrepresents the first model capable of conversing about wearable sensor data,\noffering an innovative approach to daily activity tracking that addresses\nprivacy and field-of-view limitations associated with current vision-based\nsolutions.\n","authors":["Wenqiang Chen","Jiaxuan Cheng","Leyao Wang","Wei Zhao","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2410.20034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20017v1","updated":"2024-10-26T00:17:33Z","published":"2024-10-26T00:17:33Z","title":"Off-Policy Selection for Initiating Human-Centric Experimental Design","summary":"  In human-centric tasks such as healthcare and education, the heterogeneity\namong patients and students necessitates personalized treatments and\ninstructional interventions. While reinforcement learning (RL) has been\nutilized in those tasks, off-policy selection (OPS) is pivotal to close the\nloop by offline evaluating and selecting policies without online interactions,\nyet current OPS methods often overlook the heterogeneity among participants.\nOur work is centered on resolving a pivotal challenge in human-centric systems\n(HCSs): how to select a policy to deploy when a new participant joining the\ncohort, without having access to any prior offline data collected over the\nparticipant? We introduce First-Glance Off-Policy Selection (FPS), a novel\napproach that systematically addresses participant heterogeneity through\nsub-group segmentation and tailored OPS criteria to each sub-group. By grouping\nindividuals with similar traits, FPS facilitates personalized policy selection\naligned with unique characteristics of each participant or group of\nparticipants. FPS is evaluated via two important but challenging applications,\nintelligent tutoring systems and a healthcare application for sepsis treatment\nand intervention. FPS presents significant advancement in enhancing learning\noutcomes of students and in-hospital care outcomes.\n","authors":["Ge Gao","Xi Yang","Qitong Gao","Song Ju","Miroslav Pajic","Min Chi"],"pdf_url":"https://arxiv.org/pdf/2410.20017v1.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2408.15449v2","updated":"2024-10-26T19:14:33Z","published":"2024-08-27T23:58:51Z","title":"Graph Attention Inference of Network Topology in Multi-Agent Systems","summary":"  Accurately identifying the underlying graph structures of multi-agent systems\nremains a difficult challenge. Our work introduces a novel machine\nlearning-based solution that leverages the attention mechanism to predict\nfuture states of multi-agent systems by learning node representations. The\ngraph structure is then inferred from the strength of the attention values.\nThis approach is applied to both linear consensus dynamics and the non-linear\ndynamics of Kuramoto oscillators, resulting in implicit learning of the graph\nby learning good agent representations. Our results demonstrate that the\npresented data-driven graph attention machine learning model can identify the\nnetwork topology in multi-agent systems, even when the underlying dynamic model\nis not known, as evidenced by the F1 scores achieved in the link prediction.\n","authors":["Akshay Kolli","Reza Azadeh","Kshitj Jerath"],"pdf_url":"https://arxiv.org/pdf/2408.15449v2.pdf","comment":"Accepted for publication at Modeling and Estimation Control\n  Conference 2024; 6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.20143v1","updated":"2024-10-26T10:51:22Z","published":"2024-10-26T10:51:22Z","title":"Exploring Welfare Maximization and Fairness in Participatory Budgeting","summary":"  Participatory budgeting (PB) is a voting paradigm for distributing a\ndivisible resource, usually called a budget, among a set of projects by\naggregating the preferences of individuals over these projects. It is\nimplemented quite extensively for purposes such as government allocating funds\nto public projects and funding agencies selecting research proposals to\nsupport. This PhD dissertation studies the welfare-related and fairness-related\nobjectives for different PB models. Our contribution lies in proposing and\nexploring novel PB rules that maximize welfare and promote fairness, as well\nas, in introducing and investigating a range of novel utility notions,\naxiomatic properties, and fairness notions, effectively filling the gaps in the\nexisting literature for each PB model. The thesis is divided into two main\nparts, the first focusing on dichotomous and the second focusing on ordinal\npreferences. Each part considers two cases: (i) the cost of each project is\nrestricted to a single value and partial funding is not permitted and (ii) the\ncost of each project is flexible and may assume multiple values.\n","authors":["Gogulapati Sreedurga"],"pdf_url":"https://arxiv.org/pdf/2410.20143v1.pdf","comment":"PhD Thesis"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.20145v1","updated":"2024-10-26T11:05:16Z","published":"2024-10-26T11:05:16Z","title":"Cross-Platform Neural Video Coding: A Case Study","summary":"  In this paper, we first show that current learning-based video codecs,\nspecifically the SSF codec, are not suitable for real-world applications due to\nthe mismatch between the encoder and decoder caused by floating-point round-off\nerrors. To address this issue, we propose the static quantization of the hyper\nprior decoding path. The quantization parameters are determined through an\nexhaustive search of all possible combinations of observers and quantization\nschemes from PyTorch. For the SSF codec, when encoding and decoding on\ndifferent machines, the proposed solution effectively mitigates the mismatch\nissue and enhances compression efficiency results by preventing severe image\nquality degradation. When encoding and decoding are performed on the same\nmachine, it constrains the average BD-rate increase to 9.93% and 9.02% for UVG\nand HEVC-B sequences, respectively.\n","authors":["Ruhan Concei√ß√£o","Marcelo Porto","Wen-Hsiao Peng","Luciano Agostini"],"pdf_url":"https://arxiv.org/pdf/2410.20145v1.pdf","comment":"4 pages, submitted to ISCAS2025"},{"id":"http://arxiv.org/abs/2410.20109v1","updated":"2024-10-26T07:37:43Z","published":"2024-10-26T07:37:43Z","title":"GiVE: Guiding Visual Encoder to Perceive Overlooked Information","summary":"  Multimodal Large Language Models have advanced AI in applications like\ntext-to-video generation and visual question answering. These models rely on\nvisual encoders to convert non-text data into vectors, but current encoders\neither lack semantic alignment or overlook non-salient objects. We propose the\nGuiding Visual Encoder to Perceive Overlooked Information (GiVE) approach. GiVE\nenhances visual representation with an Attention-Guided Adapter (AG-Adapter)\nmodule and an Object-focused Visual Semantic Learning module. These incorporate\nthree novel loss terms: Object-focused Image-Text Contrast (OITC) loss,\nObject-focused Image-Image Contrast (OIIC) loss, and Object-focused Image\nDiscrimination (OID) loss, improving object consideration, retrieval accuracy,\nand comprehensiveness. Our contributions include dynamic visual focus\nadjustment, novel loss functions to enhance object retrieval, and the\nMulti-Object Instruction (MOInst) dataset. Experiments show our approach\nachieves state-of-the-art performance.\n","authors":["Junjie Li","Jianghong Ma","Xiaofeng Zhang","Yuhang Li","Jianyang Shi"],"pdf_url":"https://arxiv.org/pdf/2410.20109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10811v3","updated":"2024-10-26T05:38:02Z","published":"2024-09-17T00:58:00Z","title":"Grounded GUI Understanding for Vision Based Spatial Intelligent Agent:\n  Exemplified by Virtual Reality Apps","summary":"  In recent years, spatial computing Virtual Reality (VR) has emerged as a\ntransformative technology, offering users immersive and interactive experiences\nacross diversified virtual environments. Users can interact with VR apps\nthrough interactable GUI elements (IGEs) on the stereoscopic three-dimensional\n(3D) graphical user interface (GUI). The accurate recognition of these IGEs is\ninstrumental, serving as the foundation of many software engineering tasks,\nincluding automated testing and effective GUI search. The most recent IGE\ndetection approaches for 2D mobile apps typically train a supervised object\ndetection model based on a large-scale manually-labeled GUI dataset, usually\nwith a pre-defined set of clickable GUI element categories like buttons and\nspinners. Such approaches can hardly be applied to IGE detection in VR apps,\ndue to a multitude of challenges including complexities posed by\nopen-vocabulary and heterogeneous IGE categories, intricacies of\ncontext-sensitive interactability, and the necessities of precise spatial\nperception and visual-semantic alignment for accurate IGE detection results.\nThus, it is necessary to embark on the IGE research tailored to VR apps. In\nthis paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI\nElemeNT dEtection framework for virtual Reality apps, named Orienter. By\nimitating human behaviors, Orienter observes and understands the semantic\ncontexts of VR app scenes first, before performing the detection. The detection\nprocess is iterated within a feedback-directed validation and reflection loop.\nSpecifically, Orienter contains three components, including (1) Semantic\ncontext comprehension, (2) Reflection-directed IGE candidate detection, and (3)\nContext-sensitive interactability classification. Extensive experiments\ndemonstrate that Orienter is more effective than the state-of-the-art GUI\nelement detection approaches.\n","authors":["Shuqing Li","Binchang Li","Yepang Liu","Cuiyun Gao","Jianping Zhang","Shing-Chi Cheung","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2409.10811v3.pdf","comment":null}]},"2024-10-25T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2408.16946v2","updated":"2024-10-25T18:45:56Z","published":"2024-08-29T23:57:28Z","title":"Best of two worlds: Cartesian sampling and volume computation for\n  distance-constrained configuration spaces using Cayley coordinates","summary":"  Volume calculation of configurational spaces acts as a vital part in\nconfigurational entropy calculation, which contributes towards calculating free\nenergy landscape for molecular systems. In this article, we present our\nsampling-based volume computation method using distance-based Cayley\ncoordinate, mitigating drawbacks: our method guarantees that the sampling\nprocedure stays in lower-dimensional coordinate space (instead of\nhigher-dimensional Cartesian space) throughout the whole process; and our\nmapping function, utilizing Cayley parameterization, can be applied in both\ndirections with low computational cost. Our method uniformly samples and\ncomputes a discrete volume measure of a Cartesian configuration space of point\nsets satisfying systems of distance inequality constraints. The systems belong\nto a large natural class whose feasible configuration spaces are effectively\nlower dimensional subsets of high dimensional ambient space. Their topological\ncomplexity makes discrete volume computation challenging, yet necessary in\nseveral application scenarios including free energy calculation in soft matter\nassembly modeling. The algorithm runs in linear time and empirically sub-linear\nspace in the number of grid hypercubes (used to define the discrete volume\nmeasure) \\textit{that intersect} the configuration space. In other words, the\nnumber of wasted grid cube visits is insignificant compared to prevailing\nmethods typically based on gradient descent. Specifically, the traversal stays\nwithin the feasible configuration space by viewing it as a branched covering,\nusing a recent theory of Cayley or distance coordinates to convexify the base\nspace, and by employing a space-efficient, frontier hypercube traversal data\nstructure. A software implementation and comparison with existing methods is\nprovided.\n","authors":["Yichi Zhang","Meera Sitharam"],"pdf_url":"https://arxiv.org/pdf/2408.16946v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16384v2","updated":"2024-10-25T18:14:44Z","published":"2023-09-28T12:31:35Z","title":"Multi-Swap $k$-Means++","summary":"  The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often\nthe practitioners' choice algorithm for optimizing the popular $k$-means\nclustering objective and is known to give an $O(\\log k)$-approximation in\nexpectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML\n2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local search\nsteps obtained through the $k$-means++ sampling distribution to yield a\n$c$-approximation to the $k$-means clustering problem, where $c$ is a large\nabsolute constant. Here we generalize and extend their local search algorithm\nby considering larger and more sophisticated local search neighborhoods hence\nallowing to swap multiple centers at the same time. Our algorithm achieves a $9\n+ \\varepsilon$ approximation ratio, which is the best possible for local\nsearch. Importantly we show that our approach yields substantial practical\nimprovements, we show significant quality improvements over the approach of\nLattanzi and Sohler (ICML 2019) on several datasets.\n","authors":["Lorenzo Beretta","Vincent Cohen-Addad","Silvio Lattanzi","Nikos Parotsidis"],"pdf_url":"https://arxiv.org/pdf/2309.16384v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2410.08752v2","updated":"2024-10-25T14:45:03Z","published":"2024-10-11T12:12:31Z","title":"T≈ôiVis: Versatile, Reliable, and High-Performance Tool for Computing\n  Visibility in Polygonal Environments","summary":"  Visibility is a fundamental concept in computational geometry, with numerous\napplications in surveillance, robotics, and games. This software paper presents\nT\\v{r}iVis, a C++ library developed by the authors for computing numerous\nvisibility-related queries in highly complex polygonal environments. Adapting\nthe triangular expansion algorithm, T\\v{r}iVis stands out as a versatile,\nhigh-performance, more reliable and easy-to-use alternative to current\nsolutions that is also free of heavy dependencies. Through evaluation on a\nchallenging dataset, T\\v{r}iVis has been benchmarked against existing\nvisibility libraries. The results demonstrate that T\\v{r}iVis outperforms the\ncompeting solutions by at least an order of magnitude in query times, while\nexhibiting more reliable runtime behavior. T\\v{r}iVis is freely available for\nprivate, research, and institutional use at\nhttps://github.com/janmikulacz/trivis.\n","authors":["Jan Mikula","Miroslav Kulich","Libor P≈ôeuƒçil"],"pdf_url":"https://arxiv.org/pdf/2410.08752v2.pdf","comment":"8 pages, 12 figures (including subfigures); accepted to appear at the\n  2024 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS); associated repository: https://github.com/janmikulacz/trivis"},{"id":"http://arxiv.org/abs/2410.19410v1","updated":"2024-10-25T09:12:56Z","published":"2024-10-25T09:12:56Z","title":"Embedded Graph Reconstruction under Hausdorff Noise","summary":"  Filamentary structures (topologically embedded graphs with a metric\nstructure) are ubiquitous in science and engineering. A challenging problem in\ntopological data analysis (TDA) is to reconstruct the topology and geometry of\nsuch an underlying (usually unknown) metric graph from possibly noisy data\nsampled around it. Reeb graphs have recently been successfully employed in\nabstract metric graph reconstruction under Gromov$\\unicode{x2013}$Hausdorff\nnoise: the sample is assumed to be metrically close to the ground truth.\nHowever, such a strong global density assumption is hardly achieved in\napplications, making the existing Reeb graph-based methods untractible. We\nrelax the density assumption to give provable geometric reconstruction schemes,\neven when the sample is metrically close only locally. A very different yet\nmore relevant paradigm focuses on the reconstruction of metric\ngraphs$\\unicode{x2014}$embedded in the Euclidean space$\\unicode{x2014}$from\nEuclidean samples that are only Hausdorff-close. We further extend our\nmethodologies to provide novel, provable guarantees for the successful\ngeometric reconstruction of Euclidean graphs under the Hausdorff noise model.\nOur technique produces promising results in reconstructing earthquake plate\ntectonic boundaries from the global earthquake catalog.\n","authors":["Halley Fritze","Sushovan Majhi","Marissa Masden","Atish Mitra","Michael Stickney"],"pdf_url":"https://arxiv.org/pdf/2410.19410v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.19665v1","updated":"2024-10-25T16:20:46Z","published":"2024-10-25T16:20:46Z","title":"MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular\n  Metaverse Services","summary":"  Updates of extensive Internet of Things (IoT) data are critical to the\nimmersion of vehicular metaverse services. However, providing high-quality and\nsustainable data in unstable and resource-constrained vehicular networks\nremains a significant challenge. To address this problem, we put forth a novel\nimmersion-aware model trading framework that incentivizes metaverse users (MUs)\nto contribute learning models trained by their latest local data for augmented\nreality (AR) services in the vehicular metaverse, while preserving their\nprivacy through federated learning. To comprehensively evaluate the\ncontribution of locally trained learning models provided by MUs to AR services,\nwe design a new immersion metric that captures service immersion by considering\nthe freshness and accuracy of learning models, as well as the amount and\npotential value of raw data used for training. We model the trading\ninteractions between metaverse service providers (MSPs) and MUs as an\nequilibrium problem with equilibrium constraints (EPEC) to analyze and balance\ntheir costs and gains. Moreover, considering dynamic network conditions and\nprivacy concerns, we formulate the reward decisions of MSPs as a multi-agent\nMarkov decision process. Then, a fully distributed dynamic reward method based\non deep reinforcement learning is presented, which operates without any private\ninformation about MUs and other MSPs. Experimental results demonstrate that the\nproposed framework can effectively provide higher-value models for object\ndetection and classification in AR services on real AR-related vehicle datasets\ncompared to benchmark schemes.\n","authors":["Hongjia Wu","Hui Zeng","Zehui Xiong","Jiawen Kang","Zhiping Cai","Tse-Tin Chan","Dusit Niyato","Zhu Han"],"pdf_url":"https://arxiv.org/pdf/2410.19665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18066v2","updated":"2024-10-25T15:48:34Z","published":"2024-10-23T17:42:54Z","title":"The Double-Edged Sword of Behavioral Responses in Strategic\n  Classification: Theory and User Studies","summary":"  When humans are subject to an algorithmic decision system, they can\nstrategically adjust their behavior accordingly (``game'' the system). While a\ngrowing line of literature on strategic classification has used game-theoretic\nmodeling to understand and mitigate such gaming, these existing works consider\nstandard models of fully rational agents. In this paper, we propose a strategic\nclassification model that considers behavioral biases in human responses to\nalgorithms. We show how misperceptions of a classifier (specifically, of its\nfeature weights) can lead to different types of discrepancies between biased\nand rational agents' responses, and identify when behavioral agents over- or\nunder-invest in different features. We also show that strategic agents with\nbehavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared\nto fully rational strategic agents. We complement our analytical results with\nuser studies, which support our hypothesis of behavioral biases in human\nresponses to the algorithm. Together, our findings highlight the need to\naccount for human (cognitive) biases when designing AI systems, and providing\nexplanations of them, to strategic human in the loop.\n","authors":["Raman Ebrahimi","Kristen Vaccaro","Parinaz Naghizadeh"],"pdf_url":"https://arxiv.org/pdf/2410.18066v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.19993v1","updated":"2024-10-25T22:30:52Z","published":"2024-10-25T22:30:52Z","title":"XbarSim: A Decomposition-Based Memristive Crossbar Simulator","summary":"  Given the growing focus on memristive crossbar-based in-memory computing\n(IMC) architectures as a potential alternative to current energy-hungry machine\nlearning hardware, the availability of a fast and accurate circuit-level\nsimulation framework could greatly enhance research and development efforts in\nthis field. This paper introduces XbarSim, a domain-specific circuit-level\nsimulator designed to analyze the nodal equations of memristive crossbars. The\nfirst version of XbarSim, proposed herein, leverages the lower-upper (LU)\ndecomposition approach to solve the nodal equations for the matrices associated\nwith crossbars. The XbarSim is capable of simulating interconnect parasitics\nwithin crossbars and supports batch processing of the inputs. Through\ncomprehensive experiments, we demonstrate that the XbarSim can achieve orders\nof magnitude speedup compared to HSPICE across various sizes of memristive\ncrossbars. The XbarSim's full suite of features is accessible to researchers as\nan open-source tool.\n","authors":["Anzhelika Kolinko","Md Hasibul Amin","Ramtin Zand","Jason Bakos"],"pdf_url":"https://arxiv.org/pdf/2410.19993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19708v1","updated":"2024-10-25T17:30:06Z","published":"2024-10-25T17:30:06Z","title":"Integrating LoRaWAN with Mobile Ad-hoc Networks for Enhanced Campus\n  Communication","summary":"  The integration of Long Range Wide Area Network (LoRaWAN) with Mobile Ad-hoc\nNetworks (MANETs) presents a promising solution for enhancing communication\nnetworks within campus environments. This paper explores the unique advantages\nof combining these two technologies, including scalability, energy efficiency,\nflexibility, and support for diverse applications. LoRaWAN low power\nconsumption and extended range capabilities address the challenges of\ntraditional communication methods, enabling reliable data transmission across\nvarious campus scenarios, such as emergency alerts, event coordination, and\nreal-time monitoring. We also identify key challenges faced in this integrated\narchitecture, including signal interference, data packet collisions, and energy\nmanagement. By providing a comprehensive survey of existing techniques and\nsolutions categorized by the network protocol stack layers, this study aims to\ninform future research and development efforts in creating robust, energy\nefficient communication systems tailored for modern educational institutions.\nUltimately, the findings highlight the potential of LoRaWAN MANET architectures\nto transform campus communication into a more reliable, adaptable, and cost\neffective framework.\n","authors":["Ramakant Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.19708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19593v1","updated":"2024-10-25T14:40:53Z","published":"2024-10-25T14:40:53Z","title":"Energy Efficient Dual Designs of FeFET-Based Analog In-Memory Computing\n  with Inherent Shift-Add Capability","summary":"  In-memory computing (IMC) architecture emerges as a promising paradigm,\nimproving the energy efficiency of multiply-and-accumulate (MAC) operations\nwithin DNNs by integrating the parallel computations within the memory arrays.\nVarious high-precision analog IMC array designs have been developed based on\nboth SRAM and emerging non-volatile memories. These designs perform MAC\noperations of partial input and weight, with the corresponding partial products\nthen fed into shift-add circuitry to produce the final MAC results. However,\nexisting works often present intricate shift-add process for weight. The\ntraditional digital shift-add process is limited in throughput due to\ntime-multiplexing of ADCs, and advancing the shift-add process to the analog\ndomain necessitates customized circuit implementations, resulting in\ncompromises in energy and area efficiency. Furthermore, the joint optimization\nof the partial MAC operations and the weight shift-add process is rarely\nexplored. In this paper, we propose novel, energy efficient dual designs of\nFeFET based high precision analog IMC featuring inherent shift-add capability.\nWe introduce a FeFET based IMC paradigm that performs partial MAC in each\ncolumn, and inherently integrates the shift-add process for 4-bit weights by\nleveraging FeFET's analog storage characteristics. This paradigm supports both\n2's complement mode and non-2's complement mode MAC, thereby offering flexible\nsupport for 4-/8-bit weight data in 2's complement format. Building upon this\nparadigm, we propose novel FeFET based dual designs, CurFe for the current mode\nand ChgFe for the charge mode, to accommodate the high precision analog domain\nIMC architecture.Evaluation results at circuit and system levels indicate that\nthe circuit/system-level energy efficiency of the proposed FeFET-based analog\nIMC is 1.56$\\times$/1.37$\\times$ higher when compared to SOTA analog IMC\ndesigns.\n","authors":["Zeyu Yang","Qingrong Huang","Yu Qian","Kai Ni","Thomas K√§mpfe","Xunzhao Yin"],"pdf_url":"https://arxiv.org/pdf/2410.19593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19497v1","updated":"2024-10-25T11:55:29Z","published":"2024-10-25T11:55:29Z","title":"Available Degrees of Spatial Multiplexing of a Uniform Linear Array with\n  Multiple Polarizations: a Holographic Perspective","summary":"  The capabilities of multi-antenna technology have recently been significantly\nenhanced by the proliferation of extra large array architectures. The high\ndimensionality of these systems implies that communications take place in the\nnearfield regime, which poses some questions as to their effective perfomrance\neven under simple line of sight configurations. In order to study these\nlimitations, a uniform linear array (ULA) is considered here, the elements of\nwhich are three infinitesimal dipoles transmitting different signals in the\nthree spatial dimensions. The receiver consists of a single element with three\northogonal infinitesimal dipoles and full channel state information is assumed\nto be available at both ends. A capacity analysis is presented when the number\nof elements of the ULA increases without bound while the interelement distance\nconverges to zero, so that the total aperture length is kept asymptotically\nfixed. In particular, the total number of available spatial eigenmodes is shown\nto depend crucially on the receiver position in space, and closed form\nexpressions are provided for the different achievability regions. From the\nanalysis it can be concluded that the use of three orthogonal polarizations at\nthe transmitter guarantees the almost universal availability of two spatial\nstreams, whereas the use of only two polarizations results in a more extensive\nregion where maximum multiplexing gain is available.\n","authors":["Xavier Mestre","Adrian Agustin","David Sarda"],"pdf_url":"https://arxiv.org/pdf/2410.19497v1.pdf","comment":"Submitted to IEEE Open Journal on Signal Processing"},{"id":"http://arxiv.org/abs/2410.19411v1","updated":"2024-10-25T09:13:24Z","published":"2024-10-25T09:13:24Z","title":"A potpourri of results on molecular communication with active transport","summary":"  Molecular communication (MC) is a model of information transmission where the\nsignal is transmitted by information-carrying molecules through their physical\ntransport from a transmitter to a receiver through a communication channel.\nPrior efforts have identified suitable \"information molecules\" whose efficacy\nfor signal transmission has been studied extensively in diffusive channels\n(DC). Although easy to implement, DCs are inefficient for distances longer than\ntens of nanometers. In contrast, molecular motor-driven nonequilibrium or\nactive transport can drastically increase the range of communication and may\npermit efficient communication up to tens of micrometers. In this paper, we\ninvestigate how active transport influences the efficacy of molecular\ncommunication, quantified by the mutual information between transmitted and\nreceived signals. We consider two specific scenarios: (a) active transport\nthrough relays and (b) active transport through a mixture of active and\ndiffusing particles. In each case, we discuss the efficacy of the communication\nchannel and discuss their potential pitfalls.\n","authors":["Phanindra Dewan","Sumantra Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.19411v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.19356v1","updated":"2024-10-25T07:24:49Z","published":"2024-10-25T07:24:49Z","title":"FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with\n  Ferroelectric In-Memory Computing","summary":"  In scenarios with limited training data or where explainability is crucial,\nconventional neural network-based machine learning models often face\nchallenges. In contrast, Bayesian inference-based algorithms excel in providing\ninterpretable predictions and reliable uncertainty estimation in these\nscenarios. While many state-of-the-art in-memory computing (IMC) architectures\nleverage emerging non-volatile memory (NVM) technologies to offer unparalleled\ncomputing capacity and energy efficiency for neural network workloads, their\napplication in Bayesian inference is limited. This is because the core\noperations in Bayesian inference differ significantly from the\nmultiplication-accumulation (MAC) operations common in neural networks,\nrendering them generally unsuitable for direct implementation in most existing\nIMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian\ninference engine powered by multi-bit ferroelectric field-effect transistor\n(FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a\nBayesian inference model within a compact FeFET-based crossbar. It maps\nquantized logarithmic probabilities to discrete FeFET states. As a result, the\naccumulated outputs of the crossbar naturally represent the posterior\nprobabilities, i.e., the Bayesian inference model's output given a set of\nobservations. This approach enables efficient in-memory Bayesian inference\nwithout the need for additional calculation circuitry. As the first FeFET-based\nin-memory Bayesian inference engine, FeBiM achieves an impressive storage\ndensity of 26.32 Mb/mm$^{2}$ and a computing efficiency of 581.40 TOPS/W in a\nrepresentative Bayesian classification task. These results demonstrate\n10.7$\\times$/43.4$\\times$ improvement in compactness/efficiency compared to the\nstate-of-the-art hardware implementation of Bayesian inference.\n","authors":["Chao Li","Zhicheng Xu","Bo Wen","Ruibin Mao","Can Li","Thomas K√§mpfe","Kai Ni","Xunzhao Yin"],"pdf_url":"https://arxiv.org/pdf/2410.19356v1.pdf","comment":"6 pages, 8 figures, to be published in the 61st DAC (Design\n  Automation Conference) proceedings"}],"Graphics":[{"id":"http://arxiv.org/abs/2407.00021v2","updated":"2024-10-25T18:13:23Z","published":"2024-05-06T19:44:13Z","title":"Neural Graphics Texture Compression Supporting Random Access","summary":"  Advances in rendering have led to tremendous growth in texture assets,\nincluding resolution, complexity, and novel textures components, but this\ngrowth in data volume has not been matched by advances in its compression.\nMeanwhile Neural Image Compression (NIC) has advanced significantly and shown\npromising results, but the proposed methods cannot be directly adapted to\nneural texture compression. First, texture compression requires on-demand and\nreal-time decoding with random access during parallel rendering (e.g. block\ntexture decompression on GPUs). Additionally, NIC does not support\nmulti-resolution reconstruction (mip-levels), nor does it have the ability to\nefficiently jointly compress different sets of texture channels. In this work,\nwe introduce a novel approach to texture set compression that integrates\ntraditional GPU texture representation and NIC techniques, designed to enable\nrandom access and support many-channel texture sets. To achieve this goal, we\npropose an asymmetric auto-encoder framework that employs a convolutional\nencoder to capture detailed information in a bottleneck-latent space, and at\ndecoder side we utilize a fully connected network, whose inputs are sampled\nlatent features plus positional information, for a given texture coordinate and\nmip level. This latent data is defined to enable simplified access to\nmulti-resolution data by simply changing the scanning strides. Experimental\nresults demonstrate that this approach provides much better results than\nconventional texture compression, and significant improvement over the latest\nmethod using neural networks.\n","authors":["Farzad Farhadzadeh","Qiqi Hou","Hoang Le","Amir Said","Randall Rauwendaal","Alex Bourd","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2407.00021v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2402.18116v2","updated":"2024-10-25T17:35:05Z","published":"2024-02-28T07:09:31Z","title":"Block and Detail: Scaffolding Sketch-to-Image Generation","summary":"  We introduce a novel sketch-to-image tool that aligns with the iterative\nrefinement process of artists. Our tool lets users sketch blocking strokes to\ncoarsely represent the placement and form of objects and detail strokes to\nrefine their shape and silhouettes. We develop a two-pass algorithm for\ngenerating high-fidelity images from such sketches at any point in the\niterative process. In the first pass we use a ControlNet to generate an image\nthat strictly follows all the strokes (blocking and detail) and in the second\npass we add variation by renoising regions surrounding blocking strokes. We\nalso present a dataset generation scheme that, when used to train a ControlNet\narchitecture, allows regions that do not contain strokes to be interpreted as\nnot-yet-specified regions rather than empty space. We show that this\npartial-sketch-aware ControlNet can generate coherent elements from partial\nsketches that only contain a small number of strokes. The high-fidelity images\nproduced by our approach serve as scaffolds that can help the user adjust the\nshape and proportions of objects or add additional elements to the composition.\nWe demonstrate the effectiveness of our approach with a variety of examples and\nevaluative comparisons. Quantitatively, evaluative user feedback indicates that\nnovice viewers prefer the quality of images from our algorithm over a baseline\nScribble ControlNet for 84% of the pairs and found our images had less\ndistortion in 81% of the pairs.\n","authors":["Vishnu Sarukkai","Lu Yuan","Mia Tang","Maneesh Agrawala","Kayvon Fatahalian"],"pdf_url":"https://arxiv.org/pdf/2402.18116v2.pdf","comment":"UIST 2024"},{"id":"http://arxiv.org/abs/2410.21310v1","updated":"2024-10-25T11:07:19Z","published":"2024-10-25T11:07:19Z","title":"ArCSEM: Artistic Colorization of SEM Images via Gaussian Splatting","summary":"  Scanning Electron Microscopes (SEMs) are widely renowned for their ability to\nanalyze the surface structures of microscopic objects, offering the capability\nto capture highly detailed, yet only grayscale, images. To create more\nexpressive and realistic illustrations, these images are typically manually\ncolorized by an artist with the support of image editing software. This task\nbecomes highly laborious when multiple images of a scanned object require\ncolorization. We propose facilitating this process by using the underlying 3D\nstructure of the microscopic scene to propagate the color information to all\nthe captured images, from as little as one colorized view. We explore several\nscene representation techniques and achieve high-quality colorized novel view\nsynthesis of a SEM scene. In contrast to prior work, there is no manual\nintervention or labelling involved in obtaining the 3D representation. This\nenables an artist to color a single or few views of a sequence and\nautomatically retrieve a fully colored scene or video. Project page:\nhttps://ronly2460.github.io/ArCSEM\n","authors":["Takuma Nishimura","Andreea Dogaru","Martin Oeggerli","Bernhard Egger"],"pdf_url":"https://arxiv.org/pdf/2410.21310v1.pdf","comment":"presented and published at AI for Visual Arts Workshop and Challenges\n  (AI4VA) in conjunction with European Conference on Computer Vision (ECCV)\n  2024, Milano, Italy"},{"id":"http://arxiv.org/abs/2410.01540v2","updated":"2024-10-25T09:44:10Z","published":"2024-10-02T13:29:52Z","title":"Edge-preserving noise for diffusion models","summary":"  Classical generative diffusion models learn an isotropic Gaussian denoising\nprocess, treating all spatial regions uniformly, thus neglecting potentially\nvaluable structural information in the data. Inspired by the long-established\nwork on anisotropic diffusion in image processing, we present a novel\nedge-preserving diffusion model that is a generalization of denoising diffusion\nprobablistic models (DDPM). In particular, we introduce an edge-aware noise\nscheduler that varies between edge-preserving and isotropic Gaussian noise. We\nshow that our model's generative process converges faster to results that more\nclosely match the target distribution. We demonstrate its capability to better\nlearn the low-to-mid frequencies within the dataset, which plays a crucial role\nin representing shapes and structural information. Our edge-preserving\ndiffusion process consistently outperforms state-of-the-art baselines in\nunconditional image generation. It is also more robust for generative tasks\nguided by a shape-based prior, such as stroke-to-image generation. We present\nqualitative and quantitative results showing consistent improvements (FID\nscore) of up to 30% for both tasks. We provide source code and supplementary\ncontent via the public domain edge-preserving-diffusion.mpi-inf.mpg.de .\n","authors":["Jente Vandersanden","Sascha Holl","Xingchang Huang","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2410.01540v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19430v1","updated":"2024-10-25T09:39:55Z","published":"2024-10-25T09:39:55Z","title":"Progressive Glimmer: Expanding Dimensionality in Multidimensional\n  Scaling","summary":"  Progressive dimensionality reduction algorithms allow for visually\ninvestigating intermediate results, especially for large data sets. While\ndifferent algorithms exist that progressively increase the number of data\npoints, we propose an algorithm that allows for increasing the number of\ndimensions. Especially in spatio-temporal data, where each spatial location can\nbe seen as one data point and each time step as one dimension, the data is\noften stored in a format that supports quick access to the individual\ndimensions of all points. Therefore, we propose Progressive Glimmer, a\nprogressive multidimensional scaling (MDS) algorithm. We adapt the Glimmer\nalgorithm to support progressive updates for changes in the data's\ndimensionality. We evaluate Progressive Glimmer's embedding quality and\nruntime. We observe that the algorithm provides more stable results, leading to\nvisually consistent results for progressive rendering and making the approach\napplicable to streaming data. We show the applicability of our approach to\nspatio-temporal simulation ensemble data where we add the individual ensemble\nmembers progressively.\n","authors":["Marina Evers","David H√§gele","S√∂ren D√∂ring","Daniel Weiskopf"],"pdf_url":"https://arxiv.org/pdf/2410.19430v1.pdf","comment":"6 pages, 5 figures, presented at 2024 IEEE VIS Workshop on\n  Progressive Data Analysis and Visualization (PDAV)"},{"id":"http://arxiv.org/abs/2410.19347v1","updated":"2024-10-25T07:04:17Z","published":"2024-10-25T07:04:17Z","title":"Practical High-Contrast Holography","summary":"  Holographic displays are a promising technology for immersive visual\nexperiences, and their potential for compact form factor makes them a strong\ncandidate for head-mounted displays. However, at the short propagation\ndistances needed for a compact, head-mounted architecture, image contrast is\nlow when using a traditional phase-only spatial light modulator (SLM). Although\na complex SLM could restore contrast, these modulators require bulky lenses to\noptically co-locate the amplitude and phase components, making them poorly\nsuited for a compact head-mounted design. In this work, we introduce a novel\narchitecture to improve contrast: by adding a low resolution amplitude SLM a\nshort distance away from the phase modulator, we demonstrate peak\nsignal-to-noise ratio improvement up to 31 dB in simulation compared to\nphase-only, even when the amplitude modulator is 60$\\times$ lower resolution\nthan its phase counterpart. We analyze the relationship between diffraction\nangle and amplitude modulator pixel size, and validate the concept with a\nbenchtop experimental prototype. By showing that low resolution modulation is\nsufficient to improve contrast, we pave the way towards practical high-contrast\nholography in a compact form factor.\n","authors":["Leyla Kabuli","Oliver Cossairt","Florian Schiffers","Nathan Matsuda","Grace Kuo"],"pdf_url":"https://arxiv.org/pdf/2410.19347v1.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2409.07148v5","updated":"2024-10-25T06:50:55Z","published":"2024-09-11T09:51:21Z","title":"Jump Restore Light Transport","summary":"  Markov chain Monte Carlo (MCMC) algorithms come to rescue when sampling from\na complex, high-dimensional distribution by a conventional method is\nintractable. Even though MCMC is a powerful tool, it is also hard to control\nand tune in practice. Simultaneously achieving both local exploration of the\nstate space and global discovery of the target distribution is a challenging\ntask. In this work, we present a MCMC formulation that subsumes all existing\nMCMC samplers employed in rendering. We then present a novel framework for\nadjusting an arbitrary Markov chain, making it exhibit invariance with respect\nto a specified target distribution. To showcase the potential of the proposed\nframework, we focus on a first simple application in light transport\nsimulation. As a by-product, we introduce continuous-time MCMC sampling to the\ncomputer graphics community. We show how any existing MCMC-based light\ntransport algorithm can be embedded into our framework. We empirically and\ntheoretically prove that this embedding is superior to running the standalone\nalgorithm. In fact, our approach will convert any existing algorithm into a\nhighly parallelizable variant with shorter running time, smaller error and less\nvariance.\n","authors":["Sascha Holl","Hans-Peter Seidel","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2409.07148v5.pdf","comment":null}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.22092v1","updated":"2024-10-25T21:39:55Z","published":"2024-10-25T21:39:55Z","title":"Towards Data-Informed Interventions: Opportunities and Challenges of\n  Street-level Multimodal Sensing","summary":"  Over the past decades, improvements in data collection hardware coupled with\nnovel artificial intelligence algorithms have made it possible for researchers\nto understand urban environments at an unprecedented scale. From local\ninteractions between actors to city-wide infrastructural problems, this new\ndata-driven approach enables a more informed and trustworthy decision-making\nprocess aiming at transforming cities into safer and more equitable places for\nliving. This new moment unfolded new opportunities to understand various\nphenomena that directly impact how accessible cities are to heterogeneous\npopulations. Specifically, sensing localized physical interactions among actors\nunder different scenarios can drive substantial interventions in urban\nenvironments to make them safer for all. In this manuscript, we list\nopportunities and associated challenges to leverage street-level multimodal\nsensing data to empower domain experts in making more informed decisions and,\nultimately, supporting a data-informed policymaking framework. The challenges\npresented here can motivate research in different areas, such as computer\nvision and human-computer interaction, to support cities in growing more\nsustainably.\n","authors":["Joao Rulff","Giancarlo Pereira","Maryam Hosseini","Marcos Lage","Claudio Silva"],"pdf_url":"https://arxiv.org/pdf/2410.22092v1.pdf","comment":"ASSETS 2024 UrbanAccess Workshop"},{"id":"http://arxiv.org/abs/2410.18066v2","updated":"2024-10-25T15:48:34Z","published":"2024-10-23T17:42:54Z","title":"The Double-Edged Sword of Behavioral Responses in Strategic\n  Classification: Theory and User Studies","summary":"  When humans are subject to an algorithmic decision system, they can\nstrategically adjust their behavior accordingly (``game'' the system). While a\ngrowing line of literature on strategic classification has used game-theoretic\nmodeling to understand and mitigate such gaming, these existing works consider\nstandard models of fully rational agents. In this paper, we propose a strategic\nclassification model that considers behavioral biases in human responses to\nalgorithms. We show how misperceptions of a classifier (specifically, of its\nfeature weights) can lead to different types of discrepancies between biased\nand rational agents' responses, and identify when behavioral agents over- or\nunder-invest in different features. We also show that strategic agents with\nbehavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared\nto fully rational strategic agents. We complement our analytical results with\nuser studies, which support our hypothesis of behavioral biases in human\nresponses to the algorithm. Together, our findings highlight the need to\naccount for human (cognitive) biases when designing AI systems, and providing\nexplanations of them, to strategic human in the loop.\n","authors":["Raman Ebrahimi","Kristen Vaccaro","Parinaz Naghizadeh"],"pdf_url":"https://arxiv.org/pdf/2410.18066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19599v1","updated":"2024-10-25T14:46:07Z","published":"2024-10-25T14:46:07Z","title":"Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina","summary":"  Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates for humans in social science research. However, LLMs differ\nfundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Almost\nall advanced approaches fail to replicate human behavior distributions across\nmany models, except in one case involving fine-tuning using a substantial\namount of human behavior data. Causes of failure are diverse, relating to input\nlanguage, roles, and safeguarding. These results caution against using LLMs to\nstudy human behaviors or as human surrogates.\n","authors":["Yuan Gao","Dokyun Lee","Gordon Burtch","Sina Fazelpour"],"pdf_url":"https://arxiv.org/pdf/2410.19599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14139v3","updated":"2024-10-25T13:47:16Z","published":"2024-06-20T09:32:01Z","title":"Comparing the Effects of Visual, Haptic, and Visuohaptic Encoding on\n  Memory Retention of Digital Objects in Virtual Reality","summary":"  Although Virtual Reality (VR) has undoubtedly improved human interaction with\n3D data, users still face difficulties retaining important details of complex\ndigital objects in preparation for physical tasks. To address this issue, we\nevaluated the potential of visuohaptic integration to improve the memorability\nof virtual objects in immersive visualizations. In a user study (N=20),\nparticipants performed a delayed match-to-sample task where they memorized\nstimuli of visual, haptic, or visuohaptic encoding conditions. We assessed\nperformance differences between these encoding modalities through error rates\nand response times. We found that visuohaptic encoding significantly improved\nmemorization accuracy compared to unimodal visual and haptic conditions. Our\nanalysis indicates that integrating haptics into immersive visualizations\nenhances the memorability of digital objects. We discuss its implications for\nthe optimal encoding design in VR applications that assist professionals who\nneed to memorize and recall virtual objects in their daily work.\n","authors":["Lucas Siqueira Rodrigues","Timo Torsten Schmidt","John Nyakatura","Stefan Zachow","Johann Habakuk Israel","Thomas Kosch"],"pdf_url":"https://arxiv.org/pdf/2406.14139v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19558v1","updated":"2024-10-25T13:42:03Z","published":"2024-10-25T13:42:03Z","title":"SODA: a Soft Origami Dynamic utensil for Assisted feeding","summary":"  SODA aims to revolutionize assistive feeding systems by designing a\nmulti-purpose utensil using origami-inspired artificial muscles. Traditional\nutensils, such as forks and spoons,are hard and stiff, causing discomfort and\nfear among users, especially when operated by autonomous robotic arms.\nAdditionally, these systems require frequent utensil changes to handle\ndifferent food types. Our innovative utensil design addresses these issues by\noffering a versatile, adaptive solution that can seamlessly transition between\ngripping and scooping various foods without the need for manual intervention.\nUtilizing the flexibility and strength of origami-inspired artificial muscles,\nthe utensil ensures safe and comfortable interactions, enhancing user\nexperience and efficiency. This approach not only simplifies the feeding\nprocess but also promotes greater independence for individuals with limited\nmobility, contributing to the advancement of soft robotics in healthcare\napplications.\n","authors":["Yuxin Ray Song","Shufan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19558v1.pdf","comment":"2 Pages, 4 Figures, RO-MAN 2024 Robot Design Competition"},{"id":"http://arxiv.org/abs/2408.03641v2","updated":"2024-10-25T11:45:17Z","published":"2024-08-07T08:59:49Z","title":"2D Embeddings of Multi-dimensional Partitionings","summary":"  Partitionings (or segmentations) divide a given domain into disjoint\nconnected regions whose union forms again the entire domain. Multi-dimensional\npartitionings occur, for example, when analyzing parameter spaces of simulation\nmodels, where each segment of the partitioning represents a region of similar\nmodel behavior. Having computed a partitioning, one is commonly interested in\nunderstanding how large the segments are and which segments lie next to each\nother. While visual representations of 2D domain partitionings that reveal\nsizes and neighborhoods are straightforward, this is no longer the case when\nconsidering multi-dimensional domains of three or more dimensions. We propose\nan algorithm for computing 2D embeddings of multi-dimensional partitionings.\nThe embedding shall have the following properties: It shall maintain the\ntopology of the partitioning and optimize the area sizes and joint boundary\nlengths of the embedded segments to match the respective sizes and lengths in\nthe multi-dimensional domain. We demonstrate the effectiveness of our approach\nby applying it to different use cases, including the visual exploration of 3D\nspatial domain segmentations and multi-dimensional parameter space\npartitionings of simulation ensembles. We numerically evaluate our algorithm\nwith respect to how well sizes and lengths are preserved depending on the\ndimensionality of the domain and the number of segments.\n","authors":["Marina Evers","Lars Linsen"],"pdf_url":"https://arxiv.org/pdf/2408.03641v2.pdf","comment":"9 pages paper, 2 pages references, 4 pages appendix, 13 figures in\n  the paper, 5 figures in the appendix, IEEE VIS 2024 paper"},{"id":"http://arxiv.org/abs/2410.19455v1","updated":"2024-10-25T10:29:50Z","published":"2024-10-25T10:29:50Z","title":"Project Lx Conventos: Travelling through space and time in Lisbon's\n  religious buildings","summary":"  Project Lx Conventos aims to study, in a systematic and integrated manner,\nthe impact of the dissolution of religious orders in the dynamics of urban\ntransformation in nineteenth century Lisbon. After the liberal revolution and\nthe civil war, in the 19th century, the dissolution of religious orders led to\nthe alienation, in Lisbon, of nearly 130 religious buildings which were then\ngiven profane occupations (mainly public services) or demolished and divided in\nplots, originating new urban realities. Project Lx Conventos thus aims to show\nthat the extinction of the convents was decisive in the urban development of\nLisbon, in the eighteen hundreds. The project stands on a large set of\nmultimedia data which includes historic and contemporary cartography and\ngeo-referenced photos, videos and 3D models, provided by the projects partners,\nLisbon Municipality and the Portuguese National Archive, Torre do Tombo.\nSupported by these materials, the project's team is creating an online system\nthat will implement a spatial and temporal navigation of these resources\nintegrated in an interactive Map of Lisbon. Besides spatially locating and\nanalyzing the data available for each of the religious buildings considered in\nthe project, the tool integrates cutting edge interaction technology for: 1)\nEnabling a temporal voyage over the available traces of religious buildings; 2)\nAnalyzing the evolution of religious buildings and their surroundings, through\navailable data; 3) Using 3D representations of the buildings for accessing\nrelated data, through time. In this paper, the tools under development in the\ncontext of Lx Conventos are described, as well as the technologies supporting\nthem. The current status of the system is presented and future developments are\nproposed.\n","authors":["Joao Gouveia","Fernando Branco","Armanda Rodrigues","Nuno Correia"],"pdf_url":"https://arxiv.org/pdf/2410.19455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19430v1","updated":"2024-10-25T09:39:55Z","published":"2024-10-25T09:39:55Z","title":"Progressive Glimmer: Expanding Dimensionality in Multidimensional\n  Scaling","summary":"  Progressive dimensionality reduction algorithms allow for visually\ninvestigating intermediate results, especially for large data sets. While\ndifferent algorithms exist that progressively increase the number of data\npoints, we propose an algorithm that allows for increasing the number of\ndimensions. Especially in spatio-temporal data, where each spatial location can\nbe seen as one data point and each time step as one dimension, the data is\noften stored in a format that supports quick access to the individual\ndimensions of all points. Therefore, we propose Progressive Glimmer, a\nprogressive multidimensional scaling (MDS) algorithm. We adapt the Glimmer\nalgorithm to support progressive updates for changes in the data's\ndimensionality. We evaluate Progressive Glimmer's embedding quality and\nruntime. We observe that the algorithm provides more stable results, leading to\nvisually consistent results for progressive rendering and making the approach\napplicable to streaming data. We show the applicability of our approach to\nspatio-temporal simulation ensemble data where we add the individual ensemble\nmembers progressively.\n","authors":["Marina Evers","David H√§gele","S√∂ren D√∂ring","Daniel Weiskopf"],"pdf_url":"https://arxiv.org/pdf/2410.19430v1.pdf","comment":"6 pages, 5 figures, presented at 2024 IEEE VIS Workshop on\n  Progressive Data Analysis and Visualization (PDAV)"},{"id":"http://arxiv.org/abs/2410.14826v2","updated":"2024-10-25T05:43:16Z","published":"2024-10-18T18:51:44Z","title":"SPRIG: Improving Large Language Model Performance by System Prompt\n  Optimization","summary":"  Large Language Models (LLMs) have shown impressive capabilities in many\nscenarios, but their performance depends, in part, on the choice of prompt.\nPast research has focused on optimizing prompts specific to a task. However,\nmuch less attention has been given to optimizing the general instructions\nincluded in a prompt, known as a system prompt. To address this gap, we propose\nSPRIG, an edit-based genetic algorithm that iteratively constructs prompts from\nprespecified components to maximize the model's performance in general\nscenarios. We evaluate the performance of system prompts on a collection of 47\ndifferent types of tasks to ensure generalizability. Our study finds that a\nsingle optimized system prompt performs on par with task prompts optimized for\neach individual task. Moreover, combining system and task-level optimizations\nleads to further improvement, which showcases their complementary nature.\nExperiments also reveal that the optimized system prompts generalize\neffectively across model families, parameter sizes, and languages. This study\nprovides insights into the role of system-level instructions in maximizing LLM\npotential.\n","authors":["Lechen Zhang","Tolga Ergen","Lajanugen Logeswaran","Moontae Lee","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2410.14826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07113v2","updated":"2024-10-25T04:11:17Z","published":"2022-10-13T15:50:44Z","title":"Towards End-to-End Open Conversational Machine Reading","summary":"  In open-retrieval conversational machine reading (OR-CMR) task, machines are\nrequired to do multi-turn question answering given dialogue history and a\ntextual knowledge base. Existing works generally utilize two independent\nmodules to approach this problem's two successive sub-tasks: first with a\nhard-label decision making and second with a question generation aided by\nvarious entailment reasoning methods. Such usual cascaded modeling is\nvulnerable to error propagation and prevents the two sub-tasks from being\nconsistently optimized. In this work, we instead model OR-CMR as a unified\ntext-to-text task in a fully end-to-end style. Experiments on the ShARC and\nOR-ShARC dataset show the effectiveness of our proposed end-to-end framework on\nboth sub-tasks by a large margin, achieving new state-of-the-art results.\nFurther ablation studies support that our framework can generalize to different\nbackbone models.\n","authors":["Sizhe Zhou","Siru Ouyang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2210.07113v2.pdf","comment":"13 pages, 7 figures, 10 tables; Accepted to EACL 2023 Findings"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.19962v1","updated":"2024-10-25T20:46:02Z","published":"2024-10-25T20:46:02Z","title":"The Signaler-Responder Game: Learning to Communicate using Thompson\n  Sampling","summary":"  We are interested in studying how heterogeneous agents can learn to\ncommunicate and cooperate with each other without being explicitly\npre-programmed to do so. Motivated by this goal, we present and analyze a\ndistributed solution to a two-player signaler-responder game which is defined\nas follows. The signaler agent has a random, exogenous need and can choose from\nfour different strategies: never signal, always signal, signal when need, and\nsignal when no need. The responder agent can choose to either ignore or respond\nto the signal. We define a reward to both agents when they cooperate to satisfy\nthe signaler's need, and costs associated with communication, response and\nunmet needs. We identify pure Nash equilibria of the game and the conditions\nunder which they occur. As a solution for this game, we propose two new\ndistributed Bayesian learning algorithms, one for each agent, based on the\nclassic Thompson Sampling policy for multi-armed bandits. These algorithms\nallow both agents to update beliefs about both the exogenous need and the\nbehavior of the other agent and optimize their own expected reward. We show\nthat by using these policies, the agents are able to intelligently adapt their\nstrategies over multiple iterations to attain efficient, reward-maximizing\nequilibria under different settings, communicating and cooperating when it is\nrewarding to do so, and not communicating or cooperating when it is too\nexpensive.\n","authors":["Radhika Bhuckory","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2410.19962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19718v1","updated":"2024-10-25T17:43:00Z","published":"2024-10-25T17:43:00Z","title":"Evolving Neural Networks Reveal Emergent Collective Behavior from\n  Minimal Agent Interactions","summary":"  Understanding the mechanisms behind emergent behaviors in multi-agent systems\nis critical for advancing fields such as swarm robotics and artificial\nintelligence. In this study, we investigate how neural networks evolve to\ncontrol agents' behavior in a dynamic environment, focusing on the relationship\nbetween the network's complexity and collective behavior patterns. By\nperforming quantitative and qualitative analyses, we demonstrate that the\ndegree of network non-linearity correlates with the complexity of emergent\nbehaviors. Simpler behaviors, such as lane formation and laminar flow, are\ncharacterized by more linear network operations, while complex behaviors like\nswarming and flocking show highly non-linear neural processing. Moreover,\nspecific environmental parameters, such as moderate noise, broader field of\nview, and lower agent density, promote the evolution of non-linear networks\nthat drive richer, more intricate collective behaviors. These results highlight\nthe importance of tuning evolutionary conditions to induce desired behaviors in\nmulti-agent systems, offering new pathways for optimizing coordination in\nautonomous swarms. Our findings contribute to a deeper understanding of how\nneural mechanisms influence collective dynamics, with implications for the\ndesign of intelligent, self-organizing systems.\n","authors":["Guilherme S. Y. Giardini","John F. Hardy II","Carlo R. da Cunha"],"pdf_url":"https://arxiv.org/pdf/2410.19718v1.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.19685v1","updated":"2024-10-25T16:54:39Z","published":"2024-10-25T16:54:39Z","title":"The Sound of Silence in Social Networks","summary":"  We generalize the classic multi-agent DeGroot model for opinion dynamics to\nincorporate the Spiral of Silence theory from political science. This theory\nstates that individuals may withhold their opinions when they perceive them to\nbe in the minority. As in the DeGroot model, a community of agents is\nrepresented as a weighted directed graph whose edges indicate how much agents\ninfluence one another. However, agents whose current opinions are in the\nminority become silent (i.e., they do not express their opinion). Two models\nfor opinion update are then introduced. In the memoryless opinion model\n($\\mbox{SOM}^-$), agents update their opinion by taking the weighted average of\ntheir non-silent neighbors' opinions. In the memory based opinion model\n($\\mbox{SOM}^+$), agents update their opinions by taking the weighted average\nof the opinions of all their neighbors, but for silent neighbors, their most\nrecent opinion is considered.\n  We show that for $\\mbox{SOM}^-$ convergence to consensus is guaranteed for\nclique graphs but, unlike for the classic DeGroot, not guaranteed for\nstrongly-connected aperiodic graphs. In contrast, we show that for\n$\\mbox{SOM}^+$ convergence to consensus is not guaranteed even for clique\ngraphs. We showcase our models through simulations offering experimental\ninsights that align with key aspects of the Spiral of Silence theory. These\nfindings reveal the impact of silence dynamics on opinion formation and\nhighlight the limitations of consensus in more nuanced social models.\n","authors":["Jes√∫s Aranda","Juan Francisco D√≠az","David Gaona","Frank Valencia"],"pdf_url":"https://arxiv.org/pdf/2410.19685v1.pdf","comment":"20 pages and 5 figures"},{"id":"http://arxiv.org/abs/2410.19627v1","updated":"2024-10-25T15:25:36Z","published":"2024-10-25T15:25:36Z","title":"Knowledge Graph Enhanced Language Agents for Recommendation","summary":"  Language agents have recently been used to simulate human behavior and\nuser-item interactions for recommendation systems. However, current language\nagent simulations do not understand the relationships between users and items,\nleading to inaccurate user profiles and ineffective recommendations. In this\nwork, we explore the utility of Knowledge Graphs (KGs), which contain extensive\nand reliable relationships between users and items, for recommendation. Our key\ninsight is that the paths in a KG can capture complex relationships between\nusers and items, eliciting the underlying reasons for user preferences and\nenriching user profiles. Leveraging this insight, we propose Knowledge Graph\nEnhanced Language Agents(KGLA), a framework that unifies language agents and KG\nfor recommendation systems. In the simulated recommendation scenario, we\nposition the user and item within the KG and integrate KG paths as natural\nlanguage descriptions into the simulation. This allows language agents to\ninteract with each other and discover sufficient rationale behind their\ninteractions, making the simulation more accurate and aligned with real-world\ncases, thus improving recommendation performance. Our experimental results show\nthat KGLA significantly improves recommendation performance (with a 33%-95%\nboost in NDCG@1 among three widely used benchmarks) compared to the previous\nbest baseline method.\n","authors":["Taicheng Guo","Chaochun Liu","Hai Wang","Varun Mannam","Fang Wang","Xin Chen","Xiangliang Zhang","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2410.19627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14807v4","updated":"2024-10-25T13:34:14Z","published":"2024-02-22T18:58:27Z","title":"A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit\n  Tasks in Public Health","summary":"  Restless multi-armed bandits (RMAB) have demonstrated success in optimizing\nresource allocation for large beneficiary populations in public health\nsettings. Unfortunately, RMAB models lack flexibility to adapt to evolving\npublic health policy priorities. Concurrently, Large Language Models (LLMs)\nhave emerged as adept automated planners across domains of robotic control and\nnavigation. In this paper, we propose a Decision Language Model (DLM) for\nRMABs, enabling dynamic fine-tuning of RMAB policies in public health settings\nusing human-language commands. We propose using LLMs as automated planners to\n(1) interpret human policy preference prompts, (2) propose reward functions as\ncode for a multi-agent RMAB environment, and (3) iterate on the generated\nreward functions using feedback from grounded RMAB simulations. We illustrate\nthe application of DLM in collaboration with ARMMAN, an India-based non-profit\npromoting preventative care for pregnant mothers, that currently relies on RMAB\npolicies to optimally allocate health worker calls to low-resource populations.\nWe conduct a technology demonstration in simulation using the Gemini Pro model,\nshowing DLM can dynamically shape policy outcomes using only human prompts as\ninput.\n","authors":["Nikhil Behari","Edwin Zhang","Yunfan Zhao","Aparna Taneja","Dheeraj Nagaraj","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2402.14807v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19245v1","updated":"2024-10-25T01:52:15Z","published":"2024-10-25T01:52:15Z","title":"VisionCoder: Empowering Multi-Agent Auto-Programming for Image\n  Processing with Hybrid LLMs","summary":"  In the field of automated programming, large language models (LLMs) have\ndemonstrated foundational generative capabilities when given detailed task\ndescriptions. However, their current functionalities are primarily limited to\nfunction-level development, restricting their effectiveness in complex project\nenvironments and specific application scenarios, such as complicated\nimage-processing tasks. This paper presents a multi-agent framework that\nutilises a hybrid set of LLMs, including GPT-4o and locally deployed\nopen-source models, which collaboratively complete auto-programming tasks. Each\nagent plays a distinct role in the software development cycle, collectively\nforming a virtual organisation that works together to produce software\nproducts. By establishing a tree-structured thought distribution and\ndevelopment mechanism across project, module, and function levels, this\nframework offers a cost-effective and efficient solution for code generation.\nWe evaluated our approach using benchmark datasets, and the experimental\nresults demonstrate that VisionCoder significantly outperforms existing methods\nin image processing auto-programming tasks.\n","authors":["Zixiao Zhao","Jing Sun","Zhiyuan Wei","Cheng-Hao Cai","Zhe Hou","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2410.19245v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.19702v1","updated":"2024-10-25T17:19:55Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19586v1","updated":"2024-10-25T14:28:20Z","published":"2024-10-25T14:28:20Z","title":"Diverse Sign Language Translation","summary":"  Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results.\n","authors":["Xin Shen","Lei Shen","Shaozu Yuan","Heming Du","Haiyang Sun","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2410.19586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19459v1","updated":"2024-10-25T10:40:03Z","published":"2024-10-25T10:40:03Z","title":"Evaluation of strategies for efficient rate-distortion NeRF streaming","summary":"  Neural Radiance Fields (NeRF) have revolutionized the field of 3D visual\nrepresentation by enabling highly realistic and detailed scene reconstructions\nfrom a sparse set of images. NeRF uses a volumetric functional representation\nthat maps 3D points to their corresponding colors and opacities, allowing for\nphotorealistic view synthesis from arbitrary viewpoints. Despite its\nadvancements, the efficient streaming of NeRF content remains a significant\nchallenge due to the large amount of data involved. This paper investigates the\nrate-distortion performance of two NeRF streaming strategies: pixel-based and\nneural network (NN) parameter-based streaming. While in the former, images are\ncoded and then transmitted throughout the network, in the latter, the\nrespective NeRF model parameters are coded and transmitted instead. This work\nalso highlights the trade-offs in complexity and performance, demonstrating\nthat the NN parameter-based strategy generally offers superior efficiency,\nmaking it suitable for one-to-many streaming scenarios.\n","authors":["Pedro Martin","Ant√≥nio Rodrigues","Jo√£o Ascenso","Maria Paula Queluz"],"pdf_url":"https://arxiv.org/pdf/2410.19459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19455v1","updated":"2024-10-25T10:29:50Z","published":"2024-10-25T10:29:50Z","title":"Project Lx Conventos: Travelling through space and time in Lisbon's\n  religious buildings","summary":"  Project Lx Conventos aims to study, in a systematic and integrated manner,\nthe impact of the dissolution of religious orders in the dynamics of urban\ntransformation in nineteenth century Lisbon. After the liberal revolution and\nthe civil war, in the 19th century, the dissolution of religious orders led to\nthe alienation, in Lisbon, of nearly 130 religious buildings which were then\ngiven profane occupations (mainly public services) or demolished and divided in\nplots, originating new urban realities. Project Lx Conventos thus aims to show\nthat the extinction of the convents was decisive in the urban development of\nLisbon, in the eighteen hundreds. The project stands on a large set of\nmultimedia data which includes historic and contemporary cartography and\ngeo-referenced photos, videos and 3D models, provided by the projects partners,\nLisbon Municipality and the Portuguese National Archive, Torre do Tombo.\nSupported by these materials, the project's team is creating an online system\nthat will implement a spatial and temporal navigation of these resources\nintegrated in an interactive Map of Lisbon. Besides spatially locating and\nanalyzing the data available for each of the religious buildings considered in\nthe project, the tool integrates cutting edge interaction technology for: 1)\nEnabling a temporal voyage over the available traces of religious buildings; 2)\nAnalyzing the evolution of religious buildings and their surroundings, through\navailable data; 3) Using 3D representations of the buildings for accessing\nrelated data, through time. In this paper, the tools under development in the\ncontext of Lx Conventos are described, as well as the technologies supporting\nthem. The current status of the system is presented and future developments are\nproposed.\n","authors":["Joao Gouveia","Fernando Branco","Armanda Rodrigues","Nuno Correia"],"pdf_url":"https://arxiv.org/pdf/2410.19455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02905v2","updated":"2024-10-25T08:36:33Z","published":"2023-08-05T15:54:06Z","title":"FASTER: A Font-Agnostic Scene Text Editing and Rendering framework","summary":"  Scene Text Editing (STE) is a challenging research problem, that primarily\naims towards modifying existing texts in an image while preserving the\nbackground and the font style of the original text. Despite its utility in\nnumerous real-world applications, existing style-transfer-based approaches have\nshown sub-par editing performance due to (1) complex image backgrounds, (2)\ndiverse font attributes, and (3) varying word lengths within the text. To\naddress such limitations, in this paper, we propose a novel font-agnostic scene\ntext editing and rendering framework, named FASTER, for simultaneously\ngenerating text in arbitrary styles and locations while preserving a natural\nand realistic appearance and structure. A combined fusion of target mask\ngeneration and style transfer units, with a cascaded self-attention mechanism\nhas been proposed to focus on multi-level text region edits to handle varying\nword lengths. Extensive evaluation on a real-world database with further\nsubjective human evaluation study indicates the superiority of FASTER in both\nscene text editing and rendering tasks, in terms of model performance and\nefficiency. Our code will be released upon acceptance.\n","authors":["Alloy Das","Sanket Biswas","Prasun Roy","Subhankar Ghosh","Umapada Pal","Michael Blumenstein","Joseph Llad√≥s","Saumik Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2308.02905v2.pdf","comment":"Accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2410.19310v1","updated":"2024-10-25T05:41:28Z","published":"2024-10-25T05:41:28Z","title":"Flow Generator Matching","summary":"  In the realm of Artificial Intelligence Generated Content (AIGC),\nflow-matching models have emerged as a powerhouse, achieving success due to\ntheir robust theoretical underpinnings and solid ability for large-scale\ngenerative modeling. These models have demonstrated state-of-the-art\nperformance, but their brilliance comes at a cost. The process of sampling from\nthese models is notoriously demanding on computational resources, as it\nnecessitates the use of multi-step numerical ordinary differential equations\n(ODEs). Against this backdrop, this paper presents a novel solution with\ntheoretical guarantees in the form of Flow Generator Matching (FGM), an\ninnovative approach designed to accelerate the sampling of flow-matching models\ninto a one-step generation, while maintaining the original performance. On the\nCIFAR10 unconditional generation benchmark, our one-step FGM model achieves a\nnew record Fr\\'echet Inception Distance (FID) score of 3.08 among few-step\nflow-matching-based models, outperforming original 50-step flow-matching\nmodels. Furthermore, we use the FGM to distill the Stable Diffusion 3, a\nleading text-to-image flow-matching model based on the MM-DiT architecture. The\nresulting MM-DiT-FGM one-step text-to-image model demonstrates outstanding\nindustry-level performance. When evaluated on the GenEval benchmark, MM-DiT-FGM\nhas delivered remarkable generating qualities, rivaling other multi-step models\nin light of the efficiency of a single generation step.\n","authors":["Zemin Huang","Zhengyang Geng","Weijian Luo","Guo-jun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.19310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19307v1","updated":"2024-10-25T04:57:44Z","published":"2024-10-25T04:57:44Z","title":"Semi-supervised Chinese Poem-to-Painting Generation via Cycle-consistent\n  Adversarial Networks","summary":"  Classical Chinese poetry and painting represent the epitome of artistic\nexpression, but the abstract and symbolic nature of their relationship poses a\nsignificant challenge for computational translation. Most existing methods rely\non large-scale paired datasets, which are scarce in this domain. In this work,\nwe propose a semi-supervised approach using cycle-consistent adversarial\nnetworks to leverage the limited paired data and large unpaired corpus of poems\nand paintings. The key insight is to learn bidirectional mappings that enforce\nsemantic alignment between the visual and textual modalities. We introduce\nnovel evaluation metrics to assess the quality, diversity, and consistency of\nthe generated poems and paintings. Extensive experiments are conducted on a new\nChinese Painting Description Dataset (CPDD). The proposed model outperforms\nprevious methods, showing promise in capturing the symbolic essence of artistic\nexpression. Codes are available online\n\\url{https://github.com/Mnster00/poemtopainting}.\n","authors":["Zhengyang Lu","Tianhao Guo","Feng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19307v1.pdf","comment":null}]},"2024-10-24T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2405.18680v3","updated":"2024-10-24T20:21:36Z","published":"2024-05-29T01:07:26Z","title":"Navigable Graphs for High-Dimensional Nearest Neighbor Search:\n  Constructions and Limits","summary":"  There has been significant recent interest in graph-based nearest neighbor\nsearch methods, many of which are centered on the construction of navigable\ngraphs over high-dimensional point sets. A graph is navigable if we can\nsuccessfully move from any starting node to any target node using a greedy\nrouting strategy where we always move to the neighbor that is closest to the\ndestination according to a given distance function. The complete graph is\nnavigable for any point set, but the important question for applications is if\nsparser graphs can be constructed. While this question is fairly well\nunderstood in low-dimensions, we establish some of the first upper and lower\nbounds for high-dimensional point sets. First, we give a simple and efficient\nway to construct a navigable graph with average degree $O(\\sqrt{n \\log n })$\nfor any set of $n$ points, in any dimension, for any distance function. We\ncompliment this result with a nearly matching lower bound: even under the\nEuclidean metric in $O(\\log n)$ dimensions, a random point set has no navigable\ngraph with average degree $O(n^{\\alpha})$ for any $\\alpha < 1/2$. Our lower\nbound relies on sharp anti-concentration bounds for binomial random variables,\nwhich we use to show that the near-neighborhoods of a set of random points do\nnot overlap significantly, forcing any navigable graph to have many edges.\n","authors":["Haya Diwan","Jinrui Gou","Cameron Musco","Christopher Musco","Torsten Suel"],"pdf_url":"https://arxiv.org/pdf/2405.18680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18852v1","updated":"2024-10-24T15:35:08Z","published":"2024-10-24T15:35:08Z","title":"DL-Polycube: Deep learning enhanced polycube method for high-quality\n  hexahedral mesh generation and volumetric spline construction","summary":"  In this paper, we present a novel algorithm that integrates deep learning\nwith the polycube method (DL-Polycube) to generate high-quality hexahedral\n(hex) meshes, which are then used to construct volumetric splines for\nisogeometric analysis. Our DL-Polycube algorithm begins by establishing a\nconnection between surface triangular meshes and polycube structures. We employ\ndeep neural network to classify surface triangular meshes into their\ncorresponding polycube structures. Following this, we combine the acquired\npolycube structural information with unsupervised learning to perform surface\nsegmentation of triangular meshes. This step addresses the issue of\nsegmentation not corresponding to a polycube while reducing manual\nintervention. Quality hex meshes are then generated from the polycube\nstructures, with employing octree subdivision, parametric mapping and quality\nimprovement techniques. The incorporation of deep learning for creating\npolycube structures, combined with unsupervised learning for segmentation of\nsurface triangular meshes, substantially accelerates hex mesh generation.\nFinally, truncated hierarchical B-splines are constructed on the generated hex\nmeshes. We extract trivariate B\\'ezier elements from these splines and apply\nthem directly in isogeometric analysis. We offer several examples to\ndemonstrate the robustness of our DL-Polycube algorithm.\n","authors":["Yuxuan Yu","Yuzhuo Fang","Hua Tong","Yongjie Jessica Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18609v1","updated":"2024-10-24T10:06:30Z","published":"2024-10-24T10:06:30Z","title":"Computation of symmetries of rational surfaces","summary":"  In this paper we provide, first, a general symbolic algorithm for computing\nthe symmetries of a given rational surface, based on the classical differential\ninvariants of surfaces, i.e. Gauss curvature and mean curvature. In practice,\nthe algorithm works well for sparse parametrizations (e.g. toric surfaces) and\nPN surfaces. Additionally, we provide a specific, also symbolic algorithm for\ncomputing the symmetries of ruled surfaces; this algorithm works extremely well\nin practice, since the problem is reduced to that of rational space curves,\nwhich can be efficiently solved by using existing methods. The algorithm for\nruled surfaces is based on the fact, proven in the paper, that every symmetry\nof a rational surface must also be a symmetry of its line of striction, which\nis a rational space curve. The algorithms have been implemented in the computer\nalgebra system Maple, and the implementations have been made public; evidence\nof their performance is given in the paper.\n","authors":["Juan Juan Gerardo Alc√°zar","Carlos Hermoso","H√ºsn√º Anƒ±l √áoban","Uƒüur G√∂z√ºtok"],"pdf_url":"https://arxiv.org/pdf/2410.18609v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2404.00733v2","updated":"2024-10-24T21:38:09Z","published":"2024-03-31T16:34:28Z","title":"Smooth Information Gathering in Two-Player Noncooperative Games","summary":"  We present a mathematical framework for modeling two-player noncooperative\ngames in which one player is uncertain of the other player's costs but can\npreemptively allocate information-gathering resources to reduce this\nuncertainty. We refer to the players as the uncertain player (UP) and the\ncertain player (CP), respectively. We obtain UP's decisions by solving a\ntwo-stage problem where, in Stage 1, UP allocates information-gathering\nresources that smoothly transform the information structure in the second\nstage. Then, in Stage 2, a signal (that is, a function of the Stage 1\nallocation) informs UP about CP's costs, and both players execute strategies\nwhich depend upon the signal's value. This framework allows for a smooth\nresource allocation, in contrast to existing literature on the topic. We also\nidentify conditions under which the gradient of UP's overall cost with respect\nto the information-gathering resources is well-defined. Then we provide a\ngradient-based algorithm to solve the two-stage game. Finally, we apply our\nframework to a tower-defense game which can be interpreted as a variant of a\nColonel Blotto game with smooth payoff functions and uncertainty over\nbattlefield valuations. We include an analysis of how optimal decisions shift\nwith changes in information-gathering allocations and perturbations in the cost\nfunctions.\n","authors":["Fernando Palafox","Jesse Milzman","Dong Ho Lee","Ryan Park","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2404.00733v2.pdf","comment":"https://github.com/CLeARoboticsLab/GamesVoI.jl"},{"id":"http://arxiv.org/abs/2410.19163v1","updated":"2024-10-24T21:12:14Z","published":"2024-10-24T21:12:14Z","title":"Fairness and Efficiency in Online Class Matching","summary":"  The online bipartite matching problem, extensively studied in the literature,\ndeals with the allocation of online arriving vertices (items) to a\npredetermined set of offline vertices (agents). However, little attention has\nbeen given to the concept of class fairness, where agents are categorized into\ndifferent classes, and the matching algorithm must ensure equitable\ndistribution across these classes.\n  We here focus on randomized algorithms for the fair matching of indivisible\nitems, subject to various definitions of fairness. Our main contribution is the\nfirst (randomized) non-wasteful algorithm that simultaneously achieves a $1/2$\napproximation to class envy-freeness (CEF) while simultaneously ensuring an\nequivalent approximation to the class proportionality (CPROP) and utilitarian\nsocial welfare (USW) objectives. We supplement this result by demonstrating\nthat no non-wasteful algorithm can achieve an $\\alpha$-CEF guarantee for\n$\\alpha > 0.761$. In a similar vein, we provide a novel input instance for\ndeterministic divisible matching that demonstrates a nearly tight CEF\napproximation.\n  Lastly, we define the ``price of fairness,'' which represents the trade-off\nbetween optimal and fair matching. We demonstrate that increasing the level of\nfairness in the approximation of the solution leads to a decrease in the\nobjective of maximizing USW, following an inverse proportionality relationship.\n","authors":["MohammadTaghi Hajiaghayi","Shayan Chashm Jahan","Mohammad Sharifi","Suho Shin","Max Springer"],"pdf_url":"https://arxiv.org/pdf/2410.19163v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2106.02024v5","updated":"2024-10-24T20:37:36Z","published":"2021-06-03T17:48:40Z","title":"Time-Efficient Algorithms for Nash-Bargaining-Based Matching Market\n  Models","summary":"  In the area of matching-based market design, existing models using cardinal\nutilities suffer from two deficiencies: First, the Hylland-Zeckhauser (HZ)\nmechanism, which has remained a classic in economics for one-sided matching\nmarkets, is intractable; computation of even an approximate equilibrium is\nPPAD-complete. Second, there is an extreme paucity of such models. This led\nHosseini and Vazirani (2022) to define a rich collection of\nNash-bargaining-based models for one-sided and two-sided matching markets, in\nboth Fisher and Arrow-Debreu settings, together with very fast implementations\nusing available solvers and very encouraging experimental results.\n  In this paper, we give fast algorithms with proven running times for the\nmodels introduced by Hosseini and Vazirani, using the techniques of\nmultiplicative weights update (MWU) and conditional gradient descent (CGD).\nAdditionally, we make the following contributions:\n  (1) By Tr\\\"obst and Vazirani (2024), a linear one-sided Nash-bargaining-based\nmatching market satisfies envy-freeness within factor two. We show that the\nother models satisfy approximate equal-share fairness, where the exact factor\ndepends on the utility function being used in the particular model.\n  (2) We define a Nash-bargaining-based model for non-bipartite matching\nmarkets and give fast algorithms for it using conditional gradient descent.\n","authors":["Ioannis Panageas","Thorben Tr√∂bst","Vijay V. Vazirani"],"pdf_url":"https://arxiv.org/pdf/2106.02024v5.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2410.19106v1","updated":"2024-10-24T19:13:29Z","published":"2024-10-24T19:13:29Z","title":"Quantifying the Value of Revert Protection","summary":"  Revert protection is a feature provided by some blockchain platforms that\nprevents users from incurring fees for failed transactions. This paper explores\nthe economic implications and benefits of revert protection, in the context of\npriority auctions and maximal extractable value (MEV). We develop an\nequilibrium game theoretic model that captures the behavior of users (MEV\nsearchers) bidding to have their transaction included ahead of others, in an\nenvironment where only a single transaction will succeed in realizing the\ncommon value of an opportunity, and in settings both with and without revert\nprotection. Our model applies to a broad range of settings, including Layer 1\n(L1) blockchains (e.g., Ethereum mainnet) and Layer 2 (L2) blockchains, and\nauctions such as ``bundle auctions'' (on L1s) or priority ordering auctions (on\nL2s).\n  We establish that, in the absence of revert protection, users will employ\nrandomized strategies to mitigate the impact of paying for failed transactions.\nThis will ultimately result in less auction revenue, despite the fact that\nfailed transactions still pay fees. Our results quantify in closed form how\nrevert protection enhances auction revenue, and also improves market efficiency\nand provides for more efficient use of blockspace, as a function of the\nunderlying parameters (the value of the MEV opportunity, the base fee, the\nrevert penalties, and the number of participating agents).\n","authors":["Brian Z. Zhu","Xin Wan","Ciamac C. Moallemi","Dan Robinson","Brad Bachu"],"pdf_url":"https://arxiv.org/pdf/2410.19106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02835v2","updated":"2024-10-24T17:03:11Z","published":"2024-05-05T07:23:26Z","title":"Algorithmic collusion in a two-sided market: A rideshare example","summary":"  With dynamic pricing on the rise, firms are using sophisticated algorithms\nfor price determination. These algorithms are often non-interpretable and there\nhas been a recent interest in their seemingly emergent ability to tacitly\ncollude with each other without any prior communication whatsoever. Most of the\nprevious works investigate algorithmic collusion on simple reinforcement\nlearning (RL) based algorithms operating on a basic market model. Instead, we\nexplore the collusive tendencies of Proximal Policy Optimization (PPO), a\nstate-of-the-art continuous state/action space RL algorithm, on a complex\ndouble-sided hierarchical market model of rideshare. For this purpose, we\nextend a mathematical program network (MPN) based rideshare model to a temporal\nmulti origin-destination setting and use PPO to solve for a repeated duopoly\ngame. Our results indicate that PPO can either converge to a competitive or a\ncollusive equilibrium depending upon the underlying market characteristics,\neven when the hyper-parameters are held constant.\n","authors":["Pravesh Koirala","Forrest Laine"],"pdf_url":"https://arxiv.org/pdf/2405.02835v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.18871v1","updated":"2024-10-24T15:58:14Z","published":"2024-10-24T15:58:14Z","title":"Learning Collusion in Episodic, Inventory-Constrained Markets","summary":"  Pricing algorithms have demonstrated the capability to learn tacit collusion\nthat is largely unaddressed by current regulations. Their increasing use in\nmarkets, including oligopolistic industries with a history of collusion, calls\nfor closer examination by competition authorities. In this paper, we extend the\nstudy of tacit collusion in learning algorithms from basic pricing games to\nmore complex markets characterized by perishable goods with fixed supply and\nsell-by dates, such as airline tickets, perishables, and hotel rooms. We\nformalize collusion within this framework and introduce a metric based on price\nlevels under both the competitive (Nash) equilibrium and collusive\n(monopolistic) optimum. Since no analytical expressions for these price levels\nexist, we propose an efficient computational approach to derive them. Through\nexperiments, we demonstrate that deep reinforcement learning agents can learn\nto collude in this more complex domain. Additionally, we analyze the underlying\nmechanisms and structures of the collusive strategies these agents adopt.\n","authors":["Paul Friedrich","Barna P√°sztor","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2410.18871v1.pdf","comment":"37 pages, 25 figures. Under review"},{"id":"http://arxiv.org/abs/2410.18655v1","updated":"2024-10-24T11:34:28Z","published":"2024-10-24T11:34:28Z","title":"Approximate EFX and Exact tEFX Allocations for Indivisible Chores:\n  Improved Algorithms","summary":"  We explore the fair distribution of a set of $m$ indivisible chores among $n$\nagents, where each agent's costs are evaluated using a monotone cost function.\nOur focus lies on two fairness criteria: envy-freeness up to any item (EFX) and\na relaxed notion, namely envy-freeness up to the transfer of any item (tEFX).\nWe demonstrate that a 2-approximate EFX allocation exists and is computable in\npolynomial time for three agents with subadditive cost functions, improving\nupon the previous $(2 + \\sqrt{6})$ approximation for additive cost functions.\nThis result requires extensive case analysis. Christoforidis et al. (IJCAI'24)\nindependently claim the same approximation for additive cost functions;\nhowever, we provide a counter-example to their algorithm. We expand the number\nof agents to any number to get the same approximation guarantee with the\nassumption of partially identical ordering (IDO) for the cost functions.\nAdditionally, we establish that a tEFX allocation is achievable for three\nagents if one has an additive 2-ratio bounded cost function, while the others\nmay have general monotone cost functions. This is an improvement from the prior\nrequirement of two agents with additive 2-ratio bounded cost functions. This\nallocation can also be extended to agent groups with identical valuations.\nFurther, we show various analyses of EFX allocations for chores, such as the\nrelaxations for additive $\\alpha$-ratio-bounded cost functions.\n","authors":["Mahyar Afshinmehr","Matin Ansaripour","Alireza Danaei","Kurt Mehlhorn"],"pdf_url":"https://arxiv.org/pdf/2410.18655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18602v1","updated":"2024-10-24T09:56:24Z","published":"2024-10-24T09:56:24Z","title":"Fair Diffusion Auctions","summary":"  Diffusion auction design is a new trend in mechanism design which extended\nthe original incentive compatibility property to include buyers' private\nconnection report. Reporting connections is equivalent to inviting their\nneighbors to join the auction in practice. The social welfare of a diffusion\nauction is collectively accumulated by all participants: reporting high\nvaluations or inviting high-valuation neighbors. Because of this, we can\nmeasure each participant's contribution by the marginal social welfare increase\ndue to her participation.\n  Therefore, in this paper, we introduce a new property called \\textit{Shapley\nfairness} to capture their social welfare contribution and to use it as a\nbenchmark to guide our auction design for a fairer utility allocation. Not\nsurprisingly, none of the existing diffusion auctions has ever approximated the\nfairness, because Shapley fairness depends on each buyer's own valuation and\nthis dependence can easily violate incentive compatibility. Thus, we combat\nthis challenge by proposing a new diffusion auction called \\textit{Permutation\nDiffusion Auction} (PDA) for selling $k$ homogeneous items, which is the first\ndiffusion auction satisfying $\\frac{1}{k+1}$-Shapley fairness, incentive\ncompatibility and individual rationality. Furthermore, PDA can be extended to\nthe general combinatorial auction setting where the literature did not discover\nmeaningful diffusion auctions yet.\n","authors":["Zixin Gu","Yaoxin Ge","Yao Zhang","Dengji Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.18602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18586v1","updated":"2024-10-24T09:36:49Z","published":"2024-10-24T09:36:49Z","title":"Incentives for Early Arrival in Cost Sharing","summary":"  In cooperative games, we study how values created or costs incurred by a\ncoalition are shared among the members within it, and the players may join the\ncoalition in a online manner such as investors invest a startup. Recently, Ge\net al. [10] proposed a new property called incentives for early arrival (I4EA)\nin such games, which says that the online allocation of values or costs should\nincentivize agents to join early in order to prevent mutual strategic waiting.\nIdeally, the allocation should also be fair, so that agents arriving in an\norder uniformly at random should expect to get/pay their Shapley values. Ge et\nal. [10] showed that not all monotone value functions admit such mechanisms in\nonline value sharing games. In this work, we show a sharp contrast in online\ncost sharing games. We construct a mechanism with all the properties mentioned\nabove, for every monotone cost function. To achieve this, we first solve 0-1\nvalued cost sharing games with a novel mechanism called Shapley-fair shuffle\ncost sharing mechanism (SFS-CS), and then extend SFS-CS to a family called\ngeneralized Shapley-fair shuffle cost sharing mechanisms (GSFS-CS). The\ncritical technique we invented here is a mapping from one arrival order to\nanother order so that we can directly apply marginal cost allocation on the\nshuffled orders to satisfy the properties. Finally, we solve general valued\ncost functions, by decomposing them into 0-1 valued functions in an online\nfashion.\n","authors":["Junyu Zhang","Yao Zhang","Yaoxin Ge","Dengji Zhao","Hu Fu","Zhihao Gavin Tang","Pinyan Lu"],"pdf_url":"https://arxiv.org/pdf/2410.18586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18434v1","updated":"2024-10-24T05:11:41Z","published":"2024-10-24T05:11:41Z","title":"RediSwap: MEV Redistribution Mechanism for CFMMs","summary":"  Automated Market Makers (AMMs) are essential to decentralized finance,\noffering continuous liquidity and enabling intermediary-free trading on\nblockchains. However, participants in AMMs are vulnerable to Maximal\nExtractable Value (MEV) exploitation. Users face threats such as front-running,\nback-running, and sandwich attacks, while liquidity providers (LPs) incur the\nloss-versus-rebalancing (LVR).\n  In this paper, we introduce RediSwap, a novel AMM designed to capture MEV at\nthe application level and refund it fairly among users and liquidity providers.\nAt its core, RediSwap features an MEV-redistribution mechanism that manages\narbitrage opportunities within the AMM pool. We formalize the mechanism design\nproblem and the desired game-theoretical properties. A central insight\nunderpinning our mechanism is the interpretation of the maximal MEV value as\nthe sum of LVR and individual user losses. We prove that our mechanism is\nincentive-compatible and Sybil-proof, and demonstrate that it is easy for\narbitrageurs to participate.\n  We empirically compared RediSwap with existing solutions by replaying\nhistorical AMM trades. Our results suggest that RediSwap can achieve better\nexecution than UniswapX in 89% of trades and reduce LPs' loss to under 0.5% of\nthe original LVR in most cases.\n","authors":["Mengqian Zhang","Sen Yang","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15045v2","updated":"2024-10-24T01:25:51Z","published":"2024-10-19T09:04:13Z","title":"Distribution-Aware Compensation Design for Sustainable Data Rights in\n  Machine Learning","summary":"  Modern distributed learning systems face a critical challenge when clients\nrequest the removal of their data influence from trained models, as this\nprocess can significantly destabilize system performance and affect remaining\nparticipants. We propose an innovative mechanism that views this challenge\nthrough the lens of game theory, establishing a leader-follower framework where\na central coordinator provides strategic incentives to maintain system\nstability during data removal operations. Our approach quantifies the ripple\neffects of data removal through a comprehensive analytical model that captures\nboth system-wide and participant-specific impacts. We establish mathematical\nfoundations for measuring participant utility and system outcomes, revealing\ncritical insights into how data diversity influences both individual decisions\nand overall system stability. The framework incorporates a computationally\nefficient solution method that addresses the inherent complexity of optimizing\nparticipant interactions and resource allocation.\n","authors":["Jiaqi Shao","Tao Lin","Bing Luo"],"pdf_url":"https://arxiv.org/pdf/2410.15045v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.19198v1","updated":"2024-10-24T23:16:39Z","published":"2024-10-24T23:16:39Z","title":"MAP: Multi-Human-Value Alignment Palette","summary":"  Ensuring that generative AI systems align with human values is essential but\nchallenging, especially when considering multiple human values and their\npotential trade-offs. Since human values can be personalized and dynamically\nchange over time, the desirable levels of value alignment vary across different\nethnic groups, industry sectors, and user cohorts. Within existing frameworks,\nit is hard to define human values and align AI systems accordingly across\ndifferent directions simultaneously, such as harmlessness, helpfulness, and\npositiveness. To address this, we develop a novel, first-principle approach\ncalled Multi-Human-Value Alignment Palette (MAP), which navigates the alignment\nacross multiple human values in a structured and reliable way. MAP formulates\nthe alignment problem as an optimization task with user-defined constraints,\nwhich define human value targets. It can be efficiently solved via a\nprimal-dual approach, which determines whether a user-defined alignment target\nis achievable and how to achieve it. We conduct a detailed theoretical analysis\nof MAP by quantifying the trade-offs between values, the sensitivity to\nconstraints, the fundamental connection between multi-value alignment and\nsequential alignment, and proving that linear weighted rewards are sufficient\nfor multi-value alignment. Extensive experiments demonstrate MAP's ability to\nalign multiple values in a principled manner while delivering strong empirical\nperformance across various tasks.\n","authors":["Xinran Wang","Qi Le","Ammar Ahmed","Enmao Diao","Yi Zhou","Nathalie Baracaldo","Jie Ding","Ali Anwar"],"pdf_url":"https://arxiv.org/pdf/2410.19198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02866v3","updated":"2024-10-24T19:09:03Z","published":"2024-03-05T11:19:43Z","title":"Unlocking Electro-optic Resonant Phase Shifting for Multi-dimensional,\n  Ultra-dynamic Photonic Switches","summary":"  Optical circuit switching is connection-oriented, being deterministic through\nthe reservation of a complete wavelength channel or spatial path for a certain\nperiod. However, this comes at a trade-off against link dynamics, and overall\ncapacity can thus be constrained by the time slot reservations, especially for\nswitches with microsecond- to millisecond-scale reconfiguration times. For\ndata-intensive applications, the communication patterns associated with random\ndata sets typically yield short-lived flows. This situation calls for a new\nmulti-dimensional switching paradigm that fully exploits not only the space and\nwavelength domains but also with nanosecond-scale reconfigurable capability in\nthe time domain to enable ultra-dynamic links. In this work, we focus on the\nexploitation of micro-ring resonant phase shifters (RPSs) that are wavelength\nselective for optical switching in a single plane. By proposing an innovative\nanalytical method with transmission circle chart, we fully unlock the power of\nRPS with nanosecond-scale reconfigurability and the capability to arbitrarily\nmanipulate its phase and amplitude. Such a compact model offers fresh insights\ninto designs with under and critically coupled RPSs beyond the commonly\nexplored over-coupling condition. This creates not only versatile switch\nelements but also perfect absorbers for robust multi-wavelength operations. The\nproposed device can bring about a breakthrough in the optical switching\ncapacity that potentially addresses the challenges faced by modern data center\nnetworks, as well as other photonic circuits for high-throughput signal\nprocessing.\n","authors":["Lingzhi Luo","Rui Ma","Richard V. Penty","Qixiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.02866v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.18618v1","updated":"2024-10-24T10:17:48Z","published":"2024-10-24T10:17:48Z","title":"Adiabatic training for Variational Quantum Algorithms","summary":"  This paper presents a new hybrid Quantum Machine Learning (QML) model\ncomposed of three elements: a classical computer in charge of the data\npreparation and interpretation; a Gate-based Quantum Computer running the\nVariational Quantum Algorithm (VQA) representing the Quantum Neural Network\n(QNN); and an adiabatic Quantum Computer where the optimization function is\nexecuted to find the best parameters for the VQA.\n  As of the moment of this writing, the majority of QNNs are being trained\nusing gradient-based classical optimizers having to deal with the\nbarren-plateau effect. Some gradient-free classical approaches such as\nEvolutionary Algorithms have also been proposed to overcome this effect. To the\nknowledge of the authors, adiabatic quantum models have not been used to train\nVQAs.\n  The paper compares the results of gradient-based classical algorithms against\nadiabatic optimizers showing the feasibility of integration for gate-based and\nadiabatic quantum computing models, opening the door to modern hybrid QML\napproaches for High Performance Computing.\n","authors":["Ernesto Acosta","Carlos Cano Gutierrez","Guillermo Botella","Roberto Campos"],"pdf_url":"https://arxiv.org/pdf/2410.18618v1.pdf","comment":"12 pages, 6 figures, Euro PAR 2024 EuroQHPC Workshop"},{"id":"http://arxiv.org/abs/2410.18489v1","updated":"2024-10-24T07:24:11Z","published":"2024-10-24T07:24:11Z","title":"LLM as a code generator in Agile Model Driven Development","summary":"  Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.\n","authors":["Ahmed R. Sadik","Sebastian Brulin","Markus Olhofer","Antonello Ceravola","Frank Joublin"],"pdf_url":"https://arxiv.org/pdf/2410.18489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07857v3","updated":"2024-10-24T02:43:43Z","published":"2024-04-11T15:53:39Z","title":"Optical next generation reservoir computing","summary":"  Artificial neural networks with internal dynamics exhibit remarkable\ncapability in processing information. Reservoir computing (RC) is a canonical\nexample that features rich computing expressivity and compatibility with\nphysical implementations for enhanced efficiency. Recently, a new RC paradigm\nknown as next generation reservoir computing (NGRC) further improves\nexpressivity but compromises its physical openness, posing challenges for\nrealizations in physical systems. Here we demonstrate optical NGRC with\ncomputations performed by light scattering through disordered media. In\ncontrast to conventional optical RC implementations, we drive our optical\nreservoir directly with time-delayed inputs. Much like digital NGRC that relies\non polynomial features of delayed inputs, our optical reservoir also implicitly\ngenerates these polynomial features for desired functionalities. By leveraging\nthe domain knowledge of the reservoir inputs, we show that the optical NGRC not\nonly predicts the short-term dynamics of the low-dimensional Lorenz63 and\nlarge-scale Kuramoto-Sivashinsky chaotic time series, but also replicates their\nlong-term ergodic properties. Optical NGRC shows superiority in shorter\ntraining length, increased interpretability and fewer hyperparameters compared\nto conventional optical RC based on scattering media, while achieving better\nforecasting performance. Our optical NGRC framework may inspire the realization\nof NGRC in other physical RC systems, new applications beyond time-series\nprocessing, and the development of deep and parallel architectures broadly.\n","authors":["Hao Wang","Jianqi Hu","YoonSeok Baek","Kohei Tsuchiyama","Malo Joly","Qiang Liu","Sylvain Gigan"],"pdf_url":"https://arxiv.org/pdf/2404.07857v3.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.18944v1","updated":"2024-10-24T17:35:12Z","published":"2024-10-24T17:35:12Z","title":"Path Guiding for Monte Carlo PDE Solvers","summary":"  In recent years, Monte Carlo PDE solvers have garnered increasing attention\nin computer graphics, demonstrating value across a wide range of applications.\nDespite offering clear advantages over traditional methods-such as avoiding\ndiscretization and enabling local evaluations-Monte Carlo PDE solvers face\nchallenges due to their stochastic nature, including high variance and slow\nconvergence rates. To mitigate the variance issue, we draw inspiration from\nMonte Carlo path tracing and apply the path guiding technique to the Walk on\nStars estimator. Specifically, we examine the target sampling distribution at\neach step of the Walk on Stars estimator, parameterize it, and introduce neural\nimplicit representations to model the spatially-varying guiding distribution.\nThis path guiding approach is implemented in a wavefront-style PDE solver, and\nexperimental results demonstrate that it effectively reduces variance in Monte\nCarlo PDE solvers.\n","authors":["Tianyu Huang","Jingwang Ling","Shuang Zhao","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.18944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18622v1","updated":"2024-10-24T10:27:29Z","published":"2024-10-24T10:27:29Z","title":"Environment Maps Editing using Inverse Rendering and Adversarial\n  Implicit Functions","summary":"  Editing High Dynamic Range (HDR) environment maps using an inverse\ndifferentiable rendering architecture is a complex inverse problem due to the\nsparsity of relevant pixels and the challenges in balancing light sources and\nbackground. The pixels illuminating the objects are a small fraction of the\ntotal image, leading to noise and convergence issues when the optimization\ndirectly involves pixel values. HDR images, with pixel values beyond the\ntypical Standard Dynamic Range (SDR), pose additional challenges. Higher\nlearning rates corrupt the background during optimization, while lower learning\nrates fail to manipulate light sources. Our work introduces a novel method for\nediting HDR environment maps using a differentiable rendering, addressing\nsparsity and variance between values. Instead of introducing strong priors that\nextract the relevant HDR pixels and separate the light sources, or using tricks\nsuch as optimizing the HDR image in the log space, we propose to model the\noptimized environment map with a new variant of implicit neural representations\nable to handle HDR images. The neural representation is trained with\nadversarial perturbations over the weights to ensure smooth changes in the\noutput when it receives gradients from the inverse rendering. In this way, we\nobtain novel and cheap environment maps without relying on latent spaces of\nexpensive generative models, maintaining the original visual consistency.\nExperimental results demonstrate the method's effectiveness in reconstructing\nthe desired lighting effects while preserving the fidelity of the map and\nreflections on objects in the scene. Our approach can pave the way to\ninteresting tasks, such as estimating a new environment map given a rendering\nwith novel light sources, maintaining the initial perceptual features, and\nenabling brush stroke-based editing of existing environment maps.\n","authors":["Antonio D'Orazio","Davide Sforza","Fabio Pellacini","Iacopo Masi"],"pdf_url":"https://arxiv.org/pdf/2410.18622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19702v3","updated":"2024-10-24T08:17:05Z","published":"2024-09-29T13:32:24Z","title":"RNG: Relightable Neural Gaussians","summary":"  3D Gaussian Splatting (3DGS) has shown its impressive power in novel view\nsynthesis. However, creating relightable 3D assets, especially for objects with\nill-defined shapes (e.g., fur), is still a challenging task. For these scenes,\nthe decomposition between the light, geometry, and material is more ambiguous,\nas neither the surface constraints nor the analytical shading model hold. To\naddress this issue, we propose RNG, a novel representation of relightable\nneural Gaussians, enabling the relighting of objects with both hard surfaces or\nfluffy boundaries. We avoid any assumptions in the shading model but maintain\nfeature vectors, which can be further decoded by an MLP into colors, in each\nGaussian point. Following prior work, we utilize a point light to reduce the\nambiguity and introduce a shadow-aware condition to the network. We\nadditionally propose a depth refinement network to help the shadow computation\nunder the 3DGS framework, leading to better shadow effects under point lights.\nFurthermore, to avoid the blurriness brought by the alpha-blending in 3DGS, we\ndesign a hybrid forward-deferred optimization strategy. As a result, we achieve\nabout $20\\times$ faster in training and about $600\\times$ faster in rendering\nthan prior work based on neural radiance fields, with $60$ frames per second on\nan RTX4090.\n","authors":["Jiahui Fan","Fujun Luan","Jian Yang","Milo≈° Ha≈°an","Beibei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.19702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18355v1","updated":"2024-10-24T01:34:11Z","published":"2024-10-24T01:34:11Z","title":"Real-time 3D-aware Portrait Video Relighting","summary":"  Synthesizing realistic videos of talking faces under custom lighting\nconditions and viewing angles benefits various downstream applications like\nvideo conferencing. However, most existing relighting methods are either\ntime-consuming or unable to adjust the viewpoints. In this paper, we present\nthe first real-time 3D-aware method for relighting in-the-wild videos of\ntalking faces based on Neural Radiance Fields (NeRF). Given an input portrait\nvideo, our method can synthesize talking faces under both novel views and novel\nlighting conditions with a photo-realistic and disentangled 3D representation.\nSpecifically, we infer an albedo tri-plane, as well as a shading tri-plane\nbased on a desired lighting condition for each video frame with fast\ndual-encoders. We also leverage a temporal consistency network to ensure smooth\ntransitions and reduce flickering artifacts. Our method runs at 32.98 fps on\nconsumer-level hardware and achieves state-of-the-art results in terms of\nreconstruction quality, lighting error, lighting instability, temporal\nconsistency and inference speed. We demonstrate the effectiveness and\ninteractivity of our method on various portrait videos with diverse lighting\nand viewing conditions.\n","authors":["Ziqi Cai","Kaiwen Jiang","Shu-Yu Chen","Yu-Kun Lai","Hongbo Fu","Boxin Shi","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2410.18355v1.pdf","comment":"Accepted to CVPR 2024 (Highlight). Project page:\n  http://geometrylearning.com/VideoRelighting"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.19198v1","updated":"2024-10-24T23:16:39Z","published":"2024-10-24T23:16:39Z","title":"MAP: Multi-Human-Value Alignment Palette","summary":"  Ensuring that generative AI systems align with human values is essential but\nchallenging, especially when considering multiple human values and their\npotential trade-offs. Since human values can be personalized and dynamically\nchange over time, the desirable levels of value alignment vary across different\nethnic groups, industry sectors, and user cohorts. Within existing frameworks,\nit is hard to define human values and align AI systems accordingly across\ndifferent directions simultaneously, such as harmlessness, helpfulness, and\npositiveness. To address this, we develop a novel, first-principle approach\ncalled Multi-Human-Value Alignment Palette (MAP), which navigates the alignment\nacross multiple human values in a structured and reliable way. MAP formulates\nthe alignment problem as an optimization task with user-defined constraints,\nwhich define human value targets. It can be efficiently solved via a\nprimal-dual approach, which determines whether a user-defined alignment target\nis achievable and how to achieve it. We conduct a detailed theoretical analysis\nof MAP by quantifying the trade-offs between values, the sensitivity to\nconstraints, the fundamental connection between multi-value alignment and\nsequential alignment, and proving that linear weighted rewards are sufficient\nfor multi-value alignment. Extensive experiments demonstrate MAP's ability to\nalign multiple values in a principled manner while delivering strong empirical\nperformance across various tasks.\n","authors":["Xinran Wang","Qi Le","Ammar Ahmed","Enmao Diao","Yi Zhou","Nathalie Baracaldo","Jie Ding","Ali Anwar"],"pdf_url":"https://arxiv.org/pdf/2410.19198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12944v2","updated":"2024-10-24T20:40:08Z","published":"2024-10-16T18:31:14Z","title":"How much does AI impact development speed? An enterprise-based\n  randomized controlled trial","summary":"  How much does AI assistance impact developer productivity? To date, the\nsoftware engineering literature has provided a range of answers, targeting a\ndiversity of outcomes: from perceived productivity to speed on task and\ndeveloper throughput. Our randomized controlled trial with 96 full-time Google\nsoftware engineers contributes to this literature by sharing an estimate of the\nimpact of three AI features on the time developers spent on a complex,\nenterprise-grade task. We found that AI significantly shortened the time\ndevelopers spent on task. Our best estimate of the size of this effect,\ncontrolling for factors known to influence developer time on task, stands at\nabout 21\\%, although our confidence interval is large. We also found an\ninteresting effect whereby developers who spend more hours on code-related\nactivities per day were faster with AI. Product and future research\nconsiderations are discussed. In particular, we invite further research that\nexplores the impact of AI at the ecosystem level and across multiple suites of\nAI-enhanced tools, since we cannot assume that the effect size obtained in our\nlab study will necessarily apply more broadly, or that the effect of AI found\nusing internal Google tooling in the summer of 2024 will translate across tools\nand over time.\n","authors":["Elise Paradis","Kate Grey","Quinn Madison","Daye Nam","Andrew Macvean","Vahid Meimand","Nan Zhang","Ben Ferrari-Church","Satish Chandra"],"pdf_url":"https://arxiv.org/pdf/2410.12944v2.pdf","comment":"12 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.08273v3","updated":"2024-10-24T18:55:23Z","published":"2024-06-12T14:38:34Z","title":"SonicID: User Identification on Smart Glasses with Acoustic Sensing","summary":"  Smart glasses have become more prevalent as they provide an increasing number\nof applications for users. They store various types of private information or\ncan access it via connections established with other devices. Therefore, there\nis a growing need for user identification on smart glasses. In this paper, we\nintroduce a low-power and minimally-obtrusive system called SonicID, designed\nto authenticate users on glasses. SonicID extracts unique biometric information\nfrom users by scanning their faces with ultrasonic waves and utilizes this\ninformation to distinguish between different users, powered by a customized\nbinary classifier with the ResNet-18 architecture. SonicID can authenticate\nusers by scanning their face for 0.06 seconds. A user study involving 40\nparticipants confirms that SonicID achieves a true positive rate of 97.4%, a\nfalse positive rate of 4.3%, and a balanced accuracy of 96.6% using just 1\nminute of training data collected for each new user. This performance is\nrelatively consistent across different remounting sessions and days. Given this\npromising performance, we further discuss the potential applications of SonicID\nand methods to improve its performance in the future.\n","authors":["Ke Li","Devansh Agarwal","Ruidong Zhang","Vipin Gunda","Tianjun Mo","Saif Mahmud","Boao Chen","Fran√ßois Guimbreti√®re","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.08273v3.pdf","comment":"27 pages, 6 tables, 9 figures"},{"id":"http://arxiv.org/abs/2410.18924v1","updated":"2024-10-24T17:12:51Z","published":"2024-10-24T17:12:51Z","title":"Swarm manipulation: An efficient and accurate technique for multi-object\n  manipulation in virtual reality","summary":"  The theory of swarm control shows promise for controlling multiple objects,\nhowever, scalability is hindered by cost constraints, such as hardware and\ninfrastructure. Virtual Reality (VR) can overcome these limitations, but\nresearch on swarm interaction in VR is limited. This paper introduces a novel\nSwarm Manipulation interaction technique and compares it with two baseline\ntechniques: Virtual Hand and Controller (ray-casting). We evaluated these\ntechniques in a user study ($N$ = 12) in three tasks (selection, rotation, and\nresizing) across five conditions. Our results indicate that Swarm Manipulation\nyielded superior performance, with significantly faster speeds in most\nconditions across the three tasks. It notably reduced resizing size deviations\nbut introduced a trade-off between speed and accuracy in the rotation task.\nAdditionally, we conducted a follow-up user study ($N$ = 6) using Swarm\nManipulation in two complex VR scenarios and obtained insights through\nsemi-structured interviews, shedding light on optimized swarm control\nmechanisms and perceptual changes induced by this interaction paradigm. These\nresults demonstrate the potential of the Swarm Manipulation technique to\nenhance the usability and user experience in VR compared to conventional\nmanipulation techniques. In future studies, we aim to understand and improve\nswarm interaction via internal swarm particle cooperation.\n","authors":["Xiang Li","Jin-Du Wang","John J. Dudley","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2410.18924v1.pdf","comment":"15 pages, accepted at Computers & Graphics"},{"id":"http://arxiv.org/abs/2410.18876v1","updated":"2024-10-24T16:05:38Z","published":"2024-10-24T16:05:38Z","title":"Guiding Empowerment Model: Liberating Neurodiversity in Online Higher\n  Education","summary":"  In this innovative practice full paper, we address the equity gap for\nneurodivergent and situationally limited learners by identifying the spectrum\nof dynamic factors that impact learning and function. Educators have shown a\ngrowing interest in identifying learners' cognitive abilities and learning\npreferences to measure their impact on academic achievement. Often institutions\nemploy one-size-fits-all approaches leaving the burden on disabled students to\nself-advocate or tolerate inadequate support. Emerging frameworks guide\nneurodivergent learners through instructional approaches, such as online\neducation. However, these frameworks fail to address holistic environmental\nneeds or recommend technology interventions, particularly for those with\nundisclosed learning or developmental disabilities and situational limitations.\nIn this article, we integrate a neurodivergent perspective through secondary\nresearch of around 100 articles to introduce a Guiding Empowerment Model\ninvolving key cognitive and situational factors that contextualize day-to-day\nexperiences affecting learner ability. We synthesize three sample student\nprofiles that highlight user problems in functioning. We use this model to\nevaluate sample learning platform features and other supportive technology\nsolutions. The proposed approach augments frameworks such as Universal Design\nfor Learning to consider factors including various sensory processing\ndifferences, social connection challenges, and environmental limitations. We\nsuggest that by applying the mode through technology-enabled features such as\ncustomizable task management, guided varied content access, and guided\nmulti-modal collaboration, major learning barriers of neurodivergent and\nsituationally limited learners will be removed to activate the successful\npursuit of their academic goals.\n","authors":["Hannah Beaux","Pegah Karimi","Otilia Pop","Rob Clark"],"pdf_url":"https://arxiv.org/pdf/2410.18876v1.pdf","comment":"9 pages, 1 Figure, 1 Table, Accepted in FIE 2024"},{"id":"http://arxiv.org/abs/2410.18875v1","updated":"2024-10-24T16:05:11Z","published":"2024-10-24T16:05:11Z","title":"Exploring the Universe with SNAD: Anomaly Detection in Astronomy","summary":"  SNAD is an international project with a primary focus on detecting\nastronomical anomalies within large-scale surveys, using active learning and\nother machine learning algorithms. The work carried out by SNAD not only\ncontributes to the discovery and classification of various astronomical\nphenomena but also enhances our understanding and implementation of machine\nlearning techniques within the field of astrophysics. This paper provides a\nreview of the SNAD project and summarizes the advancements and achievements\nmade by the team over several years.\n","authors":["Alina A. Volnova","Patrick D. Aleo","Anastasia Lavrukhina","Etienne Russeil","Timofey Semenikhin","Emmanuel Gangler","Emille E. O. Ishida","Matwey V. Kornilov","Vladimir Korolev","Konstantin Malanchev","Maria V. Pruzhinskaya","Sreevarsha Sreejith"],"pdf_url":"https://arxiv.org/pdf/2410.18875v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.11722v2","updated":"2024-10-24T15:48:41Z","published":"2024-10-15T15:55:00Z","title":"RClicks: Realistic Click Simulation for Benchmarking Interactive\n  Segmentation","summary":"  The emergence of Segment Anything (SAM) sparked research interest in the\nfield of interactive segmentation, especially in the context of image editing\ntasks and speeding up data annotation. Unlike common semantic segmentation,\ninteractive segmentation methods allow users to directly influence their output\nthrough prompts (e.g. clicks). However, click patterns in real-world\ninteractive segmentation scenarios remain largely unexplored. Most methods rely\non the assumption that users would click in the center of the largest erroneous\narea. Nevertheless, recent studies show that this is not always the case. Thus,\nmethods may have poor performance in real-world deployment despite high metrics\nin a baseline benchmark. To accurately simulate real-user clicks, we conducted\na large crowdsourcing study of click patterns in an interactive segmentation\nscenario and collected 475K real-user clicks. Drawing on ideas from saliency\ntasks, we develop a clickability model that enables sampling clicks, which\nclosely resemble actual user inputs. Using our model and dataset, we propose\nRClicks benchmark for a comprehensive comparison of existing interactive\nsegmentation methods on realistic clicks. Specifically, we evaluate not only\nthe average quality of methods, but also the robustness w.r.t. click patterns.\nAccording to our benchmark, in real-world usage interactive segmentation models\nmay perform worse than it has been reported in the baseline benchmark, and most\nof the methods are not robust. We believe that RClicks is a significant step\ntowards creating interactive segmentation methods that provide the best user\nexperience in real-world cases.\n","authors":["Anton Antonov","Andrey Moskalenko","Denis Shepelev","Alexander Krapukhin","Konstantin Soshin","Anton Konushin","Vlad Shakhuro"],"pdf_url":"https://arxiv.org/pdf/2410.11722v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2401.06247v3","updated":"2024-10-24T15:45:33Z","published":"2024-01-11T20:21:40Z","title":"Trickery: Exploring a Serious Game Approach to Raise Awareness of\n  Deceptive Patterns","summary":"  Deceptive patterns are often used in interface design to manipulate users\ninto taking actions they would not otherwise take, such as consenting to\nexcessive data collection. We present Trickery, a narrative serious game that\nincorporates seven gamified deceptive patterns. We designed the game as a\npotential mechanism for raising awareness of, and increasing resistance to,\ndeceptive patterns through direct consequences of player actions. We conducted\nan explorative gameplay study to examine player behavior when confronted with\nthe game Trickery. In addition, we conducted an online survey to shed light on\nthe perceived helpfulness of our gamified deceptive patterns. Our results\nreveal different player motivations and driving forces that players used to\njustify their behavior when confronted with deceptive patterns in the Trickery\ngame. In addition, we identified several influencing factors that need to be\nconsidered when adapting deceptive patterns into gameplay. Overall, the\napproach appears to be a promising solution for increasing user understanding\nand awareness of deceptive patterns.\n","authors":["Kirill Kronhardt","Kevin Rolfes","Jens Gerken"],"pdf_url":"https://arxiv.org/pdf/2401.06247v3.pdf","comment":"[V3]: Submitted to MUM 2024 [V2]: Submitted to CHI PLAY 2024 [V1]:\n  Submitted to Proceedings on Privacy Enhancing Technologies 2024"},{"id":"http://arxiv.org/abs/2410.18851v1","updated":"2024-10-24T15:33:34Z","published":"2024-10-24T15:33:34Z","title":"Intention Is All You Need","summary":"  Among the many narratives of the transformative power of Generative AI is one\nthat sees in the world a latent nation of programmers who need to wield nothing\nbut intentions and natural language to render their ideas in software. In this\npaper, this outlook is problematised in two ways. First, it is observed that\ngenerative AI is not a neutral vehicle of intention. Multiple recent studies\npaint a picture of the \"mechanised convergence\" phenomenon, namely, that\ngenerative AI has a homogenising effect on intention. Second, it is observed\nthat the formation of intention itself is immensely challenging. Constraints,\nmateriality, and resistance can offer paths to design metaphors for intentional\ntools. Finally, existentialist approaches to intention are discussed and\npossible implications for programming are proposed in the form of a\nspeculative, illustrative set of intentional programming practices.\n","authors":["Advait Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.18851v1.pdf","comment":"Proceedings of the 35th Annual Conference of the Psychology of\n  Programming Interest Group (PPIG 2024)"},{"id":"http://arxiv.org/abs/2410.18845v1","updated":"2024-10-24T15:26:34Z","published":"2024-10-24T15:26:34Z","title":"Expanding AI Awareness Through Everyday Interactions with AI: A\n  Reflective Journal Study","summary":"  As the application of AI continues to expand, students in technology programs\nare poised to be both producers and users of the technologies. They are also\npositioned to engage with AI applications within and outside the classroom.\nWhile focusing on the curriculum when examining students' AI knowledge is\ncommon, extending this connection to students' everyday interactions with AI\nprovides a more complete picture of their learning. In this paper, we explore\nstudent's awareness and engagement with AI in the context of school and their\ndaily lives. Over six weeks, 22 undergraduate students participated in a\nreflective journal study and submitted a weekly journal entry about their\ninteractions with AI. The participants were recruited from a technology and\nsociety course that focuses on the implications of technology on people,\ncommunities, and processes. In their weekly journal entries, participants\nreflected on interactions with AI on campus (coursework, advertises campus\nevents, or seminars) and beyond (social media, news, or conversations with\nfriends and family). The journal prompts were designed to help them think\nthrough what they had read, watched, or been told and reflect on the\ndevelopment of their own perspectives, knowledge, and literacy on the topic.\nOverall, students described nine categories of interactions: coursework, news\nand current events, using software and applications, university events, social\nmedia related to their work, personal discussions with friends and family,\ninteracting with content, and gaming. Students reported that completing the\ndiaries allowed them time for reflection and made them more aware of the\npresence of AI in their daily lives and of its potential benefits and\ndrawbacks. This research contributes to the ongoing work on AI awareness and\nliteracy by bringing in perspectives from beyond a formal educational context.\n","authors":["Ashish Hingle","Aditya Johri"],"pdf_url":"https://arxiv.org/pdf/2410.18845v1.pdf","comment":"Accepted and presented at the Frontiers in Education 2024 (FIE2024)"},{"id":"http://arxiv.org/abs/2410.18810v1","updated":"2024-10-24T14:57:46Z","published":"2024-10-24T14:57:46Z","title":"TangibleChannel: An Innovative Data Physicalization System for Visual\n  Channel Education","summary":"  In this paper, we provide an overview of our attempts to harness data\nphysicalizations as pedagogical tools for enhancing the understanding of visual\nchannels. We first elaborate the research goals that we have crafted for the\nphysicalization prototype, shedding light on the key principles that guided our\ndesign choices. Then we detail the materials and datasets we employed for nine\nchannels on our physicalization prototype. A preliminary pilot study is\nfollowed to validate its effectiveness. In the end, we present our upcoming\nresearch initiatives, including a comparative study for assessing the usability\nof the physicalization system. In general, the main purpose of our work is to\nstimulate a wider engagement among visualization educators and researchers,\nencouraging them to delve into the potentialities of data physicalization as an\ninnovative addition to contemporary teaching methodologies.\n","authors":["Siqi Xie","Yu Liu","Lingyun Yu"],"pdf_url":"https://arxiv.org/pdf/2410.18810v1.pdf","comment":"2 pages, 1 figure, IEEE VIS 2023 Poster"},{"id":"http://arxiv.org/abs/2301.09902v3","updated":"2024-10-24T13:38:34Z","published":"2023-01-24T10:19:24Z","title":"Investigating Labeler Bias in Face Annotation for Machine Learning","summary":"  In a world increasingly reliant on artificial intelligence, it is more\nimportant than ever to consider the ethical implications of artificial\nintelligence on humanity. One key under-explored challenge is labeler bias,\nwhich can create inherently biased datasets for training and subsequently lead\nto inaccurate or unfair decisions in healthcare, employment, education, and law\nenforcement. Hence, we conducted a study to investigate and measure the\nexistence of labeler bias using images of people from different ethnicities and\nsexes in a labeling task. Our results show that participants possess\nstereotypes that influence their decision-making process and that labeler\ndemographics impact assigned labels. We also discuss how labeler bias\ninfluences datasets and, subsequently, the models trained on them. Overall, a\nhigh degree of transparency must be maintained throughout the entire artificial\nintelligence training process to identify and correct biases in the data as\nearly as possible.\n","authors":["Luke Haliburton","Sinksar Ghebremedhin","Robin Welsch","Albrecht Schmidt","Sven Mayer"],"pdf_url":"https://arxiv.org/pdf/2301.09902v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18727v1","updated":"2024-10-24T13:33:23Z","published":"2024-10-24T13:33:23Z","title":"Breaking Down the Barriers: Investigating Non-Expert User Experiences in\n  Robotic Teleoperation in UK and Japan","summary":"  Robots are being created each year with the goal of integrating them into our\ndaily lives. As such, there is an interest in research in evaluating the trust\nof humans toward robots. In addition, teleoperating robotic arms can be\nchallenging for non-experts. In order to reduce the strain put on the user, we\ncreated TELESIM, a modular and plug-and-play framework that enables direct\nteleoperation of any robotic arm using a digital twin as the interface between\nusers and the robotic system. However, analysis of the strain put on the user\nand its ability to trust robots was omitted. This paper addresses these\nomissions by presenting the additional results of our user survey of 37\nparticipants carried out in UK. In addition, we present the results of an\nadditional user survey, under similar conditions performed in Japan, with the\ngoal of addressing the limitations of our previous approach, by interfacing a\nVR controller with a UR5e. Our experimental results show that the UR5e has a\nhigher number of towers built. Additionally, the UR5e gives the least amount of\ncognitive stress, while the combination of Senseglove and UR3 gives the user\nthe highest physical strain and causes the user to feel more frustrated.\nFinally, Japanese seems more trusting towards robots than British.\n","authors":["Florent P Audonnet","Andrew Hamilton","Yakiyasu Domae","Ixchel G Ramirez-Alpizar","Gerardo Aragon-Camarasa"],"pdf_url":"https://arxiv.org/pdf/2410.18727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18723v1","updated":"2024-10-24T13:28:40Z","published":"2024-10-24T13:28:40Z","title":"VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose\n  Estimation","summary":"  In the rapidly evolving field of computer vision, the task of accurately\nestimating the poses of multiple individuals from various viewpoints presents a\nformidable challenge, especially if the estimations should be reliable as well.\nThis work presents an extensive evaluation of the generalization capabilities\nof multi-view multi-person pose estimators to unseen datasets and presents a\nnew algorithm with strong performance in this task. It also studies the\nimprovements by additionally using depth information. Since the new approach\ncan not only generalize well to unseen datasets, but also to different\nkeypoints, the first multi-view multi-person whole-body estimator is presented.\nTo support further research on those topics, all of the work is publicly\naccessible.\n","authors":["Daniel Bermuth","Alexander Poeppel","Wolfgang Reif"],"pdf_url":"https://arxiv.org/pdf/2410.18723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18502v1","updated":"2024-10-24T07:46:06Z","published":"2024-10-24T07:46:06Z","title":"The Senses Considered as One Perceptual System","summary":"  J. J. Gibson (1966) rejected many classical assumptions about perception but\nretained 1 that dates back to classical antiquity: the assumption of separate\nsenses. We suggest that Gibson's retention of this assumption compromised his\nnovel concept of perceptual systems. We argue that lawful, 1:1 specification of\nthe animal--environment interaction, which is necessary for perception to be\ndirect, cannot exist in individual forms of ambient energy, such as light, or\nsound. We argue that specification exists exclusively in emergent, higher order\npatterns that extend across different forms of ambient energy. These emergent,\nhigher order patterns constitute the global array. If specification exists\nexclusively in the global array, then direct perception cannot be based upon\ndetection of patterns that are confined to individual forms of ambient energy\nand, therefore, Gibson's argument for the existence of several distinct\nperceptual systems cannot be correct. We argue that the senses function as a\nsingle, irreducible perceptual system that is sensitive exclusively to patterns\nin the global array. That is, rather than distinct perceptual systems there\nexists only 1 perceptual system.\n","authors":["Thomas Stoffregen","Bruno Mantel","Beno√Æt G. Bardy"],"pdf_url":"https://arxiv.org/pdf/2410.18502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18373v1","updated":"2024-10-24T02:32:56Z","published":"2024-10-24T02:32:56Z","title":"UGotMe: An Embodied System for Affective Human-Robot Interaction","summary":"  Equipping humanoid robots with the capability to understand emotional states\nof human interactants and express emotions appropriately according to\nsituations is essential for affective human-robot interaction. However,\nenabling current vision-aware multimodal emotion recognition models for\naffective human-robot interaction in the real-world raises embodiment\nchallenges: addressing the environmental noise issue and meeting real-time\nrequirements. First, in multiparty conversation scenarios, the noises inherited\nin the visual observation of the robot, which may come from either 1)\ndistracting objects in the scene or 2) inactive speakers appearing in the field\nof view of the robot, hinder the models from extracting emotional cues from\nvision inputs. Secondly, realtime response, a desired feature for an\ninteractive system, is also challenging to achieve. To tackle both challenges,\nwe introduce an affective human-robot interaction system called UGotMe designed\nspecifically for multiparty conversations. Two denoising strategies are\nproposed and incorporated into the system to solve the first issue.\nSpecifically, to filter out distracting objects in the scene, we propose\nextracting face images of the speakers from the raw images and introduce a\ncustomized active face extraction strategy to rule out inactive speakers. As\nfor the second issue, we employ efficient data transmission from the robot to\nthe local server to improve realtime response capability. We deploy UGotMe on a\nhuman robot named Ameca to validate its real-time inference capabilities in\npractical scenarios. Videos demonstrating real-world deployment are available\nat https://pi3-141592653.github.io/UGotMe/.\n","authors":["Peizhen Li","Longbing Cao","Xiao-Ming Wu","Xiaohan Yu","Runze Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18373v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.03223v5","updated":"2024-10-24T01:30:10Z","published":"2024-01-06T14:23:18Z","title":"An intelligent sociotechnical systems (iSTS) framework: Enabling a\n  hierarchical human-centered AI (hHCAI) approach","summary":"  While artificial intelligence (AI) offers significant benefits, it also has\nnegatively impacted humans and society. A human-centered AI (HCAI) approach has\nbeen proposed to address these issues. However, current HCAI practices have\nshown limited contributions due to a lack of sociotechnical thinking. To\novercome these challenges, we conducted a literature review and comparative\nanalysis of sociotechnical characteristics with respect to AI. Then, we propose\nupdated sociotechnical systems (STS) design principles. Based on these\nfindings, this paper introduces an intelligent sociotechnical systems (iSTS)\nframework to extend traditional STS theory and meet the demands with respect to\nAI. The iSTS framework emphasizes human-centered joint optimization across\nindividual, organizational, ecosystem, and societal levels. The paper further\nintegrates iSTS with current HCAI practices, proposing a hierarchical HCAI\n(hHCAI) approach. This hHCAI approach offers a structured approach to address\nchallenges in HCAI practices from a broader sociotechnical perspective.\nFinally, we provide recommendations for future iSTS and hHCAI work.\n","authors":["Wei Xu","Zaifeng Gao"],"pdf_url":"https://arxiv.org/pdf/2401.03223v5.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.04663v2","updated":"2024-10-24T21:42:20Z","published":"2024-10-07T00:22:07Z","title":"Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates","summary":"  This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.\n","authors":["Chaithanya Bandi","Abir Harrasse"],"pdf_url":"https://arxiv.org/pdf/2410.04663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00733v2","updated":"2024-10-24T21:38:09Z","published":"2024-03-31T16:34:28Z","title":"Smooth Information Gathering in Two-Player Noncooperative Games","summary":"  We present a mathematical framework for modeling two-player noncooperative\ngames in which one player is uncertain of the other player's costs but can\npreemptively allocate information-gathering resources to reduce this\nuncertainty. We refer to the players as the uncertain player (UP) and the\ncertain player (CP), respectively. We obtain UP's decisions by solving a\ntwo-stage problem where, in Stage 1, UP allocates information-gathering\nresources that smoothly transform the information structure in the second\nstage. Then, in Stage 2, a signal (that is, a function of the Stage 1\nallocation) informs UP about CP's costs, and both players execute strategies\nwhich depend upon the signal's value. This framework allows for a smooth\nresource allocation, in contrast to existing literature on the topic. We also\nidentify conditions under which the gradient of UP's overall cost with respect\nto the information-gathering resources is well-defined. Then we provide a\ngradient-based algorithm to solve the two-stage game. Finally, we apply our\nframework to a tower-defense game which can be interpreted as a variant of a\nColonel Blotto game with smooth payoff functions and uncertainty over\nbattlefield valuations. We include an analysis of how optimal decisions shift\nwith changes in information-gathering allocations and perturbations in the cost\nfunctions.\n","authors":["Fernando Palafox","Jesse Milzman","Dong Ho Lee","Ryan Park","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2404.00733v2.pdf","comment":"https://github.com/CLeARoboticsLab/GamesVoI.jl"},{"id":"http://arxiv.org/abs/2410.19112v1","updated":"2024-10-24T19:27:05Z","published":"2024-10-24T19:27:05Z","title":"Distributed Blind Source Separation based on FastICA","summary":"  With the emergence of wireless sensor networks (WSNs), many traditional\nsignal processing tasks are required to be computed in a distributed fashion,\nwithout transmissions of the raw data to a centralized processing unit, due to\nthe limited energy and bandwidth resources available to the sensors. In this\npaper, we propose a distributed independent component analysis (ICA) algorithm,\nwhich aims at identifying the original signal sources based on observations of\ntheir mixtures measured at various sensor nodes. One of the most commonly used\nICA algorithms is known as FastICA, which requires a spatial pre-whitening\noperation in the first step of the algorithm. Such a pre-whitening across all\nnodes of a WSN is impossible in a bandwidth-constrained distributed setting as\nit requires to correlate each channel with each other channel in the WSN. We\nshow that an explicit network-wide pre-whitening step can be circumvented by\nleveraging the properties of the so-called Distributed Adaptive Signal Fusion\n(DASF) framework. Despite the lack of such a network-wide pre-whitening, we can\nstill obtain the $Q$ least Gaussian independent components of the centralized\nICA solution, where $Q$ scales linearly with the required communication load.\n","authors":["Cem Ates Musluoglu","Alexander Bertrand"],"pdf_url":"https://arxiv.org/pdf/2410.19112v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2405.02835v2","updated":"2024-10-24T17:03:11Z","published":"2024-05-05T07:23:26Z","title":"Algorithmic collusion in a two-sided market: A rideshare example","summary":"  With dynamic pricing on the rise, firms are using sophisticated algorithms\nfor price determination. These algorithms are often non-interpretable and there\nhas been a recent interest in their seemingly emergent ability to tacitly\ncollude with each other without any prior communication whatsoever. Most of the\nprevious works investigate algorithmic collusion on simple reinforcement\nlearning (RL) based algorithms operating on a basic market model. Instead, we\nexplore the collusive tendencies of Proximal Policy Optimization (PPO), a\nstate-of-the-art continuous state/action space RL algorithm, on a complex\ndouble-sided hierarchical market model of rideshare. For this purpose, we\nextend a mathematical program network (MPN) based rideshare model to a temporal\nmulti origin-destination setting and use PPO to solve for a repeated duopoly\ngame. Our results indicate that PPO can either converge to a competitive or a\ncollusive equilibrium depending upon the underlying market characteristics,\neven when the hyper-parameters are held constant.\n","authors":["Pravesh Koirala","Forrest Laine"],"pdf_url":"https://arxiv.org/pdf/2405.02835v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16197v3","updated":"2024-10-24T16:13:21Z","published":"2024-10-21T17:00:03Z","title":"LASER: Script Execution by Autonomous Agents for On-demand Traffic\n  Simulation","summary":"  Autonomous Driving Systems (ADS) require diverse and safety-critical traffic\nscenarios for effective training and testing, but the existing data generation\nmethods struggle to provide flexibility and scalability. We propose LASER, a\nnovel frame-work that leverage large language models (LLMs) to conduct traffic\nsimulations based on natural language inputs. The framework operates in two\nstages: it first generates scripts from user-provided descriptions and then\nexecutes them using autonomous agents in real time. Validated in the CARLA\nsimulator, LASER successfully generates complex, on-demand driving scenarios,\nsignificantly improving ADS training and testing data generation.\n","authors":["Hao Gao","Jingyue Wang","Wenyang Fang","Jingwei Xu","Yunpeng Huang","Taolue Chen","Xiaoxing Ma"],"pdf_url":"https://arxiv.org/pdf/2410.16197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18871v1","updated":"2024-10-24T15:58:14Z","published":"2024-10-24T15:58:14Z","title":"Learning Collusion in Episodic, Inventory-Constrained Markets","summary":"  Pricing algorithms have demonstrated the capability to learn tacit collusion\nthat is largely unaddressed by current regulations. Their increasing use in\nmarkets, including oligopolistic industries with a history of collusion, calls\nfor closer examination by competition authorities. In this paper, we extend the\nstudy of tacit collusion in learning algorithms from basic pricing games to\nmore complex markets characterized by perishable goods with fixed supply and\nsell-by dates, such as airline tickets, perishables, and hotel rooms. We\nformalize collusion within this framework and introduce a metric based on price\nlevels under both the competitive (Nash) equilibrium and collusive\n(monopolistic) optimum. Since no analytical expressions for these price levels\nexist, we propose an efficient computational approach to derive them. Through\nexperiments, we demonstrate that deep reinforcement learning agents can learn\nto collude in this more complex domain. Additionally, we analyze the underlying\nmechanisms and structures of the collusive strategies these agents adopt.\n","authors":["Paul Friedrich","Barna P√°sztor","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2410.18871v1.pdf","comment":"37 pages, 25 figures. Under review"},{"id":"http://arxiv.org/abs/2410.17351v2","updated":"2024-10-24T15:57:45Z","published":"2024-10-22T18:35:05Z","title":"Hierarchical Multi-agent Reinforcement Learning for Cyber Network\n  Defense","summary":"  Recent advances in multi-agent reinforcement learning (MARL) have created\nopportunities to solve complex real-world tasks. Cybersecurity is a notable\napplication area, where defending networks against sophisticated adversaries\nremains a challenging task typically performed by teams of security operators.\nIn this work, we explore novel MARL strategies for building autonomous cyber\nnetwork defenses that address challenges such as large policy spaces, partial\nobservability, and stealthy, deceptive adversarial strategies. To facilitate\nefficient and generalized learning, we propose a hierarchical Proximal Policy\nOptimization (PPO) architecture that decomposes the cyber defense task into\nspecific sub-tasks like network investigation and host recovery. Our approach\ninvolves training sub-policies for each sub-task using PPO enhanced with domain\nexpertise. These sub-policies are then leveraged by a master defense policy\nthat coordinates their selection to solve complex network defense tasks.\nFurthermore, the sub-policies can be fine-tuned and transferred with minimal\ncost to defend against shifts in adversarial behavior or changes in network\nsettings. We conduct extensive experiments using CybORG Cage 4, the\nstate-of-the-art MARL environment for cyber defense. Comparisons with multiple\nbaselines across different adversaries show that our hierarchical learning\napproach achieves top performance in terms of convergence speed, episodic\nreturn, and several interpretable metrics relevant to cybersecurity, including\nthe fraction of clean machines on the network, precision, and false positives\non recoveries.\n","authors":["Aditya Vikram Singh","Ethan Rathbun","Emma Graham","Lisa Oakley","Simona Boboila","Alina Oprea","Peter Chin"],"pdf_url":"https://arxiv.org/pdf/2410.17351v2.pdf","comment":"9 pages, 7 figures, AAMAS preprint"},{"id":"http://arxiv.org/abs/2310.09580v4","updated":"2024-10-24T13:41:15Z","published":"2023-10-14T13:09:49Z","title":"Where to Decide? Centralized vs. Distributed Vehicle Assignment for\n  Platoon Formation","summary":"  Platooning is a promising cooperative driving application for future\nintelligent transportation systems. In order to assign vehicles to platoons,\nsome algorithm for platoon formation is required. Such vehicle-to-platoon\nassignments have to be computed on-demand, e.g., when vehicles join or leave\nthe freeways. In order to get best results from platooning, individual\nproperties of involved vehicles have to be considered during the assignment\ncomputation. In this paper, we explore the computation of vehicle-to-platoon\nassignments as an optimization problem based on similarity between vehicles. We\ndefine the similarity and, vice versa, the deviation among vehicles based on\nthe desired driving speed of vehicles and their position on the road. We create\nthree approaches to solve this assignment problem: centralized solver,\ncentralized greedy, and distributed greedy, using a Mixed Integer Programming\n(MIP) solver and greedy heuristics, respectively. Conceptually, the approaches\ndiffer in both knowledge about vehicles as well as methodology. We perform a\nlarge-scale simulation study using PlaFoSim to compare all approaches. While\nthe distributed greedy approach seems to have disadvantages due to the limited\nlocal knowledge, it performs as good as the centralized solver approach across\nmost metrics. Both outperform the centralized greedy approach, which suffers\nfrom synchronization and greedy selection effects. The centralized solver\napproach however assumes global knowledge and requires a complex MIP solver to\ncompute vehicle-to-platoon assignments. Overall, the distributed greedy\napproach achieves close to optimal results but requires the least assumptions\nand complexity. Therefore, we consider the distributed greedy approach the best\napproach among all presented approaches.\n","authors":["Julian Heinovski","Falko Dressler"],"pdf_url":"https://arxiv.org/pdf/2310.09580v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18631v1","updated":"2024-10-24T10:43:04Z","published":"2024-10-24T10:43:04Z","title":"Leveraging Graph Neural Networks and Multi-Agent Reinforcement Learning\n  for Inventory Control in Supply Chains","summary":"  Inventory control in modern supply chains has attracted significant attention\ndue to the increasing number of disruptive shocks and the challenges posed by\ncomplex dynamics, uncertainties, and limited collaboration. Traditional\nmethods, which often rely on static parameters, struggle to adapt to changing\nenvironments. This paper proposes a Multi-Agent Reinforcement Learning (MARL)\nframework with Graph Neural Networks (GNNs) for state representation to address\nthese limitations.\n  Our approach redefines the action space by parameterizing heuristic inventory\ncontrol policies, making it adaptive as the parameters dynamically adjust based\non system conditions. By leveraging the inherent graph structure of supply\nchains, our framework enables agents to learn the system's topology, and we\nemploy a centralized learning, decentralized execution scheme that allows\nagents to learn collaboratively while overcoming information-sharing\nconstraints. Additionally, we incorporate global mean pooling and\nregularization techniques to enhance performance.\n  We test the capabilities of our proposed approach on four different supply\nchain configurations and conduct a sensitivity analysis. This work paves the\nway for utilizing MARL-GNN frameworks to improve inventory management in\ncomplex, decentralized supply chain environments.\n","authors":["Niki Kotecha","Antonio del Rio Chanona"],"pdf_url":"https://arxiv.org/pdf/2410.18631v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.19388v2","updated":"2024-10-24T17:56:21Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds is a challenging task due to data scarcity and\noften insufficient caption quality, making it difficult to employ large-scale\ngenerative models for the task. In this work, we tackle this problem by\nintroducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. By using a compact audio\nrepresentation and leveraging audio metadata, AutoCap substantially enhances\ncaption quality, reaching a CIDEr score of 83.2, marking a 3.2% improvement\nfrom the best available captioning model at four times faster inference speed.\nSecond, we propose GenAu, a scalable transformer-based audio generation\narchitecture that we scale up to 1.25B parameters. Using AutoCap to generate\ncaption clips from existing audio datasets, we demonstrate the benefits of data\nscaling with synthetic captions as well as model size scaling. When compared to\nstate-of-the-art audio generators trained at similar size and data scale, GenAu\nobtains significant improvements of 4.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. Moreover, we propose an efficient and scalable\npipeline for collecting audio datasets, enabling us to compile 57M ambient\naudio clips, forming AutoReCap-XL, the largest available audio-text dataset, at\n90 times the scale of existing ones. Our code, model checkpoints, and dataset\nare publicly available.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v2.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"}]},"2024-10-23T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.17593v1","updated":"2024-10-23T06:44:13Z","published":"2024-10-23T06:44:13Z","title":"Pillow Box Design","summary":"  This paper focuses on packaging design using origami techniques, specifically\ndesigns incorporating curves, known as pillow boxes. While conventional paper\npackaging boxes are typically cuboid, pillow box designs include curved\nsurfaces, offering both aesthetic and practical advantages. This study analyzes\nthe specific curved folds of pillow boxes, clarifying the fundamental geometric\ncondition these curves must meet. Additionally, it proposes new design\nvariations for pillow boxes based on the condition. The relationship between\nthe shape of the folds and the volume of the final three-dimensional shape is\nalso explored. This research extends the boundaries of functionality and\naesthetics in origami design and explores new possibilities in packaging\nsolutions.\n","authors":["Jun Mitani"],"pdf_url":"https://arxiv.org/pdf/2410.17593v1.pdf","comment":"Submitted to the Proceedings of 8OSME"},{"id":"http://arxiv.org/abs/2410.15715v2","updated":"2024-10-23T05:02:39Z","published":"2024-10-21T07:34:04Z","title":"Timetable Nodes for Public Transport Network","summary":"  Faster pathfinding in time-dependent transport networks is an important and\nchallenging problem in navigation systems. There are two main types of\ntransport networks: road networks for car driving and public transport route\nnetwork. The solutions that work well in road networks, such as Time-dependent\nContraction Hierarchies and other graph-based approaches, do not usually apply\nin transport networks. In transport networks, non-graph solutions such as CSA\nand RAPTOR show the best results compared to graph-based techniques. In our\nwork, we propose a method that advances graph-based approaches by using\ndifferent optimization techniques from computational geometry to speed up the\nsearch process in transport networks. We apply a new pre-computation step,\nwhich we call timetable nodes (TTN). Our inspiration comes from an iterative\nsearch problem in computational geometry. We implement two versions of the TTN:\none uses a Combined Search Tree (TTN-CST), and the second uses Fractional\nCascading (TTN-FC). Both of these approaches decrease the asymptotic complexity\nof reaching new nodes from $O(k\\times \\log|C|)$ to $O(k + \\log(k) +\n\\log(|C|))$, where $k$ is the number of outgoing edges from a node and $|C|$ is\nthe size of the timetable information (total outgoing edges). Our solution\nsuits any other time-dependent networks and can be integrated into other\npathfinding algorithms. Our experiments indicate that this pre-computation\nsignificantly enhances the performance on high-density graphs. This study\nshowcases how leveraging computational geometry can enhance pathfinding in\ntransport networks, enabling faster pathfinding in scenarios involving large\nnumbers of outgoing edges.\n","authors":["Andrii Rohovyi","Peter J. Stuckey","Toby Walsh"],"pdf_url":"https://arxiv.org/pdf/2410.15715v2.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.06877v2","updated":"2024-10-23T22:41:54Z","published":"2024-10-09T13:42:37Z","title":"Best-of-Both-Worlds Fair Allocation of Indivisible and Mixed Goods","summary":"  We study the problem of fairly allocating either a set of indivisible goods\nor a set of mixed divisible and indivisible goods (i.e., mixed goods) to agents\nwith additive utilities, taking the best-of-both-worlds perspective of\nguaranteeing fairness properties both ex ante and ex post. The ex-post fairness\nnotions considered in this paper are relaxations of envy-freeness,\nspecifically, EFX for indivisible-goods allocation, and EFM for mixed-goods\nallocation. For two agents, we show that there is a polynomial-time randomized\nalgorithm that achieves ex-ante envy-freeness and ex-post EFX / EFM\nsimultaneously. For $n$ agents with bi-valued utilities, we show there exist\nrandomized allocations that are (i) ex-ante proportional and ex-post EFM, and\n(ii) ex-ante envy-free, ex-post EFX, and ex-post fractionally Pareto optimal.\n","authors":["Xiaolin Bu","Zihao Li","Shengxin Liu","Xinhang Lu","Biaoshuai Tao"],"pdf_url":"https://arxiv.org/pdf/2410.06877v2.pdf","comment":"Appears in the 20th Conference on Web and Internet Economics (WINE),\n  2024"},{"id":"http://arxiv.org/abs/2202.10986v2","updated":"2024-10-23T16:47:13Z","published":"2022-02-22T15:40:49Z","title":"Optimal Bailouts and Strategic Debt Forgiveness in Financial Networks","summary":"  A financial system is represented by a network, where nodes correspond to\nbanks, and directed labeled edges correspond to debt contracts between banks.\nOnce a payment schedule has been defined, where we assume that a bank cannot\nrefuse a payment towards one of its lenders if it has sufficient funds, the\nliquidity of the system is defined as the sum of total payments made in the\nnetwork. Maximizing systemic liquidity is a natural objective of any financial\nauthority, so, we study the setting where the financial authority offers\nbailout money to some bank(s) or forgives the debts of others in order to\nmaximize liquidity, and examine efficient ways to achieve this. We investigate\nthe approximation ratio provided by the greedy bailout policy compared to the\noptimal one, and we study the computational hardness of finding the optimal\ndebt-removal and budget-constrained optimal bailout policy, respectively.\n  We also study financial systems from a game-theoretic standpoint. We observe\nthat the removal of some incoming debt might be in the best interest of a bank,\nif that helps one of its borrowers remain solvent and avoid costs related to\ndefault. Assuming that a bank's well-being (i.e., utility) is aligned with the\nincoming payments they receive from the network, we define and analyze a game\namong banks who want to maximize their utility by strategically giving up some\nincoming payments. In addition, we extend the previous game by considering\nbailout payments. After formally defining the above games, we prove results\nabout the existence and quality of pure Nash equilibria, as well as the\ncomputational complexity of finding such equilibria.\n","authors":["Panagiotis Kanellopoulos","Maria Kyropoulou","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2202.10986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12648v2","updated":"2024-10-23T13:56:34Z","published":"2024-06-18T14:15:23Z","title":"Mitigating Information Asymmetry in Two-Stage Contracts with Non-Myopic\n  Agents","summary":"  We consider a Stackelberg game in which a principal (she) establishes a\ntwo-stage contract with a non-myopic agent (he) whose type is unknown. The\ncontract takes the form of an incentive function mapping the agent's\nfirst-stage action to his second-stage incentive. While the first-stage action\nreveals the agent's type under truthful play, a non-myopic agent could benefit\nfrom portraying a false type in the first stage to obtain a larger incentive in\nthe second stage. The challenge is thus for the principal to design the\nincentive function so as to induce truthful play. We show that this is only\npossible with a constant, non-reactive incentive functions when the type space\nis continuous, whereas it can be achieved with reactive functions for discrete\ntypes. Additionally, we show that introducing an adjustment mechanism that\npenalizes inconsistent behavior across both stages allows the principal to\ndesign more flexible incentive functions.\n","authors":["Munther A. Dahleh","Thibaut Horel","M. Umar B. Niazi"],"pdf_url":"https://arxiv.org/pdf/2406.12648v2.pdf","comment":"To appear in the Proceedings of the 5th IFAC Workshop on\n  Cyber-Physical Human Systems"},{"id":"http://arxiv.org/abs/2410.17690v1","updated":"2024-10-23T09:13:02Z","published":"2024-10-23T09:13:02Z","title":"Markov Potential Game with Final-time Reach-Avoid Objectives","summary":"  We formulate a Markov potential game with final-time reach-avoid objectives\nby integrating potential game theory with stochastic reach-avoid control. Our\nfocus is on multi-player trajectory planning where players maximize the same\nmulti-player reach-avoid objective: the probability of all participants\nreaching their designated target states by a specified time, while avoiding\ncollisions with one another. Existing approaches require centralized\ncomputation of actions via a global policy, which may have prohibitively\nexpensive communication costs. Instead, we focus on approximations of the\nglobal policy via local state feedback policies. First, we adapt the recursive\nsingle player reach-avoid value iteration to the multi-player framework with\nlocal policies, and show that the same recursion holds on the joint state\nspace. To find each player's optimal local policy, the multi-player reach-avoid\nvalue function is projected from the joint state to the local state using the\nother players' occupancy measures. Then, we propose an iterative best response\nscheme for the multi-player value iteration to converge to a pure Nash\nequilibrium. We demonstrate the utility of our approach in finding\ncollision-free policies for multi-player motion planning in simulation.\n","authors":["Sarah H. Q. Li","Abraham P. Vinod"],"pdf_url":"https://arxiv.org/pdf/2410.17690v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.17517v1","updated":"2024-10-23T02:49:37Z","published":"2024-10-23T02:49:37Z","title":"Bridging Swarm Intelligence and Reinforcement Learning","summary":"  Swarm intelligence (SI) explores how large groups of simple individuals\n(e.g., insects, fish, birds) collaborate to produce complex behaviors,\nexemplifying that the whole is greater than the sum of its parts. A fundamental\ntask in SI is Collective Decision-Making (CDM), where a group selects the best\noption among several alternatives, such as choosing an optimal foraging site.\nIn this work, we demonstrate a theoretical and empirical equivalence between\nCDM and single-agent reinforcement learning (RL) in multi-armed bandit\nproblems, utilizing concepts from opinion dynamics, evolutionary game theory,\nand RL. This equivalence bridges the gap between SI and RL and leads us to\nintroduce a novel abstract RL update rule called Maynard-Cross Learning.\nAdditionally, it provides a new population-based perspective on common RL\npractices like learning rate adjustment and batching. Our findings enable\ncross-disciplinary fertilization between RL and SI, allowing techniques from\none field to enhance the understanding and methodologies of the other.\n","authors":["Karthik Soma","Yann Bouteiller","Heiko Hamann","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17517v1.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2301.03009v4","updated":"2024-10-23T21:40:14Z","published":"2023-01-08T10:02:56Z","title":"Comparing Three Generations of D-Wave Quantum Annealers for Minor\n  Embedded Combinatorial Optimization Problems","summary":"  Quantum annealing is a novel type of analog computation that aims to use\nquantum mechanical fluctuations to search for optimal solutions of Ising\nproblems. Quantum annealing in the Transverse Ising model, implemented on\nD-Wave QPUs, are available as cloud computing resources. In this article we\nreport concise benchmarks across three generations of D-Wave quantum annealers,\nconsisting of four different devices, for the NP-Hard combinatorial\noptimization problems unweighted maximum clique and unweighted maximum cut on\nrandom graphs. The Ising, or equivalently QUBO, formulation of these problems\ndo not require auxiliary variables for order reduction, and their overall\nstructure and weights are not highly complex, which makes these problems simple\ntest cases to understand the sampling capability of current D-Wave quantum\nannealers. All-to-all minor embeddings of size $52$, with relatively uniform\nchain lengths, are used for a direct comparison across the Chimera, Pegasus,\nand Zephyr device topologies. A grid search over annealing times and the minor\nembedding chain strengths is performed in order to determine the level of\nreasonable performance for each device and problem type. Experiment metrics\nthat are reported are approximation ratios for non-broken chain samples and\nchain break proportions. How fairly the quantum annealers sample optimal\nmaximum cliques, for instances which contain multiple maximum cliques, is also\nquantified using entropy of the measured ground state distributions. The newest\ngeneration of quantum annealing hardware, which has a Zephyr hardware\nconnectivity, performed the best overall with respect to approximation ratios\nand chain break frequencies.\n","authors":["Elijah Pelofske"],"pdf_url":"https://arxiv.org/pdf/2301.03009v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18048v1","updated":"2024-10-23T17:20:35Z","published":"2024-10-23T17:20:35Z","title":"A Comparative Assessment of Technology Acceptance and Learning Outcomes\n  in Computer-based versus VR-based Pedagogical Agents","summary":"  As educational technology evolves, the potential of Pedagogical Agents (PAs)\nin supporting education is extensively explored. Typically, research on PAs has\nprimarily focused on computer-based learning environments, but their use in\nVR-based environments and integration into education is still in its infancy.\nTo address this gap, this paper presents a mixed method comparative study that\nhas been conducted to evaluate and examine how these computer-based PAs and\nVR-based PAs compare, towards their learning efficacy and technology\nacceptance. 92 Computing and Engineering undergraduate students were recruited\nand participated in an educational experience focusing on computing machinery\neducation. The findings of this study revealed that both approaches can\neffectively facilitate learning acquisition, and both technologies have been\npositively perceived by participants toward acceptance, without any significant\ndifferences. The findings of this study shed light on the potential of\nutilizing intelligent PAs to support education, contributing towards the\nadvancement of our understanding of how to integrate such technologies to\ndevelop learning interventions, and establishing the foundation for future\ninvestigations that aim to successfully integrate and use PAs in education.\n","authors":["Aimilios Hadjiliasi","Louis Nisiotis","Irene Polycarpou"],"pdf_url":"https://arxiv.org/pdf/2410.18048v1.pdf","comment":"Accepted as 4-pages poster paper in 23rd IEEE International Symposium\n  on Mixed and Augmented Reality (ISMAR), 21-25/10/2024"},{"id":"http://arxiv.org/abs/2410.17876v1","updated":"2024-10-23T13:49:25Z","published":"2024-10-23T13:49:25Z","title":"Fast classical simulation of qubit-qudit hybrid systems","summary":"  Simulating quantum circuits is a computationally intensive task that relies\nheavily on tensor products and matrix multiplications, which can be\ninefficient. Recent advancements, eliminate the need for tensor products and\nmatrix multiplications, offering significant improvements in efficiency and\nparallelization. Extending these optimizations, we adopt a block-simulation\nmethodology applicable to qubit-qudit hybrid systems. This method interprets\nthe statevector as a collection of blocks and applies gates without computing\nthe entire circuit unitary. Our method, a spiritual successor of the simulator\nQuDiet \\cite{Chatterjee_2023}, utilizes this block-simulation method, thereby\ngaining major improvements over the simulation methods used by its predecessor.\nWe exhibit that the proposed method is approximately 10$\\times$ to 1000$\\times$\nfaster than the state-of-the-art simulator for simulating multi-level quantum\nsystems with various benchmark circuits.\n","authors":["Haemanth Velmurugan","Arnav Das","Turbasu Chatterjee","Amit Saha","Anupam Chattopadhyay","Amlan Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2410.17876v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.17784v1","updated":"2024-10-23T11:34:29Z","published":"2024-10-23T11:34:29Z","title":"Holon Programming Model -- A Software-Defined Approach for System of\n  Systems","summary":"  As Systems of Systems evolve into increasingly complex networks, harnessing\ntheir collective potential becomes paramount. Traditional SoS engineering\napproaches lack the necessary programmability to develop third party SoS level\nbehaviors. To address this challenge, we propose a software defined approach to\nenable flexible and adaptive programming of SoS. We introduce the Holon\nProgramming Model, a software-defined framework designed to meet these needs.\nThe Holon Programming Model empowers developers to design and orchestrate\ncomplex system behaviors effectively, as illustrated in our disaster management\nscenario. This research outlines the Holon Programming Model theoretical\nunderpinnings and practical applications, with the aim of driving further\nexploration and advancement in the field of software defined SoS\n","authors":["Muhammad Ashfaq","Ahmed R. Sadik","Tommi Mikkonen","Muhammad Waseem","Niko Makitalo"],"pdf_url":"https://arxiv.org/pdf/2410.17784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09121v2","updated":"2024-10-23T02:55:57Z","published":"2024-10-11T00:14:31Z","title":"Comparing Quantum Encoding Techniques","summary":"  As quantum computers continue to become more capable, the possibilities of\ntheir applications increase. For example, quantum techniques are being\nintegrated with classical neural networks to perform machine learning. In order\nto be used in this way, or for any other widespread use like quantum chemistry\nsimulations or cryptographic applications, classical data must be converted\ninto quantum states through quantum encoding. There are three fundamental\nencoding methods: basis, amplitude, and rotation, as well as several proposed\ncombinations. This study explores the encoding methods, specifically in the\ncontext of hybrid quantum-classical machine learning. Using the QuClassi\nquantum neural network architecture to perform binary classification of the `3'\nand `6' digits from the MNIST datasets, this study obtains several metrics such\nas accuracy, entropy, loss, and resistance to noise, while considering resource\nusage and computational complexity to compare the three main encoding methods.\n","authors":["Nidhi Munikote"],"pdf_url":"https://arxiv.org/pdf/2410.09121v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2405.10565v2","updated":"2024-10-23T22:48:17Z","published":"2024-05-17T06:24:43Z","title":"Real-time Level-of-Detail Strand-based Hair Rendering","summary":"  Strand-based hair rendering has become increasingly popular in production for\nits realistic appearance. However, the prevailing level-of-detail solution\nemploying hair cards for distant hair models introduces a significant\ndiscontinuity in dynamics and appearance during the transition from strands to\ncards. We introduce an innovative real-time framework for strand-based hair\nrendering that ensures seamless transitions between different levels of detail\n(LOD) while maintaining a consistent hair appearance. Our method uses\nelliptical thick hairs that contain multiple hair strands at each LOD to\nmaintain the shapes of hair clusters. In addition to geometric fitting, we\nformulate an elliptical Bidirectional Curve Scattering Distribution Functions\n(BCSDF) model for a thick hair, accurately capturing single scattering and\nmultiple scattering within the hair cluster, accommodating a spectrum from\nsparse to dense hair distributions. Our framework, tested on various hairstyles\nwith dynamics as well as knits, shows that it can produce highly similar\nappearances to full hair geometries at different viewing distances with\nseamless LOD transitions, while achieving up to a 3x speedup.\n","authors":["Tao Huang","Yang Zhou","Daqi Lin","Junqiu Zhu","Ling-Qi Yan","Kui Wu"],"pdf_url":"https://arxiv.org/pdf/2405.10565v2.pdf","comment":"13 pages, 10 figures, 1 performance plot"},{"id":"http://arxiv.org/abs/2410.18161v1","updated":"2024-10-23T17:05:27Z","published":"2024-10-23T17:05:27Z","title":"Bridging the Diagnostic Divide: Classical Computer Vision and Advanced\n  AI methods for distinguishing ITB and CD through CTE Scans","summary":"  Differentiating between Intestinal Tuberculosis (ITB) and Crohn's Disease\n(CD) poses a significant clinical challenge due to their similar symptoms,\nclinical presentations, and imaging features. This study leverages Computed\nTomography Enterography (CTE) scans, deep learning, and traditional computer\nvision to address this diagnostic dilemma. A consensus among radiologists from\nrenowned institutions has recognized the visceral-to-subcutaneous fat (VF/SF)\nratio as a surrogate biomarker for differentiating between ITB and CD.\nPreviously done manually, we propose a novel 2D image computer vision algorithm\nfor auto-segmenting subcutaneous fat to automate this ratio calculation,\nenhancing diagnostic efficiency and objectivity. As a benchmark, we compare the\nresults to those obtained using the TotalSegmentator tool, a popular deep\nlearning-based software for automatic segmentation of anatomical structures,\nand manual calculations by radiologists. We also demonstrated the performance\non 3D CT volumes using a slicing method and provided a benchmark comparison of\nthe algorithm with the TotalSegmentator tool. Additionally, we propose a\nscoring approach to integrate scores from radiological features, such as the\nfat ratio and pulmonary TB probability, into a single score for diagnosis. We\ntrained a ResNet10 model on a dataset of CTE scans with samples from ITB, CD,\nand normal patients, achieving an accuracy of 75%. To enhance interpretability\nand gain clinical trust, we integrated the explainable AI technique Grad-CAM\nwith ResNet10 to explain the model's predictions. Due to the small dataset size\n(100 total cases), the feature-based scoring system is considered more reliable\nand trusted by radiologists compared to the deep learning model for disease\ndiagnosis.\n","authors":["Shashwat Gupta","L. Gokulnath","Akshan Aggarwal","Mahim Naz","Rajnikanth Yadav","Priyanka Bagade"],"pdf_url":"https://arxiv.org/pdf/2410.18161v1.pdf","comment":"9 pages, 3 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2410.18026v1","updated":"2024-10-23T16:57:51Z","published":"2024-10-23T16:57:51Z","title":"EON: A practical energy-preserving rough diffuse BRDF","summary":"  We introduce the \"Energy-preserving Oren--Nayar\" (EON) model for reflection\nfrom rough surfaces. Unlike the popular qualitative Oren--Nayar model (QON) and\nits variants, our model is energy-preserving via analytical energy\ncompensation. We include self-contained GLSL source code for efficient\nevaluation of the new model and importance sampling based on a novel technique\nwe term \"Clipped Linearly Transformed Cosine\" (CLTC) sampling.\n","authors":["Jamie Portsmouth","Peter Kutz","Stephen Hill"],"pdf_url":"https://arxiv.org/pdf/2410.18026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17932v1","updated":"2024-10-23T14:54:48Z","published":"2024-10-23T14:54:48Z","title":"VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian\n  Splatting and Neural Points","summary":"  Recent advances in novel view synthesis (NVS), particularly neural radiance\nfields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive\nresults in photorealistic scene rendering. These techniques hold great\npotential for applications in virtual tourism and teleportation, where\nimmersive realism is crucial. However, the high-performance demands of virtual\nreality (VR) systems present challenges in directly utilizing even such\nfast-to-render scene representations like 3DGS due to latency and computational\nconstraints.\n  In this paper, we propose foveated rendering as a promising solution to these\nobstacles. We analyze state-of-the-art NVS methods with respect to their\nrendering performance and compatibility with the human visual system. Our\napproach introduces a novel foveated rendering approach for Virtual Reality,\nthat leverages the sharp, detailed output of neural point rendering for the\nfoveal region, fused with a smooth rendering of 3DGS for the peripheral vision.\n  Our evaluation confirms that perceived sharpness and detail-richness are\nincreased by our approach compared to a standard VR-ready 3DGS configuration.\nOur system meets the necessary performance requirements for real-time VR\ninteractions, ultimately enhancing the user's immersive experience.\n  Project page: https://lfranke.github.io/vr_splatting\n","authors":["Linus Franke","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2410.17932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17858v1","updated":"2024-10-23T13:29:02Z","published":"2024-10-23T13:29:02Z","title":"Blendify -- Python rendering framework for Blender","summary":"  With the rapid growth of the volume of research fields like computer vision\nand computer graphics, researchers require effective and user-friendly\nrendering tools to visualize results. While advanced tools like Blender offer\npowerful capabilities, they also require a significant effort to master. This\ntechnical report introduces Blendify, a lightweight Python-based framework that\nseamlessly integrates with Blender, providing a high-level API for scene\ncreation and rendering. Blendify reduces the complexity of working with\nBlender's native API by automating object creation, handling the colors and\nmaterial linking, and implementing features such as shadow-catcher objects\nwhile maintaining support for high-quality ray-tracing rendering output. With a\nfocus on usability Blendify enables efficient and flexible rendering workflow\nfor rendering in common computer vision and computer graphics use cases. The\ncode is available at https://github.com/ptrvilya/blendify\n","authors":["Vladimir Guzov","Ilya A. Petrov","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2410.17858v1.pdf","comment":"Project page: https://virtualhumans.mpi-inf.mpg.de/blendify/"},{"id":"http://arxiv.org/abs/2410.17802v1","updated":"2024-10-23T11:59:49Z","published":"2024-10-23T11:59:49Z","title":"GenUDC: High Quality 3D Mesh Generation with Unsigned Dual Contouring\n  Representation","summary":"  Generating high-quality meshes with complex structures and realistic surfaces\nis the primary goal of 3D generative models. Existing methods typically employ\nsequence data or deformable tetrahedral grids for mesh generation. However,\nsequence-based methods have difficulty producing complex structures with many\nfaces due to memory limits. The deformable tetrahedral grid-based method\nMeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity\nin deformable grids. We propose the GenUDC framework to address these\nchallenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh\nrepresentation. UDC discretizes a mesh in a regular grid and divides it into\nthe face and vertex parts, recovering both complex structures and fine details.\nAs a result, the one-to-one mapping between UDC and mesh resolves the ambiguity\nproblem. In addition, GenUDC adopts a two-stage, coarse-to-fine generative\nprocess for 3D mesh generation. It first generates the face part as a rough\nshape and then the vertex part to craft a detailed shape. Extensive evaluations\ndemonstrate the superiority of UDC as a mesh representation and the favorable\nperformance of GenUDC in mesh generation. The code and trained models are\navailable at https://github.com/TrepangCat/GenUDC.\n","authors":["Ruowei Wang","Jiaqi Li","Dan Zeng","Xueqi Ma","Zixiang Xu","Jianwei Zhang","Qijun Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.17802v1.pdf","comment":"ACMMM 2024, code:https://github.com/TrepangCat/GenUDC"},{"id":"http://arxiv.org/abs/2410.17774v1","updated":"2024-10-23T11:23:05Z","published":"2024-10-23T11:23:05Z","title":"Quasi-Medial Distance Field (Q-MDF): A Robust Method for Approximating\n  and Discretizing Neural Medial Axis","summary":"  The medial axis, a lower-dimensional shape descriptor, plays an important\nrole in the field of digital geometry processing. Despite its importance,\nrobust computation of the medial axis transform from diverse inputs, especially\npoint clouds with defects, remains a significant challenge. In this paper, we\ntackle the challenge by proposing a new implicit method that diverges from\nmainstream explicit medial axis computation techniques. Our key technical\ninsight is the difference between the signed distance field (SDF) and the\nmedial field (MF) of a solid shape is the unsigned distance field (UDF) of the\nshape's medial axis. This allows for formulating medial axis computation as an\nimplicit reconstruction problem. Utilizing a modified double covering method,\nwe extract the medial axis as the zero level-set of the UDF. Extensive\nexperiments show that our method has enhanced accuracy and robustness in\nlearning compact medial axis transform from thorny meshes and point clouds\ncompared to existing methods.\n","authors":["Jiayi Kong","Chen Zong","Jun Luo","Shiqing Xin","Fei Hou","Hanqing Jiang","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2410.17774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09543v2","updated":"2024-10-23T08:16:15Z","published":"2024-06-27T01:50:11Z","title":"Neural Texture Block Compression","summary":"  Block compression is a widely used technique to compress textures in\nreal-time graphics applications, offering a reduction in storage size. However,\ntheir storage efficiency is constrained by the fixed compression ratio, which\nsubstantially increases storage size when hundreds of high-quality textures are\nrequired. In this paper, we propose a novel block texture compression method\nwith neural networks, Neural Texture Block Compression (NTBC). NTBC learns the\nmapping from uncompressed textures to block-compressed textures, which allows\nfor significantly reduced storage costs without any change in the shaders.Our\nexperiments show that NTBC can achieve reasonable-quality results with up to\nabout 70% less storage footprint, preserving real-time performance with a\nmodest computational overhead at the texture loading phase in the graphics\npipeline.\n","authors":["Shin Fujieda","Takahiro Harada"],"pdf_url":"https://arxiv.org/pdf/2407.09543v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.17610v1","updated":"2024-10-23T07:06:08Z","published":"2024-10-23T07:06:08Z","title":"ImDy: Human Inverse Dynamics from Imitated Observations","summary":"  Inverse dynamics (ID), which aims at reproducing the driven torques from\nhuman kinematic observations, has been a critical tool for gait analysis.\nHowever, it is hindered from wider application to general motion due to its\nlimited scalability. Conventional optimization-based ID requires expensive\nlaboratory setups, restricting its availability. To alleviate this problem, we\npropose to exploit the recently progressive human motion imitation algorithms\nto learn human inverse dynamics in a data-driven manner. The key insight is\nthat the human ID knowledge is implicitly possessed by motion imitators, though\nnot directly applicable. In light of this, we devise an efficient data\ncollection pipeline with state-of-the-art motion imitation algorithms and\nphysics simulators, resulting in a large-scale human inverse dynamics benchmark\nas Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint\ntorque and full-body ground reaction force data. With ImDy, we train a\ndata-driven human inverse dynamics solver ImDyS(olver) in a fully supervised\nmanner, which conducts ID and ground reaction force estimation simultaneously.\nExperiments on ImDy and real-world data demonstrate the impressive competency\nof ImDyS in human inverse dynamics and ground reaction force estimation.\nMoreover, the potential of ImDy(-S) as a fundamental motion analysis tool is\nexhibited with downstream applications. The project page is\nhttps://foruck.github.io/ImDy/.\n","authors":["Xinpeng Liu","Junxuan Liang","Zili Lin","Haowen Hou","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.17610v1.pdf","comment":"Yong-Lu Li and Cewu Lu are the corresponding authors"}],"Human-Computer Interaction":[{"id":"http://arxiv.org/abs/2410.18329v1","updated":"2024-10-23T23:51:53Z","published":"2024-10-23T23:51:53Z","title":"When Group Spirit Meets Personal Journeys: Exploring Motivational\n  Dynamics and Design Opportunities in Group Therapy","summary":"  Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in\ntreating various mental disorders. Technology-facilitated mental health therapy\nimproves client engagement through methods like digitization or gamification.\nHowever, these innovations largely cater to individual therapy, ignoring the\npotential of group therapy-a treatment for multiple clients concurrently, which\nenables individual clients to receive various perspectives in the treatment\nprocess and also addresses the scarcity of healthcare practitioners to reduce\ncosts. Notwithstanding its cost-effectiveness and unique social dynamics that\nfoster peer learning and community support, group therapy, such as group CBT,\nfaces the issue of attrition. While existing medical work has developed\nguidelines for therapists, such as establishing leadership and empathy to\nfacilitate group therapy, understanding about the interactions between each\nstakeholder is still missing. To bridge this gap, this study examined a group\nCBT program called the Serigaya Methamphetamine Relapse Prevention Program\n(SMARPP) as a case study to understand stakeholder coordination and\ncommunication, along with factors promoting and hindering continuous engagement\nin group therapy. In-depth interviews with eight facilitators and six former\nclients from SMARPP revealed the motivators and demotivators for\nfacilitator-facilitator, client-client, and facilitator-client communications.\nOur investigation uncovers the presence of discernible conflicts between\nclients' intrapersonal motivation as well as interpersonal motivation in the\ncontext of group therapy through the lens of self-determination theory. We\ndiscuss insights and research opportunities for the HCI community to mediate\nsuch tension and enhance stakeholder communication in future\ntechnology-assisted group therapy settings.\n","authors":["Shixian Geng","Ginshi Shimojima","Chi-Lan Yang","Zefan Sramek","Shunpei Norihama","Ayumi Takano","Simo Hosio","Koji Yatani"],"pdf_url":"https://arxiv.org/pdf/2410.18329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18299v1","updated":"2024-10-23T22:10:45Z","published":"2024-10-23T22:10:45Z","title":"CAMeleon: Interactively Exploring Craft Workflows in CAD","summary":"  Designers of physical objects make assumptions on the material and\nfabrication workflow early in the design process. Recovering from bad\nassumptions is hard, because the design and resulting CAD model are locked-in\nto those assumptions. We present CAMeleon, a software tool to interactively\nexplore fabrication workflows at any stage of the CAD process.\n  CAMeleon's modular architecture allows users to execute their design with\ndifferent workflows, and preview results. Users can freely explore alternative\nworkflows. CAMeleon's architecture, can be extended with new workflows,\nincreasing the richness of workflows available.\n  We implemented five fabrication workflows in CAMeleon. We demonstrate\nCAMeleon's extensibility through collaboration with six craftsmen whose\nworkflows we replicated. We also implemented workflows of three papers and\nreflect on the process of these nine extensions. A usability study (n=12)\nshowed that CAMeleon allowed participants to explore and select workflows for\ntheir design which they did not know before.\n","authors":["Shuo Feng","Yifan Shan","Xuening Wang","Ritik Batra","Thijs Roumen"],"pdf_url":"https://arxiv.org/pdf/2410.18299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18297v1","updated":"2024-10-23T22:02:21Z","published":"2024-10-23T22:02:21Z","title":"A Pilot Study on Clinician-AI Collaboration in Diagnosing Depression\n  from Speech","summary":"  This study investigates clinicians' perceptions and attitudes toward an\nassistive artificial intelligence (AI) system that employs a speech-based\nexplainable ML algorithm for detecting depression. The AI system detects\ndepression from vowel-based spectrotemporal variations of speech and generates\nexplanations through explainable AI (XAI) methods. It further provides\ndecisions and explanations at various temporal granularities, including\nutterance groups, individual utterances, and within each utterance. A\nsmall-scale user study was conducted to evaluate users' perceived usability of\nthe system, trust in the system, and perceptions of design factors associated\nwith several elements of the system. Quantitative and qualitative analysis of\nthe collected data indicates both positive and negative aspects that influence\nclinicians' perception toward the AI. Results from quantitative analysis\nindicate that providing more AI explanations enhances user trust but also\nincreases system complexity. Qualitative analysis indicates the potential of\nintegrating such systems into the current diagnostic and screening workflow,\nbut also highlights existing limitations including clinicians' reduced\nfamiliarity with AI/ML systems and the need for user-friendly and intuitive\nvisualizations of speech information.\n","authors":["Kexin Feng","Theodora Chaspari"],"pdf_url":"https://arxiv.org/pdf/2410.18297v1.pdf","comment":"accepted at the IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI 2024)"},{"id":"http://arxiv.org/abs/2410.18242v1","updated":"2024-10-23T19:37:19Z","published":"2024-10-23T19:37:19Z","title":"Human-Agent Coordination in Games under Incomplete Information via\n  Multi-Step Intent","summary":"  Strategic coordination between autonomous agents and human partners under\nincomplete information can be modeled as turn-based cooperative games. We\nextend a turn-based game under incomplete information, the shared-control game,\nto allow players to take multiple actions per turn rather than a single action.\nThe extension enables the use of multi-step intent, which we hypothesize will\nimprove performance in long-horizon tasks. To synthesize cooperative policies\nfor the agent in this extended game, we propose an approach featuring a memory\nmodule for a running probabilistic belief of the environment dynamics and an\nonline planning algorithm called IntentMCTS. This algorithm strategically\nselects the next action by leveraging any communicated multi-step intent via\nreward augmentation while considering the current belief. Agent-to-agent\nsimulations in the Gnomes at Night testbed demonstrate that IntentMCTS requires\nfewer steps and control switches than baseline methods. A human-agent user\nstudy corroborates these findings, showing an 18.52% higher success rate\ncompared to the heuristic baseline and a 5.56% improvement over the single-step\nprior work. Participants also report lower cognitive load, frustration, and\nhigher satisfaction with the IntentMCTS agent partner.\n","authors":["Shenghui Chen","Ruihan Zhao","Sandeep Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18230v1","updated":"2024-10-23T19:24:58Z","published":"2024-10-23T19:24:58Z","title":"Assessment of Developmental Dysgraphia Utilising a Display Tablet","summary":"  Even though the computerised assessment of developmental dysgraphia (DD)\nbased on online handwriting processing has increasing popularity, most of the\nsolutions are based on a setup, where a child writes on a paper fixed to a\ndigitizing tablet that is connected to a computer. Although this approach\nenables the standard way of writing using an inking pen, it is difficult to be\nadministered by children themselves. The main goal of this study is thus to\nexplore, whether the quantitative analysis of online handwriting recorded via a\ndisplay screen tablet could sufficiently support the assessment of DD as well.\nFor the purpose of this study, we enrolled 144 children (attending the 3rd and\n4th class of a primary school), whose handwriting proficiency was assessed by a\nspecial education counsellor, and who assessed themselves by the Handwriting\nProficiency Screening Questionnaires for Children (HPSQ C). Using machine\nlearning models based on a gradient-boosting algorithm, we were able to support\nthe DD diagnosis with up to 83.6% accuracy. The HPSQ C total score was\nestimated with a minimum error equal to 10.34 %. Children with DD spent\nsignificantly higher time in-air, they had a higher number of pen elevations, a\nbigger height of on-surface strokes, a lower in-air tempo, and a higher\nvariation in the angular velocity. Although this study shows a promising impact\nof DD assessment via display tablets, it also accents the fact that modelling\nof subjective scores is challenging and a complex and data-driven\nquantification of DD manifestations is needed.\n","authors":["Jiri Mekyska","Zoltan Galaz","Katarina Safarova","Vojtech Zvoncak","Lukas Cunek","Tomas Urbanek","Jana Marie Havigerova","Jirina Bednarova","Jan Mucha","Michal Gavenciak","Zdenek Smekal","Marcos Faundez-Zanuy"],"pdf_url":"https://arxiv.org/pdf/2410.18230v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.18048v1","updated":"2024-10-23T17:20:35Z","published":"2024-10-23T17:20:35Z","title":"A Comparative Assessment of Technology Acceptance and Learning Outcomes\n  in Computer-based versus VR-based Pedagogical Agents","summary":"  As educational technology evolves, the potential of Pedagogical Agents (PAs)\nin supporting education is extensively explored. Typically, research on PAs has\nprimarily focused on computer-based learning environments, but their use in\nVR-based environments and integration into education is still in its infancy.\nTo address this gap, this paper presents a mixed method comparative study that\nhas been conducted to evaluate and examine how these computer-based PAs and\nVR-based PAs compare, towards their learning efficacy and technology\nacceptance. 92 Computing and Engineering undergraduate students were recruited\nand participated in an educational experience focusing on computing machinery\neducation. The findings of this study revealed that both approaches can\neffectively facilitate learning acquisition, and both technologies have been\npositively perceived by participants toward acceptance, without any significant\ndifferences. The findings of this study shed light on the potential of\nutilizing intelligent PAs to support education, contributing towards the\nadvancement of our understanding of how to integrate such technologies to\ndevelop learning interventions, and establishing the foundation for future\ninvestigations that aim to successfully integrate and use PAs in education.\n","authors":["Aimilios Hadjiliasi","Louis Nisiotis","Irene Polycarpou"],"pdf_url":"https://arxiv.org/pdf/2410.18048v1.pdf","comment":"Accepted as 4-pages poster paper in 23rd IEEE International Symposium\n  on Mixed and Augmented Reality (ISMAR), 21-25/10/2024"},{"id":"http://arxiv.org/abs/2406.11757v4","updated":"2024-10-23T16:41:45Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v4.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2406.14856v2","updated":"2024-10-23T15:08:59Z","published":"2024-06-21T04:02:19Z","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis","summary":"  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2406.14856v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17909v1","updated":"2024-10-23T14:28:16Z","published":"2024-10-23T14:28:16Z","title":"AI as a Bridge Across Ages: Exploring The Opportunities of Artificial\n  Intelligence in Supporting Inter-Generational Communication in Virtual\n  Reality","summary":"  Inter-generational communication is essential for bridging generational gaps\nand fostering mutual understanding. However, maintaining it is complex due to\ncultural, communicative, and geographical differences. Recent research\nindicated that while Virtual Reality (VR) creates a relaxed atmosphere and\npromotes companionship, it inadequately addresses the complexities of\ninter-generational dialogue, including variations in values and relational\ndynamics. To address this gap, we explored the opportunities of Artificial\nIntelligence (AI) in supporting inter-generational communication in VR. We\ndeveloped three technology probes (e.g., Content Generator, Communication\nFacilitator, and Info Assistant) in VR and employed them in a probe-based\nparticipatory design study with twelve inter-generational pairs. Our results\nshow that AI-powered VR facilitates inter-generational communication by\nenhancing mutual understanding, fostering conversation fluency, and promoting\nactive participation. We also introduce several challenges when using\nAI-powered VR in supporting inter-generational communication and derive design\nimplications for future VR platforms, aiming to improve inter-generational\ncommunication.\n","authors":["Qiuxin Du","Xiaoying Wei","Jiawei Li","Emily Kuang","Jie Hao","Dongdong Weng","Mingming Fan"],"pdf_url":"https://arxiv.org/pdf/2410.17909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17783v1","updated":"2024-10-23T11:32:46Z","published":"2024-10-23T11:32:46Z","title":"Leveraging the Domain Adaptation of Retrieval Augmented Generation\n  Models for Question Answering and Reducing Hallucination","summary":"  While ongoing advancements in Large Language Models have demonstrated\nremarkable success across various NLP tasks, Retrieval Augmented Generation\nModel stands out to be highly effective on downstream applications like\nQuestion Answering. Recently, RAG-end2end model further optimized the\narchitecture and achieved notable performance improvements on domain\nadaptation. However, the effectiveness of these RAG-based architectures remains\nrelatively unexplored when fine-tuned on specialized domains such as customer\nservice for building a reliable conversational AI system. Furthermore, a\ncritical challenge persists in reducing the occurrence of hallucinations while\nmaintaining high domain-specific accuracy. In this paper, we investigated the\nperformance of diverse RAG and RAG-like architectures through domain adaptation\nand evaluated their ability to generate accurate and relevant response grounded\nin the contextual knowledge base. To facilitate the evaluation of the models,\nwe constructed a novel dataset HotelConvQA, sourced from wide range of\nhotel-related conversations and fine-tuned all the models on our domain\nspecific dataset. We also addressed a critical research gap on determining the\nimpact of domain adaptation on reducing hallucinations across different RAG\narchitectures, an aspect that was not properly measured in prior work. Our\nevaluation shows positive results in all metrics by employing domain\nadaptation, demonstrating strong performance on QA tasks and providing insights\ninto their efficacy in reducing hallucinations. Our findings clearly indicate\nthat domain adaptation not only enhances the models' performance on QA tasks\nbut also significantly reduces hallucination across all evaluated RAG\narchitectures.\n","authors":["Salman Rakin","Md. A. R. Shibly","Zahin M. Hossain","Zeeshan Khan","Md. Mostofa Akbar"],"pdf_url":"https://arxiv.org/pdf/2410.17783v1.pdf","comment":"Initial Version fine-tuned on HotelConvQA"},{"id":"http://arxiv.org/abs/2410.13247v2","updated":"2024-10-23T11:09:57Z","published":"2024-10-17T06:14:34Z","title":"Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies","summary":"  The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels.\n","authors":["Chaofeng Zhang","Jia Hou","Xueting Tan","Gaolei Li","Caijuan Chen"],"pdf_url":"https://arxiv.org/pdf/2410.13247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16137v2","updated":"2024-10-23T09:36:20Z","published":"2024-10-21T16:03:18Z","title":"Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy\n  Concerns on Social Media","summary":"  Privacy is essential to fully enjoying the benefits of social media. While\nfear around privacy risks can sometimes motivate privacy management, the\nnegative impact of such fear, particularly when it is perceived as\nunaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen\nwell-being. In a co-design study with 136 participants aged 13-18, we explored\nhow teens can protect their privacy without experiencing heightened fear. We\nidentified seven different sources of dysfunctional fear, such as `fear of a\nhostile environment' and `fear of overstepping privacy norms.' We also\nevaluated ten designs, co-created with teen participants, that address these\nfears. Our findings suggest that social media platforms can mitigate\ndysfunctional fear without compromising privacy by creating a culture where\nprivacy protection is the norm through default privacy-protective features.\nHowever, we also found that even the most effective privacy features are not\nlikely to be adopted unless they balance the multifaceted and diverse needs of\nteens. Individual teens have different needs -- for example, public and private\naccount users have different needs -- and teens often want to enjoy the\nbenefits they get from slightly reducing privacy and widening their social\nreach. Given these considerations, augmenting default privacy features by\nallowing them to be toggled on and off will allow individual users to choose\ntheir own balance while still maintaining a privacy-focused norm.\n","authors":["JaeWon Kim","Soobin Cho","Robert Wolfe","Jishnu Hari Nair","Alexis Hiniker"],"pdf_url":"https://arxiv.org/pdf/2410.16137v2.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2206.01393v6","updated":"2024-10-23T20:49:23Z","published":"2022-06-03T05:06:23Z","title":"Simulation of Crowd Egress with Environmental Stressors","summary":"  This article introduces a modeling framework to characterize evacuee response\nto environmental stimuli during emergency egress. The model is developed in\nconsistency with stress theory, which explains how an organism reacts to\nenvironmental stressors. We integrate the theory into the well-known\nsocial-force model, and develop a framework to simulate crowd evacuation\nbehavior in multi-compartment buildings. Our method serves as a theoretical\nbasis to study crowd movement at bottlenecks, and simulate their herding and\nway-finding behavior in normal and hazardous conditions. The pre-movement\nbehavior is also briefly investigated by using opinion dynamics with social\ngroup model. The algorithms have been partly tested in FDS+EVAC as well as our\nsimulation platform crowdEgress.\n","authors":["Peng Wang","Xiaoda Wang","Peter Luh","Christian Wilkie","Timo Korhonen","Neal Olderman"],"pdf_url":"https://arxiv.org/pdf/2206.01393v6.pdf","comment":"26 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.18202v1","updated":"2024-10-23T18:10:38Z","published":"2024-10-23T18:10:38Z","title":"PyTSC: A Unified Platform for Multi-Agent Reinforcement Learning in\n  Traffic Signal Control","summary":"  Multi-Agent Reinforcement Learning (MARL) presents a promising approach for\naddressing the complexity of Traffic Signal Control (TSC) in urban\nenvironments. However, existing platforms for MARL-based TSC research face\nchallenges such as slow simulation speeds and convoluted, difficult-to-maintain\ncodebases. To address these limitations, we introduce PyTSC, a robust and\nflexible simulation environment that facilitates the training and evaluation of\nMARL algorithms for TSC. PyTSC integrates multiple simulators, such as SUMO and\nCityFlow, and offers a streamlined API, empowering researchers to explore a\nbroad spectrum of MARL approaches efficiently. PyTSC accelerates\nexperimentation and provides new opportunities for advancing intelligent\ntraffic management systems in real-world applications.\n","authors":["Rohit Bokade","Xiaoning Jin"],"pdf_url":"https://arxiv.org/pdf/2410.18202v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2410.18032v1","updated":"2024-10-23T17:02:59Z","published":"2024-10-23T17:02:59Z","title":"GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration","summary":"  Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.\n","authors":["Xin Li","Qizhi Chu","Yubin Chen","Yang Liu","Yaoqi Liu","Zekai Yu","Weize Chen","Chen Qian","Chuan Shi","Cheng Yang"],"pdf_url":"https://arxiv.org/pdf/2410.18032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17898v1","updated":"2024-10-23T14:16:34Z","published":"2024-10-23T14:16:34Z","title":"Scalable Offline Reinforcement Learning for Mean Field Games","summary":"  Reinforcement learning algorithms for mean-field games offer a scalable\nframework for optimizing policies in large populations of interacting agents.\nExisting methods often depend on online interactions or access to system\ndynamics, limiting their practicality in real-world scenarios where such\ninteractions are infeasible or difficult to model. In this paper, we present\nOffline Munchausen Mirror Descent (Off-MMD), a novel mean-field RL algorithm\nthat approximates equilibrium policies in mean-field games using purely offline\ndata. By leveraging iterative mirror descent and importance sampling\ntechniques, Off-MMD estimates the mean-field distribution from static datasets\nwithout relying on simulation or environment dynamics. Additionally, we\nincorporate techniques from offline reinforcement learning to address common\nissues like Q-value overestimation, ensuring robust policy learning even with\nlimited data coverage. Our algorithm scales to complex environments and\ndemonstrates strong performance on benchmark tasks like crowd exploration or\nnavigation, highlighting its applicability to real-world multi-agent systems\nwhere online experimentation is infeasible. We empirically demonstrate the\nrobustness of Off-MMD to low-quality datasets and conduct experiments to\ninvestigate its sensitivity to hyperparameter choices.\n","authors":["Axel Brunnbauer","Julian Lemmel","Zahra Babaiee","Sophie Neubauer","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2410.17898v1.pdf","comment":"Submitted to AAMAS"},{"id":"http://arxiv.org/abs/2409.10047v2","updated":"2024-10-23T12:48:26Z","published":"2024-09-16T07:20:29Z","title":"Bearing-Distance Based Flocking with Zone-Based Interactions","summary":"  This paper presents a novel zone-based flocking control approach suitable for\ndynamic multi-agent systems (MAS). Inspired by Reynolds behavioral rules for\n$boids$, flocking behavioral rules with the zones of repulsion, conflict,\nattraction, and surveillance are introduced. For each agent, using only bearing\nand distance measurements, behavioral deviation vectors quantify the deviations\nfrom the local separation, local and global flock velocity alignment, local\ncohesion, obstacle avoidance and boundary conditions, and strategic separation\nfor avoiding alien agents. The control strategy uses the local perception-based\nbehavioral deviation vectors to guide each agent's motion. Additionally, the\ncontrol strategy incorporates a directionally-aware obstacle avoidance\nmechanism that prioritizes obstacles in the agent's forward path. Simulation\nresults validate the effectiveness of this approach in creating flexible,\nadaptable, and scalable flocking behavior.\n","authors":["Hossein B. Jond"],"pdf_url":"https://arxiv.org/pdf/2409.10047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10568v2","updated":"2024-10-23T12:37:10Z","published":"2024-09-14T04:17:24Z","title":"On the limits of agency in agent-based models","summary":"  Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch.\n","authors":["Ayush Chopra","Shashank Kumar","Nurullah Giray-Kuru","Ramesh Raskar","Arnau Quera-Bofarull"],"pdf_url":"https://arxiv.org/pdf/2409.10568v2.pdf","comment":"19 pages, 5 appendices, 5 figures"},{"id":"http://arxiv.org/abs/2410.17785v1","updated":"2024-10-23T11:35:44Z","published":"2024-10-23T11:35:44Z","title":"TranSPORTmer: A Holistic Approach to Trajectory Understanding in\n  Multi-Agent Sports","summary":"  Understanding trajectories in multi-agent scenarios requires addressing\nvarious tasks, including predicting future movements, imputing missing\nobservations, inferring the status of unseen agents, and classifying different\nglobal states. Traditional data-driven approaches often handle these tasks\nseparately with specialized models. We introduce TranSPORTmer, a unified\ntransformer-based framework capable of addressing all these tasks, showcasing\nits application to the intricate dynamics of multi-agent sports scenarios like\nsoccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively\ncaptures temporal dynamics and social interactions in an equivariant manner.\nThe model's tasks are guided by an input mask that conceals missing or\nyet-to-be-predicted observations. Additionally, we introduce a CLS extra agent\nto classify states along soccer trajectories, including passes, possessions,\nuncontrolled states, and out-of-play intervals, contributing to an enhancement\nin modeling trajectories. Evaluations on soccer and basketball datasets show\nthat TranSPORTmer outperforms state-of-the-art task-specific models in player\nforecasting, player forecasting-imputation, ball inference, and ball\nimputation. https://youtu.be/8VtSRm8oGoE\n","authors":["Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2410.17785v1.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.11642v2","updated":"2024-10-23T09:43:03Z","published":"2024-10-15T14:31:54Z","title":"Improve Value Estimation of Q Function and Reshape Reward with Monte\n  Carlo Tree Search","summary":"  Reinforcement learning has achieved remarkable success in perfect information\ngames such as Go and Atari, enabling agents to compete at the highest levels\nagainst human players. However, research in reinforcement learning for\nimperfect information games has been relatively limited due to the more complex\ngame structures and randomness. Traditional methods face challenges in training\nand improving performance in imperfect information games due to issues like\ninaccurate Q value estimation and reward sparsity. In this paper, we focus on\nUno, an imperfect information game, and aim to address these problems by\nreducing Q value overestimation and reshaping reward function. We propose a\nnovel algorithm that utilizes Monte Carlo Tree Search to average the value\nestimations in Q function. Even though we choose Double Deep Q Learning as the\nfoundational framework in this paper, our method can be generalized and used in\nany algorithm which needs Q value estimation, such as the Actor-Critic.\nAdditionally, we employ Monte Carlo Tree Search to reshape the reward structure\nin the game environment. We compare our algorithm with several traditional\nmethods applied to games such as Double Deep Q Learning, Deep Monte Carlo and\nNeural Fictitious Self Play, and the experiments demonstrate that our algorithm\nconsistently outperforms these approaches, especially as the number of players\nin Uno increases, indicating a higher level of difficulty.\n","authors":["Jiamian Li"],"pdf_url":"https://arxiv.org/pdf/2410.11642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17690v1","updated":"2024-10-23T09:13:02Z","published":"2024-10-23T09:13:02Z","title":"Markov Potential Game with Final-time Reach-Avoid Objectives","summary":"  We formulate a Markov potential game with final-time reach-avoid objectives\nby integrating potential game theory with stochastic reach-avoid control. Our\nfocus is on multi-player trajectory planning where players maximize the same\nmulti-player reach-avoid objective: the probability of all participants\nreaching their designated target states by a specified time, while avoiding\ncollisions with one another. Existing approaches require centralized\ncomputation of actions via a global policy, which may have prohibitively\nexpensive communication costs. Instead, we focus on approximations of the\nglobal policy via local state feedback policies. First, we adapt the recursive\nsingle player reach-avoid value iteration to the multi-player framework with\nlocal policies, and show that the same recursion holds on the joint state\nspace. To find each player's optimal local policy, the multi-player reach-avoid\nvalue function is projected from the joint state to the local state using the\nother players' occupancy measures. Then, we propose an iterative best response\nscheme for the multi-player value iteration to converge to a pure Nash\nequilibrium. We demonstrate the utility of our approach in finding\ncollision-free policies for multi-player motion planning in simulation.\n","authors":["Sarah H. Q. Li","Abraham P. Vinod"],"pdf_url":"https://arxiv.org/pdf/2410.17690v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16237v2","updated":"2024-10-23T08:31:20Z","published":"2024-10-21T17:41:42Z","title":"IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in\n  Communicative Multi-Agent Systems","summary":"  As large language model (LLM) agents increasingly integrate into our\ninfrastructure, their robust coordination and message synchronization become\nvital. The Byzantine Generals Problem (BGP) is a critical model for\nconstructing resilient multi-agent systems (MAS) under adversarial attacks. It\ndescribes a scenario where malicious agents with unknown identities exist in\nthe system-situations that, in our context, could result from LLM agents'\nhallucinations or external attacks. In BGP, the objective of the entire system\nis to reach a consensus on the action to be taken. Traditional BGP requires\nglobal consensus among all agents; however, in practical scenarios, global\nconsensus is not always necessary and can even be inefficient. Therefore, there\nis a pressing need to explore a refined version of BGP that aligns with the\nlocal coordination patterns observed in MAS. We refer to this refined version\nas Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To\ntackle this issue, we propose a framework that leverages consensus protocols\nwithin general MAS settings, providing provable resilience against\ncommunication attacks and adaptability to changing environments, as validated\nby empirical results. Additionally, we present a case study in a sensor network\nenvironment to illustrate the practical application of our protocol.\n","authors":["Yihuan Mao","Yipeng Kang","Peilun Li","Ning Zhang","Wei Xu","Chongjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06813v4","updated":"2024-10-23T06:39:57Z","published":"2024-07-09T12:37:54Z","title":"Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy","summary":"  Diplomacy is one of the most sophisticated activities in human society,\ninvolving complex interactions among multiple parties that require skills in\nsocial reasoning, negotiation, and long-term strategic planning. Previous AI\nagents have demonstrated their ability to handle multi-step games and large\naction spaces in multi-agent tasks. However, diplomacy involves a staggering\nmagnitude of decision spaces, especially considering the negotiation stage\nrequired. While recent agents based on large language models (LLMs) have shown\npotential in various applications, they still struggle with extended planning\nperiods in complex multi-agent settings. Leveraging recent technologies for\nLLM-based agents, we aim to explore AI's potential to create a human-like agent\ncapable of executing comprehensive multi-agent missions by integrating three\nfundamental capabilities: 1) strategic planning with memory and reflection; 2)\ngoal-oriented negotiation with social reasoning; and 3) augmenting memory\nthrough self-play games for self-evolution without human in the loop.\n","authors":["Zhenyu Guan","Xiangyu Kong","Fangwei Zhong","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.06813v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17517v1","updated":"2024-10-23T02:49:37Z","published":"2024-10-23T02:49:37Z","title":"Bridging Swarm Intelligence and Reinforcement Learning","summary":"  Swarm intelligence (SI) explores how large groups of simple individuals\n(e.g., insects, fish, birds) collaborate to produce complex behaviors,\nexemplifying that the whole is greater than the sum of its parts. A fundamental\ntask in SI is Collective Decision-Making (CDM), where a group selects the best\noption among several alternatives, such as choosing an optimal foraging site.\nIn this work, we demonstrate a theoretical and empirical equivalence between\nCDM and single-agent reinforcement learning (RL) in multi-armed bandit\nproblems, utilizing concepts from opinion dynamics, evolutionary game theory,\nand RL. This equivalence bridges the gap between SI and RL and leads us to\nintroduce a novel abstract RL update rule called Maynard-Cross Learning.\nAdditionally, it provides a new population-based perspective on common RL\npractices like learning rate adjustment and batching. Our findings enable\ncross-disciplinary fertilization between RL and SI, allowing techniques from\none field to enhance the understanding and methodologies of the other.\n","authors":["Karthik Soma","Yann Bouteiller","Heiko Hamann","Giovanni Beltrame"],"pdf_url":"https://arxiv.org/pdf/2410.17517v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.18322v1","updated":"2024-10-23T23:10:09Z","published":"2024-10-23T23:10:09Z","title":"Unified Microphone Conversion: Many-to-Many Device Mapping via\n  Feature-wise Linear Modulation","summary":"  In this study, we introduce Unified Microphone Conversion, a unified\ngenerative framework to enhance the resilience of sound event classification\nsystems against device variability. Building on the limitations of previous\nworks, we condition the generator network with frequency response information\nto achieve many-to-many device mapping. This approach overcomes the inherent\nlimitation of CycleGAN, requiring separate models for each device pair. Our\nframework leverages the strengths of CycleGAN for unpaired training to simulate\ndevice characteristics in audio recordings and significantly extends its\nscalability by integrating frequency response related information via\nFeature-wise Linear Modulation. The experiment results show that our method\noutperforms the state-of-the-art method by 2.6% and reducing variability by\n0.8% in macro-average F1 score.\n","authors":["Myeonghoon Ryu","Hongseok Oh","Suji Lee","Han Park"],"pdf_url":"https://arxiv.org/pdf/2410.18322v1.pdf","comment":"Currently under review for ICASSP 2025"},{"id":"http://arxiv.org/abs/1608.07876v2","updated":"2024-10-23T20:32:26Z","published":"2016-08-29T01:22:38Z","title":"Human Action Recognition without Human","summary":"  The objective of this paper is to evaluate \"human action recognition without\nhuman\". Motion representation is frequently discussed in human action\nrecognition. We have examined several sophisticated options, such as dense\ntrajectories (DT) and the two-stream convolutional neural network (CNN).\nHowever, some features from the background could be too strong, as shown in\nsome recent studies on human action recognition. Therefore, we considered\nwhether a background sequence alone can classify human actions in current\nlarge-scale action datasets (e.g., UCF101). In this paper, we propose a novel\nconcept for human action analysis that is named \"human action recognition\nwithout human\". An experiment clearly shows the effect of a background sequence\nfor understanding an action label.\n","authors":["Hirokatsu Kataoka","Kensho Hara","Yutaka Satoh"],"pdf_url":"https://arxiv.org/pdf/1608.07876v2.pdf","comment":"This paper is an extension of the work presented at the ECCV 2016\n  Workshop and was primarily conducted in 2017"},{"id":"http://arxiv.org/abs/2410.12018v2","updated":"2024-10-23T15:21:54Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v2.pdf","comment":"ACCV 2024 Oral"},{"id":"http://arxiv.org/abs/2409.02845v3","updated":"2024-10-23T12:12:44Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03394v3","updated":"2024-10-23T10:57:53Z","published":"2023-07-07T05:33:54Z","title":"Dual Inverse Degradation Network for Real-World SDRTV-to-HDRTV\n  Conversion","summary":"  In this study, we address the emerging necessity of converting Standard\nDynamic Range Television (SDRTV) content into High Dynamic Range Television\n(HDRTV) in light of the limited number of native HDRTV content. A principal\ntechnical challenge in this conversion is the exacerbation of coding artifacts\ninherent in SDRTV, which detrimentally impacts the quality of the resulting\nHDRTV. To address this issue, our method introduces a novel approach that\nconceptualizes the SDRTV-to-HDRTV conversion as a composite task involving dual\ndegradation restoration. This encompasses inverse tone mapping in conjunction\nwith video restoration. We propose Dual Inversion Downgraded SDRTV to HDRTV\nNetwork (DIDNet), which can accurately perform inverse tone mapping while\npreventing encoding artifacts from being amplified, thereby significantly\nimproving visual quality. DIDNet integrates an intermediate auxiliary loss\nfunction to effectively separate the dual degradation restoration tasks and\nefficient learning of both artifact reduction and inverse tone mapping during\nend-to-end training. Additionally, DIDNet introduces a spatio-temporal feature\nalignment module for video frame fusion, which augments texture quality and\nreduces artifacts. The architecture further includes a dual-modulation\nconvolution mechanism for optimized inverse tone mapping. Recognizing the\nricher texture and high-frequency information in HDRTV compared to SDRTV, we\nfurther introduce a wavelet attention module to enhance frequency features. Our\napproach demonstrates marked superiority over existing state-of-the-art\ntechniques in terms of quantitative performance and visual quality.\n","authors":["Kepeng Xu","Li Xu","Gang He","Xianyun Wu","Zhiqiang Zhang","Wenxin Yu","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2307.03394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17589v1","updated":"2024-10-23T06:35:41Z","published":"2024-10-23T06:35:41Z","title":"Challenge on Sound Scene Synthesis: Evaluating Text-to-Audio Generation","summary":"  Despite significant advancements in neural text-to-audio generation,\nchallenges persist in controllability and evaluation. This paper addresses\nthese issues through the Sound Scene Synthesis challenge held as part of the\nDetection and Classification of Acoustic Scenes and Events 2024. We present an\nevaluation protocol combining objective metric, namely Fr\\'echet Audio\nDistance, with perceptual assessments, utilizing a structured prompt format to\nenable diverse captions and effective evaluation. Our analysis reveals varying\nperformance across sound categories and model architectures, with larger models\ngenerally excelling but innovative lightweight approaches also showing promise.\nThe strong correlation between objective metrics and human ratings validates\nour evaluation approach. We discuss outcomes in terms of audio quality,\ncontrollability, and architectural considerations for text-to-audio\nsynthesizers, providing direction for future research.\n","authors":["Junwon Lee","Modan Tailleur","Laurie M. Heller","Keunwoo Choi","Mathieu Lagrange","Brian McFee","Keisuke Imoto","Yuki Okamoto"],"pdf_url":"https://arxiv.org/pdf/2410.17589v1.pdf","comment":"accepted to NeurIPS 2024 Workshop: Audio Imagination"},{"id":"http://arxiv.org/abs/2410.15573v2","updated":"2024-10-23T06:21:09Z","published":"2024-10-21T01:36:42Z","title":"OpenMU: Your Swiss Army Knife for Music Understanding","summary":"  We present OpenMU-Bench, a large-scale benchmark suite for addressing the\ndata scarcity issue in training multimodal language models to understand music.\nTo construct OpenMU-Bench, we leveraged existing datasets and bootstrapped new\nannotations. OpenMU-Bench also broadens the scope of music understanding by\nincluding lyrics understanding and music tool usage. Using OpenMU-Bench, we\ntrained our music understanding model, OpenMU, with extensive ablations,\ndemonstrating that OpenMU outperforms baseline models such as MU-Llama. Both\nOpenMU and OpenMU-Bench are open-sourced to facilitate future research in music\nunderstanding and to enhance creative music production efficiency.\n","authors":["Mengjie Zhao","Zhi Zhong","Zhuoyuan Mao","Shiqi Yang","Wei-Hsiang Liao","Shusuke Takahashi","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.15573v2.pdf","comment":"Resources: https://github.com/mzhaojp22/openmu"},{"id":"http://arxiv.org/abs/2309.12029v2","updated":"2024-10-23T04:36:26Z","published":"2023-09-21T12:51:11Z","title":"Exploring Self-Supervised Skeleton-Based Human Action Recognition under\n  Occlusions","summary":"  To integrate self-supervised skeleton-based action recognition methods into\nautonomous robotic systems, it is crucial to consider adverse situations\ninvolving target occlusions. Such a scenario, despite its practical relevance,\nis rarely addressed in existing self-supervised skeleton-based action\nrecognition methods. To empower models with the capacity to address occlusion,\nwe propose a simple and effective method. We first pre-train using occluded\nskeleton sequences, then use k-means clustering (KMeans) on sequence embeddings\nto group semantically similar samples. Next, we propose KNN-Imputation to fill\nin missing skeleton data based on the closest sample neighbors. Imputing\nincomplete skeleton sequences to create relatively complete sequences as input\nprovides significant benefits to existing skeleton-based self-supervised\nmethods. Meanwhile, building on the state-of-the-art Partial Spatio-Temporal\nLearning (PSTL), we introduce an Occluded Partial Spatio-Temporal Learning\n(OPSTL) framework. This enhancement utilizes Adaptive Spatial Masking (ASM) for\nbetter use of high-quality, intact skeletons. The new proposed method is\nverified on the challenging occluded versions of the NTURGB+D 60 and NTURGB+D\n120. The source code is publicly available at https://github.com/cyfml/OPSTL.\n","authors":["Yifei Chen","Kunyu Peng","Alina Roitberg","David Schneider","Jiaming Zhang","Junwei Zheng","Ruiping Liu","Yufan Chen","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2309.12029v2.pdf","comment":"The source code is publicly available at\n  https://github.com/cyfml/OPSTL"},{"id":"http://arxiv.org/abs/2410.18151v1","updated":"2024-10-23T03:11:01Z","published":"2024-10-23T03:11:01Z","title":"Music102: An $D_{12}$-equivariant transformer for chord progression\n  accompaniment","summary":"  We present Music102, an advanced model built upon the Music101 prototype,\naimed at enhancing chord progression accompaniment through a D12-equivariant\ntransformer. Inspired by group theory and symbolic music structures, Music102\nleverages musical symmetry--such as transposition and reflection\noperations--integrating these properties into the transformer architecture. By\nencoding prior music knowledge, the model maintains equivariance across both\nmelody and chord sequences. The POP909 dataset was employed to train and\nevaluate Music102, revealing significant improvements over Music101 in both\nweighted loss and exact accuracy metrics, despite using fewer parameters. This\nwork showcases the adaptability of self-attention mechanisms and layer\nnormalization to the discrete musical domain, addressing challenges in\ncomputational music analysis. With its stable and flexible neural framework,\nMusic102 sets the stage for further exploration in equivariant music generation\nand computational composition tools, bridging mathematical theory with\npractical music performance.\n","authors":["Weiliang Luo"],"pdf_url":"https://arxiv.org/pdf/2410.18151v1.pdf","comment":"10 pages, 3 figures"}]},"2024-10-22T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.16865v1","updated":"2024-10-22T10:07:42Z","published":"2024-10-22T10:07:42Z","title":"Polycubes via Dual Loops","summary":"  We present a complete characterization of polycubes of any genus based on\ntheir dual structure: a collection of oriented loops which run in each of the\naxis directions and capture polycubes via their intersection patterns. A\npolycube loop structure uniquely corresponds to a polycube. We also describe\nall combinatorially different ways to add a loop to a loop structure while\nmaintaining its validity; similarly, we show how to identify loops that can be\nremoved from a polycube loop structure without invalidating it. Our\ncharacterization gives rise to an iterative algorithm to construct provably\nvalid polycube-maps for a given input surface; polycube-maps are frequently\nused to solve texture mapping, spline fitting, and hexahedral meshing. We\nshowcase some results of a proof-of-concept implementation of this iterative\nalgorithm.\n","authors":["Maxim Snoep","Bettina Speckmann","Kevin Verbeek"],"pdf_url":"https://arxiv.org/pdf/2410.16865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16851v1","updated":"2024-10-22T09:37:12Z","published":"2024-10-22T09:37:12Z","title":"Toolpath Generation for High Density Spatial Fiber Printing Guided by\n  Principal Stresses","summary":"  While multi-axis 3D printing can align continuous fibers along principal\nstresses in continuous fiber-reinforced thermoplastic (CFRTP) composites to\nenhance mechanical strength, existing methods have difficulty generating\ntoolpaths with high fiber coverage. This is mainly due to the orientation\nconsistency constraints imposed by vector-field-based methods and the turbulent\nstress fields around stress concentration regions. This paper addresses these\nchallenges by introducing a 2-RoSy representation for computing the direction\nfield, which is then converted into a periodic scalar field to generate partial\niso-curves for fiber toolpaths with nearly equal hatching distance. To improve\nfiber coverage in stress-concentrated regions, such as around holes, we extend\nthe quaternion-based method for curved slicing by incorporating winding\ncompatibility considerations. Our proposed method can achieve toolpaths\ncoverage between 87.5% and 90.6% by continuous fibers with 1.1mm width. Models\nfabricated using our toolpaths show up to 84.6% improvement in failure load and\n54.4% increase in stiffness when compared to the results obtained from\nmulti-axis 3D printing with sparser fibers.\n","authors":["Tianyu Zhang","Tao Liu","Neelotpal Dutta","Yongxue Chen","Renbo Su","Zhizhou Zhang","Weiming Wang","Charlie C. L. Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01359v2","updated":"2024-10-22T08:37:13Z","published":"2022-11-02T17:57:44Z","title":"Partitioning a Polygon Into Small Pieces","summary":"  We study the problem of partitioning a given simple polygon $P$ into a\nminimum number of connected polygonal pieces, each of bounded size. We describe\na general technique for constructing such partitions that works for several\nnotions of `bounded size,' namely that each piece must be contained in an\naxis-aligned or arbitrarily rotated unit square or a unit disk, or that each\npiece has bounded perimeter, straight-line diameter or geodesic diameter. The\nproblems are motivated by practical settings in manufacturing, finite element\nanalysis, collision detection, vehicle routing, shipping and laser capture\nmicrodissection.\n  The version where each piece should be contained in an axis-aligned unit\nsquare is already known to be NP-hard [Abrahamsen and Stade, FOCS, 2024], and\nthe other versions seem no easier. Our main result is to develop\nconstant-factor approximation algorithms, which means that the number of pieces\nin the produced partition is at most a constant factor larger than the\ncardinality of an optimal partition. Existing algorithms [Damian and Pemmaraju,\nAlgorithmica, 2004] do not allow Steiner points, which means that all corners\nof the produced pieces must also be corners of $P$. This has the disappointing\nconsequence that a partition often does not exist, whereas our algorithms\nalways produce meaningful partitions. Furthermore, an optimal partition without\nSteiner points may require $\\Omega(n)$ pieces for polygons with $n$ corners\nwhere a partition consisting of just $2$ pieces exists when Steiner points are\nallowed. Other existing algorithms [Arkin, Das, Gao, Goswami, Mitchell,\nPolishchuk and T\\'oth, ESA, 2020] only allow $P$ to be split along chords (and\naim to minimize the number of chords instead of the number of pieces), whereas\nwe make no constraints on the boundaries of the pieces.\n","authors":["Mikkel Abrahamsen","Nichlas Langhoff Rasmussen"],"pdf_url":"https://arxiv.org/pdf/2211.01359v2.pdf","comment":"Updated version, to appear at SODA 2025"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.17444v1","updated":"2024-10-22T21:45:46Z","published":"2024-10-22T21:45:46Z","title":"Gains-from-Trade in Bilateral Trade with a Broker","summary":"  We study bilateral trade with a broker, where a buyer and seller interact\nexclusively through the broker. The broker strategically maximizes her payoff\nthrough arbitrage by trading with the buyer and seller at different prices. We\nstudy whether the presence of the broker interferes with the mechanism's\ngains-from-trade (GFT) achieving a constant-factor approximation to the\nfirst-best gains-from-trade (FB).\n  We first show that the GFT achieves a $1 / 36$-approximation to the FB even\nif the broker runs an optimal posted-pricing mechanism under symmetric agents\nwith monotone-hazard-rate distributions. Beyond posted-pricing mechanisms, even\nif the broker uses an arbitrary incentive-compatible (IC) and\nindividually-rational (IR) mechanism that maximizes her expected profit, we\nprove that it induces a $1 / 2$-approximation to the first-best GFT when the\nbuyer and seller's distributions are uniform distributions with arbitrary\nsupport. This bound is shown to be tight.\n  We complement such results by proving that if the broker uses an arbitrary\nprofit-maximizing IC and IR mechanism, there exists a family of problem\ninstances under which the approximation factor to the first-best GFT becomes\narbitrarily bad. We show that this phenomenon persists even if we restrict one\nof the buyer's or seller's distributions to have a singleton support, or even\nin the symmetric setting where the buyer and seller have identical\ndistributions.\n","authors":["Ilya Hajiaghayi","MohammadTaghi Hajiaghayi","Gary Peng","Suho Shin"],"pdf_url":"https://arxiv.org/pdf/2410.17444v1.pdf","comment":"To appear at SODA'25"},{"id":"http://arxiv.org/abs/2410.17431v1","updated":"2024-10-22T21:08:28Z","published":"2024-10-22T21:08:28Z","title":"Meta Stackelberg Game: Robust Federated Learning against Adaptive and\n  Mixed Poisoning Attacks","summary":"  Federated learning (FL) is susceptible to a range of security threats.\nAlthough various defense mechanisms have been proposed, they are typically\nnon-adaptive and tailored to specific types of attacks, leaving them\ninsufficient in the face of multiple uncertain, unknown, and adaptive attacks\nemploying diverse strategies. This work formulates adversarial federated\nlearning under a mixture of various attacks as a Bayesian Stackelberg Markov\ngame, based on which we propose the meta-Stackelberg defense composed of\npre-training and online adaptation. {The gist is to simulate strong attack\nbehavior using reinforcement learning (RL-based attacks) in pre-training and\nthen design meta-RL-based defense to combat diverse and adaptive attacks.} We\ndevelop an efficient meta-learning approach to solve the game, leading to a\nrobust and adaptive FL defense. Theoretically, our meta-learning algorithm,\nmeta-Stackelberg learning, provably converges to the first-order\n$\\varepsilon$-meta-equilibrium point in $O(\\varepsilon^{-2})$ gradient\niterations with $O(\\varepsilon^{-4})$ samples per iteration. Experiments show\nthat our meta-Stackelberg framework performs superbly against strong model\npoisoning and backdoor attacks of uncertain and unknown types.\n","authors":["Tao Li","Henger Li","Yunian Pan","Tianyi Xu","Zizhan Zheng","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.17431v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.05069v2","updated":"2024-10-22T19:15:35Z","published":"2024-07-06T13:23:28Z","title":"What's the Best Seat in the Game Left, Center, Right?","summary":"  Left, Center, Right is a popular dice game. We analyze the game using Markov\nchain and Monte Carlo methods. We compute the expected game length for two to\neight players and determine the probability of winning for each player in the\ngame. We discuss the surprising conclusions of which players have the highest\nand lowest chance of winning, and we propose a small rule change that makes the\ngame a little more fair.\n","authors":["Benjamin Richeson","David Richeson"],"pdf_url":"https://arxiv.org/pdf/2407.05069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17336v1","updated":"2024-10-22T18:10:50Z","published":"2024-10-22T18:10:50Z","title":"Computing Optimal Regularizers for Online Linear Optimization","summary":"  Follow-the-Regularized-Leader (FTRL) algorithms are a popular class of\nlearning algorithms for online linear optimization (OLO) that guarantee\nsub-linear regret, but the choice of regularizer can significantly impact\ndimension-dependent factors in the regret bound. We present an algorithm that\ntakes as input convex and symmetric action sets and loss sets for a specific\nOLO instance, and outputs a regularizer such that running FTRL with this\nregularizer guarantees regret within a universal constant factor of the best\npossible regret bound. In particular, for any choice of (convex, symmetric)\naction set and loss set we prove that there exists an instantiation of FTRL\nwhich achieves regret within a constant factor of the best possible learning\nalgorithm, strengthening the universality result of Srebro et al., 2011.\n  Our algorithm requires preprocessing time and space exponential in the\ndimension $d$ of the OLO instance, but can be run efficiently online assuming a\nmembership and linear optimization oracle for the action and loss sets,\nrespectively (and is fully polynomial time for the case of constant dimension\n$d$). We complement this with a lower bound showing that even deciding whether\na given regularizer is $\\alpha$-strongly-convex with respect to a given norm is\nNP-hard.\n","authors":["Khashayar Gatmiry","Jon Schneider","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2410.17336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19011v1","updated":"2024-10-22T17:10:06Z","published":"2024-10-22T17:10:06Z","title":"Local hedging approximately solves Pandora's box problems with\n  nonobligatory inspection","summary":"  We consider search problems with nonobligatory inspection and single-item or\ncombinatorial selection. A decision maker is presented with a number of items,\neach of which contains an unknown price, and can pay an inspection cost to\nobserve the item's price before selecting it. Under single-item selection, the\ndecision maker must select one item; under combinatorial selection, the\ndecision maker must select a set of items that satisfies certain constraints.\nIn our nonobligatory inspection setting, the decision maker can select items\nwithout first inspecting them. It is well-known that search with nonobligatory\ninspection is harder than the well-studied obligatory inspection case, for\nwhich the optimal policy for single-item selection (Weitzman, 1979) and\napproximation algorithms for combinatorial selection (Singla, 2018) are known.\n  We introduce a technique, local hedging, for constructing policies with good\napproximation ratios in the nonobligatory inspection setting. Local hedging\ntransforms policies for the obligatory inspection setting into policies for the\nnonobligatory inspection setting, at the cost of an extra factor in the\napproximation ratio. The factor is instance-dependent but is at most 4/3. We\nthus obtain the first approximation algorithms for a variety of combinatorial\nselection problems, including matroid basis, matching, and facility location.\n","authors":["Ziv Scully","Laura Doval"],"pdf_url":"https://arxiv.org/pdf/2410.19011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10219v2","updated":"2024-10-22T15:52:15Z","published":"2023-12-15T21:31:30Z","title":"The Complexity of Optimizing Atomic Congestion","summary":"  Atomic congestion games are a classic topic in network design, routing, and\nalgorithmic game theory, and are capable of modeling congestion and flow\noptimization tasks in various application areas. While both the price of\nanarchy for such games as well as the computational complexity of computing\ntheir Nash equilibria are by now well-understood, the computational complexity\nof computing a system-optimal set of strategies -- that is, a centrally planned\nrouting that minimizes the average cost of agents -- is severely understudied\nin the literature. We close this gap by identifying the exact boundaries of\ntractability for the problem through the lens of the parameterized complexity\nparadigm. After showing that the problem remains highly intractable even on\nextremely simple networks, we obtain a set of results which demonstrate that\nthe structural parameters which control the computational (in)tractability of\nthe problem are not vertex-separator based in nature (such as, e.g.,\ntreewidth), but rather based on edge separators. We conclude by extending our\nanalysis towards the (even more challenging) min-max variant of the problem.\n","authors":["Cornelius Brand","Robert Ganian","Subrahmanyam Kalyanasundaram","Fionn Mc Inerney"],"pdf_url":"https://arxiv.org/pdf/2312.10219v2.pdf","comment":"Short version appeared at AAAI 2024. Long version accepted in the\n  Journal of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2410.17086v1","updated":"2024-10-22T15:13:13Z","published":"2024-10-22T15:13:13Z","title":"Exploration and Persuasion","summary":"  How to incentivize self-interested agents to explore when they prefer to\nexploit? Consider a population of self-interested agents that make decisions\nunder uncertainty. They \"explore\" to acquire new information and \"exploit\" this\ninformation to make good decisions. Collectively they need to balance these two\nobjectives, but their incentives are skewed toward exploitation. This is\nbecause exploration is costly, but its benefits are spread over many agents in\nthe future.\n  \"Incentivized Exploration\" addresses this issue via strategic communication.\nConsider a benign ``principal\" which can communicate with the agents and make\nrecommendations, but cannot force the agents to comply. Moreover, suppose the\nprincipal can observe the agents' decisions and the outcomes of these\ndecisions. The goal is to design a communication and recommendation policy\nwhich (i) achieves a desirable balance between exploration and exploitation,\nand (ii) incentivizes the agents to follow recommendations. What makes it\nfeasible is \"information asymmetry\": the principal knows more than any one\nagent, as it collects information from many. It is essential that the principal\ndoes not fully reveal all its knowledge to the agents.\n  Incentivized exploration combines two important problems in, resp., machine\nlearning and theoretical economics. First, if agents always follow\nrecommendations, the principal faces a multi-armed bandit problem: essentially,\ndesign an algorithm that balances exploration and exploitation. Second,\ninteraction with a single agent corresponds to \"Bayesian persuasion\", where a\nprincipal leverages information asymmetry to convince an agent to take a\nparticular action. We provide a brief but self-contained introduction to each\nproblem through the lens of incentivized exploration, solving a key special\ncase of the former as a sub-problem of the latter.\n","authors":["Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2410.17086v1.pdf","comment":"This is a chapter published in \"Online and Matching-Based Markets\",\n  Cambridge University Press, 2023. It has been available from the author's\n  website since 2021"},{"id":"http://arxiv.org/abs/2410.16857v1","updated":"2024-10-22T09:46:09Z","published":"2024-10-22T09:46:09Z","title":"Nash Meets Wertheimer: Using Good Continuation in Jigsaw Puzzles","summary":"  Jigsaw puzzle solving is a challenging task for computer vision since it\nrequires high-level spatial and semantic reasoning. To solve the problem,\nexisting approaches invariably use color and/or shape information but in many\nreal-world scenarios, such as in archaeological fresco reconstruction, this\nkind of clues is often unreliable due to severe physical and pictorial\ndeterioration of the individual fragments. This makes state-of-the-art\napproaches entirely unusable in practice. On the other hand, in such cases,\nsimple geometrical patterns such as lines or curves offer a powerful yet\nunexplored clue. In an attempt to fill in this gap, in this paper we introduce\na new challenging version of the puzzle solving problem in which one\ndeliberately ignores conventional color and shape features and relies solely on\nthe presence of linear geometrical patterns. The reconstruction process is then\nonly driven by one of the most fundamental principles of Gestalt perceptual\norganization, namely Wertheimer's {\\em law of good continuation}. In order to\ntackle this problem, we formulate the puzzle solving problem as the problem of\nfinding a Nash equilibrium of a (noncooperative) multiplayer game and use\nclassical multi-population replicator dynamics to solve it. The proposed\napproach is general and allows us to deal with pieces of arbitrary shape, size\nand orientation. We evaluate our approach on both synthetic and real-world data\nand compare it with state-of-the-art algorithms. The results show the intrinsic\ncomplexity of our purely line-based puzzle problem as well as the relative\neffectiveness of our game-theoretic formulation.\n","authors":["Marina Khoroshiltseva","Luca Palmieri","Sinem Aslan","Sebastiano Vascon","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2410.16857v1.pdf","comment":"to be published in ACCV2024"},{"id":"http://arxiv.org/abs/2410.19852v1","updated":"2024-10-22T09:29:53Z","published":"2024-10-22T09:29:53Z","title":"Survival of the Fittest: Evolutionary Adaptation of Policies for\n  Environmental Shifts","summary":"  Reinforcement learning (RL) has been successfully applied to solve the\nproblem of finding obstacle-free paths for autonomous agents operating in\nstochastic and uncertain environments. However, when the underlying stochastic\ndynamics of the environment experiences drastic distribution shifts, the\noptimal policy obtained in the trained environment may be sub-optimal or may\nentirely fail in helping find goal-reaching paths for the agent. Approaches\nlike domain randomization and robust RL can provide robust policies, but\ntypically assume minor (bounded) distribution shifts. For substantial\ndistribution shifts, retraining (either with a warm-start policy or from\nscratch) is an alternative approach. In this paper, we develop a novel approach\ncalled {\\em Evolutionary Robust Policy Optimization} (ERPO), an adaptive\nre-training algorithm inspired by evolutionary game theory (EGT). ERPO learns\nan optimal policy for the shifted environment iteratively using a temperature\nparameter that controls the trade off between exploration and adherence to the\nold optimal policy. The policy update itself is an instantiation of the\nreplicator dynamics used in EGT. We show that under fairly common sparsity\nassumptions on rewards in such environments, ERPO converges to the optimal\npolicy in the shifted environment. We empirically demonstrate that for path\nfinding tasks in a number of environments, ERPO outperforms several popular RL\nand deep RL algorithms (PPO, A3C, DQN) in many scenarios and popular\nenvironments. This includes scenarios where the RL algorithms are allowed to\ntrain from scratch in the new environment, when they are retrained on the new\nenvironment, or when they are used in conjunction with domain randomization.\nERPO shows faster policy adaptation, higher average rewards, and reduced\ncomputational costs in policy adaptation.\n","authors":["Sheryl Paul","Jyotirmoy V. Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2410.19852v1.pdf","comment":"Pubblished in ECAI 2024"},{"id":"http://arxiv.org/abs/2410.16745v1","updated":"2024-10-22T06:58:48Z","published":"2024-10-22T06:58:48Z","title":"Characterizing the top trading cycles rule for housing markets with\n  lexicographic preferences","summary":"  We consider a housing market model with limited externalities where agents\ncare both about their own consumption via demand preferences and about the\nagent who receives their endowment via supply preferences (we extend the\nassociated lexicographic preference domains introduced in Klaus and Meo, 2023).\nIf preferences are demand lexicographic, then our model extends the classical\nShapley-Scarf housing market (Shapley and Scarf, 1974) with strict preferences\nmodel. Our main result is a characterization of the corresponding top trading\ncycles (TTC) rule by individual rationality, pair efficiency, and\nstrategy-proofness (Theorem 1), which extends that of Ekici (2024) from\nclassical Shapley-Scarf housing markets with strict preferences to our model.\nTwo further characterizations are immediately obtained by strengthening pair\nefficiency to either Pareto efficiency or pairwise stability (Corollaries 1 and\n2). Finally, we show that as soon as we extend the preference domain to include\ndemand lexicographic as well as supply lexicographic preferences (e.g., when\npreferences are separable), no rule satisfying individual rationality, pair\nefficiency, and strategy-proofness exists (Theorem 2).\n","authors":["Bettina Klaus"],"pdf_url":"https://arxiv.org/pdf/2410.16745v1.pdf","comment":"JEL codes: C71, C78, D62, D63. Keywords: Core; characterization;\n  externalities; housing markets; pair efficiency; pairwise stability; top\n  trading cycles rule"},{"id":"http://arxiv.org/abs/2410.16600v1","updated":"2024-10-22T00:55:04Z","published":"2024-10-22T00:55:04Z","title":"Convex Markov Games: A Framework for Fairness, Imitation, and Creativity\n  in Multi-Agent Learning","summary":"  Expert imitation, behavioral diversity, and fairness preferences give rise to\npreferences in sequential decision making domains that do not decompose\nadditively across time. We introduce the class of convex Markov games that\nallow general convex preferences over occupancy measures. Despite infinite time\nhorizon and strictly higher generality than Markov games, pure strategy Nash\nequilibria exist under strict convexity. Furthermore, equilibria can be\napproximated efficiently by performing gradient descent on an upper bound of\nexploitability. Our experiments imitate human choices in ultimatum games,\nreveal novel solutions to the repeated prisoner's dilemma, and find fair\nsolutions in a repeated asymmetric coordination game. In the prisoner's\ndilemma, our algorithm finds a policy profile that deviates from observed human\nplay only slightly, yet achieves higher per-player utility while also being\nthree orders of magnitude less exploitable.\n","authors":["Ian Gemp","Andreas Haupt","Luke Marris","Siqi Liu","Georgios Piliouras"],"pdf_url":"https://arxiv.org/pdf/2410.16600v1.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.17114v1","updated":"2024-10-22T15:40:44Z","published":"2024-10-22T15:40:44Z","title":"Lunar Subterra: a Self-Integrative Unit with an Automated Drilling\n  System","summary":"  As humans venture deeper into space, the need for a lunar settlement, housing\nthe first group of settlers, grows steadily. By means of new technologies such\nas in situ resource utilisation (ISRU) as well as computational design, this\ngoal can be implemented in present years. Providing the first arrivals with an\nimmediate underground habitat safe from radiation and other environmental\nconstraints is of crucial importance to initialise a prolonged mission on the\nMoon. The project's proposal revolves around the idea of establishing a base\nwhich provides an immediately habitable space with the possibility for future\nexpansion. Advanced construction methods and sustainable practices lay the\ngroundwork for a permanent human presence, predominantly based on ISRU. This\npaper outlines a two-phase initiative aimed at the foundation of the Lunar\nSubterra, followed by an extension of the habitat above ground. Following our\ncollaboration with the PoliSpace Sparc Student Association group, a Virtual\nReality (VR) reproduction of the proposed habitat enabled quick iterative\ntesting of the habitable space with the use of a Meta Quest 2 headset. This not\nonly allowed an evaluation of the environment and its impact on human residents\nbut also eradicated the need for tangible models to conceptualise the idea,\nenabling rapid user-centred design and implementation in the future of space\nexploration.\n","authors":["Anthony Sfeir","Asya Petkova","Sabine Chaaya","Karina Chichova","Marta Rossi","Anna Vock","Alessandro Mosut","Akshayanivasini Ramasamy Saravanaraj","Valentina Sumini","Tommy Nilsson"],"pdf_url":"https://arxiv.org/pdf/2410.17114v1.pdf","comment":"75th International Astronautical Congress (IAC), Milan, Italy, 14-18\n  October 2024"},{"id":"http://arxiv.org/abs/2409.18736v3","updated":"2024-10-22T09:49:44Z","published":"2024-09-27T13:27:29Z","title":"Adversarial Challenges in Network Intrusion Detection Systems: Research\n  Insights and Future Prospects","summary":"  Machine learning has brought significant advances in cybersecurity,\nparticularly in the development of Intrusion Detection Systems (IDS). These\nimprovements are mainly attributed to the ability of machine learning\nalgorithms to identify complex relationships between features and effectively\ngeneralize to unseen data. Deep neural networks, in particular, contributed to\nthis progress by enabling the analysis of large amounts of training data,\nsignificantly enhancing detection performance. However, machine learning models\nremain vulnerable to adversarial attacks, where carefully crafted input data\ncan mislead the model into making incorrect predictions. While adversarial\nthreats in unstructured data, such as images and text, have been extensively\nstudied, their impact on structured data like network traffic is less explored.\nThis survey aims to address this gap by providing a comprehensive review of\nmachine learning-based Network Intrusion Detection Systems (NIDS) and\nthoroughly analyzing their susceptibility to adversarial attacks. We critically\nexamine existing research in NIDS, highlighting key trends, strengths, and\nlimitations, while identifying areas that require further exploration.\nAdditionally, we discuss emerging challenges in the field and offer insights\nfor the development of more robust and resilient NIDS. In summary, this paper\nenhances the understanding of adversarial attacks and defenses in NIDS and\nguide future research in improving the robustness of machine learning models in\ncybersecurity applications.\n","authors":["Sabrine Ennaji","Fabio De Gaspari","Dorjan Hitaj","Alicia Kbidi","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2409.18736v3.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2410.16704v1","updated":"2024-10-22T05:18:43Z","published":"2024-10-22T05:18:43Z","title":"Resolvability of classical-quantum channels","summary":"  Channel resolvability concerns the minimum resolution for approximating the\nchannel output. We study the resolvability of classical-quantum channels in two\nsettings, for the channel output generated from the worst input, and form the\nfixed independent and identically distributed (i.i.d.) input. The direct part\nof the worst-input setting is derived from sequential hypothesis testing as it\ninvolves of non-i.i.d.~inputs. The strong converse of the worst-input setting\nis obtained via the connection to identification codes. For the fixed-input\nsetting, while the direct part follows from the known quantum soft covering\nresult, we exploit the recent alternative quantum Sanov theorem to solve the\nstrong converse.\n","authors":["Masahito Hayashi","Hao-Chung Cheng","Li Gao"],"pdf_url":"https://arxiv.org/pdf/2410.16704v1.pdf","comment":"20 pages, 3 figures. Comments are welcome!"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.17242v1","updated":"2024-10-22T17:58:28Z","published":"2024-10-22T17:58:28Z","title":"LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias","summary":"  We propose the Large View Synthesis Model (LVSM), a novel transformer-based\napproach for scalable and generalizable novel view synthesis from sparse-view\ninputs. We introduce two architectures: (1) an encoder-decoder LVSM, which\nencodes input image tokens into a fixed number of 1D latent tokens, functioning\nas a fully learned scene representation, and decodes novel-view images from\nthem; and (2) a decoder-only LVSM, which directly maps input images to\nnovel-view outputs, completely eliminating intermediate scene representations.\nBoth models bypass the 3D inductive biases used in previous methods -- from 3D\nrepresentations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar\nprojections, plane sweeps) -- addressing novel view synthesis with a fully\ndata-driven approach. While the encoder-decoder model offers faster inference\ndue to its independent latent representation, the decoder-only LVSM achieves\nsuperior quality, scalability, and zero-shot generalization, outperforming\nprevious state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive\nevaluations across multiple datasets demonstrate that both LVSM variants\nachieve state-of-the-art novel view synthesis quality. Notably, our models\nsurpass all previous methods even with reduced computational resources (1-2\nGPUs). Please see our website for more details:\nhttps://haian-jin.github.io/projects/LVSM/ .\n","authors":["Haian Jin","Hanwen Jiang","Hao Tan","Kai Zhang","Sai Bi","Tianyuan Zhang","Fujun Luan","Noah Snavely","Zexiang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.17242v1.pdf","comment":"project page: https://haian-jin.github.io/projects/LVSM/"},{"id":"http://arxiv.org/abs/2410.17019v1","updated":"2024-10-22T13:40:11Z","published":"2024-10-22T13:40:11Z","title":"Recent consumer OLED monitors can be suitable for vision science","summary":"  Vision science imposes rigorous requirements for the design and execution of\npsychophysical studies and experiments. These requirements ensure precise\ncontrol over variables, accurate measurement of perceptual responses, and\nreproducibility of results, which are essential for investigating visual\nperception and its underlying mechanisms. Since different experiments have\ndifferent requirements, not all aspects of a display system are critical for a\ngiven setting. Therefore, some display systems may be suitable for certain\ntypes of experiments but unsuitable for others. An additional challenge is that\nthe performance of consumer systems is often highly dependent on specific\nmonitor settings and firmware behavior. Here, we evaluate the performance of\nfour display systems: a consumer LCD gaming monitor, a consumer OLED gaming\nmonitor, a consumer OLED TV, and a VPixx PROPixx projector system. To allow the\nreader to assess the suitability of these systems for different experiments, we\npresent a range of different metrics: luminance behavior, luminance uniformity\nacross display surface, estimated gamma values and linearity, channel\nadditivity, channel dependency, color gamut, pixel response time, and pixel\nwaveform. In addition, we exhaustively report the monitor firmware settings\nused. Our analyses show that current consumer-level OLED display systems are\npromising, and adequate to fulfill the requirements of some critical vision\nscience experiments, allowing laboratories to run their experiments even\nwithout investing in high-quality professional display systems. For example,\nthe tested Asus OLED gaming monitor shows excellent response time, a sharp\nsquare waveform even at 240 Hz, a color gamut that covers 94% of DCI-P3 color\nspace, and the best luminance uniformity among all four tested systems, making\nit a favorable option on price-to-performance ratio.\n","authors":["Tarek Abu Haila","Korbinian Kunst","Tran Quoc Khanh","Thomas S. A. Wallis"],"pdf_url":"https://arxiv.org/pdf/2410.17019v1.pdf","comment":"29 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.16991v1","updated":"2024-10-22T13:12:47Z","published":"2024-10-22T13:12:47Z","title":"An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and\n  Geometric Reasoning Skills Using Computer Graphics Questions","summary":"  CG (Computer Graphics) is a popular field of CS (Computer Science), but many\nstudents find this topic difficult due to it requiring a large number of\nskills, such as mathematics, programming, geometric reasoning, and creativity.\nOver the past few years, researchers have investigated ways to harness the\npower of GenAI (Generative Artificial Intelligence) to improve teaching. In CS,\nmuch of the research has focused on introductory computing. A recent study\nevaluating the performance of an LLM (Large Language Model), GPT-4 (text-only),\non CG questions, indicated poor performance and reliance on detailed\ndescriptions of image content, which often required considerable insight from\nthe user to return reasonable results. So far, no studies have investigated the\nabilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG\nquestions and how these abilities can be used to improve teaching.\n  In this study, we construct two datasets of CG questions requiring varying\ndegrees of visual perception skills and geometric reasoning skills, and\nevaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find\nthat although GPT-4o exhibits great potential in solving questions with visual\ninformation independently, major limitations still exist to the accuracy and\nquality of the generated results. We propose several novel approaches for CG\neducators to incorporate GenAI into CG teaching despite these limitations. We\nhope that our guidelines further encourage learning and engagement in CG\nclassrooms.\n","authors":["Tony Haoran Feng","Paul Denny","Burkhard C. W√ºnsche","Andrew Luxton-Reilly","Jacqueline Whalley"],"pdf_url":"https://arxiv.org/pdf/2410.16991v1.pdf","comment":"8 pages, 8 figures, 1 table, to be published in SIGGRAPH Asia 2024\n  Educator's Forum"},{"id":"http://arxiv.org/abs/2410.10851v2","updated":"2024-10-22T13:08:02Z","published":"2024-10-06T12:53:07Z","title":"LLM Gesticulator: Leveraging Large Language Models for Scalable and\n  Controllable Co-Speech Gesture Synthesis","summary":"  In this work, we present LLM Gesticulator, an LLM-based audio-driven\nco-speech gesture generation framework that synthesizes full-body animations\nthat are rhythmically aligned with the input audio while exhibiting natural\nmovements and editability. Compared to previous work, our model demonstrates\nsubstantial scalability. As the size of the backbone LLM model increases, our\nframework shows proportional improvements in evaluation metrics (a.k.a. scaling\nlaw). Our method also exhibits strong controllability where the content, style\nof the generated gestures can be controlled by text prompt. To the best of our\nknowledge, LLM gesticulator is the first work that use LLM on the co-speech\ngeneration task. Evaluation with existing objective metrics and user studies\nindicate that our framework outperforms prior works.\n","authors":["Haozhou Pang","Tianwei Ding","Lanshan He","Ming Tao","Lu Zhang","Qi Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16978v1","updated":"2024-10-22T12:56:58Z","published":"2024-10-22T12:56:58Z","title":"Multi-Layer Gaussian Splatting for Immersive Anatomy Visualization","summary":"  In medical image visualization, path tracing of volumetric medical data like\nCT scans produces lifelike three-dimensional visualizations. Immersive VR\ndisplays can further enhance the understanding of complex anatomies. Going\nbeyond the diagnostic quality of traditional 2D slices, they enable interactive\n3D evaluation of anatomies, supporting medical education and planning.\nRendering high-quality visualizations in real-time, however, is computationally\nintensive and impractical for compute-constrained devices like mobile headsets.\n  We propose a novel approach utilizing GS to create an efficient but static\nintermediate representation of CT scans. We introduce a layered GS\nrepresentation, incrementally including different anatomical structures while\nminimizing overlap and extending the GS training to remove inactive Gaussians.\nWe further compress the created model with clustering across layers.\n  Our approach achieves interactive frame rates while preserving anatomical\nstructures, with quality adjustable to the target hardware. Compared to\nstandard GS, our representation retains some of the explorative qualities\ninitially enabled by immersive path tracing. Selective activation and clipping\nof layers are possible at rendering time, adding a degree of interactivity to\notherwise static GS models. This could enable scenarios where high\ncomputational demands would otherwise prohibit using path-traced medical\nvolumes.\n","authors":["Constantin Kleinbeck","Hannah Schieber","Klaus Engel","Ralf Gutjahr","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2410.16978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15629v2","updated":"2024-10-22T12:02:29Z","published":"2024-10-21T04:25:43Z","title":"Fully Explicit Dynamic Gaussian Splatting","summary":"  3D Gaussian Splatting has shown fast and high-quality rendering results in\nstatic scenes by leveraging dense 3D prior and explicit representations.\nUnfortunately, the benefits of the prior and representation do not involve\nnovel view synthesis for dynamic motions. Ironically, this is because the main\nbarrier is the reliance on them, which requires increasing training and\nrendering times to account for dynamic motions. In this paper, we design a\nExplicit 4D Gaussian Splatting(Ex4DGS). Our key idea is to firstly separate\nstatic and dynamic Gaussians during training, and to explicitly sample\npositions and rotations of the dynamic Gaussians at sparse timestamps. The\nsampled positions and rotations are then interpolated to represent both\nspatially and temporally continuous motions of objects in dynamic scenes as\nwell as reducing computational cost. Additionally, we introduce a progressive\ntraining scheme and a point-backtracking technique that improves Ex4DGS's\nconvergence. We initially train Ex4DGS using short timestamps and progressively\nextend timestamps, which makes it work well with a few point clouds. The\npoint-backtracking is used to quantify the cumulative error of each Gaussian\nover time, enabling the detection and removal of erroneous Gaussians in dynamic\nscenes. Comprehensive experiments on various scenes demonstrate the\nstate-of-the-art rendering quality from our method, achieving fast rendering of\n62 fps on a single 2080Ti GPU.\n","authors":["Junoh Lee","Chang-Yeon Won","Hyunjun Jung","Inhwan Bae","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2410.15629v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.16865v1","updated":"2024-10-22T10:07:42Z","published":"2024-10-22T10:07:42Z","title":"Polycubes via Dual Loops","summary":"  We present a complete characterization of polycubes of any genus based on\ntheir dual structure: a collection of oriented loops which run in each of the\naxis directions and capture polycubes via their intersection patterns. A\npolycube loop structure uniquely corresponds to a polycube. We also describe\nall combinatorially different ways to add a loop to a loop structure while\nmaintaining its validity; similarly, we show how to identify loops that can be\nremoved from a polycube loop structure without invalidating it. Our\ncharacterization gives rise to an iterative algorithm to construct provably\nvalid polycube-maps for a given input surface; polycube-maps are frequently\nused to solve texture mapping, spline fitting, and hexahedral meshing. We\nshowcase some results of a proof-of-concept implementation of this iterative\nalgorithm.\n","authors":["Maxim Snoep","Bettina Speckmann","Kevin Verbeek"],"pdf_url":"https://arxiv.org/pdf/2410.16865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16851v1","updated":"2024-10-22T09:37:12Z","published":"2024-10-22T09:37:12Z","title":"Toolpath Generation for High Density Spatial Fiber Printing Guided by\n  Principal Stresses","summary":"  While multi-axis 3D printing can align continuous fibers along principal\nstresses in continuous fiber-reinforced thermoplastic (CFRTP) composites to\nenhance mechanical strength, existing methods have difficulty generating\ntoolpaths with high fiber coverage. This is mainly due to the orientation\nconsistency constraints imposed by vector-field-based methods and the turbulent\nstress fields around stress concentration regions. This paper addresses these\nchallenges by introducing a 2-RoSy representation for computing the direction\nfield, which is then converted into a periodic scalar field to generate partial\niso-curves for fiber toolpaths with nearly equal hatching distance. To improve\nfiber coverage in stress-concentrated regions, such as around holes, we extend\nthe quaternion-based method for curved slicing by incorporating winding\ncompatibility considerations. Our proposed method can achieve toolpaths\ncoverage between 87.5% and 90.6% by continuous fibers with 1.1mm width. Models\nfabricated using our toolpaths show up to 84.6% improvement in failure load and\n54.4% increase in stiffness when compared to the results obtained from\nmulti-axis 3D printing with sparser fibers.\n","authors":["Tianyu Zhang","Tao Liu","Neelotpal Dutta","Yongxue Chen","Renbo Su","Zhizhou Zhang","Weiming Wang","Charlie C. L. Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16851v1.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.17382v1","updated":"2024-10-22T19:34:53Z","published":"2024-10-22T19:34:53Z","title":"Cooperative Multi-Agent Constrained Stochastic Linear Bandits","summary":"  In this study, we explore a collaborative multi-agent stochastic linear\nbandit setting involving a network of $N$ agents that communicate locally to\nminimize their collective regret while keeping their expected cost under a\nspecified threshold $\\tau$. Each agent encounters a distinct linear bandit\nproblem characterized by its own reward and cost parameters, i.e., local\nparameters. The goal of the agents is to determine the best overall action\ncorresponding to the average of these parameters, or so-called global\nparameters. In each round, an agent is randomly chosen to select an action\nbased on its current knowledge of the system. This chosen action is then\nexecuted by all agents, then they observe their individual rewards and costs.\nWe propose a safe distributed upper confidence bound algorithm, so called\n\\textit{MA-OPLB}, and establish a high probability bound on its $T$-round\nregret. MA-OPLB utilizes an accelerated consensus method, where agents can\ncompute an estimate of the average rewards and costs across the network by\ncommunicating the proper information with their neighbors. We show that our\nregret bound is of order $\n\\mathcal{O}\\left(\\frac{d}{\\tau-c_0}\\frac{\\log(NT)^2}{\\sqrt{N}}\\sqrt{\\frac{T}{\\log(1/|\\lambda_2|)}}\\right)$,\nwhere $\\lambda_2$ is the second largest (in absolute value) eigenvalue of the\ncommunication matrix, and $\\tau-c_0$ is the known cost gap of a feasible\naction. We also experimentally show the performance of our proposed algorithm\nin different network structures.\n","authors":["Amirhossein Afsharrad","Parisa Oftadeh","Ahmadreza Moradipari","Sanjay Lall"],"pdf_url":"https://arxiv.org/pdf/2410.17382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00222v3","updated":"2024-10-22T19:16:39Z","published":"2024-03-01T01:49:57Z","title":"Efficient Reinforcement Learning for Global Decision Making in the\n  Presence of Local Agents at Scale","summary":"  We study reinforcement learning for global decision-making in the presence of\nlocal agents, where the global decision-maker makes decisions affecting all\nlocal agents, and the objective is to learn a policy that maximizes the joint\nrewards of all the agents. Such problems find many applications, e.g. demand\nresponse, EV charging, queueing, etc. In this setting, scalability has been a\nlong-standing challenge due to the size of the state space which can be\nexponential in the number of agents. This work proposes the\n\\texttt{SUBSAMPLE-Q} algorithm where the global agent subsamples $k\\leq n$\nlocal agents to compute a policy in time that is polynomial in $k$. We show\nthat this learned policy converges to the optimal policy in the order of\n$\\tilde{O}(1/\\sqrt{k}+{\\epsilon}_{k,m})$ as the number of sub-sampled agents\n$k$ increases, where ${\\epsilon}_{k,m}$ is the Bellman noise. Finally, we\nvalidate the theory through numerical simulations in a demand-response setting\nand a queueing setting.\n","authors":["Emile Anand","Guannan Qu"],"pdf_url":"https://arxiv.org/pdf/2403.00222v3.pdf","comment":"34 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.17373v1","updated":"2024-10-22T19:12:42Z","published":"2024-10-22T19:12:42Z","title":"Episodic Future Thinking Mechanism for Multi-agent Reinforcement\n  Learning","summary":"  Understanding cognitive processes in multi-agent interactions is a primary\ngoal in cognitive science. It can guide the direction of artificial\nintelligence (AI) research toward social decision-making in multi-agent\nsystems, which includes uncertainty from character heterogeneity. In this\npaper, we introduce an episodic future thinking (EFT) mechanism for a\nreinforcement learning (RL) agent, inspired by cognitive processes observed in\nanimals. To enable future thinking functionality, we first develop a\nmulti-character policy that captures diverse characters with an ensemble of\nheterogeneous policies. Here, the character of an agent is defined as a\ndifferent weight combination on reward components, representing distinct\nbehavioral preferences. The future thinking agent collects observation-action\ntrajectories of the target agents and uses the pre-trained multi-character\npolicy to infer their characters. Once the character is inferred, the agent\npredicts the upcoming actions of target agents and simulates the potential\nfuture scenario. This capability allows the agent to adaptively select the\noptimal action, considering the predicted future scenario in multi-agent\ninteractions. To evaluate the proposed mechanism, we consider the multi-agent\nautonomous driving scenario with diverse driving traits and multiple particle\nenvironments. Simulation results demonstrate that the EFT mechanism with\naccurate character inference leads to a higher reward than existing multi-agent\nsolutions. We also confirm that the effect of reward improvement remains valid\nacross societies with different levels of character diversity.\n","authors":["Dongsu Lee","Minhae Kwon"],"pdf_url":"https://arxiv.org/pdf/2410.17373v1.pdf","comment":"NeurIPS 2024 (Web: https://sites.google.com/view/eftm-neurips2024)"},{"id":"http://arxiv.org/abs/2402.12416v3","updated":"2024-10-22T18:10:01Z","published":"2024-02-19T08:18:53Z","title":"Aligning Individual and Collective Objectives in Multi-Agent Cooperation","summary":"  Among the research topics in multi-agent learning, mixed-motive cooperation\nis one of the most prominent challenges, primarily due to the mismatch between\nindividual and collective goals. The cutting-edge research is focused on\nincorporating domain knowledge into rewards and introducing additional\nmechanisms to incentivize cooperation. However, these approaches often face\nshortcomings such as the effort on manual design and the absence of theoretical\ngroundings. To close this gap, we model the mixed-motive game as a\ndifferentiable game for the ease of illuminating the learning dynamics towards\ncooperation. More detailed, we introduce a novel optimization method named\n\\textbf{\\textit{A}}ltruistic \\textbf{\\textit{G}}radient\n\\textbf{\\textit{A}}djustment (\\textbf{\\textit{AgA}}) that employs gradient\nadjustments to progressively align individual and collective objectives.\nFurthermore, we theoretically prove that AgA effectively attracts gradients to\nstable fixed points of the collective objective while considering individual\ninterests, and we validate these claims with empirical evidence. We evaluate\nthe effectiveness of our algorithm AgA through benchmark environments for\ntesting mixed-motive collaboration with small-scale agents such as the\ntwo-player public good game and the sequential social dilemma games, Cleanup\nand Harvest, as well as our self-developed large-scale environment in the game\nStarCraft II.\n","authors":["Yang Li","Wenhao Zhang","Jianhong Wang","Shao Zhang","Yali Du","Ying Wen","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2402.12416v3.pdf","comment":"20 pages; Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.17221v1","updated":"2024-10-22T17:45:45Z","published":"2024-10-22T17:45:45Z","title":"Scalable spectral representations for network multiagent control","summary":"  Network Markov Decision Processes (MDPs), a popular model for multi-agent\ncontrol, pose a significant challenge to efficient learning due to the\nexponential growth of the global state-action space with the number of agents.\nIn this work, utilizing the exponential decay property of network dynamics, we\nfirst derive scalable spectral local representations for network MDPs, which\ninduces a network linear subspace for the local $Q$-function of each agent.\nBuilding on these local spectral representations, we design a scalable\nalgorithmic framework for continuous state-action network MDPs, and provide\nend-to-end guarantees for the convergence of our algorithm. Empirically, we\nvalidate the effectiveness of our scalable representation-based approach on two\nbenchmark problems, and demonstrate the advantages of our approach over generic\nfunction approximation approaches to representing the local $Q$-functions.\n","authors":["Zhaolin Ren"," Runyu"," Zhang","Bo Dai","Na Li"],"pdf_url":"https://arxiv.org/pdf/2410.17221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17068v1","updated":"2024-10-22T14:47:08Z","published":"2024-10-22T14:47:08Z","title":"Delay-Constrained Grant-Free Random Access in MIMO Systems: Distributed\n  Pilot Allocation and Power Control","summary":"  We study a delay-constrained grant-free random access system with a\nmulti-antenna base station. The users randomly generate data packets with\nexpiration deadlines, which are then transmitted from data queues on a first-in\nfirst-out basis. To deliver a packet, a user needs to succeed in both random\naccess phase (sending a pilot without collision) and data transmission phase\n(achieving a required data rate with imperfect channel information) before the\npacket expires. We develop a distributed, cross-layer policy that allows the\nusers to dynamically and independently choose their pilots and transmit powers\nto achieve a high effective sum throughput with fairness consideration. Our\npolicy design involves three key components: 1) a proxy of the instantaneous\ndata rate that depends only on macroscopic environment variables and\ntransmission decisions, considering pilot collisions and imperfect channel\nestimation; 2) a quantitative, instantaneous measure of fairness within each\ncommunication round; and 3) a deep learning-based, multi-agent control\nframework with centralized training and distributed execution. The proposed\nframework benefits from an accurate, differentiable objective function for\ntraining, thereby achieving a higher sample efficiency compared with a\nconventional application of model-free, multi-agent reinforcement learning\nalgorithms. The performance of the proposed approach is verified by simulations\nunder highly dynamic and heterogeneous scenarios.\n","authors":["Jianan Bai","Zheng Chen","Erik. G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2410.17068v1.pdf","comment":"15 pages, 7 figures. Accepted for publication in IEEE Transactions on\n  Cognitive Communications and Networking"},{"id":"http://arxiv.org/abs/2410.19855v1","updated":"2024-10-22T14:11:26Z","published":"2024-10-22T14:11:26Z","title":"Personalized Recommendation Systems using Multimodal, Autonomous, Multi\n  Agent Systems","summary":"  This paper describes a highly developed personalised recommendation system\nusing multimodal, autonomous, multi-agent systems. The system focuses on the\nincorporation of futuristic AI tech and LLMs like Gemini-1.5- pro and LLaMA-70B\nto improve customer service experiences especially within e-commerce. Our\napproach uses multi agent, multimodal systems to provide best possible\nrecommendations to its users. The system is made up of three agents as a whole.\nThe first agent recommends products appropriate for answering the given\nquestion, while the second asks follow-up questions based on images that belong\nto these recommended products and is followed up with an autonomous search by\nthe third agent. It also features a real-time data fetch, user\npreferences-based recommendations and is adaptive learning. During complicated\nqueries the application processes with Symphony, and uses the Groq API to\nanswer quickly with low response times. It uses a multimodal way to utilize\ntext and images comprehensively, so as to optimize product recommendation and\ncustomer interaction.\n","authors":["Param Thakkar","Anushka Yadav"],"pdf_url":"https://arxiv.org/pdf/2410.19855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16946v1","updated":"2024-10-22T12:20:23Z","published":"2024-10-22T12:20:23Z","title":"Self-Evolving Multi-Agent Collaboration Networks for Software\n  Development","summary":"  LLM-driven multi-agent collaboration (MAC) systems have demonstrated\nimpressive capabilities in automatic software development at the function\nlevel. However, their heavy reliance on human design limits their adaptability\nto the diverse demands of real-world software development. To address this\nlimitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC\nnetworks. Inspired by traditional neural network training, EvoMAC obtains\ntext-based environmental feedback by verifying the MAC network's output against\na target proxy and leverages a novel textual backpropagation to update the\nnetwork. To extend coding capabilities beyond function-level tasks to more\nchallenging software-level development, we further propose rSDE-Bench, a\nrequirement-oriented software development benchmark, which features complex and\ndiverse software requirements along with automatic evaluation of requirement\ncorrectness. Our experiments show that: i) The automatic requirement-aware\nevaluation in rSDE-Bench closely aligns with human evaluations, validating its\nreliability as a software-level coding benchmark. ii) EvoMAC outperforms\nprevious SOTA methods on both the software-level rSDE-Bench and the\nfunction-level HumanEval benchmarks, reflecting its superior coding\ncapabilities. The benchmark can be downloaded at\nhttps://yuzhu-cai.github.io/rSDE-Bench/.\n","authors":["Yue Hu","Yuzhu Cai","Yaxin Du","Xinyu Zhu","Xiangrui Liu","Zijie Yu","Yuchen Hou","Shuo Tang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.16946v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2305.05747v2","updated":"2024-10-22T08:24:47Z","published":"2023-05-09T19:59:48Z","title":"Persistent synchronization of heterogeneous networks with time-dependent\n  linear diffusive coupling","summary":"  We study synchronization for linearly coupled temporal networks of\nheterogeneous time-dependent nonlinear agents via the convergence of attracting\ntrajectories of each node. The results are obtained by constructing and\nstudying the stability of a suitable linear nonautonomous problem bounding the\nevolution of the synchronization errors. Both, the case of the entire network\nand only a cluster, are addressed and the persistence of the obtained\nsynchronization against perturbation is also discussed. Furthermore, a\nsufficient condition for the existence of attracting trajectories of each node\nis given. In all cases, the considered dependence on time requires only local\nintegrability, which is a very mild regularity assumption. Moreover, our\nresults mainly depend on the network structure and its properties, and achieve\nsynchronization up to a constant in finite time. Hence they are quite suitable\nfor applications. The applicability of the results is showcased via several\nexamples: coupled van-der-Pol/FitzHugh-Nagumo oscillators, weighted/signed\nopinion dynamics, and coupled Lorenz systems.\n","authors":["Hildeberto Jard√≥n-Kojakhmetov","Christian Kuehn","Iacopo P. Longo"],"pdf_url":"https://arxiv.org/pdf/2305.05747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15490v2","updated":"2024-10-22T07:46:35Z","published":"2024-10-20T20:07:36Z","title":"Dynamic Intelligence Assessment: Benchmarking LLMs on the Road to AGI\n  with a Focus on Model Confidence","summary":"  As machine intelligence evolves, the need to test and compare the\nproblem-solving abilities of different AI models grows. However, current\nbenchmarks are often overly simplistic, allowing models to perform uniformly\nwell, making it difficult to distinguish their capabilities. Additionally,\nbenchmarks typically rely on static question-answer pairs, which models might\nmemorize or guess. To address these limitations, we introduce the Dynamic\nIntelligence Assessment (DIA), a novel methodology for testing AI models using\ndynamic question templates and improved metrics across multiple disciplines\nsuch as mathematics, cryptography, cybersecurity, and computer science. The\naccompanying DIA-Bench dataset, which includes 150 diverse and challenging task\ntemplates with mutable parameters, is presented in various formats such as\ntext, PDFs, compiled binaries, and visual puzzles. Our framework introduces\nfour new metrics to assess a model's reliability and confidence across multiple\nattempts. These metrics revealed that even simple questions are frequently\nanswered incorrectly when posed in varying forms, highlighting significant gaps\nin models' reliability. Notably, models like GPT-4o tended to overestimate\ntheir mathematical abilities, while ChatGPT-4o demonstrated better\ndecision-making and performance through effective tool usage. We evaluated\neight state-of-the-art large language models (LLMs) using DIA-Bench, showing\nthat current models struggle with complex tasks and often display unexpectedly\nlow confidence, even with simpler questions. The DIA framework sets a new\nstandard for assessing not only problem-solving but also a model's adaptive\nintelligence and ability to assess its own limitations. The dataset is publicly\navailable on our project's website.\n","authors":["Norbert Tihanyi","Tamas Bisztray","Richard A. Dubniczky","Rebeka Toth","Bertalan Borsos","Bilel Cherif","Mohamed Amine Ferrag","Lajos Muzsai","Ridhi Jain","Ryan Marinelli","Lucas C. Cordeiro","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2410.15490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16686v1","updated":"2024-10-22T04:35:57Z","published":"2024-10-22T04:35:57Z","title":"SERN: Simulation-Enhanced Realistic Navigation for Multi-Agent Robotic\n  Systems in Contested Environments","summary":"  The increasing deployment of autonomous systems in complex environments\nnecessitates efficient communication and task completion among multiple agents.\nThis paper presents SERN (Simulation-Enhanced Realistic Navigation), a novel\nframework integrating virtual and physical environments for real-time\ncollaborative decision-making in multi-robot systems. SERN addresses key\nchallenges in asset deployment and coordination through a bi-directional\ncommunication framework using the AuroraXR ROS Bridge. Our approach advances\nthe SOTA through accurate real-world representation in virtual environments\nusing Unity high-fidelity simulator; synchronization of physical and virtual\nrobot movements; efficient ROS data distribution between remote locations; and\nintegration of SOTA semantic segmentation for enhanced environmental\nperception. Our evaluations show a 15% to 24% improvement in latency and up to\na 15% increase in processing efficiency compared to traditional ROS setups.\nReal-world and virtual simulation experiments with multiple robots demonstrate\nsynchronization accuracy, achieving less than 5 cm positional error and under\n2-degree rotational error. These results highlight SERN's potential to enhance\nsituational awareness and multi-agent coordination in diverse, contested\nenvironments.\n","authors":["Jumman Hossain","Emon Dey","Snehalraj Chugh","Masud Ahmed","MS Anwar","Abu-Zaher Faridee","Jason Hoppes","Theron Trout","Anjon Basak","Rafidh Chowdhury","Rishabh Mistry","Hyun Kim","Jade Freeman","Niranjan Suri","Adrienne Raglin","Carl Busart","Timothy Gregory","Anuradha Ravi","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2410.16686v1.pdf","comment":"Under Review for ICRA 2025"},{"id":"http://arxiv.org/abs/2410.16629v1","updated":"2024-10-22T02:18:44Z","published":"2024-10-22T02:18:44Z","title":"Cutting Through the Confusion and Hype: Understanding the True Potential\n  of Generative AI","summary":"  This paper explores the nuanced landscape of generative AI (genAI),\nparticularly focusing on neural network-based models like Large Language Models\n(LLMs). While genAI garners both optimistic enthusiasm and sceptical criticism,\nthis work seeks to provide a balanced examination of its capabilities,\nlimitations, and the profound impact it may have on societal functions and\npersonal interactions. The first section demystifies language-based genAI\nthrough detailed discussions on how LLMs learn, their computational needs,\ndistinguishing features from supporting technologies, and the inherent\nlimitations in their accuracy and reliability. Real-world examples illustrate\nthe practical applications and implications of these technologies. The latter\npart of the paper adopts a systems perspective, evaluating how the integration\nof LLMs with existing technologies can enhance productivity and address\nemerging concerns. It highlights the need for significant investment to\nunderstand the implications of recent advancements, advocating for a\nwell-informed dialogue to ethically and responsibly integrate genAI into\ndiverse sectors. The paper concludes with prospective developments and\nrecommendations, emphasizing a forward-looking approach to harnessing genAI`s\npotential while mitigating its risks.\n","authors":["Ante Prodan","Jo-An Occhipinti","Rehez Ahlip","Goran Ujdur","Harris A. Eyre","Kyle Goosen","Luke Penza","Mark Heffernan"],"pdf_url":"https://arxiv.org/pdf/2410.16629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16600v1","updated":"2024-10-22T00:55:04Z","published":"2024-10-22T00:55:04Z","title":"Convex Markov Games: A Framework for Fairness, Imitation, and Creativity\n  in Multi-Agent Learning","summary":"  Expert imitation, behavioral diversity, and fairness preferences give rise to\npreferences in sequential decision making domains that do not decompose\nadditively across time. We introduce the class of convex Markov games that\nallow general convex preferences over occupancy measures. Despite infinite time\nhorizon and strictly higher generality than Markov games, pure strategy Nash\nequilibria exist under strict convexity. Furthermore, equilibria can be\napproximated efficiently by performing gradient descent on an upper bound of\nexploitability. Our experiments imitate human choices in ultimatum games,\nreveal novel solutions to the repeated prisoner's dilemma, and find fair\nsolutions in a repeated asymmetric coordination game. In the prisoner's\ndilemma, our algorithm finds a policy profile that deviates from observed human\nplay only slightly, yet achieves higher per-player utility while also being\nthree orders of magnitude less exploitable.\n","authors":["Ian Gemp","Andreas Haupt","Luke Marris","Siqi Liu","Georgios Piliouras"],"pdf_url":"https://arxiv.org/pdf/2410.16600v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.11402v2","updated":"2024-10-22T23:13:34Z","published":"2024-09-17T17:59:06Z","title":"NVLM: Open Frontier-Class Multimodal LLMs","summary":"  We introduce NVLM 1.0, a family of frontier-class multimodal large language\nmodels (LLMs) that achieve state-of-the-art results on vision-language tasks,\nrivaling the leading proprietary models (e.g., GPT-4o) and open-access models\n(e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved\ntext-only performance over its LLM backbone after multimodal training. In terms\nof model design, we perform a comprehensive comparison between decoder-only\nmultimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g.,\nFlamingo). Based on the strengths and weaknesses of both approaches, we propose\na novel architecture that enhances both training efficiency and multimodal\nreasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for\ntile-based dynamic high-resolution images, which significantly boosts\nperformance on multimodal reasoning and OCR-related tasks. Regarding training\ndata, we meticulously curate and provide detailed information on our multimodal\npretraining and supervised fine-tuning datasets. Our findings indicate that\ndataset quality and task diversity are more important than scale, even during\nthe pretraining phase, across all architectures. Notably, we develop\nproduction-grade multimodality for the NVLM-1.0 models, enabling them to excel\nin vision-language tasks while maintaining and even improving text-only\nperformance compared to their LLM backbones. To achieve this, we craft and\nintegrate a high-quality text-only dataset into multimodal training, alongside\na substantial amount of multimodal math and reasoning data, leading to enhanced\nmath and coding capabilities across modalities. To advance research in the\nfield, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B\nand will open-source the training code for the community soon.\n","authors":["Wenliang Dai","Nayeon Lee","Boxin Wang","Zhuolin Yang","Zihan Liu","Jon Barker","Tuomas Rintamaki","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2409.11402v2.pdf","comment":"Fixed the typos. For more information, please visit our project page\n  at: https://research.nvidia.com/labs/adlr/NVLM-1"},{"id":"http://arxiv.org/abs/2408.11982v3","updated":"2024-10-22T16:58:09Z","published":"2024-08-21T20:32:45Z","title":"AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results","summary":"  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n","authors":["Maksim Smirnov","Aleksandr Gushchin","Anastasia Antsiferova","Dmitry Vatolin","Radu Timofte","Ziheng Jia","Zicheng Zhang","Wei Sun","Jiaying Qian","Yuqin Cao","Yinan Sun","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Kanjar De","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Wenhui Meng","Zhenzhong Chen","Zhengxue Cheng","Jiahao Xiao","Jun Xu","Chenlong He","Qi Zheng","Ruoxi Zhu","Min Li","Yibo Fan","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2408.11982v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17073v1","updated":"2024-10-22T14:50:19Z","published":"2024-10-22T14:50:19Z","title":"Personalized Playback Technology: How Short Video Services Create\n  Excellent User Experience","summary":"  Short-form video content has become increasingly popular and influential in\nrecent years. Its concise yet engaging format aligns well with todays'\nfast-paced and on-the-go lifestyles, making it a dominating trend in the\ndigital world. As one of the front runners in the short video platform space,\nByteDance has been highly successful in delivering a one-of-a-kind short video\nexperience and attracting billions of users worldwide. One key contributing\nfactor is its advanced end-to-end personalized short video playback technology,\nwhere we pioneered and developed the new technical field over the past five\nyears to optimize user experience. This paper introduces the major concepts and\nmethodologies of this personalized video playback technology that distinguish\nit from traditional multimedia technologies. More details, including goal\nsetting, iterative process, modeling, experimental methods and required\nsupporting systems, are also provided to encourage deeper research in this\narea.\n","authors":["Weihui Deng","Zhiwei Fan","Deliang Fu","Yun Gong","Shenglan Huang","Xiaocheng Li","Zheng Li","Yiting Liao","He Liu","Chunyu Qiao","Bin Wang","Zhen Wang","Zhengyu Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.17073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16532v2","updated":"2024-10-22T14:40:15Z","published":"2024-08-29T13:43:36Z","title":"WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling","summary":"  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n","authors":["Shengpeng Ji","Ziyue Jiang","Wen Wang","Yifu Chen","Minghui Fang","Jialong Zuo","Qian Yang","Xize Cheng","Zehan Wang","Ruiqi Li","Ziang Zhang","Xiaoda Yang","Rongjie Huang","Yidi Jiang","Qian Chen","Siqi Zheng","Wen Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16532v2.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.21303v1","updated":"2024-10-22T10:12:11Z","published":"2024-10-22T10:12:11Z","title":"VEMOCLAP: A video emotion classification web application","summary":"  We introduce VEMOCLAP: Video EMOtion Classifier using Pretrained features,\nthe first readily available and open-source web application that analyzes the\nemotional content of any user-provided video. We improve our previous work,\nwhich exploits open-source pretrained models that work on video frames and\naudio, and then efficiently fuse the resulting pretrained features using\nmulti-head cross-attention. Our approach increases the state-of-the-art\nclassification accuracy on the Ekman-6 video emotion dataset by 4.3% and offers\nan online application for users to run our model on their own videos or YouTube\nvideos. We invite the readers to try our application at serkansulun.com/app.\n","authors":["Serkan Sulun","Paula Viana","Matthew E. P. Davies"],"pdf_url":"https://arxiv.org/pdf/2410.21303v1.pdf","comment":"Accepted to 2024 IEEE International Symposium on Multimedia (ISM),\n  Tokyo, Japan"},{"id":"http://arxiv.org/abs/2410.15778v2","updated":"2024-10-22T05:01:28Z","published":"2024-10-21T08:42:30Z","title":"Reducing Hallucinations in Vision-Language Models via Latent Space\n  Steering","summary":"  Hallucination poses a challenge to the deployment of large vision-language\nmodels (LVLMs) in applications. Unlike in large language models (LLMs),\nhallucination in LVLMs often arises from misalignments between visual inputs\nand textual outputs. This paper investigates the underlying mechanisms of\nhallucination, focusing on the unique structure of LVLMs that distinguishes\nthem from large language models (LLMs). We identify that hallucinations often\narise from the sensitivity of text decoders to vision inputs, a natural\nphenomenon when image encoders and text decoders are pre-trained separately.\nInspired by this, we introduce Visual and Textual Intervention (VTI), a novel\ntechnique designed to reduce hallucinations by steering latent space\nrepresentations during inference to enhance the stability of vision features.\nAs a task-agnostic test-time intervention, VTI can be easily applied to any\nproblem without additional cost. Extensive experiments demonstrate that it can\neffectively reduce hallucinations and outperform baseline methods across\nmultiple metrics, highlighting the critical role of vision feature stability in\nLVLMs.\n","authors":["Sheng Liu","Haotian Ye","Lei Xing","James Zou"],"pdf_url":"https://arxiv.org/pdf/2410.15778v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2404.01617v2","updated":"2024-10-22T04:09:15Z","published":"2024-04-02T03:43:55Z","title":"Designing Network Algorithms via Large Language Models","summary":"  We introduce NADA, the first framework to autonomously design network\nalgorithms by leveraging the generative capabilities of large language models\n(LLMs). Starting with an existing algorithm implementation, NADA enables LLMs\nto create a wide variety of alternative designs in the form of code blocks. It\nthen efficiently identifies the top-performing designs through a series of\nfiltering techniques, minimizing the need for full-scale evaluations and\nsignificantly reducing computational costs. Using adaptive bitrate (ABR)\nstreaming as a case study, we demonstrate that NADA produces novel ABR\nalgorithms -- previously unknown to human developers -- that consistently\noutperform the original algorithm in diverse network environments, including\nbroadband, satellite, 4G, and 5G.\n","authors":["Zhiyuan He","Aashish Gottipati","Lili Qiu","Xufang Luo","Kenuo Xu","Yuqing Yang","Francis Y. Yan"],"pdf_url":"https://arxiv.org/pdf/2404.01617v2.pdf","comment":null}]},"2024-10-21T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.16553v1","updated":"2024-10-21T22:23:01Z","published":"2024-10-21T22:23:01Z","title":"Distributed Computation of Persistent Cohomology","summary":"  Persistent (co)homology is a central construction in topological data\nanalysis, where it is used to quantify prominence of features in data to\nproduce stable descriptors suitable for downstream analysis. Persistence is\nchallenging to compute in parallel because it relies on global connectivity of\nthe data. We propose a new algorithm to compute persistent cohomology in the\ndistributed setting. It combines domain and range partitioning. The former is\nused to reduce and sparsify the coboundary matrix locally. After this initial\nlocal reduction, we redistribute the matrix across processors for the global\nreduction. We experimentally compare our cohomology algorithm with DIPHA, the\nonly publicly available code for distributed computation of persistent\n(co)homology; our algorithm demonstrates a significant improvement in strong\nscaling.\n","authors":["Arnur Nigmetov","Dmitriy Morozov"],"pdf_url":"https://arxiv.org/pdf/2410.16553v1.pdf","comment":"11 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.15023v4","updated":"2024-10-21T13:41:50Z","published":"2024-09-23T13:50:39Z","title":"Efficient Nearest Neighbor Search Using Dynamic Programming","summary":"  Given a collection of points in R^3, KD-Tree and R-Tree are well-known\nnearest neighbor search (NNS) algorithms that rely on space partitioning and\nspatial indexing techniques. However, when the query point is far from the data\npoints or the data points inherently represent a 2-manifold surface, their\nquery performance may degrade. To address this, we propose a novel dynamic\nprogramming technique that precomputes a Directed Acyclic Graph (DAG) to encode\nthe proximity structure between data points. More specifically, the DAG\ncaptures how the proximity structure evolves during the incremental\nconstruction of the Voronoi diagram of the data points. Experimental results\ndemonstrate that our method achieves a 1x-10x speedup. Additionally, our\nalgorithm offers several valuable features. For instance, it naturally supports\nan O(k \\log n) algorithm for farthest point sampling, where k is the desired\nnumber of sample points. Moreover, density peak clustering, which involves\nfinding the nearest point among the top K points, is typically considered to\nhave a time complexity of O(n^2). With our algorithm, this can be reduced to\nO(n \\log n). We believe this work will inspire further research on the NNS\nproblem.\n","authors":["Pengfei Wang","Jiantao Song","Shiqing Xin","Shuangmin Chen","Changhe Tu","Wenping Wang","Jiaye Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17093v3","updated":"2024-10-21T10:50:05Z","published":"2023-12-28T16:11:11Z","title":"Discrete transforms of quantized persistence diagrams","summary":"  Topological data analysis leverages topological features to analyze datasets,\nwith applications in diverse fields like medical sciences and biology. A key\ntool of this theory is the persistence diagram, which encodes topological\ninformation but poses challenges for integration into standard machine learning\npipelines. We introduce Qupid (QUantized Persistence and Integral transforms of\nDiagrams), a novel and simple method for vectorizing persistence diagrams.\nFirst, Qupid uses a binning procedure to turn persistence diagrams into finite\nmeasures on a grid and then applies discrete transforms to these measures. Key\nfeatures are the choice of log-scaled grids that emphasize information\ncontained near the diagonal in persistence diagrams, combined with the use of\ndiscrete transforms to enhance and efficiently encode the obtained topological\ninformation. We conduct an in-depth experimental analysis of Qupid, showing\nthat the simplicity of our method results in very low computational costs while\npreserving highly competitive performances compared to state-of-the-art methods\nacross numerous classification tasks on both synthetic and real-world datasets.\nFinally, we provide experimental evidence that our method is robust to a\ndecrease in the grid resolution used.\n","authors":["Michael Etienne Van Huffel","Olympio Hacquard","Vadim Lebovici","Matteo Palo"],"pdf_url":"https://arxiv.org/pdf/2312.17093v3.pdf","comment":"13 pages, 6 figures, major changes to paper"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.16566v1","updated":"2024-10-21T22:55:59Z","published":"2024-10-21T22:55:59Z","title":"The Social Cost of Growth: Evaluating GMV-Centric and Welfare-Centric\n  Strategies in Online Food Delivery Platforms","summary":"  This paper develops a comprehensive theoretical framework to analyze the\ntrade-offs between Gross Merchandise Volume (GMV) maximization and social\nwelfare optimization in online food delivery platforms. Using a multi-agent\nsimulation and a dual-model approach based on two-sided market theory and\nwelfare economics, we evaluate the impact of GMV-centric and welfare-centric\nstrategies on platform dynamics, including pricing mechanisms, stakeholder\nwelfare, and market efficiency. Our results show that GMV maximization\nstrategies drive rapid short-term transaction growth but lead to uneven welfare\ndistribution, particularly disadvantaging delivery workers. In contrast,\nwelfare-centric strategies promote a more balanced and equitable distribution\nof benefits among consumers, restaurants, and delivery workers, enhancing\nplatform sustainability in the long run. These findings provide actionable\ninsights for platform operators and policymakers to design strategies that\nbalance growth with social welfare, ensuring both economic efficiency and\nfairness.\n","authors":["Yukun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.16566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16548v1","updated":"2024-10-21T22:19:15Z","published":"2024-10-21T22:19:15Z","title":"On the Uniqueness of Nash Equilibria in Multiagent Matrix Games","summary":"  We provide a complete characterization for uniqueness of equilibria in\nunconstrained polymatrix games. We show that while uniqueness is natural for\ncoordination and general polymatrix games, zero-sum games require that the\ndimension of the combined strategy space is even. Therefore, non-uniqueness is\ncommon in zero-sum polymatrix games. In addition, we study the impact of\nnon-uniqueness on classical learning dynamics for multiagent systems and show\nthat the classical methods still yield unique estimates even when there is not\na unique equilibrium.\n","authors":["James P. Bailey"],"pdf_url":"https://arxiv.org/pdf/2410.16548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16441v1","updated":"2024-10-21T19:10:49Z","published":"2024-10-21T19:10:49Z","title":"Policies with Sparse Inter-Agent Dependencies in Dynamic Games: A\n  Dynamic Programming Approach","summary":"  Common feedback strategies in multi-agent dynamic games require all players'\nstate information to compute control strategies. However, in real-world\nscenarios, sensing and communication limitations between agents make full state\nfeedback expensive or impractical, and such strategies can become fragile when\nstate information from other agents is inaccurate. To this end, we propose a\nregularized dynamic programming approach for finding sparse feedback policies\nthat selectively depend on the states of a subset of agents in dynamic games.\nThe proposed approach solves convex adaptive group Lasso problems to compute\nsparse policies approximating Nash equilibrium solutions. We prove the\nregularized solutions' asymptotic convergence to a neighborhood of Nash\nequilibrium policies in linear-quadratic (LQ) games. We extend the proposed\napproach to general non-LQ games via an iterative algorithm. Empirical results\nin multi-robot interaction scenarios show that the proposed approach\neffectively computes feedback policies with varying sparsity levels. When\nagents have noisy observations of other agents' states, simulation results\nindicate that the proposed regularized policies consistently achieve lower\ncosts than standard Nash equilibrium policies by up to 77% for all interacting\nagents whose costs are coupled with other agents' states.\n","authors":["Xinjie Liu","Jingqi Li","Filippos Fotiadis","Mustafa O. Karabag","Jesse Milzman","David Fridovich-Keil","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.16441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16276v3","updated":"2024-10-21T15:59:42Z","published":"2024-05-25T15:24:21Z","title":"Mechanism Design for LLM Fine-tuning with Multiple Reward Models","summary":"  Fine-tuning large language models (LLMs) to aggregate multiple preferences\nhas attracted considerable research attention. With aggregation algorithms\nadvancing, a potential economic scenario arises where fine-tuning services are\nprovided to agents with different preferences. In this context, agents may\nbenefit from strategically misreporting their preferences, which could affect\nthe fine-tuned outcomes. This paper addresses such incentive issues by framing\nit as a mechanism design problem: an LLM provider determines the fine-tuning\nobjective (training rule) and the pricing scheme (payment rule) for agents. We\nprimarily focus on a representative class of training rules that maximize\nsocial welfare subject to certain regularizations, referred to as \\tr\\ rules.\nFirstly, we show that under most circumstances, truthful reporting is\nsub-optimal with simply a training rule, thereby highlighting the necessity of\npayments. Secondly, we design affine maximizer payment rules that implement\n\\tr\\ rules in dominant-strategy incentive compatibility (DSIC). We characterize\nsufficient conditions for payment equivalence properties. For a training rule\nthat satisfies these conditions, we have found all the payment rules that\nimplement it in DSIC, as they only differ by a constant term irrelevant to\nagents' reports from each other. Thirdly, we demonstrate that our mechanism is\napproximately DSIC even with perturbed input, showcasing its robustness against\nthe inevitable errors in real-world applications. Experiments on real LLM\nsetups further confirm the practical implications of our results.\n","authors":["Haoran Sun","Yurong Chen","Siwei Wang","Wei Chen","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2405.16276v3.pdf","comment":"35 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.16131v1","updated":"2024-10-21T15:56:08Z","published":"2024-10-21T15:56:08Z","title":"Constrained Truthful Obnoxious Two-Facility Location with Optional\n  Preferences","summary":"  We consider a truthful facility location problem with agents that have\nprivate positions on the line of real numbers and known optional preferences\nover two obnoxious facilities that must be placed at locations chosen from a\ngiven set of candidate ones. Each agent wants to be as far away as possible\nfrom the facilities that affect her, and our goal is to design mechanisms that\ndecide where to place the facilities so as to maximize the total happiness of\nthe agents as well as provide the right incentives to them to truthfully report\ntheir positions. We consider separately the setting in which all agents are\naffected by both facilities (i.e., they have non-optional preferences) and the\ngeneral optional setting. We show tight bounds on the approximation ratio of\ndeterministic strategyproof mechanisms for both settings, and almost tight\nbounds for randomized mechanisms.\n","authors":["Panagiotis Kanellopoulos","Alexandros A. Voudouris"],"pdf_url":"https://arxiv.org/pdf/2410.16131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04787v2","updated":"2024-10-21T12:36:33Z","published":"2024-10-07T06:54:42Z","title":"A Differentially Private Energy Trading Mechanism Approaching Social\n  Optimum","summary":"  This paper proposes a differentially private energy trading mechanism for\nprosumers in peer-to-peer (P2P) markets, offering provable privacy guarantees\nwhile approaching the Nash equilibrium with nearly socially optimal efficiency.\nWe first model the P2P energy trading as a (generalized) Nash game and prove\nthe vulnerability of traditional distributed algorithms to privacy attacks\nthrough an adversarial inference model. To address this challenge, we develop a\nprivacy-preserving Nash equilibrium seeking algorithm incorporating carefully\ncalibrated Laplacian noise. We prove that the proposed algorithm achieves\n$\\epsilon$-differential privacy while converging in expectation to the Nash\nequilibrium with a suitable stepsize. Numerical experiments are conducted to\nevaluate the algorithm's robustness against privacy attacks, convergence\nbehavior, and optimality compared to the non-private solution. Results\ndemonstrate that our mechanism effectively protects prosumers' sensitive\ninformation while maintaining near-optimal market outcomes, offering a\npractical approach for privacy-preserving coordination in P2P markets.\n","authors":["Yuji Cao","Yue Chen"],"pdf_url":"https://arxiv.org/pdf/2410.04787v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15738v1","updated":"2024-10-21T07:58:16Z","published":"2024-10-21T07:58:16Z","title":"A Fair Allocation is Approximately Optimal for Indivisible Chores, or Is\n  It?","summary":"  In this paper, we study the allocation of indivisible chores and consider the\nproblem of finding a fair allocation that is approximately efficient. We shift\nour attention from the multiplicative approximation to the additive one. Our\nresults are twofold, with (1) bounding how the optimal social cost escalates\nresulting from fairness requirements and (2) presenting the hardness of\napproximation for the problems of finding fair allocations with the minimum\nsocial cost. To quantify the escalation, we introduce cost of fairness (CoF)\n$\\unicode{x2014}$ an alternative to the price of fairness (PoF)\n$\\unicode{x2014}$ to bound the difference (v.s. ratio for PoF) between the\noptimal social cost with and without fairness constraints in the worst-case\ninstance. We find that CoF is more informative than PoF for chores in the sense\nthat the PoF is infinity regarding all EQX (equitable up to any item), EQ1\n(equitable up to one item) and EF1 (envy-free up to one item), while the CoF is\n$n$ regarding EQX and 1 regarding EQ1 and EF1, where $n$ is the number of\nagents. For inapproximability, we present a detailed picture of hardness of\napproximation. We prove that finding the optimal EQX allocation within an\nadditive approximation factor of $n$ is NP-hard for any $n \\geq 2$ where $n$ is\nthe number of agents and the cost functions are normalized to 1. For EQ1 and\nEF1, the problem is NP-hard when the additive factor is a constant and $n \\geq\n3$. When $n = 2$, we design additive approximation schemes for EQ1 and EF1.\n","authors":["Bo Li","Ankang Sun","Shiji Xing"],"pdf_url":"https://arxiv.org/pdf/2410.15738v1.pdf","comment":"Appears in the 20th Conference on Web and Internet Economics (WINE),\n  2024"},{"id":"http://arxiv.org/abs/2410.15684v1","updated":"2024-10-21T06:51:16Z","published":"2024-10-21T06:51:16Z","title":"A Machine Learning Approach to Detect Strategic Behavior from\n  Large-Population Observational Data Applied to Game Mode Prediction on a\n  Team-Based Video Game","summary":"  Modeling the strategic behavior of agents in a real-world multi-agent system\nusing existing state-of-the-art computational game-theoretic tools can be a\ndaunting task, especially when only the actions taken by the agents can be\nobserved. Before attempting such a task, it would be useful to gain insight\ninto whether or not agents are in fact acting strategically at all, from a\ngame-theoretic perspective. In this paper, we present an initial step toward\naddressing this problem by proposing a general approach based on machine\nlearning fundamentals for detecting potentially strategic behavior. We\ninstantiate the approach by applying state-of-the-art machine learning tools\nfor model selection and performance evaluation of prediction models in the\ncontext of detecting the strategic behavior of players for game mode selection\nin the multiplayer online video game Heroes of the Storm. Specifically, as a\nbaseline, we first train neural networks to predict players' game mode\nselections using only information about the state of the player themselves.\nThen, we train a new set of neural networks using the same architectures, this\ntime incorporating \"historical co-play\" features that encode players' past\ninteractions with other players. We find that including these new features led\nto statistically significant improvements in game mode prediction accuracy,\nproviding a sufficiently strong signal that players indeed make decisions\nstrategically, which justifies the development of more complex computational\ngame-theoretic tools in the hope of improving modeling and predictive power. We\ndiscuss remaining research work about potential approaches to validate the\neffectiveness of this initial step to detect strategic behavior.\n","authors":["Boshen Wang","Luis E. Ortiz"],"pdf_url":"https://arxiv.org/pdf/2410.15684v1.pdf","comment":"8 pages, 1 figure, submitted to AAMAS2025"},{"id":"http://arxiv.org/abs/2410.15600v1","updated":"2024-10-21T02:53:18Z","published":"2024-10-21T02:53:18Z","title":"Patrol Security Game: Defending Against Adversary with Freedom in Attack\n  Timing, Location, and Duration","summary":"  We explored the Patrol Security Game (PSG), a robotic patrolling problem\nmodeled as an extensive-form Stackelberg game, where the attacker determines\nthe timing, location, and duration of their attack. Our objective is to devise\na patrolling schedule with an infinite time horizon that minimizes the\nattacker's payoff. We demonstrated that PSG can be transformed into a\ncombinatorial minimax problem with a closed-form objective function. By\nconstraining the defender's strategy to a time-homogeneous first-order Markov\nchain (i.e., the patroller's next move depends solely on their current\nlocation), we proved that the optimal solution in cases of zero penalty\ninvolves either minimizing the expected hitting time or return time, depending\non the attacker model, and that these solutions can be computed efficiently.\nAdditionally, we observed that increasing the randomness in the patrol schedule\nreduces the attacker's expected payoff in high-penalty cases. However, the\nminimax problem becomes non-convex in other scenarios. To address this, we\nformulated a bi-criteria optimization problem incorporating two objectives:\nexpected maximum reward and entropy. We proposed three graph-based algorithms\nand one deep reinforcement learning model, designed to efficiently balance the\ntrade-off between these two objectives. Notably, the third algorithm can\nidentify the optimal deterministic patrol schedule, though its runtime grows\nexponentially with the number of patrol spots. Experimental results validate\nthe effectiveness and scalability of our solutions, demonstrating that our\napproaches outperform state-of-the-art baselines on both synthetic and\nreal-world crime datasets.\n","authors":["Hao-Tsung Yang","Ting-Kai Weng","Ting-Yu Chang","Kin Sum Liu","Shan Lin","Jie Gao","Shih-Yu Tsai"],"pdf_url":"https://arxiv.org/pdf/2410.15600v1.pdf","comment":"Under review of TCPS"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.05494v2","updated":"2024-10-21T19:43:45Z","published":"2024-10-07T21:05:45Z","title":"Tactile Displays Driven by Projected Light","summary":"  Tactile displays that lend tangible form to digital content could transform\ncomputing interactions. However, achieving the resolution, speed, and dynamic\nrange needed for perceptual fidelity remains challenging. We present a tactile\ndisplay that directly converts projected light into visible tactile patterns\nvia a photomechanical surface populated with millimeter-scale optotactile\npixels. The pixels transduce incident light into mechanical displacements\nthrough photostimulated thermal gas expansion, yielding millimeter scale\ndisplacements with response times of 2 to 100 milliseconds. Employing projected\nlight for power transmission and addressing renders these displays highly\nscalable. We demonstrate devices with up to 1511 addressable pixels. Perceptual\nstudies confirm that they can reproduce diverse spatiotemporal tactile patterns\nwith high fidelity. This research establishes a foundation for practical,\nversatile high-resolution tactile displays driven by light.\n","authors":["Max Linnander","Dustin Goetz","Gregory Reardon","Elliot Hawkes","Yon Visell"],"pdf_url":"https://arxiv.org/pdf/2410.05494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16397v1","updated":"2024-10-21T18:08:42Z","published":"2024-10-21T18:08:42Z","title":"Towards a Reliable Offline Personal AI Assistant for Long Duration\n  Spaceflight","summary":"  As humanity prepares for new missions to the Moon and Mars, astronauts will\nneed to operate with greater autonomy, given the communication delays that make\nreal-time support from Earth difficult. For instance, messages between Mars and\nEarth can take up to 24 minutes, making quick responses impossible. This\nlimitation poses a challenge for astronauts who must rely on in-situ tools to\naccess the large volume of data from spacecraft sensors, rovers, and\nsatellites, data that is often fragmented and difficult to use. To bridge this\ngap, systems like the Mars Exploration Telemetry-Driven Information System\n(METIS) are being developed. METIS is an AI assistant designed to handle\nroutine tasks, monitor spacecraft systems, and detect anomalies, all while\nreducing the reliance on mission control. Current Generative Pretrained\nTransformer (GPT) Models, while powerful, struggle in safety-critical\nenvironments. They can generate plausible but incorrect responses, a phenomenon\nknown as \"hallucination,\" which could endanger astronauts. To overcome these\nlimitations, this paper proposes enhancing systems like METIS by integrating\nGPTs, Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and\nAugmented Reality (AR). The idea is to allow astronauts to interact with their\ndata more intuitively, using natural language queries and visualizing real-time\ninformation through AR. KGs will be used to easily access live telemetry and\nmultimodal data, ensuring that astronauts have the right information at the\nright time. By combining AI, KGs, and AR, this new system will empower\nastronauts to work more autonomously, safely, and efficiently during future\nspace missions.\n","authors":["Oliver Bensch","Leonie Bensch","Tommy Nilsson","Florian Saling","Wafa M. Sadri","Carsten Hartmann","Tobias Hecking","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2410.16397v1.pdf","comment":"75th International Astronautical Congress (IAC), Milan, Italy, 14-18\n  October 2024"},{"id":"http://arxiv.org/abs/2410.16079v1","updated":"2024-10-21T14:58:16Z","published":"2024-10-21T14:58:16Z","title":"SAIM: Scalable Analog Ising Machine for Solving Quadratic Binary\n  Optimization Problems","summary":"  This paper presents a CMOS-compatible Lechner-Hauke-Zoller (LHZ)--based\nanalog tile structure as a fundamental unit for developing scalable analog\nIsing machines (IMs). In the designed LHZ tile, the voltage-controlled\noscillators are employed as the physical Ising spins, while for the ancillary\nspins, we introduce an oscillator-based circuit to emulate the constraint\nneeded to ensure the correct functionality of the tile. We implement the\nproposed LHZ tile in 12nm FinFET technology using the Cadence Virtuoso.\nSimulation results show the proposed tile could converge to the results in\nabout 31~ns. Also, the designed spins could operate at approximately 13~GHz.\n","authors":["Sasan Razmkhah","Jui-Yu Huang","Mehdi Kamal","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2410.16079v1.pdf","comment":"5 pages, 8 figures, prepared in IEEE format"},{"id":"http://arxiv.org/abs/2410.15943v1","updated":"2024-10-21T12:21:08Z","published":"2024-10-21T12:21:08Z","title":"Molecular Signal Reception in Complex Vessel Networks: The Role of the\n  Network Topology","summary":"  The notion of synthetic molecular communication (MC) refers to the\ntransmission of information via molecules and is largely foreseen for use\nwithin the human body, where traditional electromagnetic wave (EM)-based\ncommunication is impractical. MC is anticipated to enable innovative medical\napplications, such as early-stage tumor detection, targeted drug delivery, and\nholistic approaches like the Internet of Bio-Nano Things (IoBNT). Many of these\napplications involve parts of the human cardiovascular system (CVS), here\nreferred to as networks, posing challenges for MC due to their complex, highly\nbranched vessel structures. To gain a better understanding of how the topology\nof such branched vessel networks affects the reception of a molecular signal at\na target location, e.g., the network outlet, we present a generic analytical\nend-to-end model that characterizes molecule propagation and reception in\nlinear branched vessel networks (LBVNs). We specialize this generic model to\nany MC system employing superparamagnetic iron-oxide nanoparticles (SPIONs) as\nsignaling molecules and a planar coil as receiver (RX). By considering\ncomponents that have been previously established in testbeds, we effectively\nisolate the impact of the network topology and validate our theoretical model\nwith testbed data. Additionally, we propose two metrics, namely the molecule\ndelay and the multi-path spread, that relate the LBVN topology to the molecule\ndispersion induced by the network, thereby linking the network structure to the\nsignal-to-noise ratio (SNR) at the target location. This allows the\ncharacterization of the SNR at any point in the network solely based on the\nnetwork topology. Consequently, our framework can, e.g., be exploited for\noptimal sensor placement in the CVS or identification of suitable testbed\ntopologies for given SNR requirements.\n","authors":["Timo Jakumeit","Lukas Brand","Jens Kirchner","Robert Schober","Sebastian Lotter"],"pdf_url":"https://arxiv.org/pdf/2410.15943v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.15893v1","updated":"2024-10-21T11:13:06Z","published":"2024-10-21T11:13:06Z","title":"ATOMIC: Automatic Tool for Memristive IMPLY-based Circuit-level\n  Simulation and Validation","summary":"  Since performance improvements of computers are stagnating, new technologies\nand computer paradigms are hot research topics. Memristor-based In-Memory\nComputing is one of the promising candidates for the post-CMOS era, which comes\nin many flavors. Processing In memory Array (PIA) or using memory, is on of\nthem which is a relatively new approach, and substantially different than\ntraditional CMOS-based logic design. Consequently, there is a lack of publicly\navailable CAD tools for memristive PIA design and evaluation. Here, we present\nATOMIC: an Automatic Tool for Memristive IMPLY-based Circuit-level Simulation\nand Validation. Using our tool, a large portion of the simulation, evaluation,\nand validation process can be performed automatically, drastically reducing the\ndevelopment time for memristive PIA systems, in particular those using IMPLY\nlogic. The code is available at https://github.com/fabianseiler/ATOMIC.\n","authors":["Fabian Seiler","Nima TaheriNejad"],"pdf_url":"https://arxiv.org/pdf/2410.15893v1.pdf","comment":"4 pages, 5 figures, Submitted and Presented at the Embedded Systems\n  Software Competition 2024 at ESWEEK"},{"id":"http://arxiv.org/abs/2410.15854v1","updated":"2024-10-21T10:30:24Z","published":"2024-10-21T10:30:24Z","title":"TEXEL: A neuromorphic processor with on-chip learning for beyond-CMOS\n  device integration","summary":"  Recent advances in memory technologies, devices and materials have shown\ngreat potential for integration into neuromorphic electronic systems. However,\na significant gap remains between the development of these materials and the\nrealization of large-scale, fully functional systems. One key challenge is\ndetermining which devices and materials are best suited for specific functions\nand how they can be paired with CMOS circuitry. To address this, we introduce\nTEXEL, a mixed-signal neuromorphic architecture designed to explore the\nintegration of on-chip learning circuits and novel two- and three-terminal\ndevices. TEXEL serves as an accessible platform to bridge the gap between\nCMOS-based neuromorphic computation and the latest advancements in emerging\ndevices. In this paper, we demonstrate the readiness of TEXEL for device\nintegration through comprehensive chip measurements and simulations. TEXEL\nprovides a practical system for testing bio-inspired learning algorithms\nalongside emerging devices, establishing a tangible link between brain-inspired\ncomputation and cutting-edge device research.\n","authors":["Hugh Greatorex","Ole Richter","Michele Mastella","Madison Cotteret","Philipp Klein","Maxime Fabre","Arianna Rubino","Willian Soares Gir√£o","Junren Chen","Martin Ziegler","Laura B√©gon-Lours","Giacomo Indiveri","Elisabetta Chicca"],"pdf_url":"https://arxiv.org/pdf/2410.15854v1.pdf","comment":"17 pages, 7 figures. Supplementary material: 8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.15736v1","updated":"2024-10-21T07:56:41Z","published":"2024-10-21T07:56:41Z","title":"Design of a 64-bit SQRT-CSLA with Reduced Area and High-Speed\n  Applications in Low Power VLSI Circuits","summary":"  The main areas of research in VLSI system design include area, high speed,\nand power-efficient data route logic systems. The amount of time needed to send\na carry through the adder limits the pace at which addition can occur in\ndigital adders. One of the quickest adders, the Carry Select Adder (CSLA), is\nutilized by various data processing processors to carry out quick arithmetic\noperations. It is evident from the CSLA's structure that there is room to cut\nback on both the area and the delay. This work employs a straightforward and\neffective gate-level adjustment (in a regular structure) that significantly\nlowers the CSLA's area and delay. In light of this adjustment Square-Root Carry\nSelect Adder (SQRT CSLA) designs with bit lengths of 8, 16, 32, and 64. When\ncompared to the standard SQRT CSLA, the suggested design significantly reduces\nboth area and latency. Xilinx ISE tool is used for Simulation and synthesis.\nThe performance of the recommended designs in terms of delay is estimated in\nthis study using the standard designs. The study of the findings indicates that\nthe suggested CSLA structure outperforms the standard SQRT CSLA.\n","authors":["CH. Pallavi","C. Padma","R. Kiran Kumar","T. Suguna","C. Nalini"],"pdf_url":"https://arxiv.org/pdf/2410.15736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15724v1","updated":"2024-10-21T07:43:20Z","published":"2024-10-21T07:43:20Z","title":"Efficient and Universally Accessible Cross-Chain Options without Upfront\n  Holder Collateral","summary":"  Options are fundamental to blockchain-based financial markets, offering\nessential tools for risk management and price speculation, which enhance\nliquidity, flexibility, and market efficiency in decentralized finance (DeFi).\nDespite the growing interest in options for blockchain-resident assets, such as\ncryptocurrencies, current option mechanisms face significant challenges,\nincluding limited asset support, high trading delays, and the requirement for\noption holders to provide upfront collateral.\n  In this paper, we present a protocol that addresses the aforementioned issues\nby facilitating efficient and universally accessible option trading without\nrequiring holders to post collateral when establishing options. Our protocol's\nuniversality allows for cross-chain options involving nearly $\\textit{any}$\nassets on $\\textit{any}$ two different blockchains, provided the chains'\nprogramming languages can enforce and execute the necessary contract logic. A\nkey innovation in our approach is the use of Double-Authentication-Preventing\nSignatures (DAPS), which significantly reduces trading latency. Additionally,\nby introducing a guarantee from the option writer, our protocol removes the\nneed of upfront collateral from holders. Our evaluation demonstrates that the\nproposed scheme reduces option transfer latency to less than half of that in\nexisting methods. Rigorous security analysis proves that our protocol achieves\nsecure option trading, even when facing adversarial behaviors.\n","authors":["Zifan Peng","Yingjie Xue","Jingyu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15724v1.pdf","comment":"19 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.15626v1","updated":"2024-10-21T04:10:54Z","published":"2024-10-21T04:10:54Z","title":"Hybrid Quantum-HPC Solutions for Max-Cut: Bridging Classical and Quantum\n  Algorithms","summary":"  This research explores the integration of the Quantum Approximate\nOptimization Algorithm (QAOA) into Hybrid Quantum-HPC systems for solving the\nMax-Cut problem, comparing its performance with classical algorithms like\nbrute-force search and greedy heuristics. We develop a theoretical model to\nanalyze the time complexity, scalability, and communication overhead in hybrid\nsystems. Using simulations, we evaluate QAOA's performance on small-scale\nMax-Cut instances, benchmarking its runtime, solution accuracy, and resource\nutilization. The study also investigates the scalability of QAOA with\nincreasing problem size, offering insights into its potential advantages over\nclassical methods for large-scale combinatorial optimization problems, with\nimplications for future Quantum computing applications in HPC environments.\n","authors":["Ishan Patwardhan","Akhil Akkapelli"],"pdf_url":"https://arxiv.org/pdf/2410.15626v1.pdf","comment":"Submitted to IEEE PuneCon"}],"Graphics":[{"id":"http://arxiv.org/abs/2409.10695v2","updated":"2024-10-21T20:01:13Z","published":"2024-09-16T19:52:24Z","title":"Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large\n  Language Models","summary":"  We introduce Playground v3 (PGv3), our latest text-to-image model that\nachieves state-of-the-art (SoTA) performance across multiple testing\nbenchmarks, excels in graphic design abilities and introduces new capabilities.\nUnlike traditional text-to-image generative models that rely on pre-trained\nlanguage models like T5 or CLIP text encoders, our approach fully integrates\nLarge Language Models (LLMs) with a novel structure that leverages text\nconditions exclusively from a decoder-only LLM. Additionally, to enhance image\ncaptioning quality-we developed an in-house captioner, capable of generating\ncaptions with varying levels of detail, enriching the diversity of text\nstructures. We also introduce a new benchmark CapsBench to evaluate detailed\nimage captioning performance. Experimental results demonstrate that PGv3 excels\nin text prompt adherence, complex reasoning, and accurate text rendering. User\npreference studies indicate the super-human graphic design ability of our model\nfor common design applications, such as stickers, posters, and logo designs.\nFurthermore, PGv3 introduces new capabilities, including precise RGB color\ncontrol and robust multilingual understanding.\n","authors":["Bingchen Liu","Ehsan Akhgari","Alexander Visheratin","Aleks Kamko","Linmiao Xu","Shivam Shrirao","Chase Lambert","Joao Souza","Suhail Doshi","Daiqing Li"],"pdf_url":"https://arxiv.org/pdf/2409.10695v2.pdf","comment":"Project page: https://playground.com/pg-v3"},{"id":"http://arxiv.org/abs/2410.16395v1","updated":"2024-10-21T18:07:33Z","published":"2024-10-21T18:07:33Z","title":"Joker: Conditional 3D Head Synthesis with Extreme Facial Expressions","summary":"  We introduce Joker, a new method for the conditional synthesis of 3D human\nheads with extreme expressions. Given a single reference image of a person, we\nsynthesize a volumetric human head with the reference identity and a new\nexpression. We offer control over the expression via a 3D morphable model\n(3DMM) and textual inputs. This multi-modal conditioning signal is essential\nsince 3DMMs alone fail to define subtle emotional changes and extreme\nexpressions, including those involving the mouth cavity and tongue\narticulation. Our method is built upon a 2D diffusion-based prior that\ngeneralizes well to out-of-domain samples, such as sculptures, heavy makeup,\nand paintings while achieving high levels of expressiveness. To improve view\nconsistency, we propose a new 3D distillation technique that converts\npredictions of our 2D prior into a neural radiance field (NeRF). Both the 2D\nprior and our distillation technique produce state-of-the-art results, which\nare confirmed by our extensive evaluations. Also, to the best of our knowledge,\nour method is the first to achieve view-consistent extreme tongue articulation.\n","authors":["Malte Prinzler","Egor Zakharov","Vanessa Sklyarova","Berna Kabadayi","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2410.16395v1.pdf","comment":"Project Page: https://malteprinzler.github.io/projects/joker/"},{"id":"http://arxiv.org/abs/2410.16259v1","updated":"2024-10-21T17:57:50Z","published":"2024-10-21T17:57:50Z","title":"Agent-to-Sim: Learning Interactive Behavior Models from Casual\n  Longitudinal Videos","summary":"  We present Agent-to-Sim (ATS), a framework for learning interactive behavior\nmodels of 3D agents from casual longitudinal video collections. Different from\nprior works that rely on marker-based tracking and multiview cameras, ATS\nlearns natural behaviors of animal and human agents non-invasively through\nvideo observations recorded over a long time-span (e.g., a month) in a single\nenvironment. Modeling 3D behavior of an agent requires persistent 3D tracking\n(e.g., knowing which point corresponds to which) over a long time period. To\nobtain such data, we develop a coarse-to-fine registration method that tracks\nthe agent and the camera over time through a canonical 3D space, resulting in a\ncomplete and persistent spacetime 4D representation. We then train a generative\nmodel of agent behaviors using paired data of perception and motion of an agent\nqueried from the 4D reconstruction. ATS enables real-to-sim transfer from video\nrecordings of an agent to an interactive behavior simulator. We demonstrate\nresults on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos\ncaptured by a smartphone.\n","authors":["Gengshan Yang","Andrea Bajcsy","Shunsuke Saito","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.16259v1.pdf","comment":"Project page: https://gengshan-y.github.io/agent2sim-www/"},{"id":"http://arxiv.org/abs/2409.15023v4","updated":"2024-10-21T13:41:50Z","published":"2024-09-23T13:50:39Z","title":"Efficient Nearest Neighbor Search Using Dynamic Programming","summary":"  Given a collection of points in R^3, KD-Tree and R-Tree are well-known\nnearest neighbor search (NNS) algorithms that rely on space partitioning and\nspatial indexing techniques. However, when the query point is far from the data\npoints or the data points inherently represent a 2-manifold surface, their\nquery performance may degrade. To address this, we propose a novel dynamic\nprogramming technique that precomputes a Directed Acyclic Graph (DAG) to encode\nthe proximity structure between data points. More specifically, the DAG\ncaptures how the proximity structure evolves during the incremental\nconstruction of the Voronoi diagram of the data points. Experimental results\ndemonstrate that our method achieves a 1x-10x speedup. Additionally, our\nalgorithm offers several valuable features. For instance, it naturally supports\nan O(k \\log n) algorithm for farthest point sampling, where k is the desired\nnumber of sample points. Moreover, density peak clustering, which involves\nfinding the nearest point among the top K points, is typically considered to\nhave a time complexity of O(n^2). With our algorithm, this can be reduced to\nO(n \\log n). We believe this work will inspire further research on the NNS\nproblem.\n","authors":["Pengfei Wang","Jiantao Song","Shiqing Xin","Shuangmin Chen","Changhe Tu","Wenping Wang","Jiaye Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15023v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15891v1","updated":"2024-10-21T11:10:07Z","published":"2024-10-21T11:10:07Z","title":"TexPro: Text-guided PBR Texturing with Procedural Material Modeling","summary":"  In this paper, we present TexPro, a novel method for high-fidelity material\ngeneration for input 3D meshes given text prompts. Unlike existing\ntext-conditioned texture generation methods that typically generate RGB\ntextures with baked lighting, TexPro is able to produce diverse texture maps\nvia procedural material modeling, which enables physical-based rendering,\nrelighting, and additional benefits inherent to procedural materials.\nSpecifically, we first generate multi-view reference images given the input\ntextual prompt by employing the latest text-to-image model. We then derive\ntexture maps through a rendering-based optimization with recent differentiable\nprocedural materials. To this end, we design several techniques to handle the\nmisalignment between the generated multi-view images and 3D meshes, and\nintroduce a novel material agent that enhances material classification and\nmatching by exploring both part-level understanding and object-aware material\nreasoning. Experiments demonstrate the superiority of the proposed method over\nexisting SOTAs and its capability of relighting.\n","authors":["Ziqiang Dang","Wenqi Dong","Zesong Yang","Bangbang Yang","Liang Li","Yuewen Ma","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2410.15891v1.pdf","comment":"In submission. Supplementary material is included at the end of the\n  main paper (5 pages, 2 figures)"},{"id":"http://arxiv.org/abs/2410.15768v1","updated":"2024-10-21T08:28:11Z","published":"2024-10-21T08:28:11Z","title":"Learning to Synthesize Graphics Programs for Geometric Artworks","summary":"  Creating and understanding art has long been a hallmark of human ability.\nWhen presented with finished digital artwork, professional graphic artists can\nintuitively deconstruct and replicate it using various drawing tools, such as\nthe line tool, paint bucket, and layer features, including opacity and blending\nmodes. While most recent research in this field has focused on art generation,\nproposing a range of methods, these often rely on the concept of artwork being\nrepresented as a final image. To bridge the gap between pixel-level results and\nthe actual drawing process, we present an approach that treats a set of drawing\ntools as executable programs. This method predicts a sequence of steps to\nachieve the final image, allowing for understandable and resolution-independent\nreproductions under the usage of a set of drawing commands. Our experiments\ndemonstrate that our program synthesizer, Art2Prog, can comprehensively\nunderstand complex input images and reproduce them using high-quality\nexecutable programs. The experimental results evidence the potential of\nmachines to grasp higher-level information from images and generate compact\nprogram-level descriptions.\n","authors":["Qi Bing","Chaoyi Zhang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2410.15768v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2403.06841v3","updated":"2024-10-21T08:16:46Z","published":"2024-03-11T16:01:07Z","title":"Inverse Garment and Pattern Modeling with a Differentiable Simulator","summary":"  The capability to generate simulation-ready garment models from 3D shapes of\nclothed humans will significantly enhance the interpretability of captured\ngeometry of real garments, as well as their faithful reproduction in the\nvirtual world. This will have notable impact on fields like shape capture in\nsocial VR, and virtual try-on in the fashion industry. To align with the\ngarment modeling process standardized by the fashion industry as well as cloth\nsimulation softwares, it is required to recover 2D patterns. This involves an\ninverse garment design problem, which is the focus of our work here: Starting\nwith an arbitrary target garment geometry, our system estimates an animatable\ngarment model by automatically adjusting its corresponding 2D template pattern,\nalong with the material parameters of the physics-based simulation (PBS). Built\nupon a differentiable cloth simulator, the optimization process is directed\ntowards minimizing the deviation of the simulated garment shape from the target\ngeometry. Moreover, our produced patterns meet manufacturing requirements such\nas left-to-right-symmetry, making them suited for reverse garment fabrication.\nWe validate our approach on examples of different garment types, and show that\nour method faithfully reproduces both the draped garment shape and the sewing\npattern.\n","authors":["Boyang Yu","Frederic Cordier","Hyewon Seo"],"pdf_url":"https://arxiv.org/pdf/2403.06841v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01597v2","updated":"2024-10-21T03:11:29Z","published":"2024-04-09T14:37:54Z","title":"End-to-End Rate-Distortion Optimized 3D Gaussian Representation","summary":"  3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable\npotential in 3D representation and image rendering. However, the substantial\nstorage overhead of 3DGS significantly impedes its practical applications. In\nthis work, we formulate the compact 3D Gaussian learning as an end-to-end\nRate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can\nachieve flexible and continuous rate control. RDO-Gaussian addresses two main\nissues that exist in current schemes: 1) Different from prior endeavors that\nminimize the rate under the fixed distortion, we introduce dynamic pruning and\nentropy-constrained vector quantization (ECVQ) that optimize the rate and\ndistortion at the same time. 2) Previous works treat the colors of each\nGaussian equally, while we model the colors of different regions and materials\nwith learnable numbers of parameters. We verify our method on both real and\nsynthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D\nGaussian over 40x, and surpasses existing methods in rate-distortion\nperformance.\n","authors":["Henan Wang","Hanxin Zhu","Tianyu He","Runsen Feng","Jiajun Deng","Jiang Bian","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01597v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2410.15584v1","updated":"2024-10-21T02:10:49Z","published":"2024-10-21T02:10:49Z","title":"Deep Learning and Machine Learning -- Object Detection and Semantic\n  Segmentation: From Theory to Applications","summary":"  This book offers an in-depth exploration of object detection and semantic\nsegmentation, combining theoretical foundations with practical applications. It\ncovers state-of-the-art advancements in machine learning and deep learning,\nwith a focus on convolutional neural networks (CNNs), YOLO architectures, and\ntransformer-based approaches like DETR. The book also delves into the\nintegration of artificial intelligence (AI) techniques and large language\nmodels for enhanced object detection in complex environments. A thorough\ndiscussion of big data analysis is presented, highlighting the importance of\ndata processing, model optimization, and performance evaluation metrics. By\nbridging the gap between traditional methods and modern deep learning\nframeworks, this book serves as a comprehensive guide for researchers, data\nscientists, and engineers aiming to leverage AI-driven methodologies in\nlarge-scale object detection tasks.\n","authors":["Jintao Ren","Ziqian Bi","Qian Niu","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jinlang Wang","Keyu Chen","Caitlyn Heqi Yin","Pohsun Feng","Yizhu Wen","Tianyang Wang","Silin Chen","Ming Li","Jiawei Xu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15584v1.pdf","comment":"167 pages"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.16529v1","updated":"2024-10-21T21:37:55Z","published":"2024-10-21T21:37:55Z","title":"Distributed Online Life-Long Learning (DOL3) for Multi-agent Trust and\n  Reputation Assessment in E-commerce","summary":"  Trust and Reputation Assessment of service providers in citizen-focused\nenvironments like e-commerce is vital to maintain the integrity of the\ninteractions among agents. The goals and objectives of both the service\nprovider and service consumer agents are relevant to the goals of the\nrespective citizens (end users). The provider agents often pursue selfish goals\nthat can make the service quality highly volatile, contributing towards the\nnon-stationary nature of the environment. The number of active service\nproviders tends to change over time resulting in an open environment. This\nnecessitates a rapid and continual assessment of the Trust and Reputation. A\nlarge number of service providers in the environment require a distributed\nmulti-agent Trust and Reputation assessment. This paper addresses the problem\nof multi-agent Trust and Reputation Assessment in a non-stationary environment\ninvolving transactions between providers and consumers. In this setting, the\nobserver agents carry out the assessment and communicate their assessed trust\nscores with each other over a network. We propose a novel Distributed Online\nLife-Long Learning (DOL3) algorithm that involves real-time rapid learning of\ntrust and reputation scores of providers. Each observer carries out an adaptive\nlearning and weighted fusion process combining their own assessment along with\nthat of their neighbour in the communication network. Simulation studies reveal\nthat the state-of-the-art methods, which usually involve training a model to\nassess an agent's trust and reputation, do not work well in such an\nenvironment. The simulation results show that the proposed DOL3 algorithm\noutperforms these methods and effectively handles the volatility in such\nenvironments. From the statistical evaluation, it is evident that DOL3 performs\nbetter compared to other models in 90% of the cases.\n","authors":["Hariprasauth Ramamoorthy","Shubhankar Gupta","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2410.16529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02161v2","updated":"2024-10-21T21:20:29Z","published":"2024-05-03T15:08:25Z","title":"Simulating the Economic Impact of Rationality through Reinforcement\n  Learning and Agent-Based Modelling","summary":"  Agent-based models (ABMs) are simulation models used in economics to overcome\nsome of the limitations of traditional frameworks based on general equilibrium\nassumptions. However, agents within an ABM follow predetermined 'bounded\nrational' behavioural rules which can be cumbersome to design and difficult to\njustify. Here we leverage multi-agent reinforcement learning (RL) to expand the\ncapabilities of ABMs with the introduction of 'fully rational' agents that\nlearn their policy by interacting with the environment and maximising a reward\nfunction. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by\nextending a paradigmatic macro ABM from the economic literature. We show that\ngradually substituting ABM firms in the model with RL agents, trained to\nmaximise profits, allows for studying the impact of rationality on the economy.\nWe find that RL agents spontaneously learn three distinct strategies for\nmaximising profits, with the optimal strategy depending on the level of market\ncompetition and rationality. We also find that RL agents with independent\npolicies, and without the ability to communicate with each other, spontaneously\nlearn to segregate into different strategic groups, thus increasing market\npower and overall profits. Finally, we find that a higher number of rational\n(RL) agents in the economy always improves the macroeconomic environment as\nmeasured by total output. Depending on the specific rational policy, this can\ncome at the cost of higher instability. Our R-MABM framework allows for stable\nmulti-agent learning, is available in open source, and represents a principled\nand robust direction to extend economic simulators.\n","authors":["Simone Brusatin","Tommaso Padoan","Andrea Coletta","Domenico Delli Gatti","Aldo Glielmo"],"pdf_url":"https://arxiv.org/pdf/2405.02161v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.16464v1","updated":"2024-10-21T19:46:06Z","published":"2024-10-21T19:46:06Z","title":"Beyond Browsing: API-Based Web Agents","summary":"  Web browsers are a portal to the internet, where much of human activity is\nundertaken. Thus, there has been significant research work in AI agents that\ninteract with the internet through web browsing. However, there is also another\ninterface designed specifically for machine interaction with online content:\napplication programming interfaces (APIs). In this paper we ask -- what if we\nwere to take tasks traditionally tackled by browsing agents, and give AI agents\naccess to APIs? To do so, we propose two varieties of agents: (1) an\nAPI-calling agent that attempts to perform online tasks through APIs only,\nsimilar to traditional coding agents, and (2) a Hybrid Agent that can interact\nwith online data through both web browsing and APIs. In experiments on\nWebArena, a widely-used and realistic benchmark for web navigation tasks, we\nfind that API-based agents outperform web browsing agents. Hybrid Agents\nout-perform both others nearly uniformly across tasks, resulting in a more than\n20.0% absolute improvement over web browsing alone, achieving a success rate of\n35.8%, achiving the SOTA performance among task-agnostic agents. These results\nstrongly suggest that when APIs are available, they present an attractive\nalternative to relying on web browsing alone.\n","authors":["Yueqi Song","Frank Xu","Shuyan Zhou","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2410.16464v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.16441v1","updated":"2024-10-21T19:10:49Z","published":"2024-10-21T19:10:49Z","title":"Policies with Sparse Inter-Agent Dependencies in Dynamic Games: A\n  Dynamic Programming Approach","summary":"  Common feedback strategies in multi-agent dynamic games require all players'\nstate information to compute control strategies. However, in real-world\nscenarios, sensing and communication limitations between agents make full state\nfeedback expensive or impractical, and such strategies can become fragile when\nstate information from other agents is inaccurate. To this end, we propose a\nregularized dynamic programming approach for finding sparse feedback policies\nthat selectively depend on the states of a subset of agents in dynamic games.\nThe proposed approach solves convex adaptive group Lasso problems to compute\nsparse policies approximating Nash equilibrium solutions. We prove the\nregularized solutions' asymptotic convergence to a neighborhood of Nash\nequilibrium policies in linear-quadratic (LQ) games. We extend the proposed\napproach to general non-LQ games via an iterative algorithm. Empirical results\nin multi-robot interaction scenarios show that the proposed approach\neffectively computes feedback policies with varying sparsity levels. When\nagents have noisy observations of other agents' states, simulation results\nindicate that the proposed regularized policies consistently achieve lower\ncosts than standard Nash equilibrium policies by up to 77% for all interacting\nagents whose costs are coupled with other agents' states.\n","authors":["Xinjie Liu","Jingqi Li","Filippos Fotiadis","Mustafa O. Karabag","Jesse Milzman","David Fridovich-Keil","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.16441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00352v6","updated":"2024-10-21T17:22:45Z","published":"2023-08-01T07:49:10Z","title":"MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework","summary":"  Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT\n","authors":["Sirui Hong","Mingchen Zhuge","Jonathan Chen","Xiawu Zheng","Yuheng Cheng","Ceyao Zhang","Jinlin Wang","Zili Wang","Steven Ka Shing Yau","Zijuan Lin","Liyang Zhou","Chenyu Ran","Lingfeng Xiao","Chenglin Wu","J√ºrgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2308.00352v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16175v1","updated":"2024-10-21T16:41:35Z","published":"2024-10-21T16:41:35Z","title":"Spiking Neural Networks as a Controller for Emergent Swarm Agents","summary":"  Drones which can swarm and loiter in a certain area cost hundreds of dollars,\nbut mosquitos can do the same and are essentially worthless. To control swarms\nof low-cost robots, researchers may end up spending countless hours\nbrainstorming robot configurations and policies to ``organically\" create\nbehaviors which do not need expensive sensors and perception. Existing research\nexplores the possible emergent behaviors in swarms of robots with only a binary\nsensor and a simple but hand-picked controller structure. Even agents in this\nhighly limited sensing, actuation, and computational capability class can\nexhibit relatively complex global behaviors such as aggregation, milling, and\ndispersal, but finding the local interaction rules that enable more collective\nbehaviors remains a significant challenge. This paper investigates the\nfeasibility of training spiking neural networks to find those local interaction\nrules that result in particular emergent behaviors. In this paper, we focus on\nsimulating a specific milling behavior already known to be producible using\nvery simple binary sensing and acting agents. To do this, we use evolutionary\nalgorithms to evolve not only the parameters (the weights, biases, and delays)\nof a spiking neural network, but also its structure. To create a baseline, we\nalso show an evolutionary search strategy over the parameters for the incumbent\nhand-picked binary controller structure. Our simulations show that spiking\nneural networks can be evolved in binary sensing agents to form a mill.\n","authors":["Kevin Zhu","Connor Mattson","Shay Snyder","Ricardo Vega","Daniel S. Brown","Maryam Parsa","Cameron Nowzari"],"pdf_url":"https://arxiv.org/pdf/2410.16175v1.pdf","comment":"8 pages, 7 figures, presented at the 2024 International Conference on\n  Neuromorphic Systems"},{"id":"http://arxiv.org/abs/2311.07105v2","updated":"2024-10-21T14:15:54Z","published":"2023-11-13T06:40:31Z","title":"Collaborative Goal Tracking of Multiple Mobile Robots Based on Geometric\n  Graph Neural Network","summary":"  Multiple mobile robots play a significant role in various spatially\ndistributed tasks, highlighting the importance of collaborative path planning\nto enhance operational efficiency. In unfamiliar and non-repetitive scenarios,\nreconstructing the global map can be time-inefficient and sometimes\nunrealistic. Therefore, research has focused on achieving real-time\ncollaborative planning by utilizing sensor data from multiple robots located at\ndifferent positions, without relying on a global map. This paper introduces a\nMulti-Robot Collaborative Path Planning method based on a Geometric Graph\nNeural Network (MRPP-GeoGNN). First, the features of each neighboring robot's\nsensory data are extracted, and the relative positions of neighboring robots\nare integrated into each interaction layer to incorporate obstacle information\nalong with location details. Subsequently, GeoGNN maps the amalgamated local\nenvironment features to multiple forward directions for the robot's actual\nmovement. An expert data generation method is devised for the robot to advance\nstep by step in the physical environment, generating different expert data in\nROS to train the network. We conducted both simulations and physical\nexperiments to validate the effectiveness of the proposed method. Simulation\nresults demonstrate approximately a 5% improvement in accuracy compared to the\nmodel based solely on CNN using expert datasets. In the ROS simulation test,\nthe success rate is enhanced by about 4% compared to CNN, and the flow time\nincrease is reduced by approximately 8%, surpassing other GNN models. The\nphysical experimental results indicate that the proposed method enables the\nrobot to navigate successfully in the actual environment and achieve the\nshortest average path length compared to the benchmark method.\n","authors":["Weining Lu","Qingquan Lin","Litong Meng","Chenxi Li","Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2311.07105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16029v1","updated":"2024-10-21T14:05:06Z","published":"2024-10-21T14:05:06Z","title":"Natural GaLore: Accelerating GaLore for memory-efficient LLM Training\n  and Fine-tuning","summary":"  Training LLMs presents significant memory challenges due to growing size of\ndata, weights, and optimizer states. Techniques such as data and model\nparallelism, gradient checkpointing, and offloading strategies address this\nissue but are often infeasible due to hardware constraints. To mitigate memory\nusage, alternative methods like Parameter-Efficient-Fine-Tuning (PEFT) and\nGaLore approximate weights or optimizer states. PEFT methods, such as LoRA,\nhave gained popularity for fine-tuning LLMs, though they require a full-rank\nwarm start. In contrast, GaLore allows full-parameter learning while being more\nmemory-efficient. This work introduces Natural GaLore, a simple drop in\nreplacement for AdamW, which efficiently applies the inverse Empirical Fisher\nInformation Matrix to low-rank gradients using Woodbury's Identity. We\ndemonstrate that incorporating second-order information speeds up optimization\nsignificantly, especially when the iteration budget is limited. Empirical\npretraining on 60M, 130M, 350M, and 1.1B parameter Llama models on C4 data\ndemonstrate significantly lower perplexity over GaLore without additional\nmemory overhead. By fine-tuning RoBERTa on the GLUE benchmark using Natural\nGaLore, we demonstrate significant reduction in gap 86.05% vs 86.28% for\nfull-finetuning. Furthermore, fine-tuning the TinyLlama 1.1B model for function\ncalling using the TinyAgent framework shows that Natural GaLore achieving\n83.09% accuracy on the TinyAgent dataset, significantly outperforms 16-bit LoRA\nat 80.06% and even surpasses GPT4-Turbo by 4%, all while using 30% less memory.\n  All code to reproduce the results are available at:\nhttps://github.com/selfsupervised-ai/Natural-GaLore.git\n","authors":["Arijit Das"],"pdf_url":"https://arxiv.org/pdf/2410.16029v1.pdf","comment":"10 pages, 3 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.04202v6","updated":"2024-10-21T13:47:44Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v6.pdf","comment":"Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)\n  https://ojs.aaai.org/index.php/AIES/article/view/31736"},{"id":"http://arxiv.org/abs/2410.15987v1","updated":"2024-10-21T13:16:58Z","published":"2024-10-21T13:16:58Z","title":"Analyzing Closed-loop Training Techniques for Realistic Traffic Agent\n  Models in Autonomous Highway Driving Simulations","summary":"  Simulation plays a crucial role in the rapid development and safe deployment\nof autonomous vehicles. Realistic traffic agent models are indispensable for\nbridging the gap between simulation and the real world. Many existing\napproaches for imitating human behavior are based on learning from\ndemonstration. However, these approaches are often constrained by focusing on\nindividual training strategies. Therefore, to foster a broader understanding of\nrealistic traffic agent modeling, in this paper, we provide an extensive\ncomparative analysis of different training principles, with a focus on\nclosed-loop methods for highway driving simulation. We experimentally compare\n(i) open-loop vs. closed-loop multi-agent training, (ii) adversarial vs.\ndeterministic supervised training, (iii) the impact of reinforcement losses,\nand (iv) the impact of training alongside log-replayed agents to identify\nsuitable training techniques for realistic agent modeling. Furthermore, we\nidentify promising combinations of different closed-loop training methods.\n","authors":["Matthias Bitzer","Reinis Cimurs","Benjamin Coors","Johannes Goth","Sebastian Ziesche","Philipp Geiger","Maximilian Naumann"],"pdf_url":"https://arxiv.org/pdf/2410.15987v1.pdf","comment":"15 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.15538v2","updated":"2024-10-21T11:32:57Z","published":"2024-08-28T05:11:16Z","title":"TrafficGamer: Reliable and Flexible Traffic Simulation for\n  Safety-Critical Scenarios with Game-Theoretic Oracles","summary":"  While modern Autonomous Vehicle (AV) systems can develop reliable driving\npolicies under regular traffic conditions, they frequently struggle with\nsafety-critical traffic scenarios. This difficulty primarily arises from the\nrarity of such scenarios in driving datasets and the complexities associated\nwith predictive modeling among multiple vehicles. To support the testing and\nrefinement of AV policies, simulating safety-critical traffic events is an\nessential challenge to be addressed. In this work, we introduce TrafficGamer,\nwhich facilitates game-theoretic traffic simulation by viewing common road\ndriving as a multi-agent game. In evaluating the empirical performance across\nvarious real-world datasets, TrafficGamer ensures both fidelity and\nexploitability of the simulated scenarios, guaranteeing that they not only\nstatically align with real-world traffic distribution but also efficiently\ncapture equilibriums for representing safety-critical scenarios involving\nmultiple agents. Additionally, the results demonstrate that TrafficGamer\nexhibits highly flexible simulation across various contexts. Specifically, we\ndemonstrate that the generated scenarios can dynamically adapt to equilibriums\nof varying tightness by configuring risk-sensitive constraints during\noptimization. To the best of our knowledge, TrafficGamer is the first simulator\ncapable of generating diverse traffic scenarios involving multiple agents. We\nhave provided a demo webpage for the project at\nhttps://qiaoguanren.github.io/trafficgamer-demo/.\n","authors":["Guanren Qiao","Guorui Quan","Jiawei Yu","Shujun Jia","Guiliang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15876v1","updated":"2024-10-21T10:57:45Z","published":"2024-10-21T10:57:45Z","title":"FlickerFusion: Intra-trajectory Domain Generalizing Multi-Agent RL","summary":"  Multi-agent reinforcement learning has demonstrated significant potential in\naddressing complex cooperative tasks across various real-world applications.\nHowever, existing MARL approaches often rely on the restrictive assumption that\nthe number of entities (e.g., agents, obstacles) remains constant between\ntraining and inference. This overlooks scenarios where entities are dynamically\nremoved or added during the inference trajectory -- a common occurrence in\nreal-world environments like search and rescue missions and dynamic combat\nsituations. In this paper, we tackle the challenge of intra-trajectory dynamic\nentity composition under zero-shot out-of-domain (OOD) generalization, where\nsuch dynamic changes cannot be anticipated beforehand. Our empirical studies\nreveal that existing MARL methods suffer significant performance degradation\nand increased uncertainty in these scenarios. In response, we propose\nFlickerFusion, a novel OOD generalization method that acts as a universally\napplicable augmentation technique for MARL backbone methods. Our results show\nthat FlickerFusion not only achieves superior inference rewards but also\nuniquely reduces uncertainty vis-\\`a-vis the backbone, compared to existing\nmethods. For standardized evaluation, we introduce MPEv2, an enhanced version\nof Multi Particle Environments (MPE), consisting of 12 benchmarks. Benchmarks,\nimplementations, and trained models are organized and open-sourced at\nflickerfusion305.github.io, accompanied by ample demo video renderings.\n","authors":["Woosung Koh","Wonbeen Oh","Siyeol Kim","Suhin Shin","Hyeongjin Kim","Jaein Jang","Junghyun Lee","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2410.15876v1.pdf","comment":"NeurIPS '24 Open-World Agents Workshop"},{"id":"http://arxiv.org/abs/2410.15841v1","updated":"2024-10-21T10:03:21Z","published":"2024-10-21T10:03:21Z","title":"Towards Efficient Collaboration via Graph Modeling in Reinforcement\n  Learning","summary":"  In multi-agent reinforcement learning, a commonly considered paradigm is\ncentralized training with decentralized execution. However, in this framework,\ndecentralized execution restricts the development of coordinated policies due\nto the local observation limitation. In this paper, we consider the cooperation\namong neighboring agents during execution and formulate their interactions as a\ngraph. Thus, we introduce a novel encoder-decoder architecture named\nFactor-based Multi-Agent Transformer ($f$-MAT) that utilizes a transformer to\nenable the communication between neighboring agents during both training and\nexecution. By dividing agents into different overlapping groups and\nrepresenting each group with a factor, $f$-MAT fulfills efficient message\npassing among agents through factor-based attention layers. Empirical results\non networked systems such as traffic scheduling and power control demonstrate\nthat $f$-MAT achieves superior performance compared to strong baselines,\nthereby paving the way for handling complex collaborative problems.\n","authors":["Wenzhe Fan","Zishun Yu","Chengdong Ma","Changye Li","Yaodong Yang","Xinhua Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15686v1","updated":"2024-10-21T06:54:27Z","published":"2024-10-21T06:54:27Z","title":"NetSafe: Exploring the Topological Safety of Multi-agent Networks","summary":"  Large language models (LLMs) have empowered nodes within multi-agent networks\nwith intelligence, showing growing applications in both academia and industry.\nHowever, how to prevent these networks from generating malicious information\nremains unexplored with previous research on single LLM's safety be challenging\nto transfer. In this paper, we focus on the safety of multi-agent networks from\na topological perspective, investigating which topological properties\ncontribute to safer networks. To this end, we propose a general framework,\nNetSafe along with an iterative RelCom interaction to unify existing diverse\nLLM-based agent frameworks, laying the foundation for generalized topological\nsafety research. We identify several critical phenomena when multi-agent\nnetworks are exposed to attacks involving misinformation, bias, and harmful\ninformation, termed as Agent Hallucination and Aggregation Safety. Furthermore,\nwe find that highly connected networks are more susceptible to the spread of\nadversarial attacks, with task performance in a Star Graph Topology decreasing\nby 29.7%. Besides, our proposed static metrics aligned more closely with\nreal-world dynamic evaluations than traditional graph-theoretic metrics,\nindicating that networks with greater average distances from attackers exhibit\nenhanced safety. In conclusion, our work introduces a new topological\nperspective on the safety of LLM-based multi-agent networks and discovers\nseveral unreported phenomena, paving the way for future research to explore the\nsafety of such networks.\n","authors":["Miao Yu","Shilong Wang","Guibin Zhang","Junyuan Mao","Chenlong Yin","Qijiong Liu","Qingsong Wen","Kun Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13147v3","updated":"2024-10-21T02:15:42Z","published":"2024-06-19T01:51:15Z","title":"A Simulation Environment for the Neuroevolution of Ant Colony Dynamics","summary":"  We introduce a simulation environment to facilitate research into emergent\ncollective behaviour, with a focus on replicating the dynamics of ant colonies.\nBy leveraging real-world data, the environment simulates a target ant trail\nthat a controllable agent must learn to replicate, using sensory data observed\nby the target ant. This work aims to contribute to the neuroevolution of models\nfor collective behaviour, focusing on evolving neural architectures that encode\ndomain-specific behaviours in the network topology. By evolving models that can\nbe modified and studied in a controlled environment, we can uncover the\nnecessary conditions required for collective behaviours to emerge. We hope this\nenvironment will be useful to those studying the role of interactions in\nemergent behaviour within collective systems.\n","authors":["Michael Crosscombe","Ilya Horiguchi","Norihiro Maruyama","Shigeto Dobata","Takashi Ikegami"],"pdf_url":"https://arxiv.org/pdf/2406.13147v3.pdf","comment":"Accepted for publication at The 2024 Conference on Artificial Life. 2\n  page extended abstract"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.16438v1","updated":"2024-10-21T19:02:13Z","published":"2024-10-21T19:02:13Z","title":"AlignVSR: Audio-Visual Cross-Modal Alignment for Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to recognize corresponding text by\nanalyzing visual information from lip movements. Due to the high variability\nand weak information of lip movements, VSR tasks require effectively utilizing\nany information from any source and at any level. In this paper, we propose a\nVSR method based on audio-visual cross-modal alignment, named AlignVSR. The\nmethod leverages the audio modality as an auxiliary information source and\nutilizes the global and local correspondence between the audio and visual\nmodalities to improve visual-to-text inference. Specifically, the method first\ncaptures global alignment between video and audio through a cross-modal\nattention mechanism from video frames to a bank of audio units. Then, based on\nthe temporal correspondence between audio and video, a frame-level local\nalignment loss is introduced to refine the global alignment, improving the\nutility of the audio information. Experimental results on the LRS2 and\nCNVSRC.Single datasets consistently show that AlignVSR outperforms several\nmainstream VSR methods, demonstrating its superior and robust performance.\n","authors":["Zehua Liu","Xiaolou Li","Chen Chen","Li Guo","Lantian Li","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16407v1","updated":"2024-10-21T18:19:09Z","published":"2024-10-21T18:19:09Z","title":"Enhancing Multimodal Affective Analysis with Learned Live Comment\n  Features","summary":"  Live comments, also known as Danmaku, are user-generated messages that are\nsynchronized with video content. These comments overlay directly onto streaming\nvideos, capturing viewer emotions and reactions in real-time. While prior work\nhas leveraged live comments in affective analysis, its use has been limited due\nto the relative rarity of live comments across different video platforms. To\naddress this, we first construct the Live Comment for Affective Analysis\n(LCAffect) dataset which contains live comments for English and Chinese videos\nspanning diverse genres that elicit a wide spectrum of emotions. Then, using\nthis dataset, we use contrastive learning to train a video encoder to produce\nsynthetic live comment features for enhanced multimodal affective content\nanalysis. Through comprehensive experimentation on a wide range of affective\nanalysis tasks (sentiment, emotion recognition, and sarcasm detection) in both\nEnglish and Chinese, we demonstrate that these synthetic live comment features\nsignificantly improve performance over state-of-the-art methods.\n","authors":["Zhaoyuan Deng","Amith Ananthram","Kathleen McKeown"],"pdf_url":"https://arxiv.org/pdf/2410.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v8","updated":"2024-10-21T15:24:04Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v8.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2410.16058v1","updated":"2024-10-21T14:37:26Z","published":"2024-10-21T14:37:26Z","title":"Shorter Is Different: Characterizing the Dynamics of Short-Form Video\n  Platforms","summary":"  The emerging short-form video platforms have been growing tremendously and\nbecome one of the leading social media recently. Although the expanded\npopularity of these platforms has attracted increasing research attention,\nthere has been a lack of understanding of whether and how they deviate from\ntraditional long-form video-sharing platforms such as YouTube and Bilibili. To\naddress this, we conduct a large-scale data-driven analysis of Kuaishou, one of\nthe largest short-form video platforms in China. Based on 248 million videos\nuploaded to the platform across all categories, we identify their notable\ndifferences from long-form video platforms through a comparison study with\nBilibili, a leading long-form video platform in China. We find that videos are\nshortened by multiples on Kuaishou, with distinctive categorical distributions\nover-represented by life-related rather than interest-based videos. Users\ninteract with videos less per view, but top videos can even more effectively\nacquire users' collective attention. More importantly, ordinary content\ncreators have higher probabilities of producing hit videos. Our results shed\nlight on the uniqueness of short-form video platforms and pave the way for\nfuture research and design for better short-form video ecology.\n","authors":["Zhilong Chen","Peijie Liu","Jinghua Piao","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.16058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15846v1","updated":"2024-10-21T10:16:56Z","published":"2024-10-21T10:16:56Z","title":"Modelling Concurrent RTP Flows for End-to-end Predictions of QoS in Real\n  Time Communications","summary":"  The Real-time Transport Protocol (RTP)-based real-time communications (RTC)\napplications, exemplified by video conferencing, have experienced an\nunparalleled surge in popularity and development in recent years. In pursuit of\noptimizing their performance, the prediction of Quality of Service (QoS)\nmetrics emerges as a pivotal endeavor, bolstering network monitoring and\nproactive solutions. However, contemporary approaches are confined to\nindividual RTP flows and metrics, falling short in relationship capture and\ncomputational efficiency. To this end, we propose Packet-to-Prediction (P2P), a\nnovel deep learning (DL) framework that hinges on raw packets to simultaneously\nprocess concurrent RTP flows and perform end-to-end prediction of multiple QoS\nmetrics. Specifically, we implement a streamlined architecture, namely\nlength-free Transformer with cross and neighbourhood attention, capable of\nhandling an unlimited number of RTP flows, and employ a multi-task learning\nparadigm to forecast four key metrics in a single shot. Our work is based on\nextensive traffic collected during real video calls, and conclusively, P2P\nexcels comparative models in both prediction performance and temporal\nefficiency.\n","authors":["Tailai Song","Paolo Garza","Michela Meo","Maurizio Matteo Munaf√≤"],"pdf_url":"https://arxiv.org/pdf/2410.15846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08813v3","updated":"2024-10-21T03:08:08Z","published":"2024-05-14T17:59:02Z","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","summary":"  Current datasets for long-form video understanding often fall short of\nproviding genuine long-form comprehension challenges, as many tasks derived\nfrom these datasets can be successfully tackled by analyzing just one or a few\nrandom frames from a video. To address this issue, we present a novel dataset\nand benchmark, CinePile, specifically designed for authentic long-form video\nunderstanding. This paper details our innovative approach for creating a\nquestion-answer dataset, utilizing advanced LLMs with human-in-the-loop and\nbuilding upon human-generated raw data. Our comprehensive dataset comprises\n305,000 multiple-choice questions (MCQs), covering various visual and\nmultimodal aspects, including temporal comprehension, understanding\nhuman-object interactions, and reasoning about events or actions within a\nscene. Additionally, we fine-tuned open-source Video-LLMs on the training split\nand evaluated both open-source and proprietary video-centric LLMs on the test\nsplit of our dataset. The findings indicate that although current models\nunderperform compared to humans, fine-tuning these models can lead to\nsignificant improvements in their performance.\n","authors":["Ruchit Rawal","Khalid Saifullah","Miquel Farr√©","Ronen Basri","David Jacobs","Gowthami Somepalli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2405.08813v3.pdf","comment":"Project page with all the artifacts -\n  https://ruchitrawal.github.io/cinepile/. Updated version with adversarial\n  refinement pipeline and more model evaluations"}]},"2024-10-20T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.15541v1","updated":"2024-10-20T23:56:53Z","published":"2024-10-20T23:56:53Z","title":"A Proper Definition of Higher Order Rigidity","summary":"  [Connelly and Servatius, 1994] shows the difficulty of properly defining n-th\norder rigidity and flexiblity of a bar-and-joint framework for higher order (n\n>= 3) through the introduction of a cusp mechanism. The author proposes a\n\"proper\" definition of the order of rigidity by the order of elongation of the\nbars with respect to the arclength along the path in the configuration space.\nWe show that the classic definition using formal n-th derivative of the length\nconstraint is a sufficient condition for the n-th flexiblity in the proposed\ndefinition and also a necessary condition only for n = 1, 2.\n","authors":["Tomohiro Tachi"],"pdf_url":"https://arxiv.org/pdf/2410.15541v1.pdf","comment":"This is a note the author originally shared with Bob Connelly, Erik\n  Demaine, and Simon Guest on October 17th, 2017"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2402.06626v3","updated":"2024-10-20T04:27:09Z","published":"2024-02-09T18:59:20Z","title":"Computing Optimal Commitments to Strategies and Outcome-Conditional\n  Utility Transfers","summary":"  Prior work has studied the computational complexity of computing optimal\nstrategies to commit to in Stackelberg or leadership games, where a leader\ncommits to a strategy which is observed by one or more followers. We extend\nthis setting to one where the leader can additionally commit to\noutcome-conditional utility transfers. We characterize the computational\ncomplexity of finding optimal strategies in normal-form and Bayesian games,\ngiving a mix of efficient algorithms and NP-hardness results. Finally, we allow\nthe leader to also commit to a signaling scheme which induces a correlated\nequilibrium. In this setting, optimal commitments can be found in polynomial\ntime for arbitrarily many players.\n","authors":["Nathaniel Sauerberg","Caspar Oesterheld"],"pdf_url":"https://arxiv.org/pdf/2402.06626v3.pdf","comment":"Minor update to the version published at AAMAS 2024"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.11295v2","updated":"2024-10-20T23:57:21Z","published":"2024-10-15T05:33:16Z","title":"BRC20 Pinning Attack","summary":"  BRC20 tokens are a type of non-fungible asset on the Bitcoin network. They\nallow users to embed customized content within Bitcoin satoshis. The related\ntoken frenzy has reached a market size of US$2,650b over the past year\n(2023Q3-2024Q3). However, this intuitive design has not undergone serious\nsecurity scrutiny.\n  We present the first in-depth analysis of the BRC20 transfer mechanism and\nidentify a critical attack vector. A typical BRC20 transfer involves two\nbundled on-chain transactions with different fee levels: the first (i.e., Tx1)\nwith a lower fee inscribes the transfer request, while the second (i.e., Tx2)\nwith a higher fee finalizes the actual transfer. We find that an adversary can\nexploit this by sending a manipulated fee transaction (falling between the two\nfee levels), which allows Tx1 to be processed while Tx2 remains pinned in the\nmempool. This locks the BRC20 liquidity and disrupts normal transfers for\nusers. We term this BRC20 pinning attack.\n  Our attack exposes an inherent design flaw that can be applied to 90+%\ninscription-based tokens within the Bitcoin ecosystem.\n  We also conducted the attack on Binance's ORDI hot wallet (the most prevalent\nBRC20 token and the most active wallet), resulting in a temporary suspension of\nORDI withdrawals on Binance for 3.5 hours, which were shortly resumed after our\ncommunication.\n","authors":["Minfeng Qi","Qin Wang","Zhipeng Wang","Lin Zhong","Tianqing Zhu","Shiping Chen","William Knottenbelt"],"pdf_url":"https://arxiv.org/pdf/2410.11295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15296v1","updated":"2024-10-20T05:52:03Z","published":"2024-10-20T05:52:03Z","title":"A Remedy to Compute-in-Memory with Dynamic Random Access Memory:\n  1FeFET-1C Technology for Neuro-Symbolic AI","summary":"  Neuro-symbolic artificial intelligence (AI) excels at learning from noisy and\ngeneralized patterns, conducting logical inferences, and providing\ninterpretable reasoning. Comprising a 'neuro' component for feature extraction\nand a 'symbolic' component for decision-making, neuro-symbolic AI has yet to\nfully benefit from efficient hardware accelerators. Additionally, current\nhardware struggles to accommodate applications requiring dynamic resource\nallocation between these two components. To address these challenges-and\nmitigate the typical data-transfer bottleneck of classical Von Neumann\narchitectures-we propose a ferroelectric charge-domain compute-in-memory (CiM)\narray as the foundational processing element for neuro-symbolic AI. This array\nseamlessly handles both the critical multiply-accumulate (MAC) operations of\nthe 'neuro' workload and the parallel associative search operations of the\n'symbolic' workload. To enable this approach, we introduce an innovative\n1FeFET-1C cell, combining a ferroelectric field-effect transistor (FeFET) with\na capacitor. This design, overcomes the destructive sensing limitations of DRAM\nin CiM applications, while capable of capitalizing decades of DRAM expertise\nwith a similar cell structure as DRAM, achieves high immunity against FeFET\nvariation-crucial for neuro-symbolic AI-and demonstrates superior energy\nefficiency. The functionalities of our design have been successfully validated\nthrough SPICE simulations and prototype fabrication and testing. Our hardware\nplatform has been benchmarked in executing typical neuro-symbolic AI reasoning\ntasks, showing over 2x improvement in latency and 1000x improvement in energy\nefficiency compared to GPU-based implementations.\n","authors":["Xunzhao Yin","Hamza Errahmouni Barkam","Franz M√ºller","Yuxiao Jiang","Mohsen Imani","Sukhrob Abdulazhanov","Alptekin Vardar","Nellie Laleni","Zijian Zhao","Jiahui Duan","Zhiguo Shi","Siddharth Joshi","Michael Niemier","Xiaobo Sharon Hu","Cheng Zhuo","Thomas K√§mpfe","Kai Ni"],"pdf_url":"https://arxiv.org/pdf/2410.15296v1.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2008.08937v5","updated":"2024-10-20T20:14:36Z","published":"2020-08-20T12:38:55Z","title":"Institutional Grammar 2.0 Codebook","summary":"  The Grammar of Institutions, or Institutional Grammar, is an established\napproach to encode policy information in terms of institutional statements\nbased on a set of pre-defined syntactic components. This codebook provides\ncoding guidelines for a revised version of the Institutional Grammar, the\nInstitutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at\nfacilitating the encoding of policy to meet varying analytical objectives. To\nthis end, it revises the grammar with respect to comprehensiveness,\nflexibility, and specificity by offering multiple levels of expressiveness (IG\nCore, IG Extended, IG Logico). In addition to the encoding of regulative\nstatements, it further introduces the encoding of constitutive institutional\nstatements, as well as statements that exhibit both constitutive and regulative\ncharacteristics. Introducing those aspects, the codebook initially covers\nfundamental concepts of IG 2.0, before providing an overview of pre-coding\nsteps relevant for document preparation. Detailed coding guidelines are\nprovided for both regulative and constitutive statements across all levels of\nexpressiveness, along with the encoding guidelines for statements of mixed form\n-- hybrid and polymorphic institutional statements. The document further\nprovides an overview of taxonomies used in the encoding process and referred to\nthroughout the codebook. The codebook concludes with a summary and discussion\nof relevant considerations to facilitate the coding process. An initial\nReader's Guide helps the reader tailor the content to her interest.\n  Note that this codebook specifically focuses on operational aspects of IG 2.0\nin the context of policy coding. Links to additional resources such as the\nunderlying scientific literature (that offers a comprehensive treatment of the\nunderlying theoretical concepts) are referred to in the DOI and the concluding\nsection of the codebook.\n","authors":["Christopher K. Frantz","Saba N. Siddiki"],"pdf_url":"https://arxiv.org/pdf/2008.08937v5.pdf","comment":"122 pages, 16 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.15394v1","updated":"2024-10-20T13:49:44Z","published":"2024-10-20T13:49:44Z","title":"A Semi-decentralized and Variational-Equilibrium-Based Trajectory\n  Planner for Connected and Autonomous Vehicles","summary":"  This paper designs a novel trajectory planning approach to resolve the\ncomputational efficiency and safety problems in uncoordinated methods by\nexploiting vehicle-to-everything (V2X) technology. The trajectory planning for\nconnected and autonomous vehicles (CAVs) is formulated as a game with coupled\nsafety constraints. We then define interaction-fair trajectories and prove that\nthey correspond to the variational equilibrium (VE) of this game. We propose a\nsemi-decentralized planner for the vehicles to seek VE-based fair trajectories,\nwhich can significantly improve computational efficiency through parallel\ncomputing among CAVs and enhance the safety of planned trajectories by ensuring\nequilibrium concordance among CAVs. Finally, experimental results show the\nadvantages of the approach, including fast computation speed, high scalability,\nequilibrium concordance, and safety.\n","authors":["Zhengqin Liu","Jinlong Lei","Peng Yi"],"pdf_url":"https://arxiv.org/pdf/2410.15394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04702v2","updated":"2024-10-20T02:25:44Z","published":"2024-05-07T22:42:04Z","title":"Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment","summary":"  When independently trained or designed robots are deployed in a shared\nenvironment, their combined actions can lead to unintended negative side\neffects (NSEs). To ensure safe and efficient operation, robots must optimize\ntask performance while minimizing the penalties associated with NSEs, balancing\nindividual objectives with collective impact. We model the problem of\nmitigating NSEs in a cooperative multi-agent system as a bi-objective\nlexicographic decentralized Markov decision process. We assume independence of\ntransitions and rewards with respect to the robots' tasks, but the joint NSE\npenalty creates a form of dependence in this setting. To improve scalability,\nthe joint NSE penalty is decomposed into individual penalties for each robot\nusing credit assignment, which facilitates decentralized policy computation. We\nempirically demonstrate, using mobile robots and in simulation, the\neffectiveness and scalability of our approach in mitigating NSEs.\n","authors":["Pulkit Rustagi","Sandhya Saisubramanian"],"pdf_url":"https://arxiv.org/pdf/2405.04702v2.pdf","comment":"8 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.15461v1","updated":"2024-10-20T18:24:00Z","published":"2024-10-20T18:24:00Z","title":"EVA: An Embodied World Model for Future Video Anticipation","summary":"  World models integrate raw data from various modalities, such as images and\nlanguage to simulate comprehensive interactions in the world, thereby\ndisplaying crucial roles in fields like mixed reality and robotics. Yet,\napplying the world model for accurate video prediction is quite challenging due\nto the complex and dynamic intentions of the various scenes in practice. In\nthis paper, inspired by the human rethinking process, we decompose the complex\nvideo prediction into four meta-tasks that enable the world model to handle\nthis issue in a more fine-grained manner. Alongside these tasks, we introduce a\nnew benchmark named Embodied Video Anticipation Benchmark (EVA-Bench) to\nprovide a well-rounded evaluation. EVA-Bench focused on evaluating the video\nprediction ability of human and robot actions, presenting significant\nchallenges for both the language model and the generation model. Targeting\nembodied video prediction, we propose the Embodied Video Anticipator (EVA), a\nunified framework aiming at video understanding and generation. EVA integrates\na video generation model with a visual language model, effectively combining\nreasoning capabilities with high-quality generation. Moreover, to enhance the\ngeneralization of our framework, we tailor-designed a multi-stage pretraining\nparadigm that adaptatively ensembles LoRA to produce high-fidelity results.\nExtensive experiments on EVA-Bench highlight the potential of EVA to\nsignificantly improve performance in embodied scenes, paving the way for\nlarge-scale pre-trained models in real-world prediction tasks.\n","authors":["Xiaowei Chi","Hengyuan Zhang","Chun-Kai Fan","Xingqun Qi","Rongyu Zhang","Anthony Chen","Chi-min Chan","Wei Xue","Wenhan Luo","Shanghang Zhang","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2410.15461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15364v1","updated":"2024-10-20T11:40:31Z","published":"2024-10-20T11:40:31Z","title":"Scene Graph Generation with Role-Playing Large Language Models","summary":"  Current approaches for open-vocabulary scene graph generation (OVSGG) use\nvision-language models such as CLIP and follow a standard zero-shot pipeline --\ncomputing similarity between the query image and the text embeddings for each\ncategory (i.e., text classifiers). In this work, we argue that the text\nclassifiers adopted by existing OVSGG methods, i.e., category-/part-level\nprompts, are scene-agnostic as they remain unchanged across contexts. Using\nsuch fixed text classifiers not only struggles to model visual relations with\nhigh variance, but also falls short in adapting to distinct contexts. To plug\nthese intrinsic shortcomings, we devise SDSGG, a scene-specific description\nbased OVSGG framework where the weights of text classifiers are adaptively\nadjusted according to the visual content. In particular, to generate\ncomprehensive and diverse descriptions oriented to the scene, an LLM is asked\nto play different roles (e.g., biologist and engineer) to analyze and discuss\nthe descriptive features of a given scene from different views. Unlike previous\nefforts simply treating the generated descriptions as mutually equivalent text\nclassifiers, SDSGG is equipped with an advanced renormalization mechanism to\nadjust the influence of each text classifier based on its relevance to the\npresented scene (this is what the term \"specific\" means). Furthermore, to\ncapture the complicated interplay between subjects and objects, we propose a\nnew lightweight module called mutual visual adapter. It refines CLIP's ability\nto recognize relations by learning an interaction-aware semantic space.\nExtensive experiments on prevalent benchmarks show that SDSGG outperforms\ntop-leading methods by a clear margin.\n","authors":["Guikun Chen","Jin Li","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.15364v1.pdf","comment":"NeurIPS 2024. Code: https://github.com/guikunchen/SDSGG"},{"id":"http://arxiv.org/abs/2410.15279v1","updated":"2024-10-20T04:28:19Z","published":"2024-10-20T04:28:19Z","title":"ContextDet: Temporal Action Detection with Adaptive Context Aggregation","summary":"  Temporal action detection (TAD), which locates and recognizes action\nsegments, remains a challenging task in video understanding due to variable\nsegment lengths and ambiguous boundaries. Existing methods treat neighboring\ncontexts of an action segment indiscriminately, leading to imprecise boundary\npredictions. We introduce a single-stage ContextDet framework, which makes use\nof large-kernel convolutions in TAD for the first time. Our model features a\npyramid adaptive context aggragation (ACA) architecture, capturing long context\nand improving action discriminability. Each ACA level consists of two novel\nmodules. The context attention module (CAM) identifies salient contextual\ninformation, encourages context diversity, and preserves context integrity\nthrough a context gating block (CGB). The long context module (LCM) makes use\nof a mixture of large- and small-kernel convolutions to adaptively gather\nlong-range context and fine-grained local features. Additionally, by varying\nthe length of these large kernels across the ACA pyramid, our model provides\nlightweight yet effective context aggregation and action discrimination. We\nconducted extensive experiments and compared our model with a number of\nadvanced TAD methods on six challenging TAD benchmarks: MultiThumos, Charades,\nFineAction, EPIC-Kitchens 100, Thumos14, and HACS, demonstrating superior\naccuracy at reduced inference speed.\n","authors":["Ning Wang","Yun Xiao","Xiaopeng Peng","Xiaojun Chang","Xuanhong Wang","Dingyi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.15279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15266v1","updated":"2024-10-20T03:45:50Z","published":"2024-10-20T03:45:50Z","title":"GSSF: Generalized Structural Sparse Function for Deep Cross-modal Metric\n  Learning","summary":"  Cross-modal metric learning is a prominent research topic that bridges the\nsemantic heterogeneity between vision and language. Existing methods frequently\nutilize simple cosine or complex distance metrics to transform the pairwise\nfeatures into a similarity score, which suffers from an inadequate or\ninefficient capability for distance measurements. Consequently, we propose a\nGeneralized Structural Sparse Function to dynamically capture thorough and\npowerful relationships across modalities for pair-wise similarity learning\nwhile remaining concise but efficient. Specifically, the distance metric\ndelicately encapsulates two formats of diagonal and block-diagonal terms,\nautomatically distinguishing and highlighting the cross-channel relevancy and\ndependency inside a structured and organized topology. Hence, it thereby\nempowers itself to adapt to the optimal matching patterns between the paired\nfeatures and reaches a sweet spot between model complexity and capability.\nExtensive experiments on cross-modal and two extra uni-modal retrieval tasks\n(image-text retrieval, person re-identification, fine-grained image retrieval)\nhave validated its superiority and flexibility over various popular retrieval\nframeworks. More importantly, we further discover that it can be seamlessly\nincorporated into multiple application scenarios, and demonstrates promising\nprospects from Attention Mechanism to Knowledge Distillation in a plug-and-play\nmanner. Our code is publicly available at: https://github.com/Paranioar/GSSF.\n","authors":["Haiwen Diao","Ying Zhang","Shang Gao","Jiawen Zhu","Long Chen","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2410.15266v1.pdf","comment":"12 pages, 9 figures, Accepted by TIP2024"}]},"2024-10-19T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2403.13702v2","updated":"2024-10-19T11:48:13Z","published":"2024-03-20T16:06:45Z","title":"Constrained and Ordered Level Planarity Parameterized by the Number of\n  Levels","summary":"  The problem Level Planarity asks for a crossing-free drawing of a graph in\nthe plane such that vertices are placed at prescribed y-coordinates (called\nlevels) and such that every edge is realized as a y-monotone curve. In the\nvariant Constrained Level Planarity (CLP), each level $y$ is equipped with a\npartial order $\\prec_y$ on its vertices and in the desired drawing the\nleft-to-right order of vertices on level $y$ has to be a linear extension of\n$\\prec_y$. Ordered Level Planarity (OLP) corresponds to the special case of CLP\nwhere the given partial orders $\\prec_y$ are total orders. Previous results by\nBr\\\"uckner and Rutter [SODA 2017] and Klemz and Rote [ACM Trans. Alg. 2019]\nstate that both CLP and OLP are NP-hard even in severely restricted cases. In\nparticular, they remain NP-hard even when restricted to instances whose width\n(the maximum number of vertices that may share a common level) is at most two.\nIn this paper, we focus on the other dimension: we study the parameterized\ncomplexity of CLP and OLP with respect to the height (the number of levels).\n  We show that OLP parameterized by the height is complete with respect to the\ncomplexity class XNLP, which was first studied by Elberfeld et al.\n[Algorithmica 2015] (under a different name) and recently made more prominent\nby Bodlaender et al. [FOCS 2021]. It contains all parameterized problems that\ncan be solved nondeterministically in time $f(k) n^{O(1)}$ and space $f(k) \\log\nn$ (where $f$ is a computable function, $n$ is the input size, and $k$ is the\nparameter). If a problem is XNLP-complete, it lies in XP, but is W[$t$]-hard\nfor every $t$.\n  In contrast to the fact that OLP parameterized by the height lies in XP, it\nturns out that CLP is NP-hard even when restricted to instances of height 4. We\ncomplement this result by showing that CLP can be solved in polynomial time for\ninstances of height at most 3.\n","authors":["V√°clav Bla≈æej","Boris Klemz","Felix Klesen","Marie Diana Sieper","Alexander Wolff","Johannes Zink"],"pdf_url":"https://arxiv.org/pdf/2403.13702v2.pdf","comment":"A preliminary version of this paper appeared in the proceedings of\n  the 40th International Symposium on Computational Geometry (SoCG 2024)"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.15214v1","updated":"2024-10-19T21:13:09Z","published":"2024-10-19T21:13:09Z","title":"Relay Incentive Mechanisms Using Wireless Power Transfer in\n  Non-Cooperative Networks","summary":"  This paper studies the use of a multi-attribute auction in a communication\nsystem to bring about efficient relaying in a non-cooperative setting. We\nconsider a system where a source seeks to offload data to an access point (AP)\nwhile balancing both the timeliness and energy-efficiency of the transmission.\nA deep fade in the communication channel (due to, e.g., a line-of-sight\nblockage) makes direct communication costly, and the source may alternatively\nrely on non-cooperative UEs to act as relays. We propose a multi-attribute\nauction to select a UE and to determine the duration and power of the\ntransmission, with payments to the UE taking the form of energy sent via\nwireless power transfer (WPT). The quality of the channel from a UE to the AP\nconstitutes private information, and bids consist of a transmission time and\ntransmission power. We show that under a second-preferred-offer auction,\ntruthful bidding by all candidate UEs forms a Nash Equilibrium. However, this\nauction is not incentive compatible, and we present a modified auction in which\ntruthful bidding is in fact a dominant strategy. Extensive numerical\nexperimentation illustrates the efficacy of our approach, which we compare to a\ncooperative baseline. We demonstrate that with as few as two candidates, our\nimproved mechanism leads to as much as a 76% reduction in energy consumption,\nand that with as few as three candidates, the transmission time decreases by as\nmuch as 55%. Further, we see that as the number of candidates increases, the\nperformance of our mechanism approaches that of the cooperative baseline.\nOverall, our findings highlight the potential of multi-attribute auctions to\nenhance the efficiency of data transfer in non-cooperative settings.\n","authors":["Winston Hurst","Yasamin Mostofi"],"pdf_url":"https://arxiv.org/pdf/2410.15214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15093v1","updated":"2024-10-19T13:01:44Z","published":"2024-10-19T13:01:44Z","title":"DPVS-Shapley:Faster and Universal Contribution Evaluation Component in\n  Federated Learning","summary":"  In the current era of artificial intelligence, federated learning has emerged\nas a novel approach to addressing data privacy concerns inherent in centralized\nlearning paradigms. This decentralized learning model not only mitigates the\nrisk of data breaches but also enhances the system's scalability and\nrobustness. However, this approach introduces a new challenge: how to fairly\nand accurately assess the contribution of each participant. Developing an\neffective contribution evaluation mechanism is crucial for federated learning.\nSuch a mechanism incentivizes participants to actively contribute their data\nand computational resources, thereby improving the overall performance of the\nfederated learning system. By allocating resources and rewards based on the\nsize of the contributions, it ensures that each participant receives fair\ntreatment, fostering sustained engagement.Currently, Shapley value-based\nmethods are widely used to evaluate participants' contributions, with many\nresearchers proposing modifications to adapt these methods to real-world\nscenarios. In this paper, we introduce a component called Dynamic Pruning\nValidation Set Shapley (DPVS-Shapley). This method accelerates the contribution\nassessment process by dynamically pruning the original dataset without\ncompromising the evaluation's accuracy. Furthermore, this component can assign\ndifferent weights to various samples, thereby allowing clients capable of\ndistinguishing difficult examples to receive higher contribution scores.\n","authors":["Ketin Yin","Zonghao Guo","ZhengHan Qin"],"pdf_url":"https://arxiv.org/pdf/2410.15093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14936v1","updated":"2024-10-19T01:56:43Z","published":"2024-10-19T01:56:43Z","title":"Optimizing Individualized Incentives from Grid Measurements and Limited\n  Knowledge of Agent Behavior","summary":"  As electrical generation becomes more distributed and volatile, and loads\nbecome more uncertain, controllability of distributed energy resources (DERs),\nregardless of their ownership status, will be necessary for grid reliability.\nGrid operators lack direct control over end-users' grid interactions, such as\nenergy usage, but incentives can influence behavior -- for example, an end-user\nthat receives a grid-driven incentive may adjust their consumption or expose\nrelevant control variables in response. A key challenge in studying such\nincentives is the lack of data about human behavior, which usually motivates\nstrong assumptions, such as distributional assumptions on compliance or\nrational utility-maximization. In this paper, we propose a general incentive\nmechanism in the form of a constrained optimization problem -- our approach is\ndistinguished from prior work by modeling human behavior (e.g., reactions to an\nincentive) as an arbitrary unknown function. We propose feedback-based\noptimization algorithms to solve this problem that each leverage different\namounts of information and/or measurements. We show that each converges to an\nasymptotically stable incentive with (near)-optimality guarantees given mild\nassumptions on the problem. Finally, we evaluate our proposed techniques in\nvoltage regulation simulations on standard test beds. We test a variety of\nsettings, including those that break assumptions required for theoretical\nconvergence (e.g., convexity, smoothness) to capture realistic settings. In\nthis evaluation, our proposed algorithms are able to find near-optimal\nincentives even when the reaction to an incentive is modeled by a theoretically\ndifficult (yet realistic) function.\n","authors":["Adam Lechowicz","Joshua Comden","Andrey Bernstein"],"pdf_url":"https://arxiv.org/pdf/2410.14936v1.pdf","comment":"28 pages, 10 figures"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.16331v1","updated":"2024-10-19T13:01:31Z","published":"2024-10-19T13:01:31Z","title":"Exploring Quantum Neural Networks for Demand Forecasting","summary":"  Forecasting demand for assets and services can be addressed in various\nmarkets, providing a competitive advantage when the predictive models used\ndemonstrate high accuracy. However, the training of machine learning models\nincurs high computational costs, which may limit the training of prediction\nmodels based on available computational capacity. In this context, this paper\npresents an approach for training demand prediction models using quantum neural\nnetworks. For this purpose, a quantum neural network was used to forecast\ndemand for vehicle financing. A classical recurrent neural network was used to\ncompare the results, and they show a similar predictive capacity between the\nclassical and quantum models, with the advantage of using a lower number of\ntraining parameters and also converging in fewer steps. Utilizing quantum\ncomputing techniques offers a promising solution to overcome the limitations of\ntraditional machine learning approaches in training predictive models for\ncomplex market dynamics.\n","authors":["Gleydson Fernandes de Jesus","Maria Helo√≠sa Fraga da Silva","Otto Menegasso Pires","Lucas Cruz da Silva","Clebson dos Santos Cruz","Val√©ria Loureiro da Silva"],"pdf_url":"https://arxiv.org/pdf/2410.16331v1.pdf","comment":"22 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2410.15087v1","updated":"2024-10-19T12:23:59Z","published":"2024-10-19T12:23:59Z","title":"The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for\n  Effective Carbon-Aware Scheduling","summary":"  The rapid increase in computing demand and its corresponding energy\nconsumption have focused attention on computing's impact on the climate and\nsustainability. Prior work proposes metrics that quantify computing's carbon\nfootprint across several lifecycle phases, including its supply chain,\noperation, and end-of-life. Industry uses these metrics to optimize the carbon\nfootprint of manufacturing hardware and running computing applications.\nUnfortunately, prior work on optimizing datacenters' carbon footprint often\nsuccumbs to the \\emph{sunk cost fallacy} by considering embodied carbon\nemissions (a sunk cost) when making operational decisions (i.e., job scheduling\nand placement), which leads to operational decisions that do not always reduce\nthe total carbon footprint.\n  In this paper, we evaluate carbon-aware job scheduling and placement on a\ngiven set of servers for a number of carbon accounting metrics. Our analysis\nreveals state-of-the-art carbon accounting metrics that include embodied carbon\nemissions when making operational decisions can actually increase the total\ncarbon footprint of executing a set of jobs. We study the factors that affect\nthe added carbon cost of such suboptimal decision-making. We then use a\nreal-world case study from a datacenter to demonstrate how the sunk carbon\nfallacy manifests itself in practice. Finally, we discuss the implications of\nour findings in better guiding effective carbon-aware scheduling in on-premise\nand cloud datacenters.\n","authors":["Noman Bashir","Varun Gohil","Anagha Belavadi","Mohammad Shahrad","David Irwin","Elsa Olivetti","Christina Delimitrou"],"pdf_url":"https://arxiv.org/pdf/2410.15087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14945v1","updated":"2024-10-19T02:28:53Z","published":"2024-10-19T02:28:53Z","title":"ImmerseDiffusion: A Generative Spatial Audio Latent Diffusion Model","summary":"  We introduce ImmerseDiffusion, an end-to-end generative audio model that\nproduces 3D immersive soundscapes conditioned on the spatial, temporal, and\nenvironmental conditions of sound objects. ImmerseDiffusion is trained to\ngenerate first-order ambisonics (FOA) audio, which is a conventional spatial\naudio format comprising four channels that can be rendered to multichannel\nspatial output. The proposed generative system is composed of a spatial audio\ncodec that maps FOA audio to latent components, a latent diffusion model\ntrained based on various user input types, namely, text prompts, spatial,\ntemporal and environmental acoustic parameters, and optionally a spatial audio\nand text encoder trained in a Contrastive Language and Audio Pretraining (CLAP)\nstyle. We propose metrics to evaluate the quality and spatial adherence of the\ngenerated spatial audio. Finally, we assess the model performance in terms of\ngeneration quality and spatial conformance, comparing the two proposed modes:\n``descriptive\", which uses spatial text prompts) and ``parametric\", which uses\nnon-spatial text prompts and spatial parameters. Our evaluations demonstrate\npromising results that are consistent with the user conditions and reflect\nreliable spatial fidelity.\n","authors":["Mojtaba Heydari","Mehrez Souden","Bruno Conejo","Joshua Atkins"],"pdf_url":"https://arxiv.org/pdf/2410.14945v1.pdf","comment":"This work pioneers a Latent Diffusion Model for generating\n  text-prompted ambisonic spatial audio"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.15199v1","updated":"2024-10-19T20:11:11Z","published":"2024-10-19T20:11:11Z","title":"CLIPtortionist: Zero-shot Text-driven Deformation for Manufactured 3D\n  Shapes","summary":"  We propose a zero-shot text-driven 3D shape deformation system that deforms\nan input 3D mesh of a manufactured object to fit an input text description. To\ndo this, our system optimizes the parameters of a deformation model to maximize\nan objective function based on the widely used pre-trained vision language\nmodel CLIP. We find that CLIP-based objective functions exhibit many spurious\nlocal optima; to circumvent them, we parameterize deformations using a novel\ndeformation model called BoxDefGraph which our system automatically computes\nfrom an input mesh, the BoxDefGraph is designed to capture the object aligned\nrectangular/circular geometry features of most manufactured objects. We then\nuse the CMA-ES global optimization algorithm to maximize our objective, which\nwe find to work better than popular gradient-based optimizers. We demonstrate\nthat our approach produces appealing results and outperforms several baselines.\n","authors":["Xianghao Xu","Srinath Sridhar","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2410.15199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15139v1","updated":"2024-10-19T15:48:17Z","published":"2024-10-19T15:48:17Z","title":"The discrete charm of iterated function systems. A computer scientist's\n  perspective on approximation of IFS invariant sets and measures","summary":"  We study invariant sets and measures generated by iterated function systems\ndefined on countable discrete spaces that are uniform grids of a finite\ndimension. The discrete spaces of this type can be considered as models of\nspaces in which actual numerical computation takes place. In this context, we\ninvestigate the possibility of the application of the random iteration\nalgorithm to approximate these discrete IFS invariant sets and measures. The\nproblems concerning a discretization of hyperbolic IFSs are considered as\nspecial cases of this more general setting.\n","authors":["Tomasz Martyn"],"pdf_url":"https://arxiv.org/pdf/2410.15139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15068v1","updated":"2024-10-19T11:11:58Z","published":"2024-10-19T11:11:58Z","title":"A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for\n  Unpaired LDR-to-HDR Image Translation","summary":"  Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is an\nimportant computer vision problem. There is a significant amount of research\nutilizing both conventional non-learning methods and modern data-driven\napproaches, focusing on using both single-exposed and multi-exposed LDR for HDR\nimage reconstruction. However, most current state-of-the-art methods require\nhigh-quality paired {LDR,HDR} datasets for model training. In addition, there\nis limited literature on using unpaired datasets for this task where the model\nlearns a mapping between domains, i.e., LDR to HDR. To address limitations of\ncurrent methods, such as the paired data constraint , as well as unwanted\nblurring and visual artifacts in the reconstructed HDR, we propose a method\nthat uses a modified cycle-consistent adversarial architecture and utilizes\nunpaired {LDR,HDR} datasets for training. The method introduces novel\ngenerators to address visual artifact removal and an encoder and loss to\naddress semantic consistency, another under-explored topic. The method achieves\nstate-of-the-art results across several benchmark datasets and reconstructs\nhigh-quality HDR images.\n","authors":["Hrishav Bakul Barua","Stefanov Kalin","Lemuel Lai En Che","Dhall Abhinav","Wong KokSheik","Krishnasamy Ganesh"],"pdf_url":"https://arxiv.org/pdf/2410.15068v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2410.19831v1","updated":"2024-10-19T04:49:13Z","published":"2024-10-19T04:49:13Z","title":"GL-NeRF: Gauss-Laguerre Quadrature Enables Training-Free NeRF\n  Acceleration","summary":"  Volume rendering in neural radiance fields is inherently time-consuming due\nto the large number of MLP calls on the points sampled per ray. Previous works\nwould address this issue by introducing new neural networks or data structures.\nIn this work, We propose GL-NeRF, a new perspective of computing volume\nrendering with the Gauss-Laguerre quadrature. GL-NeRF significantly reduces the\nnumber of MLP calls needed for volume rendering, introducing no additional data\nstructures or neural networks. The simple formulation makes adopting GL-NeRF in\nany NeRF model possible. In the paper, we first justify the use of the\nGauss-Laguerre quadrature and then demonstrate this plug-and-play attribute by\nimplementing it in two different NeRF models. We show that with a minimal drop\nin performance, GL-NeRF can significantly reduce the number of MLP calls,\nshowing the potential to speed up any NeRF model.\n","authors":["Silong Yong","Yaqi Xie","Simon Stepputtis","Katia Sycara"],"pdf_url":"https://arxiv.org/pdf/2410.19831v1.pdf","comment":"NeurIPS 2024. Project page:\n  https://silongyong.github.io/GL-NeRF_project_page/"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.15221v1","updated":"2024-10-19T21:34:24Z","published":"2024-10-19T21:34:24Z","title":"IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual\n  Reinforcement Learning","summary":"  Despite the popularity of multi-agent reinforcement learning (RL) in\nsimulated and two-player applications, its success in messy real-world\napplications has been limited. A key challenge lies in its generalizability\nacross problem variations, a common necessity for many real-world problems.\nContextual reinforcement learning (CRL) formalizes learning policies that\ngeneralize across problem variations. However, the lack of standardized\nbenchmarks for multi-agent CRL has hindered progress in the field. Such\nbenchmarks are desired to be based on real-world applications to naturally\ncapture the many open challenges of real-world problems that affect\ngeneralization. To bridge this gap, we propose IntersectionZoo, a comprehensive\nbenchmark suite for multi-agent CRL through the real-world application of\ncooperative eco-driving in urban road networks. The task of cooperative\neco-driving is to control a fleet of vehicles to reduce fleet-level vehicular\nemissions. By grounding IntersectionZoo in a real-world application, we\nnaturally capture real-world problem characteristics, such as partial\nobservability and multiple competing objectives. IntersectionZoo is built on\ndata-informed simulations of 16,334 signalized intersections derived from 10\nmajor US cities, modeled in an open-source industry-grade microscopic traffic\nsimulator. By modeling factors affecting vehicular exhaust emissions (e.g.,\ntemperature, road conditions, travel demand), IntersectionZoo provides one\nmillion data-driven traffic scenarios. Using these traffic scenarios, we\nbenchmark popular multi-agent RL and human-like driving algorithms and\ndemonstrate that the popular multi-agent RL algorithms struggle to generalize\nin CRL settings.\n","authors":["Vindula Jayawardana","Baptiste Freydt","Ao Qu","Cameron Hickert","Zhongxia Yan","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2410.15221v1.pdf","comment":"In review"},{"id":"http://arxiv.org/abs/2410.15205v1","updated":"2024-10-19T20:40:18Z","published":"2024-10-19T20:40:18Z","title":"DTPPO: Dual-Transformer Encoder-based Proximal Policy Optimization for\n  Multi-UAV Navigation in Unseen Complex Environments","summary":"  Existing multi-agent deep reinforcement learning (MADRL) methods for\nmulti-UAV navigation face challenges in generalization, particularly when\napplied to unseen complex environments. To address these limitations, we\npropose a Dual-Transformer Encoder-based Proximal Policy Optimization (DTPPO)\nmethod. DTPPO enhances multi-UAV collaboration through a Spatial Transformer,\nwhich models inter-agent dynamics, and a Temporal Transformer, which captures\ntemporal dependencies to improve generalization across diverse environments.\nThis architecture allows UAVs to navigate new, unseen environments without\nretraining. Extensive simulations demonstrate that DTPPO outperforms current\nMADRL methods in terms of transferability, obstacle avoidance, and navigation\nefficiency across environments with varying obstacle densities. The results\nconfirm DTPPO's effectiveness as a robust solution for multi-UAV navigation in\nboth known and unseen scenarios.\n","authors":["Anning Wei","Jintao Liang","Kaiyuan Lin","Ziyue Li","Rui Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.15205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15156v1","updated":"2024-10-19T17:00:23Z","published":"2024-10-19T17:00:23Z","title":"Simulation-Based Optimistic Policy Iteration For Multi-Agent MDPs with\n  Kullback-Leibler Control Cost","summary":"  This paper proposes an agent-based optimistic policy iteration (OPI) scheme\nfor learning stationary optimal stochastic policies in multi-agent Markov\nDecision Processes (MDPs), in which agents incur a Kullback-Leibler (KL)\ndivergence cost for their control efforts and an additional cost for the joint\nstate. The proposed scheme consists of a greedy policy improvement step\nfollowed by an m-step temporal difference (TD) policy evaluation step. We use\nthe separable structure of the instantaneous cost to show that the policy\nimprovement step follows a Boltzmann distribution that depends on the current\nvalue function estimate and the uncontrolled transition probabilities. This\nallows agents to compute the improved joint policy independently. We show that\nboth the synchronous (entire state space evaluation) and asynchronous (a\nuniformly sampled set of substates) versions of the OPI scheme with finite\npolicy evaluation rollout converge to the optimal value function and an optimal\njoint policy asymptotically. Simulation results on a multi-agent MDP with KL\ncontrol cost variant of the Stag-Hare game validates our scheme's performance\nin terms of minimizing the cost return.\n","authors":["Khaled Nakhleh","Ceyhun Eksin","Sabit Ekin"],"pdf_url":"https://arxiv.org/pdf/2410.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15137v1","updated":"2024-10-19T15:34:15Z","published":"2024-10-19T15:34:15Z","title":"Collaborative State Fusion in Partially Known Multi-agent Environments","summary":"  In this paper, we study the collaborative state fusion problem in a\nmulti-agent environment, where mobile agents collaborate to track movable\ntargets. Due to the limited sensing range and potential errors of on-board\nsensors, it is necessary to aggregate individual observations to provide target\nstate fusion for better target state estimation. Existing schemes do not\nperform well due to (1) impractical assumption of the fully known prior target\nstate-space model and (2) observation outliers from individual sensors. To\naddress the issues, we propose a two-stage collaborative fusion framework,\nnamely \\underline{L}earnable Weighted R\\underline{o}bust \\underline{F}usion\n(\\textsf{LoF}). \\textsf{LoF} combines a local state estimator (e.g., Kalman\nFilter) with a learnable weight generator to address the mismatch between the\nprior state-space model and underlying patterns of moving targets. Moreover,\ngiven observation outliers, we develop a time-series soft medoid(TSM) scheme to\nperform robust fusion. We evaluate \\textsf{LoF} in a collaborative detection\nsimulation environment with promising results. In an example setting with 4\nagents and 2 targets, \\textsf{LoF} leads to a 9.1\\% higher fusion gain compared\nto the state-of-the-art.\n","authors":["Tianlong Zhou","Jun Shang","Weixiong Rao"],"pdf_url":"https://arxiv.org/pdf/2410.15137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11084v3","updated":"2024-10-19T08:12:53Z","published":"2023-12-18T10:23:50Z","title":"Multi-Agent Reinforcement Learning for Connected and Automated Vehicles\n  Control: Recent Advancements and Future Prospects","summary":"  Connected and automated vehicles (CAVs) are considered a potential solution\nfor future transportation challenges, aiming to develop systems that are\nefficient, safe, and environmentally friendly. However, CAV control presents\nsignificant challenges due to the complexity of interconnectivity and\ncoordination required among vehicles. Multi-agent reinforcement learning\n(MARL), which has shown notable advancements in addressing complex problems in\nautonomous driving, robotics, and human-vehicle interaction, emerges as a\npromising tool to enhance CAV capabilities. Despite its potential, there is a\nnotable absence of current reviews on mainstream MARL algorithms for CAVs. To\nfill this gap, this paper offers a comprehensive review of MARL's application\nin CAV control. The paper begins with an introduction to MARL, explaining its\nunique advantages in handling complex and multi-agent scenarios. It then\npresents a detailed survey of MARL applications across various control\ndimensions for CAVs, including critical scenarios such as platooning control,\nlane-changing, and unsignalized intersections. Additionally, the paper reviews\nprominent simulation platforms essential for developing and testing MARL\nalgorithms. Lastly, it examines the current challenges in deploying MARL for\nCAV control, including macro-micro optimization, communication, mixed traffic,\nand sim-to-real challenges. Potential solutions discussed include hierarchical\nMARL, decentralized MARL, adaptive interactions, and offline MARL.\n","authors":["Min Hua","Dong Chen","Xinda Qi","Kun Jiang","Zemin Eitan Liu","Quan Zhou","Hongming Xu"],"pdf_url":"https://arxiv.org/pdf/2312.11084v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14947v1","updated":"2024-10-19T02:34:13Z","published":"2024-10-19T02:34:13Z","title":"Optimally Solving Colored Generalized Sliding-Tile Puzzles: Complexity\n  and Bounds","summary":"  The Generalized Sliding-Tile Puzzle (GSTP), allowing many square tiles on a\nboard to move in parallel while enforcing natural geometric collision\nconstraints on the movement of neighboring tiles, provide a high-fidelity\nmathematical model for many high-utility existing and future multi-robot\napplications, e.g., at mobile robot-based warehouses or autonomous garages.\nMotivated by practical relevance, this work examines a further generalization\nof GSTP called the Colored Generalized Sliding-Tile Puzzle (CGSP), where tiles\ncan now assume varying degrees of distinguishability, a common occurrence in\nthe aforementioned applications. Our study establishes the computational\ncomplexity of CGSP and its key sub-problems under a broad spectrum of possible\nconditions and characterizes solution makespan lower and upper bounds that\ndiffer by at most a logarithmic factor. These results are further extended to\nhigher-dimensional versions of the puzzle game.\n","authors":["Marcus Gozon","Jingjin Yu"],"pdf_url":"https://arxiv.org/pdf/2410.14947v1.pdf","comment":"WAFR 2024 Conference Version"},{"id":"http://arxiv.org/abs/2410.14916v1","updated":"2024-10-19T00:10:52Z","published":"2024-10-19T00:10:52Z","title":"Cooperation and Fairness in Multi-Agent Reinforcement Learning","summary":"  Multi-agent systems are trained to maximize shared cost objectives, which\ntypically reflect system-level efficiency. However, in the resource-constrained\nenvironments of mobility and transportation systems, efficiency may be achieved\nat the expense of fairness -- certain agents may incur significantly greater\ncosts or lower rewards compared to others. Tasks could be distributed\ninequitably, leading to some agents receiving an unfair advantage while others\nincur disproportionately high costs. It is important to consider the tradeoffs\nbetween efficiency and fairness. We consider the problem of fair multi-agent\nnavigation for a group of decentralized agents using multi-agent reinforcement\nlearning (MARL). We consider the reciprocal of the coefficient of variation of\nthe distances traveled by different agents as a measure of fairness and\ninvestigate whether agents can learn to be fair without significantly\nsacrificing efficiency (i.e., increasing the total distance traveled). We find\nthat by training agents using min-max fair distance goal assignments along with\na reward term that incentivizes fairness as they move towards their goals, the\nagents (1) learn a fair assignment of goals and (2) achieve almost perfect goal\ncoverage in navigation scenarios using only local observations. For goal\ncoverage scenarios, we find that, on average, our model yields a 14%\nimprovement in efficiency and a 5% improvement in fairness over a baseline\ntrained using random assignments. Furthermore, an average of 21% improvement in\nfairness can be achieved compared to a model trained on optimally efficient\nassignments; this increase in fairness comes at the expense of only a 7%\ndecrease in efficiency. Finally, we extend our method to environments in which\nagents must complete coverage tasks in prescribed formations and show that it\nis possible to do so without tailoring the models to specific formation shapes.\n","authors":["Jasmine Jerry Aloor","Siddharth Nayak","Sydney Dolan","Hamsa Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2410.14916v1.pdf","comment":"Manuscript accepted in ACM Journal on Autonomous Transportation\n  Systems"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.03650v2","updated":"2024-10-19T14:23:57Z","published":"2024-08-07T09:25:17Z","title":"Towards Multimodal Emotional Support Conversation Systems","summary":"  The integration of conversational artificial intelligence (AI) into mental\nhealth care promises a new horizon for therapist-client interactions, aiming to\nclosely emulate the depth and nuance of human conversations. Despite the\npotential, the current landscape of conversational AI is markedly limited by\nits reliance on single-modal data, constraining the systems' ability to\nempathize and provide effective emotional support. This limitation stems from a\npaucity of resources that encapsulate the multimodal nature of human\ncommunication essential for therapeutic counseling. To address this gap, we\nintroduce the Multimodal Emotional Support Conversation (MESC) dataset, a\nfirst-of-its-kind resource enriched with comprehensive annotations across text,\naudio, and video modalities. This dataset captures the intricate interplay of\nuser emotions, system strategies, system emotion, and system responses, setting\na new precedent in the field. Leveraging the MESC dataset, we propose a general\nSequential Multimodal Emotional Support framework (SMES) grounded in\nTherapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES\nframework incorporates an LLM-based reasoning model that sequentially generates\nuser emotion recognition, system strategy prediction, system emotion\nprediction, and response generation. Our rigorous evaluations demonstrate that\nthis framework significantly enhances the capability of AI systems to mimic\ntherapist behaviors with heightened empathy and strategic responsiveness. By\nintegrating multimodal data in this innovative manner, we bridge the critical\ngap between emotion recognition and emotional support, marking a significant\nadvancement in conversational AI for mental health support.\n","authors":["Yuqi Chu","Lizi Liao","Zhiyuan Zhou","Chong-Wah Ngo","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2408.03650v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2410.15007v1","updated":"2024-10-19T06:42:43Z","published":"2024-10-19T06:42:43Z","title":"DiffuseST: Unleashing the Capability of the Diffusion Model for Style\n  Transfer","summary":"  Style transfer aims to fuse the artistic representation of a style image with\nthe structural information of a content image. Existing methods train specific\nnetworks or utilize pre-trained models to learn content and style features.\nHowever, they rely solely on textual or spatial representations that are\ninadequate to achieve the balance between content and style. In this work, we\npropose a novel and training-free approach for style transfer, combining\ntextual embedding with spatial features and separating the injection of content\nor style. Specifically, we adopt the BLIP-2 encoder to extract the textual\nrepresentation of the style image. We utilize the DDIM inversion technique to\nextract intermediate embeddings in content and style branches as spatial\nfeatures. Finally, we harness the step-by-step property of diffusion models by\nseparating the injection of content and style in the target branch, which\nimproves the balance between content preservation and style fusion. Various\nexperiments have demonstrated the effectiveness and robustness of our proposed\nDiffeseST for achieving balanced and controllable style transfer results, as\nwell as the potential to extend to other tasks.\n","authors":["Ying Hu","Chenyi Zhuang","Pan Gao"],"pdf_url":"https://arxiv.org/pdf/2410.15007v1.pdf","comment":"Accepted to ACMMM Asia 2024. Code is available at\n  https://github.com/I2-Multimedia-Lab/DiffuseST"},{"id":"http://arxiv.org/abs/2410.14922v1","updated":"2024-10-19T00:49:05Z","published":"2024-10-19T00:49:05Z","title":"Testing and validation of innovative eXtended Reality technologies for\n  astronaut training in a partial-gravity parabolic flight campaign","summary":"  The use of eXtended Reality (XR) technologies in the space domain has\nincreased significantly over the past few years as it can offer many advantages\nwhen simulating complex and challenging environments. Space agencies are\ncurrently using these disruptive tools to train astronauts for Extravehicular\nActivities (EVAs), to test equipment and procedures, and to assess spacecraft\nand hardware designs. With the Moon being the current focus of the next\ngeneration of space exploration missions, simulating its harsh environment is\none of the key areas where XR can be applied, particularly for astronaut\ntraining. Peculiar lunar lighting conditions in combination with reduced\ngravity levels will highly impact human locomotion especially for movements\nsuch as walking, jumping, and running. In order to execute operations on the\nlunar surface and to safely live on the Moon for an extended period of time,\ninnovative training methodologies and tools such as XR are becoming paramount\nto perform pre-mission validation and certification. This research work\npresents the findings of the experiments aimed at exploring the integration of\nXR technology and parabolic flight activities for astronaut training. In\naddition, the study aims to consolidate these findings into a set of guidelines\nthat can assist future researchers who wish to incorporate XR technology into\nlunar training and preparation activities, including the use of such XR tools\nduring long duration missions.\n","authors":["Florian Saling","Andrea Emanuele Maria Casini","Andreas Treuer","Martial Costantini","Leonie Bensch","Tommy Nilsson","Lionel Ferra"],"pdf_url":"https://arxiv.org/pdf/2410.14922v1.pdf","comment":"75th International Astronautical Congress (IAC), Milan, Italy, 14-18\n  October 2024"}]},"2024-10-18T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2407.05231v2","updated":"2024-10-18T15:51:31Z","published":"2024-07-07T01:54:54Z","title":"Fr√©chet Distance in Subquadratic Time","summary":"  Let $m$ and $n$ be the numbers of vertices of two polygonal curves in\n$\\mathbb{R}^d$ for any fixed $d$ such that $m \\leq n$. Since it was known in\n1995 how to compute the Fr\\'{e}chet distance of these two curves in $O(mn\\log\n(mn))$ time, it has been an open problem whether the running time can be\nreduced to $o(n^2)$ when $m = \\Omega(n)$. In the mean time, several well-known\nquadratic time barriers in computational geometry have been overcome: 3SUM,\nsome 3SUM-hard problems, and the computation of some distances between two\npolygonal curves, including the discrete Fr\\'{e}chet distance, the dynamic time\nwarping distance, and the geometric edit distance. It is curious that the\nquadratic time barrier for Fr\\'{e}chet distance still stands. We present an\nalgorithm to compute the Fr\\'echet distance in $O(mn(\\log\\log n)^{2+\\mu}\\log\nn/\\log^{1+\\mu} m)$ expected time for some constant $\\mu \\in (0,1)$. It is the\nfirst algorithm that returns the Fr\\'{e}chet distance in $o(mn)$ time when $m =\n\\Omega(n^{\\varepsilon})$ for any fixed $\\varepsilon \\in (0,1]$.\n","authors":["Siu-Wing Cheng","Haoqiang Huang"],"pdf_url":"https://arxiv.org/pdf/2407.05231v2.pdf","comment":"To appear at SODA25"},{"id":"http://arxiv.org/abs/2410.14318v1","updated":"2024-10-18T09:25:29Z","published":"2024-10-18T09:25:29Z","title":"Scalable Field-Aligned Reparameterization for Trimmed NURBS","summary":"  In engineering design, one of the most daunting problems in the\ndesign-through-analysis workflow is to deal with trimmed NURBS (Non-Uniform\nRational B-Splines), which often involve topological/geometric issues and lead\nto inevitable gaps and overlaps in the model. Given the dominance of the\ntrimming technology in CAD systems, reconstructing such a model as a watertight\nrepresentation is highly desired. While remarkable progress has been made in\nrecent years, especially with the advancement of isogeometric analysis (IGA),\nthere still lack a fully automatic and scalable tool to achieve this\nreconstruction goal. To address this issue, we present a semi-automatic and\nscalable reparameterization pipeline based on a scalable and feature-aligned\nmeshing tool, QuadriFlow [1]. On top of it, we provide support for open\nsurfaces to deal with engineering shell structures, and perform sophisticated\npatch simplification to remove undesired tiny/slender patches. As a result, we\nobtain a watertight spline surface (multi-patch NURBS or unstructured splines)\nwith a simple quadrilateral layout. Through several challenging models from\nindustry applications, we demonstrate the efficacy and efficiency of the\nproposed pipeline as well as its integration with IGA. Our source code is\npublicly available on GitHub [2].\n","authors":["Zheng Wei","Xiaodong Wei"],"pdf_url":"https://arxiv.org/pdf/2410.14318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14124v1","updated":"2024-10-18T02:41:12Z","published":"2024-10-18T02:41:12Z","title":"Additive design of 2-dimensional scissor lattices","summary":"  We introduce an additive approach for the design of a class of transformable\nstructures based on two-bar linkages (\"scissor mechanisms\") joined at vertices\nto form a two dimensional lattice. Our discussion traces an underlying\nmathematical similarity between linkage mechanisms, origami, and kirigami and\ninspires our name for these structures: karigami. We show how to design\nkarigami which unfold from a one dimensional collapsed state to two-dimensional\nsurfaces of single and double curvature. Our algorithm for growing karigami\nstructures is provably complete in providing the ability to explore the full\nspace of possible mechanisms, and we use it to computationally design and\nphysically realize a series of examples of varying complexity.\n","authors":["Noah Toyonaga","L Mahadevan"],"pdf_url":"https://arxiv.org/pdf/2410.14124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11073v2","updated":"2024-10-18T01:28:58Z","published":"2024-10-14T20:33:20Z","title":"An Interface Tracking Method with Triangle Edge Cuts","summary":"  This paper introduces a volume-conserving interface tracking algorithm on\nunstructured triangle meshes. We propose to discretize the interface via\ntriangle edge cuts which represent the intersections between the interface and\nthe triangle mesh edges using a compact 6 numbers per triangle. This enables an\nefficient implicit representation of the sub-triangle polygonal material\nregions without explicitly storing connectivity information. Moreover, we\npropose an efficient advection algorithm for this interface representation that\nis based on geometric queries and does not require an optimization process.\nThis advection algorithm is extended via an area correction step that enforces\nvolume-conservation of the materials. We demonstrate the efficacy of our method\non a variety of advection problems on a triangle mesh and compare its\nperformance to existing interface tracking methods including VOF and MOF.\n","authors":["Mengdi Wang","Matthew Cong","Bo Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.11073v2.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.14904v1","updated":"2024-10-18T23:11:31Z","published":"2024-10-18T23:11:31Z","title":"Switchback Price Experiments with Forward-Looking Demand","summary":"  We consider a retailer running a switchback experiment for the price of a\nsingle product, with infinite supply. In each period, the seller chooses a\nprice $p$ from a set of predefined prices that consist of a reference price and\na few discounted price levels. The goal is to estimate the demand gradient at\nthe reference price point, with the goal of adjusting the reference price to\nimprove revenue after the experiment. In our model, in each period, a unit mass\nof buyers arrives on the market, with values distributed based on a\ntime-varying process. Crucially, buyers are forward looking with a discounted\nutility and will choose to not purchase now if they expect to face a discounted\nprice in the near future. We show that forward-looking demand introduces bias\nin naive estimators of the demand gradient, due to intertemporal interference.\nFurthermore, we prove that there is no estimator that uses data from price\nexperiments with only two price points that can recover the correct demand\ngradient, even in the limit of an infinitely long experiment with an\ninfinitesimal price discount. Moreover, we characterize the form of the bias of\nnaive estimators. Finally, we show that with a simple three price level\nexperiment, the seller can remove the bias due to strategic forward-looking\nbehavior and construct an estimator for the demand gradient that asymptotically\nrecovers the truth.\n","authors":["Yifan Wu","Ramesh Johari","Vasilis Syrgkanis","Gabriel Y. Weintraub"],"pdf_url":"https://arxiv.org/pdf/2410.14904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14864v1","updated":"2024-10-18T21:13:23Z","published":"2024-10-18T21:13:23Z","title":"Double Distributionally Robust Bid Shading for First Price Auctions","summary":"  Bid shading has become a standard practice in the digital advertising\nindustry, in which most auctions for advertising (ad) opportunities are now of\nfirst price type. Given an ad opportunity, performing bid shading requires\nestimating not only the value of the opportunity but also the distribution of\nthe highest bid from competitors (i.e. the competitive landscape). Since these\ntwo estimates tend to be very noisy in practice, first-price auction\nparticipants need a bid shading policy that is robust against relatively\nsignificant estimation errors. In this work, we provide a max-min formulation\nin which we maximize the surplus against an adversary that chooses a\ndistribution both for the value and the competitive landscape, each from a\nKullback-Leibler-based ambiguity set. As we demonstrate, the two ambiguity sets\nare essential to adjusting the shape of the bid-shading policy in a principled\nway so as to effectively cope with uncertainty. Our distributionally robust bid\nshading policy is efficient to compute and systematically outperforms its\nnon-robust counterpart on real datasets provided by Yahoo DSP.\n","authors":["Yanlin Qu","Ravi Kant","Yan Chen","Brendan Kitts","San Gultekin","Aaron Flores","Jose Blanchet"],"pdf_url":"https://arxiv.org/pdf/2410.14864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.03076v2","updated":"2024-10-18T20:26:03Z","published":"2022-07-07T04:15:26Z","title":"Playing Divide-and-Choose Given Uncertain Preferences","summary":"  We study the classic divide-and-choose method for equitably allocating\ndivisible goods between two players who are rational, self-interested Bayesian\nagents. The players have additive values for the goods. The prior distributions\non those values are common knowledge. We consider both the cases of independent\nvalues and values that are correlated across players (as occurs when there is a\ncommon-value component).\n  We describe the structure of optimal divisions in the divide-and-choose game\nand identify several cases where it is possible to efficiently compute\nequilibria. An approximation algorithm is presented for the case when the\ndistribution over the chooser's value for each good follows a normal\ndistribution, along with a randomized approximation algorithm for the case of\nuniform distributions over intervals.\n  A mixture of analytic results and computational simulations illuminates\nseveral striking differences between optimal strategies in the cases of known\nversus unknown preferences. Most notably, given unknown preferences, the\ndivider has a compelling \"diversification\" incentive in creating the chooser's\ntwo options. This incentive leads to multiple goods being divided at\nequilibrium, quite contrary to the divider's optimal strategy when preferences\nare known.\n  In many contexts, such as buy-and-sell provisions between partners, or in\njudging fairness, it is important to assess the relative expected utilities of\nthe divider and chooser. Those utilities, we show, depend on the players'\nlevels of knowledge about each other's values, the correlations between the\nplayers' values, and the number of goods being divided. Under fairly mild\nassumptions, we show that the chooser is strictly better off for a small number\nof goods, while the divider is strictly better off for a large number of goods.\n","authors":["Jamie Tucker-Foltz","Richard Zeckhauser"],"pdf_url":"https://arxiv.org/pdf/2207.03076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14593v1","updated":"2024-10-18T16:43:36Z","published":"2024-10-18T16:43:36Z","title":"Temporal Fair Division of Indivisible Items","summary":"  We study a fair division model where indivisible items arrive sequentially,\nand must be allocated immediately and irrevocably. Previous work on online fair\ndivision has shown impossibility results in achieving approximate envy-freeness\nunder these constraints. In contrast, we consider an informed setting where the\nalgorithm has complete knowledge of future items, and aim to ensure that the\ncumulative allocation at each round satisfies approximate envy-freeness --\nwhich we define as temporal envy-freeness up to one item (TEF1). We focus on\nsettings where items can be exclusively goods or exclusively chores. For goods,\nwhile TEF1 allocations may not always exist, we identify several special cases\nwhere they do -- two agents, two item types, generalized binary valuations,\nunimodal preferences -- and provide polynomial-time algorithms for these cases.\nWe also prove that determining the existence of a TEF1 allocation is NP-hard.\nFor chores, we establish analogous results for the special cases, but present a\nslightly weaker intractability result. We also establish the incompatibility\nbetween TEF1 and Pareto-optimality, with the implication that it is intractable\nto find a TEF1 allocation that maximizes any $p$-mean welfare, even for two\nagents.\n","authors":["Edith Elkind","Alexander Lam","Mohamad Latifian","Tzeh Yuan Neoh","Nicholas Teh"],"pdf_url":"https://arxiv.org/pdf/2410.14593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14421v1","updated":"2024-10-18T12:33:24Z","published":"2024-10-18T12:33:24Z","title":"Fair Division in a Variable Setting","summary":"  We study the classic problem of fairly dividing a set of indivisible items\namong a set of agents and consider the popular fairness notion of envy-freeness\nup to one item (EF1). While in reality, the set of agents and items may vary,\nprevious works have studied static settings, where no change can occur in the\nsystem. We initiate and develop a formal model to understand fair division\nunder the variable input setting: here, there is an EF1 allocation that gets\ndisrupted because of the loss/deletion of an item, or the arrival of a new\nagent, resulting in a near-EF1 allocation. The objective is to perform a\nsequence of transfers of items between agents to regain EF1 fairness by\ntraversing only via near-EF1 allocations. We refer to this as the\nEF1-Restoration problem.\n  In this work, we present algorithms for the above problem when agents have\nidentical monotone valuations, and items are either all goods or all chores.\nBoth of these algorithms achieve an optimal number of transfers (at most $m/n$,\nwhere $m$ and $n$ are the number of items and agents respectively) for\nidentical additive valuations. Next, we consider a valuation class with\ngraphical structure, introduced by Christodoulou et al. (EC'23), where each\nitem is valued by at most two agents, and hence can be seen as an edge between\nthese two agents in a graph. Here, we consider EF1 orientations on\n(multi)graphs - allocations in which each item is allocated to an agent who\nvalues it. While considering EF1 orientations on multi-graphs with additive\nbinary valuations, we present an optimal algorithm for the EF1-Restoration\nproblem. Finally, for monotone binary valuations, we show that the problem of\ndeciding whether EF1-Restoration is possible is PSPACE-complete.\n","authors":["Harish Chandramouleeswaran","Prajakta Nimbhorkar","Nidhi Rathi"],"pdf_url":"https://arxiv.org/pdf/2410.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14363v1","updated":"2024-10-18T10:49:54Z","published":"2024-10-18T10:49:54Z","title":"Skill vs. Chance Quantification for Popular Card & Board Games","summary":"  We consider a few online and offline games under actual playing conditions.\nGenerally it is expected that initially a player obtains additional skill with\nexperience of playing more number of games and then it should finally saturate.\nThis phase is identified when a player, with the experience of very few games,\nloses more when she plays against players with much longer history. Then the\nwinning proportion curve moves up and finally it saturates. We benchmark our\nanalysis and discussion against Chess, the most skilled one among the games we\nconsider here. We use proprietary data from actual games (online and offline)\nas well as experiments for our statistical analysis. In this regard, we show\nthat Rummy has stronger skill and learning effects. Ludo has similar\ncharacteristics as Rummy, but at a weaker level. Similarly, a game that is\nperceived as almost no skill such as Teen Patti indeed presents much less skill\nin the analysis. In the next section we describe the game structures.\n","authors":["Tathagata Banerjee","Anushka De","Subhamoy Maitra","Diganta Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.14363v1.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.14311v1","updated":"2024-10-18T09:17:18Z","published":"2024-10-18T09:17:18Z","title":"Game Theory with Simulation in the Presence of Unpredictable\n  Randomisation","summary":"  AI agents will be predictable in certain ways that traditional agents are\nnot. Where and how can we leverage this predictability in order to improve\nsocial welfare? We study this question in a game-theoretic setting where one\nagent can pay a fixed cost to simulate the other in order to learn its mixed\nstrategy. As a negative result, we prove that, in contrast to prior work on\npure-strategy simulation, enabling mixed-strategy simulation may no longer lead\nto improved outcomes for both players in all so-called \"generalised trust\ngames\". In fact, mixed-strategy simulation does not help in any game where the\nsimulatee's action can depend on that of the simulator. We also show that, in\ngeneral, deciding whether simulation introduces Pareto-improving Nash\nequilibria in a given game is NP-hard. As positive results, we establish that\nmixed-strategy simulation can improve social welfare if the simulator has the\noption to scale their level of trust, if the players face challenges with both\ntrust and coordination, or if maintaining some level of privacy is essential\nfor enabling cooperation.\n","authors":["Vojtech Kovarik","Nathaniel Sauerberg","Lewis Hammond","Vincent Conitzer"],"pdf_url":"https://arxiv.org/pdf/2410.14311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14272v1","updated":"2024-10-18T08:26:07Z","published":"2024-10-18T08:26:07Z","title":"Envy-Free and Efficient Allocations for Graphical Valuations","summary":"  We consider the complexity of finding envy-free allocations for the class of\ngraphical valuations. Graphical valuations were introduced by Christodoulou et.\nal.(2023) as a structured class of valuations that admit allocations that are\nenvy-free up to any item (EFX). These are valuations where every item is valued\nby two agents, lending a (simple) graph structure to the utilities, where the\nagents are vertices and are adjacent if and only if they value a (unique)\ncommon item. Finding envy-free allocations for general valuations is known to\nbe computationally intractable even for very special cases: in particular, even\nfor binary valuations, and even for identical valuations with two agents. We\nshow that, for binary graphical valuations, the existence of envy-free\nallocations can be determined in polynomial time. In contrast, we also show\nthat allowing for even slightly more general utilities {0, 1, d} leads to\nintractability even for graphical valuations. This motivates other approaches\nto tractability, and to that end, we exhibit the fixed-parameter tractability\nof the problem parameterized by the vertex cover number of the graph when the\nnumber of distinct utilities is bounded. We also show that, all graphical\ninstances that admit EF allocations also admit one that is non-wasteful. Since\nEFX allocations are possibly wasteful, we also address the question of\ndetermining the price of fairness of EFX allocations. We show that the price of\nEFX with respect to utilitarian welfare is one for binary utilities, but can be\narbitrarily large {0, 1, d} valuations. We also show the hardness of deciding\nthe existence of an EFX allocation which is also welfare-maximizing and of\nfinding a welfare-maximizing allocation within the set of EFX allocations.\n","authors":["Neeldhara Misra","Aditi Sethia"],"pdf_url":"https://arxiv.org/pdf/2410.14272v1.pdf","comment":"18 pages, 2 figures, 1 table. A version of this work was accepted for\n  presentation at Algorithmic Decision Theory (ADT) 2024"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.14766v1","updated":"2024-10-18T15:50:59Z","published":"2024-10-18T15:50:59Z","title":"Evaluating Quantized Large Language Models for Code Generation on\n  Low-Resource Language Benchmarks","summary":"  Democratization of AI is an important topic within the broader topic of the\ndigital divide. This issue is relevant to LLMs, which are becoming popular as\nAI co-pilots but suffer from a lack of accessibility due to high computational\ndemand. In this study, we evaluate whether quantization is a viable approach\ntoward enabling LLMs on generic consumer devices. The study assesses the\nperformance of five quantized code LLMs in Lua code generation tasks. To\nevaluate the impact of quantization, the models with 7B parameters were tested\non a consumer laptop at 2-, 4-, and 8-bit integer precisions and compared to\nnon-quantized code LLMs with 1.3, 2, and 3 billion parameters. Lua is chosen as\na low-level resource language to avoid models' biases related to high-resource\nlanguages. The results suggest that the models quantized at the 4-bit integer\nprecision offer the best trade-off between performance and model size. These\nmodels can be comfortably deployed on an average laptop without a dedicated\nGPU. The performance significantly drops at the 2-bit integer precision. The\nmodels at 8-bit integer precision require more inference time that does not\neffectively translate to better performance. The 4-bit models with 7 billion\nparameters also considerably outperform non-quantized models with lower\nparameter numbers despite having comparable model sizes with respect to storage\nand memory demand. While quantization indeed increases the accessibility of\nsmaller LLMs with 7 billion parameters, these LLMs demonstrate overall low\nperformance (less than 50\\%) on high-precision and low-resource tasks such as\nLua code generation. While accessibility is improved, usability is still not at\nthe practical level comparable to foundational LLMs such as GPT-4o or Llama 3.1\n405B.\n","authors":["Enkhbold Nyamsuren"],"pdf_url":"https://arxiv.org/pdf/2410.14766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14537v1","updated":"2024-10-18T15:25:36Z","published":"2024-10-18T15:25:36Z","title":"Assessing the Impact of AR-Assisted Warnings on Roadway Workers' Stress\n  Under Different Workload Conditions","summary":"  Recent data from the Federal Highway Administration highlights an alarming\nincrease in fatalities and injuries in roadway work zones, emphasizing the need\nfor enhanced worker safety measures. This study addresses this concern by\nevaluating stress levels among roadway workers equipped with AR-assisted\nmulti-sensory warning technology during varying work intensities. The research\nleverages a high-fidelity Virtual Reality environment to simulate realistic\nwork scenarios, enabling safe evaluation of high-risk situations. Unlike\nprevious studies focusing on external factors, this research investigates the\ninternal physiological impact on workers. Utilizing wearable sensors, the study\ncollected physiological data, including photoplethysmography (PPG),\nelectrodermal activity (EDA), and skin temperature (ST), to assess stress\nlevels continuously and non-invasively. Our findings from 18 participants\nreveal significant differences between light- and medium-intensity activities\nin heart rate variability metrics. These metrics commonly used to assess\nautonomic nervous system function and stress levels, included mean heart rate,\nNN50, pNN50, and HF-HRV. By examining the relationship between AR-enabled\nwarnings, work intensity, and stress levels, the study contributes to enhancing\nworker safety and well-being. The proposed methodology offers potential for\nactive stress monitoring in the field, contributing to enhanced safety\npractices and worker productivity in construction sites. By providing real-time\nphysiological data, this approach enables informed stress management and more\neffective hazard warning systems in roadway work zones. This research bridges a\ngap in understanding the physiological impacts of AR-assisted warnings on\nroadway workers. The insights gained from this study can inform future safety\ninterventions and guide the development of more effective warning systems.\n","authors":["Fatemeh Banani Ardecani","Amit Kumar","Omidreza Shoghli"],"pdf_url":"https://arxiv.org/pdf/2410.14537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15681v2","updated":"2024-10-18T13:16:02Z","published":"2024-06-21T23:11:45Z","title":"Dyna-5G: A Dynamic, Flexible, and Self-Organizing 5G Network for M2M\n  Ecosystems","summary":"  In this work, we present Dyna-5G, a dynamic, self-organizing 5G New Radio\n(5G-NR) network designed for massive Machine-to-Machine (M2M) networks.\nTraditional 5G NR networks, characterized by their centralized architecture,\nface challenges in supporting applications that require dynamic, decentralized\ncommunication, such as autonomous vehicles and drone swarms for emergency\nresponses. These scenarios often suffer from the centralized model's single\npoint of failure, undermining the reliability required in critical and fully\nautonomous applications. Dyna-5G addresses these challenges by allowing each\ndevice in the network to function as either part of the Radio Access Network\n(RAN) and Core Network, or as User Equipment (UE), thus maintaining network\nfunctionality even when conventional infrastructure components are compromised.\nDyna-5G has built-in mechanisms carefully designed specifically for M2M\nnetworks, such as failure-recovery and ad-hoc entry and exit. We demonstrate\nthe performance and feasibility of Dyna-5G using a custom-built testbed that\nsimulates real-world missions, demonstrating our network's robustness,\nadaptability, and failure recovery capabilities. The results indicate that our\nentire 5G network model can fully re-organize in 6 seconds at maximum, without\ncompromising the mission.\n","authors":["Evangelos Bitsikas","Adam Belfki","Aanjhan Ranganathan"],"pdf_url":"https://arxiv.org/pdf/2406.15681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14320v1","updated":"2024-10-18T09:29:39Z","published":"2024-10-18T09:29:39Z","title":"Not Sure Your Car Withstands Cyberwarfare","summary":"  Data and derived information about target victims has always been key for\nsuccessful attacks, both during historical wars and modern cyber wars. Ours\nturns out to be an era in which modern cars generate a plethora of data about\ntheir drivers, and such data could be extremely attractive for offenders. This\npaper seeks to assess how well modern cars protect their drivers' data. It\npursues its goal at a requirement level by analysing the gaps of the privacy\npolicies of chief automakers such as BMW and Mercedes with respect to the\nGeneral Data Protection Regulation (GDPR). It is found that both brands are\nstill imprecise about how they comply with a number of GDPR articles, hence\ncompliance often results non-verifiable. Most importantly, while BMW exhibits\nslightly broader compliance, both brands still fail to comply with a number of\nrelevant articles of the regulation. An interpretation of these findings is a\nnon-negligible likelihood that your car may turn against you should\ncyberwarfare break out.\n","authors":["Giampaolo Bella","Gianpietro Castiglione","Sergio Esposito","Mario Raciti","Salvatore Riccobene"],"pdf_url":"https://arxiv.org/pdf/2410.14320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14111v1","updated":"2024-10-18T01:46:25Z","published":"2024-10-18T01:46:25Z","title":"HyCiM: A Hybrid Computing-in-Memory QUBO Solver for General\n  Combinatorial Optimization Problems with Inequality Constraints","summary":"  Computationally challenging combinatorial optimization problems (COPs) play a\nfundamental role in various applications. To tackle COPs, many Ising machines\nand Quadratic Unconstrained Binary Optimization (QUBO) solvers have been\nproposed, which typically involve direct transformation of COPs into Ising\nmodels or equivalent QUBO forms (D-QUBO). However, when addressing COPs with\ninequality constraints, this D-QUBO approach introduces numerous extra\nauxiliary variables, resulting in a substantially larger search space,\nincreased hardware costs, and reduced solving efficiency. In this work, we\npropose HyCiM, a novel hybrid computing-in-memory (CiM) based QUBO solver\nframework, designed to overcome aforementioned challenges. The proposed\nframework consists of (i) an innovative transformation method (first to our\nknown) that converts COPs with inequality constraints into an inequality-QUBO\nform, thus eliminating the need of expensive auxiliary variables and associated\ncalculations; (ii) \"inequality filter\", a ferroelectric FET (FeFET)-based CiM\ncircuit that accelerates the inequality evaluation, and filters out infeasible\ninput configurations; (iii) %When feasible solutions are detected, a\nFeFET-based CiM annealer that is capable of approaching global solutions of\nCOPs via iterative QUBO computations within a simulated annealing process. The\nevaluation results show that HyCiM drastically narrows down the search space,\neliminating $2^{100} \\text{ to } 2^{2536}$ infeasible input configurations\ncompared to the conventional D-QUBO approach.\n","authors":["Yu Qian","Zeyu Yang","Kai Ni","Alptekin Vardar","Thomas K√§mpfe","Xunzhao Yin"],"pdf_url":"https://arxiv.org/pdf/2410.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14093v1","updated":"2024-10-18T00:18:27Z","published":"2024-10-18T00:18:27Z","title":"Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable\n  Ising Machines","summary":"  A cognitive function of tracking multiple objects, needed in autonomous\nmobile vehicles, comprises object detection and their temporal association.\nWhile great progress owing to machine learning has been recently seen for\nelaborating the similarity matrix between the objects that have been recognized\nand the objects detected in a current video frame, less for the assignment\nproblem that finally determines the temporal association, which is a\ncombinatorial optimization problem. Here we show an in-vehicle multiple object\ntracking system with a flexible assignment function for tracking through\nmultiple long-term occlusion events. To solve the flexible assignment problem\nformulated as a nondeterministic polynomial time-hard problem, the system\nrelies on an embeddable Ising machine based on a quantum-inspired algorithm\ncalled simulated bifurcation. Using a vehicle-mountable computing platform, we\ndemonstrate a realtime system-wide throughput (23 frames per second on average)\nwith the enhanced functionality.\n","authors":["Kosuke Tatsumura","Yohei Hamakawa","Masaya Yamasaki","Koji Oya","Hiroshi Fujimoto"],"pdf_url":"https://arxiv.org/pdf/2410.14093v1.pdf","comment":"18 pages, 7 figures, 2 tables"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.14844v1","updated":"2024-10-18T19:46:12Z","published":"2024-10-18T19:46:12Z","title":"SYNOSIS: Image synthesis pipeline for machine vision in metal surface\n  inspection","summary":"  The use of machine learning (ML) methods for development of robust and\nflexible visual inspection system has shown promising. However their\nperformance is highly dependent on the amount and diversity of training data.\nThis is often restricted not only due to costs but also due to a wide variety\nof defects and product surfaces which occur with varying frequency. As such,\none can not guarantee that the acquired dataset contains enough defect and\nproduct surface occurrences which are needed to develop a robust model. Using\nparametric synthetic dataset generation, it is possible to avoid these issues.\nIn this work, we introduce a complete pipeline which describes in detail how to\napproach image synthesis for surface inspection - from first acquisition, to\ntexture and defect modeling, data generation, comparison to real data and\nfinally use of the synthetic data to train a defect segmentation model. The\npipeline is in detail evaluated for milled and sandblasted aluminum surfaces.\nIn addition to providing an in-depth view into each step, discussion of chosen\nmethods, and presentation of ML results, we provide a comprehensive dual\ndataset containing both real and synthetic images.\n","authors":["Juraj Fulir","Natascha Jeziorski","Lovro Bosnar","Hans Hagen","Claudia Redenbach","Petra Gospodnetiƒá","Tobias Herrfurth","Marcus Trost","Thomas Gischkat"],"pdf_url":"https://arxiv.org/pdf/2410.14844v1.pdf","comment":"Initial preprint, 21 pages, 21 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.14770v1","updated":"2024-10-18T17:53:07Z","published":"2024-10-18T17:53:07Z","title":"A Survey on Computational Solutions for Reconstructing Complete Objects\n  by Reassembling Their Fractured Parts","summary":"  Reconstructing a complete object from its parts is a fundamental problem in\nmany scientific domains. The purpose of this article is to provide a systematic\nsurvey on this topic. The reassembly problem requires understanding the\nattributes of individual pieces and establishing matches between different\npieces. Many approaches also model priors of the underlying complete object.\nExisting approaches are tightly connected problems of shape segmentation, shape\nmatching, and learning shape priors. We provide existing algorithms in this\ncontext and emphasize their similarities and differences to general-purpose\napproaches. We also survey the trends from early non-deep learning approaches\nto more recent deep learning approaches. In addition to algorithms, this survey\nwill also describe existing datasets, open-source software packages, and\napplications. To the best of our knowledge, this is the first comprehensive\nsurvey on this topic in computer graphics.\n","authors":["Jiaxin Lu","Yongqing Liang","Huijun Han","Jiacheng Hua","Junfeng Jiang","Xin Li","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14770v1.pdf","comment":"36 pages, 22 figures"},{"id":"http://arxiv.org/abs/2410.14508v1","updated":"2024-10-18T14:43:05Z","published":"2024-10-18T14:43:05Z","title":"LEAD: Latent Realignment for Human Motion Diffusion","summary":"  Our goal is to generate realistic human motion from natural language. Modern\nmethods often face a trade-off between model expressiveness and text-to-motion\nalignment. Some align text and motion latent spaces but sacrifice\nexpressiveness; others rely on diffusion models producing impressive motions,\nbut lacking semantic meaning in their latent space. This may compromise\nrealism, diversity, and applicability. Here, we address this by combining\nlatent diffusion with a realignment mechanism, producing a novel, semantically\nstructured space that encodes the semantics of language. Leveraging this\ncapability, we introduce the task of textual motion inversion to capture novel\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\nterms of realism, diversity, and text-motion consistency. Our qualitative\nanalysis and user study reveal that our synthesized motions are sharper, more\nhuman-like and comply better with the text compared to modern methods. For\nmotion textual inversion, our method demonstrates improved capacity in\ncapturing out-of-distribution characteristics in comparison to traditional\nVAEs.\n","authors":["Nefeli Andreou","Xi Wang","Victoria Fern√°ndez Abrevaya","Marie-Paule Cani","Yiorgos Chrysanthou","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2410.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14505v1","updated":"2024-10-18T14:37:37Z","published":"2024-10-18T14:37:37Z","title":"Neural Real-Time Recalibration for Infrared Multi-Camera Systems","summary":"  Currently, there are no learning-free or neural techniques for real-time\nrecalibration of infrared multi-camera systems. In this paper, we address the\nchallenge of real-time, highly-accurate calibration of multi-camera infrared\nsystems, a critical task for time-sensitive applications. Unlike traditional\ncalibration techniques that lack adaptability and struggle with on-the-fly\nrecalibrations, we propose a neural network-based method capable of dynamic\nreal-time calibration. The proposed method integrates a differentiable\nprojection model that directly correlates 3D geometries with their 2D image\nprojections and facilitates the direct optimization of both intrinsic and\nextrinsic camera parameters. Key to our approach is the dynamic camera pose\nsynthesis with perturbations in camera parameters, emulating realistic\noperational challenges to enhance model robustness. We introduce two model\nvariants: one designed for multi-camera systems with onboard processing of 2D\npoints, utilizing the direct 2D projections of 3D fiducials, and another for\nimage-based systems, employing color-coded projected points for implicitly\nestablishing correspondence. Through rigorous experimentation, we demonstrate\nour method is more accurate than traditional calibration techniques with or\nwithout perturbations while also being real-time, marking a significant leap in\nthe field of real-time multi-camera system calibration. The source code can be\nfound at https://github.com/theICTlab/neural-recalibration\n","authors":["Benyamin Mehmandar","Reza Talakoob","Charalambos Poullis"],"pdf_url":"https://arxiv.org/pdf/2410.14505v1.pdf","comment":"real-time camera calibration, infrared camera, neural calibration"},{"id":"http://arxiv.org/abs/2410.14159v1","updated":"2024-10-18T03:58:29Z","published":"2024-10-18T03:58:29Z","title":"Assessing Open-world Forgetting in Generative Image Model Customization","summary":"  Recent advances in diffusion models have significantly enhanced image\ngeneration capabilities. However, customizing these models with new classes\noften leads to unintended consequences that compromise their reliability. We\nintroduce the concept of open-world forgetting to emphasize the vast scope of\nthese unintended alterations, contrasting it with the well-studied closed-world\nforgetting, which is measurable by evaluating performance on a limited set of\nclasses or skills. Our research presents the first comprehensive investigation\ninto open-world forgetting in diffusion models, focusing on semantic and\nappearance drift of representations. We utilize zero-shot classification to\nanalyze semantic drift, revealing that even minor model adaptations lead to\nunpredictable shifts affecting areas far beyond newly introduced concepts, with\ndramatic drops in zero-shot classification of up to 60%. Additionally, we\nobserve significant changes in texture and color of generated content when\nanalyzing appearance drift. To address these issues, we propose a mitigation\nstrategy based on functional regularization, designed to preserve original\ncapabilities while accommodating new concepts. Our study aims to raise\nawareness of unintended changes due to model customization and advocates for\nthe analysis of open-world forgetting in future research on model customization\nand finetuning methods. Furthermore, we provide insights for developing more\nrobust adaptation methodologies.\n","authors":["H√©ctor Laria","Alex Gomez-Villa","Imad Eddine Marouf","Kai Wang","Bogdan Raducanu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2410.14159v1.pdf","comment":"Project page: https://hecoding.github.io/open-world-forgetting/"},{"id":"http://arxiv.org/abs/2410.14128v1","updated":"2024-10-18T02:48:57Z","published":"2024-10-18T02:48:57Z","title":"Hybrid Voxel Formats for Efficient Ray Tracing","summary":"  Voxels are a geometric representation used for rendering volumes,\nmulti-resolution models, and indirect lighting effects. Since the memory\nconsumption of uncompressed voxel volumes scales cubically with resolution,\npast works have introduced data structures for exploiting spatial sparsity and\nhomogeneity to compress volumes and accelerate ray tracing. However, these\nworks don't systematically evaluate the trade-off between compression and ray\nintersection performance for a variety of storage formats. We show that a\nhierarchical combination of voxel formats can achieve Pareto optimal trade-offs\nbetween memory consumption and rendering speed. We present a formulation of\n\"hybrid\" voxel formats, where each level of a hierarchical format can have a\ndifferent structure. For evaluation, we implement a metaprogramming system to\nautomatically generate construction and ray intersection code for arbitrary\nhybrid formats. We also identify transformations on these formats that can\nimprove compression and rendering performance. We evaluate this system with\nseveral models and hybrid formats, demonstrating that compared to standalone\nbase formats, hybrid formats achieve a new Pareto frontier in ray intersection\nperformance and storage cost.\n","authors":["Russel Arbore","Jeffrey Liu","Aidan Wefel","Steven Gao","Eric Shaffer"],"pdf_url":"https://arxiv.org/pdf/2410.14128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10403v3","updated":"2024-10-18T01:44:05Z","published":"2024-02-16T02:01:24Z","title":"Polyhedral Complex Derivation from Piecewise Trilinear Networks","summary":"  Recent advancements in visualizing deep neural networks provide insights into\ntheir structures and mesh extraction from Continuous Piecewise Affine (CPWA)\nfunctions. Meanwhile, developments in neural surface representation learning\nincorporate non-linear positional encoding, addressing issues like spectral\nbias; however, this poses challenges in applying mesh extraction techniques\nbased on CPWA functions. Focusing on trilinear interpolating methods as\npositional encoding, we present theoretical insights and an analytical mesh\nextraction, showing the transformation of hypersurfaces to flat planes within\nthe trilinear region under the eikonal constraint. Moreover, we introduce a\nmethod for approximating intersecting points among three hypersurfaces\ncontributing to broader applications. We empirically validate correctness and\nparsimony through chamfer distance and efficiency, and angular distance, while\nexamining the correlation between the eikonal loss and the planarity of the\nhypersurfaces.\n","authors":["Jin-Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2402.10403v3.pdf","comment":"Accepted at NeurIPS 2024. Updated with the camera-ready version"},{"id":"http://arxiv.org/abs/2410.11073v2","updated":"2024-10-18T01:28:58Z","published":"2024-10-14T20:33:20Z","title":"An Interface Tracking Method with Triangle Edge Cuts","summary":"  This paper introduces a volume-conserving interface tracking algorithm on\nunstructured triangle meshes. We propose to discretize the interface via\ntriangle edge cuts which represent the intersections between the interface and\nthe triangle mesh edges using a compact 6 numbers per triangle. This enables an\nefficient implicit representation of the sub-triangle polygonal material\nregions without explicitly storing connectivity information. Moreover, we\npropose an efficient advection algorithm for this interface representation that\nis based on geometric queries and does not require an optimization process.\nThis advection algorithm is extended via an area correction step that enforces\nvolume-conservation of the materials. We demonstrate the efficacy of our method\non a variety of advection problems on a triangle mesh and compare its\nperformance to existing interface tracking methods including VOF and MOF.\n","authors":["Mengdi Wang","Matthew Cong","Bo Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.11073v2.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2311.11144v3","updated":"2024-10-18T19:13:25Z","published":"2023-11-18T18:43:22Z","title":"A Model for Multi-Agent Autonomy That Uses Opinion Dynamics and\n  Multi-Objective Behavior Optimization","summary":"  This paper reports a new hierarchical architecture for modeling autonomous\nmulti-robot systems (MRSs): a nonlinear dynamical opinion process is used to\nmodel high-level group choice, and multi-objective behavior optimization is\nused to model individual decisions. Using previously reported theoretical\nresults, we show it is possible to design the behavior of the MRS by the\nselection of a relatively small set of parameters. The resulting behavior -\nboth collective actions and individual actions - can be understood intuitively.\nThe approach is entirely decentralized and the communication cost scales by the\nnumber of group options, not agents. We demonstrated the effectiveness of this\napproach using a hypothetical `explore-exploit-migrate' scenario in a two hour\nfield demonstration with eight unmanned surface vessels (USVs). The results\nfrom our preliminary field experiment show the collective behavior is robust\neven with time-varying network topology and agent dropouts.\n","authors":["Tyler M. Paine","Michael R. Benjamin"],"pdf_url":"https://arxiv.org/pdf/2311.11144v3.pdf","comment":"v1) 7 pages, 7 figures. v2) To appear at the 2024 IEEE International\n  Conference on Robotics and Automation (ICRA) in Yokohama, Japan. v3) Fixed\n  typos and added publication info"},{"id":"http://arxiv.org/abs/2409.13964v2","updated":"2024-10-18T18:56:37Z","published":"2024-09-21T00:54:15Z","title":"Adaptive bias for dissensus in nonlinear opinion dynamics with\n  application to evolutionary division of labor games","summary":"  This paper addresses the problem of adaptively controlling the bias parameter\nin nonlinear opinion dynamics (NOD) to allocate agents into groups of arbitrary\nsizes for the purpose of maximizing collective rewards. In previous work, an\nalgorithm based on the coupling of NOD with an multi-objective behavior\noptimization was successfully deployed as part of a multi-robot system in an\nautonomous task allocation field experiment. Motivated by the field results, in\nthis paper we propose and analyze a new task allocation model that synthesizes\nNOD with an evolutionary game framework. We prove sufficient conditions under\nwhich it is possible to control the opinion state in the group to a desired\nallocation of agents between two tasks through an adaptive bias using\ndecentralized feedback. We then verify the theoretical results with a\nsimulation study of a collaborative evolutionary division of labor game.\n","authors":["Tyler M. Paine","Anastasia Bizyaeva","Michael R. Benjamin"],"pdf_url":"https://arxiv.org/pdf/2409.13964v2.pdf","comment":"v1) To appear at the 2024 IEEE Conference on Decision and Control\n  (CDC) in Milan, Italy. 8 Pages, 5 Figures. v2) Fixed typo"},{"id":"http://arxiv.org/abs/2410.14406v1","updated":"2024-10-18T12:12:14Z","published":"2024-10-18T12:12:14Z","title":"On the Benefits of Robot Platooning for Navigating Crowded Environments","summary":"  This paper studies how groups of robots can effectively navigate through a\ncrowd of agents. It quantifies the performance of platooning and less\nconstrained, greedy strategies, and the extent to which these strategies\ndisrupt the crowd agents. Three scenarios are considered: (i) passive crowds,\n(ii) counter-flow crowds, and (iii) perpendicular-flow crowds. Through\nsimulations consisting of up to 200 robots, we show that for navigating passive\nand counter-flow crowds, the platooning strategy is less disruptive and more\neffective in dense crowds than the greedy strategy, whereas for navigating\nperpendicular-flow crowds, the greedy strategy outperforms the platooning\nstrategy in either aspect. Moreover, we propose an adaptive strategy that can\nswitch between platooning and greedy behavioral states, and demonstrate that it\ncombines the strengths of both strategies in all the scenarios considered.\n","authors":["Jahir Argote-Gerald","Genki Miyauchi","Paul Trodden","Roderich Gross"],"pdf_url":"https://arxiv.org/pdf/2410.14406v1.pdf","comment":"14 pages, 7 figures, to be published in DARS 2024"},{"id":"http://arxiv.org/abs/2410.14374v1","updated":"2024-10-18T11:02:13Z","published":"2024-10-18T11:02:13Z","title":"A Model Checker for Natural Strategic Ability","summary":"  In the last two decades, Alternating-time Temporal Logic (ATL) has been\nproved to be very useful in modeling strategic reasoning for Multi-Agent\nSystems (MAS). However, this logic struggles to capture the bounded rationality\ninherent in human decision-making processes. To overcome these limitations,\nNatural Alternating-time Temporal Logic (NatATL) has been recently introduced.\nAs an extension of ATL, NatATL incorporates bounded memory constraints into\nagents' strategies, which allows to resemble human cognitive limitations. In\nthis paper, we present a model checker tool for NatATL specifications - both\nfor memoryless strategies and strategies with recall - integrated into VITAMIN,\nan open-source model checker designed specifically for MAS verification. By\nembedding NatATL into VITAMIN, we transform theoretical advancements into a\npractical verification framework, enabling comprehensive analysis and\nvalidation of strategic reasoning in complex multi-agent environments. Our\nnovel tool paves the way for applications in areas such as explainable AI and\nhuman-in-the-loop systems, highlighting NatATL's substantial potential.\n","authors":["Marco Aruta","Vadim Malvone","Aniello Murano"],"pdf_url":"https://arxiv.org/pdf/2410.14374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08975v2","updated":"2024-10-18T10:14:58Z","published":"2022-03-16T22:39:46Z","title":"A Survey of Multi-Agent Deep Reinforcement Learning with Communication","summary":"  Communication is an effective mechanism for coordinating the behaviors of\nmultiple agents, broadening their views of the environment, and to support\ntheir collaborations. In the field of multi-agent deep reinforcement learning\n(MADRL), agents can improve the overall learning performance and achieve their\nobjectives by communication. Agents can communicate various types of messages,\neither to all agents or to specific agent groups, or conditioned on specific\nconstraints. With the growing body of research work in MADRL with communication\n(Comm-MADRL), there is a lack of a systematic and structural approach to\ndistinguish and classify existing Comm-MADRL approaches. In this paper, we\nsurvey recent works in the Comm-MADRL field and consider various aspects of\ncommunication that can play a role in designing and developing multi-agent\nreinforcement learning systems. With these aspects in mind, we propose 9\ndimensions along which Comm-MADRL approaches can be analyzed, developed, and\ncompared. By projecting existing works into the multi-dimensional space, we\ndiscover interesting trends. We also propose some novel directions for\ndesigning future Comm-MADRL systems through exploring possible combinations of\nthe dimensions.\n","authors":["Changxi Zhu","Mehdi Dastani","Shihan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.08975v2.pdf","comment":"34 pages, 5 figures, 13 tables; published on Autonomous Agents and\n  Multi-Agent Systems"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.17821v2","updated":"2024-10-18T21:55:23Z","published":"2024-04-27T08:00:43Z","title":"An automatic mixing speech enhancement system for multi-track audio","summary":"  We propose a speech enhancement system for multitrack audio. The system will\nminimize auditory masking while allowing one to hear multiple simultaneous\nspeakers. The system can be used in multiple communication scenarios e.g.,\nteleconferencing, invoice gaming, and live streaming. The ITU-R BS.1387\nPerceptual Evaluation of Audio Quality (PEAQ) model is used to evaluate the\namount of masking in the audio signals. Different audio effects e.g., level\nbalance, equalization, dynamic range compression, and spatialization are\napplied via an iterative Harmony searching algorithm that aims to minimize the\nmasking. In the subjective listening test, the designed system can compete with\nmixes by professional sound engineers and outperforms mixes by existing\nauto-mixing systems.\n","authors":["Xiaojing Liu","Hongwei Ai","Joshua D. Reiss"],"pdf_url":"https://arxiv.org/pdf/2404.17821v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06729v2","updated":"2024-10-18T13:45:09Z","published":"2024-10-09T09:55:51Z","title":"Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n","authors":["Dongshuai Duan","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.13754v2","updated":"2024-10-18T08:56:52Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any, real-world benchmark designed to optimize and\nstandardize evaluations across diverse input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions. Meanwhile, MixEval-X's model rankings correlate strongly with\nthat of crowd-sourced real-world evaluations (up to 0.98) while being much more\nefficient. We provide comprehensive leaderboards to rerank existing models and\norganizations and offer insights to enhance understanding of multi-modal\nevaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14154v1","updated":"2024-10-18T03:45:19Z","published":"2024-10-18T03:45:19Z","title":"RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping\n  Language-Image Pre-training","summary":"  Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.\n","authors":["Muhe Ding","Yang Ma","Pengda Qin","Jianlong Wu","Yuhong Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.14154v1.pdf","comment":"10 pages, 6 figures, Journal"},{"id":"http://arxiv.org/abs/2402.07640v3","updated":"2024-10-18T02:50:53Z","published":"2024-02-12T13:27:22Z","title":"Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image\n  Data","summary":"  The ability to generate sentiment-controlled feedback in response to\nmultimodal inputs comprising text and images addresses a critical gap in\nhuman-computer interaction. This capability allows systems to provide\nempathetic, accurate, and engaging responses, with useful applications in\neducation, healthcare, marketing, and customer service. To this end, we have\nconstructed a large-scale Controllable Multimodal Feedback Synthesis (CMFeed)\ndataset and propose a controllable feedback synthesis system. The system\nfeatures an encoder, decoder, and controllability block for textual and visual\ninputs. It extracts features using a transformer and Faster R-CNN networks,\ncombining them to generate feedback. The CMFeed dataset includes images, texts,\nreactions to the posts, human comments with relevance scores, and reactions to\nthese comments. These reactions train the model to produce feedback with\nspecified sentiments, achieving a sentiment classification accuracy of 77.23\\%,\nwhich is 18.82\\% higher than the accuracy without controllability. The system\nalso incorporates a similarity module for assessing feedback relevance through\nrank-based metrics and an interpretability technique to analyze the\ncontributions of textual and visual features during feedback generation. Access\nto the CMFeed dataset and the system's code is available at\nhttps://github.com/MIntelligence-Group/CMFeed.\n","authors":["Puneet Kumar","Sarthak Malik","Balasubramanian Raman","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2402.07640v3.pdf","comment":null}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2205.15279v3","updated":"2024-10-18T10:23:04Z","published":"2022-05-22T15:44:04Z","title":"The openESEA Modelling Language for Ethical, Social and Environmental\n  Accounting: Technical Report","summary":"  Over the years ethical, social and environmental accounting (ESEA) has become\na common practice among responsible organisations. ESEA entails assessing and\nreporting organisations\" performance on environmental, social and governance\ntopics. In this report, we present a textual grammar for specifying ESEA\nmethods. With the grammar ESEA models can be created. Such models can be\ninterpreted by our open-source, model-driven tool, called openESEA. The report\npresents the metamodel of the grammar, the grammar itself, and explanations of\neach grammar primitive.\n","authors":["Sergio Espa√±a","Vijanti Ramautar"],"pdf_url":"https://arxiv.org/pdf/2205.15279v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16323v1","updated":"2024-10-18T07:23:55Z","published":"2024-10-18T07:23:55Z","title":"Matheuristic Local Search for the Placement of Analog Integrated\n  Circuits","summary":"  The suboptimal physical design of the integrated circuits may not only\nincrease the manufacturing costs due to the larger size of the chip but can\nalso impact its performance by placing interconnected rectangular devices too\nfar from each other. In the domain of Analog and Mixed-Signal Integrated\nCircuits (AMS ICs), placement automation is lacking behind its digital\ncounterpart, mainly due to the variety of components and complex constraints\nthe placement needs to satisfy. Integer Linear Programming (ILP) is a suitable\napproach to modeling the placement problem for AMS ICs. However, not even\nstate-of-the-art solvers can create high-quality placements for large problem\ninstances. In this paper, we study how to improve the results of our previous\nILP model, first by introducing additional constraints and second by using\nmatheuristics. Given the initial solution we obtain using our original ILP\nmodel, we use the solver to perform a local search. We try to improve the\ncriterion by considering only a few spatially close rectangles while keeping\nthe rest of the placement fixed. This local search approach enables us to\nsignificantly improve the quality of instances whose solution space we could\nnot sufficiently explore before, even when the computation time reserved for\nthe matheuristic is limited. Finally, we evaluate our revised approach on\nsynthetically generated instances containing more than 200 independent\nrectangles and on real-life problems.\n","authors":["Josef Grus","Zdenƒõk Hanz√°lek"],"pdf_url":"https://arxiv.org/pdf/2410.16323v1.pdf","comment":"24 pages"}]},"2024-10-17T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.13811v1","updated":"2024-10-17T17:44:29Z","published":"2024-10-17T17:44:29Z","title":"Pentagonal bipyramids lead to the smallest flexible embedded polyhedron","summary":"  Steffen's polyhedron was believed to have the least number of vertices among\npolyhedra that can flex without self-intersections. Maksimov clarified that the\npentagonal bipyramid with one face subdivided into three is the only polyhedron\nwith fewer vertices for which the existence of a self-intersection-free flex\nwas open. Since subdividing a face into three does not change the mobility, we\nfocus on flexible pentagonal bipyramids. When a bipyramid flexes, the distance\nbetween the two opposite vertices of the two pyramids changes; associating the\nposition of the bipyramid to this distance yields an algebraic map that\ndetermines a nontrivial extension of rational function fields. We classify\nflexible pentagonal bipyramids with respect to the Galois group of this field\nextension and provide examples for each class, building on a construction\nproposed by Nelson. Surprisingly, one of our constructions yields a flexible\npentagonal bipyramid that can be extended to an embedded flexible polyhedron\nwith 8 vertices. The latter hence solves the open question.\n","authors":["Matteo Gallet","Georg Grasegger","Jan Legersk√Ω","Josef Schicho"],"pdf_url":"https://arxiv.org/pdf/2410.13811v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2410.13723v1","updated":"2024-10-17T16:27:31Z","published":"2024-10-17T16:27:31Z","title":"A Subsequence Approach to Topological Data Analysis for\n  Irregularly-Spaced Time Series","summary":"  A time-delay embedding (TDE), grounded in the framework of Takens's Theorem,\nprovides a mechanism to represent and analyze the inherent dynamics of\ntime-series data. Recently, topological data analysis (TDA) methods have been\napplied to study this time series representation mainly through the lens of\npersistent homology. Current literature on the fusion of TDE and TDA are adept\nat analyzing uniformly-spaced time series observations. This work introduces a\nnovel {\\em subsequence} embedding method for irregularly-spaced time-series\ndata. We show that this method preserves the original state space topology\nwhile reducing spurious homological features. Theoretical stability results and\nconvergence properties of the proposed method in the presence of noise and\nvarying levels of irregularity in the spacing of the time series are\nestablished. Numerical studies and an application to real data illustrates the\nperformance of the proposed method.\n","authors":["Sixtus Dakurah","Jessi Cisewski-Kehe"],"pdf_url":"https://arxiv.org/pdf/2410.13723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13536v1","updated":"2024-10-17T13:27:20Z","published":"2024-10-17T13:27:20Z","title":"A Simple Partially Embedded Planarity Test Based on Vertex-Addition","summary":"  In the Partially Embedded Planarity problem, we are given a graph $G$\ntogether with a topological drawing of a subgraph $H$ of $G$. The task is to\ndecide whether the drawing can be extended to a drawing of the whole graph such\nthat no two edges cross. Angelini et al. gave a linear-time algorithm for\nsolving this problem in 2010 (SODA '10). While their paper constitutes a\nsignificant result, the algorithm described therein is highly complex: it uses\nseveral layers of decompositions according to connectivity of both $G$ and $H$,\nits description spans more than 30 pages, and can hardly be considered\nimplementable. We give an independent linear-time algorithm that works along\nthe well-known vertex-addition planarity test by Booth and Lueker. We modify\nthe PC-tree as underlying data structure used for representing all planar\ndrawing possibilities in a natural way to also respect the restrictions given\nby the prescribed drawing of the subgraph $H$. The testing algorithm and its\nproof of correctness only require small adaptations from the comparatively much\nsimpler generic planarity test, of which several implementations exist. If the\ntest succeeds, an embedding can be constructed using the same approaches that\nare used for the generic planarity test.\n","authors":["Simon D. Fink","Ignaz Rutter","Sandhya T. P"],"pdf_url":"https://arxiv.org/pdf/2410.13536v1.pdf","comment":"accepted for SOSA 2025"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.14078v1","updated":"2024-10-17T22:53:43Z","published":"2024-10-17T22:53:43Z","title":"Computational Social Choice: Parameterized Complexity and Challenges","summary":"  We survey two key problems-Multi-Winner Determination and Hedonic Games in\nComputational Social Choice, with a special focus on their parameterized\ncomplexity, and propose some research challenges in the field.\n","authors":["Jiehua Chena","Christian Hatschka","Sofia Simola"],"pdf_url":"https://arxiv.org/pdf/2410.14078v1.pdf","comment":"Submitted to Computer Science Review"},{"id":"http://arxiv.org/abs/2410.13978v1","updated":"2024-10-17T19:14:06Z","published":"2024-10-17T19:14:06Z","title":"Incentivizing Information Acquisition","summary":"  I study a principal-agent model in which a principal hires an agent to\ncollect information about an unknown continuous state. The agent acquires a\nsignal whose distribution is centered around the state, controlling the\nsignal's precision at a cost. The principal observes neither the precision nor\nthe signal, but rather, using transfers that can depend on the state,\nincentivizes the agent to choose high precision and report the signal\ntruthfully. I identify a sufficient and necessary condition on the agent's\ninformation structure which ensures that there exists an optimal transfer with\na simple cutoff structure: the agent receives a fixed prize when his prediction\nis close enough to the state and receives nothing otherwise. This condition is\nmild and applies to all signal distributions commonly used in the literature.\n","authors":["Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.13978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13624v1","updated":"2024-10-17T15:00:01Z","published":"2024-10-17T15:00:01Z","title":"Optimal MEV Extraction Using Absolute Commitments","summary":"  We propose a new, more potent attack on decentralized exchanges. This attack\nleverages absolute commitments, which are commitments that can condition on the\nstrategies made by other agents. This attack allows an adversary to charge\nmonopoly prices by committing to undercut those other miners that refuse to\ncharge an even higher fee. This allows the miner to extract the maximum\npossible price from the user, potentially through side channels that evade the\ninefficiencies and fees usually incurred. This is considerably more efficient\nthan the prevailing strategy of `sandwich attacks', wherein the adversary\ninduces and profits from fluctuations in the market price to the detriment of\nusers. The attack we propose can, in principle, be realized by the irrevocable\nand self-executing nature of smart contracts, which are readily available on\nmany major blockchains. Thus, the attack could potentially be used against a\ndecentralized exchange and could drastically reduce the utility of the affected\nexchange.\n","authors":["Daji Landis","Nikolaj I. Schwartzbach"],"pdf_url":"https://arxiv.org/pdf/2410.13624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08711v2","updated":"2024-10-17T14:28:53Z","published":"2024-08-16T12:49:26Z","title":"Weighted Envy-free Allocation with Subsidy","summary":"  We consider the problem of fair allocation of indivisible items with\nsubsidies when agents have weighted entitlements. After highlighting several\nimportant differences from the unweighted case, we present several results\nconcerning weighted envy-freeability including general characterizations,\nalgorithms for achieving and testing weighted envy-freeability, lower and upper\nbounds of the amount of subsidies for envy-freeable allocations, and algorithms\nfor achieving weighted envy-freeability along with other properties.\n","authors":["Haris Aziz","Xin Huang","Kei Kimura","Indrajit Saha","Zhaohong Sun","Mashbat Suzuki","Makoto Yokoo"],"pdf_url":"https://arxiv.org/pdf/2408.08711v2.pdf","comment":"26 pages, 1 Table, 1 Figure"},{"id":"http://arxiv.org/abs/2410.13587v1","updated":"2024-10-17T14:22:51Z","published":"2024-10-17T14:22:51Z","title":"A Sequential Game Framework for Target Tracking","summary":"  This paper investigates the application of game-theoretic principles combined\nwith advanced Kalman filtering techniques to enhance maritime target tracking\nsystems. Specifically, the paper presents a two-player, imperfect information,\nnon-cooperative, sequential game framework for optimal decision making for a\ntracker and an evader. The paper also investigates the effectiveness of this\ngame-theoretic decision making framework by comparing it with single-objective\noptimisation methods based on minimising tracking uncertainty. Rather than\nmodelling a zero-sum game between the tracker and the evader, which presupposes\nthe availability of perfect information, in this paper we model both the\ntracker and the evader as playing separate zero-sum games at each time step\nwith an internal (and imperfect) model of the other player. The study defines\nmulti-faceted winning criteria for both tracker and evader, and computes\nwinning percentages for both by simulating their interaction for a range of\nspeed ratios. The results indicate that game theoretic decision making improves\nthe win percentage of the tracker compared to traditional covariance\nminimization procedures in all cases, regardless of the speed ratios and the\nactions of the evader. In the case of the evader, we find that a simpler linear\nescape action is most effective for the evader in most scenarios. Overall, the\nresults indicate that the presented sequential-game based decision making\nframework significantly improves win percentages for a player in scenarios\nwhere that player does not have inherent advantages in terms of starting\nposition, speed ratio, or available time (to track / escape), highlighting that\ngame theoretic decision making is particularly useful in scenarios where\nwinning by using more traditional decision making procedures is highly\nunlikely.\n","authors":["Daniel Leal","Ngoc Hung Nguyen","Alex Skvortsov","Sanjeev Arulampalam","Mahendra Piraveenan"],"pdf_url":"https://arxiv.org/pdf/2410.13587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13580v1","updated":"2024-10-17T14:15:08Z","published":"2024-10-17T14:15:08Z","title":"EFX Exists for Three Types of Agents","summary":"  In this paper, we study the problem of finding an envy-free allocation of\nindivisible goods among multiple agents. EFX, which stands for envy-freeness up\nto any good, is a well-studied relaxation of the envy-free allocation problem\nand has been shown to exist for specific scenarios. For instance, EFX is known\nto exist when there are only three agents [Chaudhury et al, EC 2020], and for\nany number of agents when there are only two types of valuations [Mahara,\nDiscret. Appl. Math 2023].\n  We show that EFX allocations exist for any number of agents when there are at\nmost three types of additive valuations.\n","authors":["Vishwa Prakash H. V.","Pratik Ghosal","Prajakta Nimbhorkar","Nithin Varma"],"pdf_url":"https://arxiv.org/pdf/2410.13580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13520v1","updated":"2024-10-17T13:09:08Z","published":"2024-10-17T13:09:08Z","title":"Contracting With a Reinforcement Learning Agent by Playing Trick or\n  Treat","summary":"  We study principal-agent problems where a farsighted agent takes costly\nactions in an MDP. The core challenge in these settings is that agent's actions\nare hidden to the principal, who can only observe their outcomes, namely state\ntransitions and their associated rewards. Thus, the principal's goal is to\ndevise a policy that incentives the agent to take actions leading to desirable\noutcomes. This is accomplished by committing to a payment scheme (a.k.a.\ncontract) at each step, specifying a monetary transfer from the principal to\nthe agent for every possible outcome. Interestingly, we show that Markovian\npolicies are unfit in these settings, as they do not allow to achieve the\noptimal principal's utility and are constitutionally intractable. Thus,\naccounting for history in unavoidable, and this begets considerable additional\nchallenges compared to standard MDPs. Nevertheless, we design an efficient\nalgorithm to compute an optimal policy, leveraging a compact way of\nrepresenting histories for this purpose. Unfortunately, the policy produced by\nsuch an algorithm cannot be readily implemented, as it is only approximately\nincentive compatible, meaning that the agent is incentivized to take the\ndesired actions only approximately. To fix this, we design an efficient method\nto make such a policy incentive compatible, by only introducing a negligible\nloss in principal's utility. This method can be generally applied to any\napproximately-incentive-compatible policy, and it generalized a related\napproach that has already been discovered for classical principal-agent\nproblems to more general settings in MDPs.\n","authors":["Matteo Bollini","Francesco Bacchiocchi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti"],"pdf_url":"https://arxiv.org/pdf/2410.13520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01963v2","updated":"2024-10-17T12:47:42Z","published":"2024-09-03T14:59:32Z","title":"Achieving Maximin Share and EFX/EF1 Guarantees Simultaneously","summary":"  We study the problem of computing \\emph{fair} divisions of a set of\nindivisible goods among agents with \\emph{additive} valuations. For the past\nmany decades, the literature has explored various notions of fairness, that can\nbe primarily seen as either having \\emph{envy-based} or \\emph{share-based}\nlens. For the discrete setting of resource-allocation problems, \\emph{envy-free\nup to any good} (EFX) and \\emph{maximin share} (MMS) are widely considered as\nthe flag-bearers of fairness notions in the above two categories, thereby\ncapturing different aspects of fairness herein. Due to lack of existence\nresults of these notions and the fact that a good approximation of EFX or MMS\ndoes not imply particularly strong guarantees of the other, it becomes\nimportant to understand the compatibility of EFX and MMS allocations with one\nanother.\n  In this work, we identify a novel way to simultaneously achieve MMS\nguarantees with EFX/EF1 notions of fairness, while beating the best known\napproximation factors [Chaudhury et al., 2021, Amanatidis et al., 2020]. Our\nmain contribution is to constructively prove the existence of (i) a partial\nallocation that is both $2/3$-MMS and EFX, and (ii) a complete allocation that\nis both $2/3$-MMS and EF1. Our algorithms run in pseudo-polynomial time if the\napproximation factor for MMS is relaxed to $2/3-\\varepsilon$ for any constant\n$\\varepsilon > 0$ and in polynomial time if, in addition, the EFX (or EF1)\nguarantee is relaxed to $(1-\\delta)$-EFX (or $(1-\\delta)$-EF1) for any constant\n$\\delta>0$. In particular, we improve from the best approximation factor known\nprior to our work, which computes partial allocations that are $1/2$-MMS and\nEFX in pseudo-polynomial time [Chaudhury et al., 2021].\n","authors":["Hannaneh Akrami","Nidhi Rathi"],"pdf_url":"https://arxiv.org/pdf/2409.01963v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14613v4","updated":"2024-10-17T02:10:53Z","published":"2024-01-26T03:10:51Z","title":"Multiplayer General Lotto game","summary":"  In this paper, we investigate the multiplayer General Lotto game across\nmultiple battlefields, a significant variant of the Colonel Blotto game. In\nthis version, each player employs a probability distribution for resource\nallocation, ensuring that their expected expenditure does not exceed their\nbudget. We first establish the existence of the Nash equilibrium in a general\nsetting, where players' budgets are asymmetric and the values of the\nbattlefields are heterogeneous and asymmetric among players. Next, we provide a\ndetailed characterization of the Nash equilibrium for multiple players on a\nsingle battlefield. In this characterization, we observe that the upper\nendpoints of the supports of players' equilibrium strategies coincide, and that\nthe minimum value of a player's support above zero inversely correlates with\nhis budget. We demonstrate the uniqueness of Nash equilibrium over a single\nbattlefield in some scenarios. In the multi-battlefield setting, we prove that\nthere is an upper bound on the average number of battlefields each player\nparticipates in. Additionally, we provide an example demonstrating the\nnon-uniqueness of the Nash equilibrium in the context of multiple battlefields\nwith multiple players. Finally, we present a solution for the Nash equilibrium\nin a symmetric case.\n","authors":["Yan Liu","Bonan Ni","Weiran Shen","Zihe Wang","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.14613v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10340v2","updated":"2024-10-17T01:09:51Z","published":"2023-09-19T05:51:13Z","title":"Striking a Balance: An Optimal Mechanism Design for Heterogenous\n  Differentially Private Data Acquisition for Logistic Regression","summary":"  We address the challenge of solving machine learning tasks using data from\nprivacy-sensitive sellers. Since the data is private, we design a data market\nthat incentivizes sellers to provide their data in exchange for payments.\nTherefore our objective is to design a mechanism that optimizes a weighted\ncombination of test loss, seller privacy, and payment, striking a balance\nbetween building a good privacy-preserving ML model and minimizing payments to\nthe sellers. To achieve this, we first propose an approach to solve logistic\nregression with known heterogeneous differential privacy guarantees. Building\non these results and leveraging standard mechanism design theory, we develop a\ntwo-step optimization framework. We further extend this approach to an online\nalgorithm that handles the sequential arrival of sellers.\n","authors":["Ameya Anjarlekar","Rasoul Etesami","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2309.10340v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.14058v1","updated":"2024-10-17T22:02:19Z","published":"2024-10-17T22:02:19Z","title":"An AI Guide to Enhance Accessibility of Social Virtual Reality for Blind\n  People","summary":"  The rapid growth of virtual reality (VR) has led to increased use of social\nVR platforms for interaction. However, these platforms lack adequate features\nto support blind and low vision (BLV) users, posing significant challenges in\nnavigation, visual interpretation, and social interaction. One promising\napproach to these challenges is employing human guides in VR. However, this\napproach faces limitations with a lack of availability of humans to serve as\nguides, or the inability to customize the guidance a user receives from the\nhuman guide. We introduce an AI-powered guide to address these limitations. The\nAI guide features six personas, each offering unique behaviors and appearances\nto meet diverse user needs, along with visual interpretation and navigation\nassistance. We aim to use this AI guide in the future to help us understand BLV\nusers' preferences for guide forms and functionalities.\n","authors":["Jazmin Collins","Kaylah Myranda Nicholson","Yusuf Khadir","Andrea Stevenson Won","Shiri Azenkot"],"pdf_url":"https://arxiv.org/pdf/2410.14058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13623v1","updated":"2024-10-17T14:56:44Z","published":"2024-10-17T14:56:44Z","title":"Neural Correlates of Augmented Reality Safety Warnings: EEG Analysis of\n  Situational Awareness and Cognitive Performance in Roadway Work Zones","summary":"  Despite the research and implementation efforts involving various safety\nstrategies, protocols, and technologies, work zone crashes and fatalities\ncontinue to occur at an alarming rate each year. This study investigates the\nneurophysiological responses to Augmented Reality safety warnings in roadway\nwork zones under varying workload conditions. Using electroencephalogram (EEG)\ntechnology, we objectively assessed situational awareness, attention, and\ncognitive load in simulated low-intensity (LA) and moderate-intensity (MA) work\nactivities. The research analyzed key EEG indicators including beta, gamma,\nalpha, and theta waves, as well as various combined wave ratios. Results\nrevealed that AR warnings effectively triggered neurological responses\nassociated with increased situational awareness and attention across both\nworkload conditions. However, significant differences were observed in the\ntiming and intensity of these responses. In the LA condition, peak responses\noccurred earlier (within 125 ms post-warning) and were more pronounced,\nsuggesting a more robust cognitive response when physical demands were lower.\nConversely, the MA condition showed delayed peak responses (125-250 ms\npost-warning) and more gradual changes, indicating a potential impact of\nincreased physical activity on cognitive processing speed. These findings\nunderscore the importance of considering physical workload when designing\nAR-based safety systems for roadway work zones. The research contributes to the\nunderstanding of how AR can enhance worker safety and provides insights for\ndeveloping more effective, context-aware safety interventions in high-risk work\nenvironments.\n","authors":["Fatemeh Banani Ardecani","Amit Kumar","Sepehr Sabeti","Omidreza Shoghli"],"pdf_url":"https://arxiv.org/pdf/2410.13623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07980v2","updated":"2024-10-17T14:07:13Z","published":"2024-10-10T14:36:24Z","title":"D-Wave's Nonlinear-Program Hybrid Solver: Description and Performance\n  Analysis","summary":"  The development of advanced quantum-classical algorithms is among the most\nprominent strategies in quantum computing. Numerous hybrid solvers have been\nintroduced recently. Many of these methods are created ad hoc to address\nspecific use cases. However, several well-established schemes are frequently\nutilized to address optimization problems. In this context, D-Wave launched the\nHybrid Solver Service in 2020, offering a portfolio of methods designed to\naccelerate time-to-solution for users aiming to optimize performance and\noperational processes. Recently, a new technique has been added to this\nportfolio: the Nonlinear-Program Hybrid Solver. This paper describes this\nsolver and evaluates its performance through a benchmark of 45 instances across\nthree combinatorial optimization problems: the Traveling Salesman Problem, the\nKnapsack Problem, and the Maximum Cut Problem. To facilitate the use of this\nrelatively unexplored solver, we provide details of the implementation used to\nsolve these three optimization problems.\n","authors":["Eneko Osaba","Pablo Miranda-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2410.07980v2.pdf","comment":"10 pages, 8 figures and 7 tables"},{"id":"http://arxiv.org/abs/2410.13462v1","updated":"2024-10-17T11:47:26Z","published":"2024-10-17T11:47:26Z","title":"EOSpython Version 0.0.11: A Framework for Scenario Generation and a\n  Solution System for the Agile Earth Observation Satellite Scheduling Problem","summary":"  EOSpython is a PyPI published Python package that encompass everything within\na centralized earth observation satellite scheduling system in terms of\ncustomer database setup, scenario generation, pre-processing, problem setup,\nscheduling solution approach, decision maker preference integration, and\nvisualization. The package is tailored to easily configure internal parameters\nand contribute with other solution approaches.\n","authors":["Alex Elkj√¶r Vasegaard","Andreas K√ºhne Larsen"],"pdf_url":"https://arxiv.org/pdf/2410.13462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13104v1","updated":"2024-10-17T00:29:29Z","published":"2024-10-17T00:29:29Z","title":"Cost-Effective Realization of n-Bit Toffoli Gates for IBM Quantum\n  Computers Using the Bloch Sphere Approach and IBM Native Gates","summary":"  A cost-effective n-bit Toffoli gate is proposed to be realized (or\ntranspiled) based on the layouts (linear, T-like, and I-like) and the number of\nn physical qubits for IBM quantum computers. This proposed gate is termed the\n\"layout-aware n-bit Toffoli gate\". The layout-aware n-bit Toffoli gate is\ndesigned using the visual approach of the Bloch sphere, from the visual\nrepresentations of the rotational quantum operations for IBM native gates. In\nthis paper, we also proposed a new formula for the quantum cost, which\ncalculates the total number of native gates, the crossing connections, and the\ndepth of the final transpiled quantum circuit. This formula is termed the\n\"transpilation quantum cost\". After transpilation, our proposed layout-aware\nn-bit Toffoli gate always has a much lower transpilation quantum cost than that\nof the conventional n-bit Toffoli gate, where 3 <= n <= 7 qubits, for different\nIBM quantum computers.\n","authors":["Ali Al-Bayaty","Marek Perkowski"],"pdf_url":"https://arxiv.org/pdf/2410.13104v1.pdf","comment":"22 figures and 5 tables. arXiv admin note: text overlap with\n  arXiv:2311.06760"}],"Graphics":[{"id":"http://arxiv.org/abs/2309.17329v3","updated":"2024-10-17T19:23:45Z","published":"2023-09-29T15:40:58Z","title":"Efficient Anatomical Labeling of Pulmonary Tree Structures via Deep\n  Point-Graph Representation-based Implicit Fields","summary":"  Pulmonary diseases rank prominently among the principal causes of death\nworldwide. Curing them will require, among other things, a better understanding\nof the complex 3D tree-shaped structures within the pulmonary system, such as\nairways, arteries, and veins. Traditional approaches using high-resolution\nimage stacks and standard CNNs on dense voxel grids face challenges in\ncomputational efficiency, limited resolution, local context, and inadequate\npreservation of shape topology. Our method addresses these issues by shifting\nfrom dense voxel to sparse point representation, offering better memory\nefficiency and global context utilization. However, the inherent sparsity in\npoint representation can lead to a loss of crucial connectivity in tree-shaped\nstructures. To mitigate this, we introduce graph learning on skeletonized\nstructures, incorporating differentiable feature fusion for improved topology\nand long-distance context capture. Furthermore, we employ an implicit function\nfor efficient conversion of sparse representations into dense reconstructions\nend-to-end. The proposed method not only delivers state-of-the-art performance\nin labeling accuracy, both overall and at key locations, but also enables\nefficient inference and the generation of closed surface shapes. Addressing\ndata scarcity in this field, we have also curated a comprehensive dataset to\nvalidate our approach. Data and code are available at\n\\url{https://github.com/M3DV/pulmonary-tree-labeling}.\n","authors":["Kangxian Xie","Jiancheng Yang","Donglai Wei","Ziqiao Weng","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2309.17329v3.pdf","comment":"Accepted by Medical Image Analysis"},{"id":"http://arxiv.org/abs/2410.13851v1","updated":"2024-10-17T17:59:02Z","published":"2024-10-17T17:59:02Z","title":"Differentiable Robot Rendering","summary":"  Vision foundation models trained on massive amounts of visual data have shown\nunprecedented reasoning and planning skills in open-world settings. A key\nchallenge in applying them to robotic tasks is the modality gap between visual\ndata and action data. We introduce differentiable robot rendering, a method\nallowing the visual appearance of a robot body to be directly differentiable\nwith respect to its control parameters. Our model integrates a kinematics-aware\ndeformable model and Gaussians Splatting and is compatible with any robot form\nfactors and degrees of freedom. We demonstrate its capability and usage in\napplications including reconstruction of robot poses from images and\ncontrolling robots through vision language models. Quantitative and qualitative\nresults show that our differentiable rendering model provides effective\ngradients for robotic control directly from pixels, setting the foundation for\nthe future applications of vision foundation models in robotics.\n","authors":["Ruoshi Liu","Alper Canberk","Shuran Song","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2410.13851v1.pdf","comment":"Project Page: https://drrobot.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2410.13760v1","updated":"2024-10-17T16:55:14Z","published":"2024-10-17T16:55:14Z","title":"Eyelid Fold Consistency in Facial Modeling","summary":"  Eyelid shape is integral to identity and likeness in human facial modeling.\nHuman eyelids are diverse in appearance with varied skin fold and epicanthal\nfold morphology between individuals. Existing parametric face models express\neyelid shape variation to an extent, but do not preserve sufficient likeness\nacross a diverse range of individuals. We propose a new definition of eyelid\nfold consistency and implement geometric processing techniques to model diverse\neyelid shapes in a unified topology. Using this method we reprocess data used\nto train a parametric face model and demonstrate significant improvements in\nface-related machine learning tasks.\n","authors":["Lohit Petikam","Charlie Hewitt","Fatemeh Saleh","Tadas Baltru≈°aitis"],"pdf_url":"https://arxiv.org/pdf/2410.13760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.02791v3","updated":"2024-10-17T16:11:28Z","published":"2021-07-06T17:58:35Z","title":"Depth-supervised NeRF: Fewer Views and Faster Training for Free","summary":"  A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting\nincorrect geometries when given an insufficient number of input views. One\npotential reason is that standard volumetric rendering does not enforce the\nconstraint that most of a scene's geometry consist of empty space and opaque\nsurfaces. We formalize the above assumption through DS-NeRF (Depth-supervised\nNeural Radiance Fields), a loss for learning radiance fields that takes\nadvantage of readily-available depth supervision. We leverage the fact that\ncurrent NeRF pipelines require images with known camera poses that are\ntypically estimated by running structure-from-motion (SFM). Crucially, SFM also\nproduces sparse 3D points that can be used as \"free\" depth supervision during\ntraining: we add a loss to encourage the distribution of a ray's terminating\ndepth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can\nrender better images given fewer training views while training 2-3x faster.\nFurther, we show that our loss is compatible with other recently proposed NeRF\nmethods, demonstrating that depth is a cheap and easily digestible supervisory\nsignal. And finally, we find that DS-NeRF can support other types of depth\nsupervision such as scanned depth sensors and RGB-D reconstruction outputs.\n","authors":["Kangle Deng","Andrew Liu","Jun-Yan Zhu","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2107.02791v3.pdf","comment":"Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:\n  https://github.com/dunbar12138/DSNeRF"},{"id":"http://arxiv.org/abs/2402.13251v3","updated":"2024-10-17T15:45:06Z","published":"2024-02-20T18:59:00Z","title":"FlashTex: Fast Relightable Mesh Texturing with LightControlNet","summary":"  Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\nalgorithm is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.\n","authors":["Kangle Deng","Timothy Omernick","Alexander Weiss","Deva Ramanan","Jun-Yan Zhu","Tinghui Zhou","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2402.13251v3.pdf","comment":"Project page: https://flashtex.github.io/"},{"id":"http://arxiv.org/abs/2410.13613v1","updated":"2024-10-17T14:47:08Z","published":"2024-10-17T14:47:08Z","title":"MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes","summary":"  4D Gaussian Splatting (4DGS) has recently emerged as a promising technique\nfor capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D\nGaussian representation and a GPU-friendly rasterizer, enabling rapid rendering\nspeeds. Despite its advantages, 4DGS faces significant challenges, notably the\nrequirement of millions of 4D Gaussians, each with extensive associated\nattributes, leading to substantial memory and storage cost. This paper\nintroduces a memory-efficient framework for 4DGS. We streamline the color\nattribute by decomposing it into a per-Gaussian direct color component with\nonly 3 parameters and a shared lightweight alternating current color predictor.\nThis approach eliminates the need for spherical harmonics coefficients, which\ntypically involve up to 144 parameters in classic 4DGS, thereby creating a\nmemory-efficient 4D Gaussian representation. Furthermore, we introduce an\nentropy-constrained Gaussian deformation technique that uses a deformation\nfield to expand the action range of each Gaussian and integrates an\nopacity-based entropy loss to limit the number of Gaussians, thus forcing our\nmodel to use as few Gaussians as possible to fit a dynamic scene well. With\nsimple half-precision storage and zip compression, our framework achieves a\nstorage reduction by approximately 190$\\times$ and 125$\\times$ on the\nTechnicolor and Neural 3D Video datasets, respectively, compared to the\noriginal 4DGS. Meanwhile, it maintains comparable rendering speeds and scene\nrepresentation quality, setting a new standard in the field.\n","authors":["Xinjie Zhang","Zhening Liu","Yifan Zhang","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Zehong Lin","Shuicheng Yan","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13566v1","updated":"2024-10-17T14:03:53Z","published":"2024-10-17T14:03:53Z","title":"360U-Former: HDR Illumination Estimation with Panoramic Adapted Vision\n  Transformers","summary":"  Recent illumination estimation methods have focused on enhancing the\nresolution and improving the quality and diversity of the generated textures.\nHowever, few have explored tailoring the neural network architecture to the\nEquirectangular Panorama (ERP) format utilised in image-based lighting.\nConsequently, high dynamic range images (HDRI) results usually exhibit a seam\nat the side borders and textures or objects that are warped at the poles. To\naddress this shortcoming we propose a novel architecture, 360U-Former, based on\na U-Net style Vision-Transformer which leverages the work of PanoSWIN, an\nadapted shifted window attention tailored to the ERP format. To the best of our\nknowledge, this is the first purely Vision-Transformer model used in the field\nof illumination estimation. We train 360U-Former as a GAN to generate HDRI from\na limited field of view low dynamic range image (LDRI). We evaluate our method\nusing current illumination estimation evaluation protocols and datasets,\ndemonstrating that our approach outperforms existing and state-of-the-art\nmethods without the artefacts typically associated with the use of the ERP\nformat.\n","authors":["Jack Hilliard","Adrian Hilton","Jean-Yves Guillemaut"],"pdf_url":"https://arxiv.org/pdf/2410.13566v1.pdf","comment":"Accepted at AIM Workshop 2024 at ECCV 2024, 18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.13530v1","updated":"2024-10-17T13:19:32Z","published":"2024-10-17T13:19:32Z","title":"L3DG: Latent 3D Gaussian Diffusion","summary":"  We propose L3DG, the first approach for generative 3D modeling of 3D\nGaussians through a latent 3D Gaussian diffusion formulation. This enables\neffective generative 3D modeling, scaling to generation of entire room-scale\nscenes which can be very efficiently rendered. To enable effective synthesis of\n3D Gaussians, we propose a latent diffusion formulation, operating in a\ncompressed latent space of 3D Gaussians. This compressed latent space is\nlearned by a vector-quantized variational autoencoder (VQ-VAE), for which we\nemploy a sparse convolutional architecture to efficiently operate on room-scale\nscenes. This way, the complexity of the costly generation process via diffusion\nis substantially reduced, allowing higher detail on object-level generation, as\nwell as scalability to large scenes. By leveraging the 3D Gaussian\nrepresentation, the generated scenes can be rendered from arbitrary viewpoints\nin real-time. We demonstrate that our approach significantly improves visual\nquality over prior work on unconditional object-level radiance field synthesis\nand showcase its applicability to room-scale scene generation.\n","authors":["Barbara Roessle","Norman M√ºller","Lorenzo Porzi","Samuel Rota Bul√≤","Peter Kontschieder","Angela Dai","Matthias Nie√üner"],"pdf_url":"https://arxiv.org/pdf/2410.13530v1.pdf","comment":"SIGGRAPH Asia 2024, project page:\n  https://barbararoessle.github.io/l3dg , video: https://youtu.be/UHEEiXCYeLU"},{"id":"http://arxiv.org/abs/2410.13503v1","updated":"2024-10-17T12:48:39Z","published":"2024-10-17T12:48:39Z","title":"NePHIM: A Neural Physics-Based Head-Hand Interaction Model","summary":"  Due to the increasing use of virtual avatars, the animation of head-hand\ninteractions has recently gained attention. To this end, we present a novel\nvolumetric and physics-based interaction simulation. In contrast to previous\nwork, our simulation incorporates temporal effects such as collision paths,\nrespects anatomical constraints, and can detect and simulate skin pulling. As a\nresult, we can achieve more natural-looking interaction animations and take a\nstep towards greater realism. However, like most complex and computationally\nexpensive simulations, ours is not real-time capable even on high-end machines.\nTherefore, we train small and efficient neural networks as accurate\napproximations that achieve about 200 FPS on consumer GPUs, about 50 FPS on\nCPUs, and are learned in less than four hours for one person. In general, our\nfocus is not to generalize the approximation networks to low-resolution head\nmodels but to adapt them to more detailed personalized avatars. Nevertheless,\nwe show that these networks can learn to approximate our head-hand interaction\nmodel for multiple identities while maintaining computational efficiency.\n  Since the quality of the simulations can only be judged subjectively, we\nconducted a comprehensive user study which confirms the improved realism of our\napproach. In addition, we provide extensive visual results and inspect the\nneural approximations quantitatively. All data used in this work has been\nrecorded with a multi--view camera rig and will be made available upon\npublication. We will also publish relevant implementations.\n","authors":["Nicolas Wagner","Mario Botsch","Ulrich Schwanecke"],"pdf_url":"https://arxiv.org/pdf/2410.13503v1.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.14078v1","updated":"2024-10-17T22:53:43Z","published":"2024-10-17T22:53:43Z","title":"Computational Social Choice: Parameterized Complexity and Challenges","summary":"  We survey two key problems-Multi-Winner Determination and Hedonic Games in\nComputational Social Choice, with a special focus on their parameterized\ncomplexity, and propose some research challenges in the field.\n","authors":["Jiehua Chena","Christian Hatschka","Sofia Simola"],"pdf_url":"https://arxiv.org/pdf/2410.14078v1.pdf","comment":"Submitted to Computer Science Review"},{"id":"http://arxiv.org/abs/2404.12474v2","updated":"2024-10-17T18:45:57Z","published":"2024-04-18T19:11:34Z","title":"Learning a Stable, Safe, Distributed Feedback Controller for a\n  Heterogeneous Platoon of Autonomous Vehicles","summary":"  Platooning of autonomous vehicles has the potential to increase safety and\nfuel efficiency on highways. The goal of platooning is to have each vehicle\ndrive at a specified speed (set by the leader) while maintaining a safe\ndistance from its neighbors. Many prior works have analyzed various controllers\nfor platooning, most commonly linear feedback and distributed model predictive\ncontrollers. In this work, we introduce an algorithm for learning a stable,\nsafe, distributed controller for a heterogeneous platoon. Our algorithm relies\non recent developments in learning neural network stability certificates. We\ntrain a controller for autonomous platooning in simulation and evaluate its\nperformance on hardware with a platoon of four F1Tenth vehicles. We then\nperform further analysis in simulation with a platoon of 100 vehicles.\nExperimental results demonstrate the practicality of the algorithm and the\nlearned controller by comparing the performance of the neural network\ncontroller to linear feedback and distributed model predictive controllers.\n","authors":["Michael H. Shaham","Taskin Padir"],"pdf_url":"https://arxiv.org/pdf/2404.12474v2.pdf","comment":"Accepted to the International Symposium of Robotics Research (ISRR)\n  2024"},{"id":"http://arxiv.org/abs/2410.13769v1","updated":"2024-10-17T17:06:41Z","published":"2024-10-17T17:06:41Z","title":"Transformer Guided Coevolution: Improved Team Formation in Multiagent\n  Adversarial Games","summary":"  We consider the problem of team formation within multiagent adversarial\ngames. We propose BERTeam, a novel algorithm that uses a transformer-based deep\nneural network with Masked Language Model training to select the best team of\nplayers from a trained population. We integrate this with coevolutionary deep\nreinforcement learning, which trains a diverse set of individual players to\nchoose teams from. We test our algorithm in the multiagent adversarial game\nMarine Capture-The-Flag, and we find that BERTeam learns non-trivial team\ncompositions that perform well against unseen opponents. For this game, we find\nthat BERTeam outperforms MCAA, an algorithm that similarly optimizes team\nformation.\n","authors":["Pranav Rajbhandari","Prithviraj Dasgupta","Donald Sofge"],"pdf_url":"https://arxiv.org/pdf/2410.13769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13768v1","updated":"2024-10-17T17:06:26Z","published":"2024-10-17T17:06:26Z","title":"Rapid and Automated Alloy Design with Graph Neural Network-Powered\n  LLM-Driven Multi-Agent Systems","summary":"  A multi-agent AI model is used to automate the discovery of new metallic\nalloys, integrating multimodal data and external knowledge including insights\nfrom physics via atomistic simulations. Our multi-agent system features three\nkey components: (a) a suite of LLMs responsible for tasks such as reasoning and\nplanning, (b) a group of AI agents with distinct roles and expertise that\ndynamically collaborate, and (c) a newly developed graph neural network (GNN)\nmodel for rapid retrieval of key physical properties. A set of LLM-driven AI\nagents collaborate to automate the exploration of the vast design space of\nMPEAs, guided by predictions from the GNN. We focus on the NbMoTa family of\nbody-centered cubic (bcc) alloys, modeled using an ML-based interatomic\npotential, and target two key properties: the Peierls barrier and solute/screw\ndislocation interaction energy. Our GNN model accurately predicts these\natomic-scale properties, providing a faster alternative to costly brute-force\ncalculations and reducing the computational burden on multi-agent systems for\nphysics retrieval. This AI system revolutionizes materials discovery by\nreducing reliance on human expertise and overcoming the limitations of direct\nall-atom simulations. By synergizing the predictive power of GNNs with the\ndynamic collaboration of LLM-based agents, the system autonomously navigates\nvast alloy design spaces, identifying trends in atomic-scale material\nproperties and predicting macro-scale mechanical strength, as demonstrated by\nseveral computational experiments. This approach accelerates the discovery of\nadvanced alloys and holds promise for broader applications in other complex\nsystems, marking a significant step forward in automated materials design.\n","authors":["Alireza Ghafarollahi","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2410.13768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13757v1","updated":"2024-10-17T16:53:50Z","published":"2024-10-17T16:53:50Z","title":"MobA: A Two-Level Agent System for Efficient Mobile Task Automation","summary":"  Current mobile assistants are limited by dependence on system APIs or\nstruggle with complex user instructions and diverse interfaces due to\nrestricted comprehension and decision-making abilities. To address these\nchallenges, we propose MobA, a novel Mobile phone Agent powered by multimodal\nlarge language models that enhances comprehension and planning capabilities\nthrough a sophisticated two-level agent architecture. The high-level Global\nAgent (GA) is responsible for understanding user commands, tracking history\nmemories, and planning tasks. The low-level Local Agent (LA) predicts detailed\nactions in the form of function calls, guided by sub-tasks and memory from the\nGA. Integrating a Reflection Module allows for efficient task completion and\nenables the system to handle previously unseen complex tasks. MobA demonstrates\nsignificant improvements in task execution efficiency and completion rate in\nreal-life evaluations, underscoring the potential of MLLM-empowered mobile\nassistants.\n","authors":["Zichen Zhu","Hao Tang","Yansi Li","Kunyao Lan","Yixuan Jiang","Hao Zhou","Yixiao Wang","Situo Zhang","Liangtai Sun","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13757v1.pdf","comment":"27 pages, 6 figures, and 5 tables. We will release our source code in\n  a few days"},{"id":"http://arxiv.org/abs/2404.16312v3","updated":"2024-10-17T15:49:34Z","published":"2024-04-25T03:38:07Z","title":"3D Guidance Law for Flexible Target Enclosing with Inherent Safety","summary":"  In this paper, we address the problem of enclosing an arbitrarily moving\ntarget in three dimensions by a single pursuer while ensuring the pursuer's\nsafety by preventing collisions with the target. The proposed guidance strategy\nsteers the pursuer to a safe region of space surrounding and excluding the\ntarget, allowing it to maintain a certain distance from the latter while\noffering greater flexibility in positioning and converging to any orbit within\nthis safe zone. We leverage the concept of the Lyapunov Barrier Function as a\npowerful tool to constrain the distance between the pursuer and the target\nwithin asymmetric bounds, thereby ensuring the pursuer's safety within the\npredefined region. Further, we demonstrate the effectiveness of the proposed\nguidance law in managing arbitrarily maneuvering targets and other\nuncertainties (such as vehicle/autopilot dynamics and external disturbances) by\nenabling the pursuer to consistently achieve stable global enclosing behaviors\nby switching between stable enclosing trajectories within the safe region\nwhenever necessary, even in response to aggressive target maneuvers. To attest\nto the merits of our work, we conduct experimental tests with various plant\nmodels, including a high-fidelity quadrotor model within Software-in-the-loop\n(SITL) simulations, encompassing various challenging target maneuver scenarios\nand requiring only relative information for successful execution.\n","authors":["Praveen Kumar Ranjan","Abhinav Sinha","Yongcan Cao"],"pdf_url":"https://arxiv.org/pdf/2404.16312v3.pdf","comment":"Supplementary video at https://youtu.be/UU704o_966s"},{"id":"http://arxiv.org/abs/2410.13580v1","updated":"2024-10-17T14:15:08Z","published":"2024-10-17T14:15:08Z","title":"EFX Exists for Three Types of Agents","summary":"  In this paper, we study the problem of finding an envy-free allocation of\nindivisible goods among multiple agents. EFX, which stands for envy-freeness up\nto any good, is a well-studied relaxation of the envy-free allocation problem\nand has been shown to exist for specific scenarios. For instance, EFX is known\nto exist when there are only three agents [Chaudhury et al, EC 2020], and for\nany number of agents when there are only two types of valuations [Mahara,\nDiscret. Appl. Math 2023].\n  We show that EFX allocations exist for any number of agents when there are at\nmost three types of additive valuations.\n","authors":["Vishwa Prakash H. V.","Pratik Ghosal","Prajakta Nimbhorkar","Nithin Varma"],"pdf_url":"https://arxiv.org/pdf/2410.13580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13454v1","updated":"2024-10-17T11:31:22Z","published":"2024-10-17T11:31:22Z","title":"Byzantine-Resilient Output Optimization of Multiagent via Self-Triggered\n  Hybrid Detection Approach","summary":"  How to achieve precise distributed optimization despite unknown attacks,\nespecially the Byzantine attacks, is one of the critical challenges for\nmultiagent systems. This paper addresses a distributed resilient optimization\nfor linear heterogeneous multi-agent systems faced with adversarial threats. We\nestablish a framework aimed at realizing resilient optimization for\ncontinuous-time systems by incorporating a novel self-triggered hybrid\ndetection approach. The proposed hybrid detection approach is able to identify\nattacks on neighbors using both error thresholds and triggering intervals,\nthereby optimizing the balance between effective attack detection and the\nreduction of excessive communication triggers. Through using an edge-based\nadaptive self-triggered approach, each agent can receive its neighbors'\ninformation and determine whether these information is valid. If any neighbor\nprove invalid, each normal agent will isolate that neighbor by disconnecting\ncommunication along that specific edge. Importantly, our adaptive algorithm\nguarantees the accuracy of the optimization solution even when an agent is\nisolated by its neighbors.\n","authors":["Chenhang Yan","Liping Yan","Yuezu Lv","Bolei Dong","Yuanqing Xia"],"pdf_url":"https://arxiv.org/pdf/2410.13454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14928v2","updated":"2024-10-17T10:30:41Z","published":"2024-06-21T07:37:19Z","title":"Autonomous Agents for Collaborative Task under Information Asymmetry","summary":"  Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great\nprogress in solving complex tasks. It performs communication among agents\nwithin the system to collaboratively solve tasks, under the premise of shared\ninformation. However, when agents' collaborations are leveraged to perform\nmulti-person tasks, a new challenge arises due to information asymmetry, since\neach agent can only access the information of its human user. Previous MAS\nstruggle to complete tasks under this condition. To address this, we propose a\nnew MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems.\nIn iAgents, the human social network is mirrored in the agent network, where\nagents proactively exchange human information necessary for task resolution,\nthereby overcoming information asymmetry. iAgents employs a novel agent\nreasoning mechanism, InfoNav, to navigate agents' communication toward\neffective information exchange. Together with InfoNav, iAgents organizes human\ninformation in a mixed memory to provide agents with accurate and comprehensive\ninformation for exchange. Additionally, we introduce InformativeBench, the\nfirst benchmark tailored for evaluating LLM agents' task-solving ability under\ninformation asymmetry. Experimental results show that iAgents can collaborate\nwithin a social network of 140 individuals and 588 relationships, autonomously\ncommunicate over 30 turns, and retrieve information from nearly 70,000 messages\nto complete tasks within 3 minutes.\n","authors":["Wei Liu","Chenxi Wang","Yifei Wang","Zihao Xie","Rennai Qiu","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2406.14928v2.pdf","comment":"32 pages, 12 figures, 6 tables, accepted by NeurIPS 2024, see detail\n  at https://thinkwee.top/iagents"},{"id":"http://arxiv.org/abs/2410.12475v2","updated":"2024-10-17T09:58:09Z","published":"2024-10-16T11:43:17Z","title":"Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional\n  Safety Engineering","summary":"  Functional safety is a critical aspect of automotive engineering,\nencompassing all phases of a vehicle's lifecycle, including design,\ndevelopment, production, operation, and decommissioning. This domain involves\nhighly knowledge-intensive tasks. This paper introduces Aegis: An Advanced\nLLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is\nspecifically designed to support complex functional safety tasks within the\nautomotive sector. It is tailored to perform Hazard Analysis and Risk\nAssessment(HARA), document Functional Safety Requirements(FSR), and plan test\ncases for Automatic Emergency Braking(AEB) systems. The most advanced version,\nAegis-Max, leverages Retrieval-Augmented Generation(RAG) and reflective\nmechanisms to enhance its capability in managing complex, knowledge-intensive\ntasks. Additionally, targeted prompt refinement by professional functional\nsafety practitioners can significantly optimize Aegis's performance in the\nfunctional safety domain. This paper demonstrates the potential of Aegis to\nimprove the efficiency and effectiveness of functional safety processes in\nautomotive engineering.\n","authors":["Lu Shi","Bin Qi","Jiarui Luo","Yang Zhang","Zhanzhao Liang","Zhaowei Gao","Wenke Deng","Lin Sun"],"pdf_url":"https://arxiv.org/pdf/2410.12475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12464v2","updated":"2024-10-17T09:01:11Z","published":"2024-10-16T11:25:13Z","title":"Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning","summary":"  While many studies prove more advanced LLMs perform better on tasks such as\nmath and coding, we notice that in cryptocurrency trading, stronger LLMs work\nworse than weaker LLMs often. To study how this counter-intuitive phenomenon\noccurs, we examine the LLM reasoning processes on making trading decisions. We\nfind that separating the reasoning process into factual and subjective\ncomponents can lead to higher profits. Building on this insight, we introduce a\nmulti-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and\nlearn from both factual and subjective reasoning. Extensive experiments\ndemonstrate that this framework enhances LLM trading performance in\ncryptocurrency markets. Additionally, an ablation study reveals that relying on\nsubjective news tends to generate higher returns in bull markets, whereas\nfocusing on factual information yields better results in bear markets. Our code\nand data are available at\n\\url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}.\n","authors":["Qian Wang","Yuchen Gao","Zhenheng Tang","Bingqiao Luo","Bingsheng He"],"pdf_url":"https://arxiv.org/pdf/2410.12464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13139v1","updated":"2024-10-17T01:51:58Z","published":"2024-10-17T01:51:58Z","title":"See Behind Walls in Real-time Using Aerial Drones and Augmented Reality","summary":"  This work presents ARD2, a framework that enables real-time through-wall\nsurveillance using two aerial drones and an augmented reality (AR) device. ARD2\nconsists of two main steps: target direction estimation and contour\nreconstruction. In the first stage, ARD2 leverages geometric relationships\nbetween the drones, the user, and the target to project the target's direction\nonto the user's AR display. In the second stage, images from the drones are\nsynthesized to reconstruct the target's contour, allowing the user to visualize\nthe target behind walls. Experimental results demonstrate the system's accuracy\nin both direction estimation and contour reconstruction.\n","authors":["Sikai Yang","Kang Yang","Yuning Chen","Fan Zhao","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2410.13139v1.pdf","comment":"6 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.13733v1","updated":"2024-10-17T16:36:38Z","published":"2024-10-17T16:36:38Z","title":"Improving Multi-modal Large Language Model through Boosting Vision\n  Capabilities","summary":"  We focus on improving the visual understanding capability for boosting the\nvision-language models. We propose \\textbf{Arcana}, a multiModal language\nmodel, which introduces two crucial techniques. First, we present Multimodal\nLoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional\nlanguage-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for\nvision and one for language -- each with its own parameters. This disentangled\nparameters design allows for more specialized learning in each modality and\nbetter integration of multimodal information. Second, we introduce the Query\nLadder adapter (QLadder) to improve the visual encoder. QLadder employs a\nlearnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate\nrepresentations from the frozen pretrained visual encoder (e.g., CLIP image\nencoder). This enables the model to learn new and informative visual features,\nas well as remaining the powerful capabilities of the pretrained visual\nencoder. These techniques collectively enhance Arcana's visual perception\npower, enabling it to leverage improved visual information for more accurate\nand contextually relevant outputs across various multimodal scenarios.\nExtensive experiments and ablation studies demonstrate the effectiveness and\ngeneralization capability of our Arcana. The code and re-annotated data are\navailable at \\url{https://arcana-project-page.github.io}.\n","authors":["Yanpeng Sun","Huaxin Zhang","Qiang Chen","Xinyu Zhang","Nong Sang","Gang Zhang","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2410.13733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.13647v1","updated":"2024-10-17T15:13:26Z","published":"2024-10-17T15:13:26Z","title":"Multimodal growth and development assessment model","summary":"  With the development of social economy and the improvement of people's\nattention to health, the growth and development of children and adolescents has\nbecome an important indicator to measure the level of national health.\nTherefore, accurate and timely assessment of children's growth and development\nhas become increasingly important. At the same time, global health\ninequalities, especially child malnutrition and stunting in developing\ncountries, urgently require effective assessment tools to monitor and\nintervene. In recent years, the rapid development of technologies such as big\ndata, artificial intelligence, and cloud computing, and the cross-integration\nof multiple disciplines such as biomedicine, statistics, and computer science\nhave promoted the rapid development of large-scale models for growth and\ndevelopment assessment. However, there are still problems such as too single\nevaluation factors, inaccurate diagnostic results, and inability to give\naccurate and reasonable recommendations. The multi-modal growth and development\nassessment model uses the public data set of RSNA ( North American College of\nRadiology ) as the training set, and the data set of the Department of\nPediatrics of Huaibei People's Hospital as the open source test set. The\nembedded ICL module enables the model to quickly adapt and identify the tasks\nthat need to be done to ensure that under the premise of considering multiple\nevaluation factors, accurate diagnosis results and reasonable medical\nrecommendations are given, so as to provide solutions to the above problems and\npromote the development of the medical field.\n","authors":["Ying Li","Zichen Song","Zijie Gong","Sitan Huang","Jiewei Ge"],"pdf_url":"https://arxiv.org/pdf/2410.13647v1.pdf","comment":"7 Pages 7 Figures"},{"id":"http://arxiv.org/abs/2410.13419v1","updated":"2024-10-17T10:41:52Z","published":"2024-10-17T10:41:52Z","title":"MeloTrans: A Text to Symbolic Music Generation Model Following Human\n  Composition Habit","summary":"  At present, neural network models show powerful sequence prediction ability\nand are used in many automatic composition models. In comparison, the way\nhumans compose music is very different from it. Composers usually start by\ncreating musical motifs and then develop them into music through a series of\nrules. This process ensures that the music has a specific structure and\nchanging pattern. However, it is difficult for neural network models to learn\nthese composition rules from training data, which results in a lack of\nmusicality and diversity in the generated music. This paper posits that\nintegrating the learning capabilities of neural networks with human-derived\nknowledge may lead to better results. To archive this, we develop the\nPOP909$\\_$M dataset, the first to include labels for musical motifs and their\nvariants, providing a basis for mimicking human compositional habits. Building\non this, we propose MeloTrans, a text-to-music composition model that employs\nprinciples of motif development rules. Our experiments demonstrate that\nMeloTrans excels beyond existing music generation models and even surpasses\nLarge Language Models (LLMs) like ChatGPT-4. This highlights the importance of\nmerging human insights with neural network capabilities to achieve superior\nsymbolic music generation.\n","authors":["Yutian Wang","Wanyin Yang","Zhenrong Dai","Yilong Zhang","Kun Zhao","Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07728v4","updated":"2024-10-17T09:54:06Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature\n  Disentanglement and Enhancement","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.\n","authors":["Zihao Wang","Le Ma","Yongsheng Feng","Xin Pan","Yuhang Jin","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v4.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v2","updated":"2024-10-17T08:18:14Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19809v1","updated":"2024-10-17T07:59:54Z","published":"2024-10-17T07:59:54Z","title":"ScreenWriter: Automatic Screenplay Generation and Movie Summarisation","summary":"  The proliferation of creative video content has driven demand for textual\ndescriptions or summaries that allow users to recall key plot points or get an\noverview without watching. The volume of movie content and speed of turnover\nmotivates automatic summarisation, which is nevertheless challenging, requiring\nidentifying character intentions and very long-range temporal dependencies. The\nfew existing methods attempting this task rely heavily on textual screenplays\nas input, greatly limiting their applicability. In this work, we propose the\ntask of automatic screenplay generation, and a method, ScreenWriter, that\noperates only on video and produces output which includes dialogue, speaker\nnames, scene breaks, and visual descriptions. ScreenWriter introduces a novel\nalgorithm to segment the video into scenes based on the sequence of visual\nvectors, and a novel method for the challenging problem of determining\ncharacter names, based on a database of actors' faces. We further demonstrate\nhow these automatic screenplays can be used to generate plot synopses with a\nhierarchical summarisation method based on scene breaks. We test the quality of\nthe final summaries on the recent MovieSum dataset, which we augment with\nvideos, and show that they are superior to a number of comparison models which\nassume access to goldstandard screenplays.\n","authors":["Louis Mahon","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.19809v1.pdf","comment":null}]},"2024-10-16T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.12649v1","updated":"2024-10-16T15:13:27Z","published":"2024-10-16T15:13:27Z","title":"Faster Algorithms for Growing Collision-Free Convex Polytopes in Robot\n  Configuration Space","summary":"  We propose two novel algorithms for constructing convex collision-free\npolytopes in robot configuration space. Finding these polytopes enables the\napplication of stronger motion-planning frameworks such as trajectory\noptimization with Graphs of Convex Sets [1] and is currently a major roadblock\nin the adoption of these approaches. In this paper, we build upon IRIS-NP\n(Iterative Regional Inflation by Semidefinite & Nonlinear Programming) [2] to\nsignificantly improve tunability, runtimes, and scaling to complex\nenvironments. IRIS-NP uses nonlinear programming paired with uniform random\ninitialization to find configurations on the boundary of the free configuration\nspace. Our key insight is that finding near-by configuration-space obstacles\nusing sampling is inexpensive and greatly accelerates region generation. We\npropose two algorithms using such samples to either employ nonlinear\nprogramming more efficiently (IRIS-NP2 ) or circumvent it altogether using a\nmassively-parallel zero-order optimization strategy (IRIS-ZO). We also propose\na termination condition that controls the probability of exceeding a\nuser-specified permissible fraction-in-collision, eliminating a significant\nsource of tuning difficulty in IRIS-NP. We compare performance across eight\nrobot environments, showing that IRIS-ZO achieves an order-of-magnitude speed\nadvantage over IRIS-NP. IRISNP2, also significantly faster than IRIS-NP, builds\nlarger polytopes using fewer hyperplanes, enabling faster downstream\ncomputation. Website: https://sites.google.com/view/fastiris\n","authors":["Peter Werner","Thomas Cohn","Rebecca H. Jiang","Tim Seyde","Max Simchowitz","Russ Tedrake","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.12649v1.pdf","comment":"16 pages, 6 figures, accepted for publication in the proceedings of\n  the International Symposium for Robotics Research 2024"},{"id":"http://arxiv.org/abs/2405.17257v2","updated":"2024-10-16T10:08:50Z","published":"2024-05-27T15:14:47Z","title":"Topological reconstruction of sampled surfaces via Morse theory","summary":"  In this work, we study the perception problem for sampled surfaces (possibly\nwith boundary) using tools from computational topology, specifically, how to\nidentify their underlying topology starting from point-cloud samples in space,\nsuch as those obtained with 3D scanners. We present a reconstruction algorithm\nbased on a careful topological study of the point sample that allows us to\nobtain a cellular decomposition of it using a Morse function. No triangulation\nor local implicit equations are used as intermediate steps, avoiding in this\nway reconstruction-induced artifices. The algorithm can be run without any\nprior knowledge of the surface topology, density or regularity of the\npoint-sample. The results consist of a piece-wise decomposition of the given\nsurface as a union of Morse cells (i.e. topological disks), suitable for tasks\nsuch as mesh-independent reparametrization or noise-filtering, and a small-rank\ncellular complex determining the topology of the surface. The algorithm, which\nwe test with several real and synthetic surfaces, can be applied to smooth\nsurfaces with or without boundary, embedded in an ambient space of any\ndimension.\n","authors":["Franco Coltraro","Jaume Amor√≥s","Maria Alberich-Carrami√±ana","Carme Torras"],"pdf_url":"https://arxiv.org/pdf/2405.17257v2.pdf","comment":"39 pages, 17 figures, 1 table, 1 algorithm, 1 appendix"},{"id":"http://arxiv.org/abs/2410.12331v1","updated":"2024-10-16T07:52:32Z","published":"2024-10-16T07:52:32Z","title":"Ellipsoidal Density-Equalizing Map for Genus-0 Closed Surfaces","summary":"  Surface parameterization is a fundamental task in geometry processing and\nplays an important role in many science and engineering applications. In recent\nyears, the density-equalizing map, a shape deformation technique based on the\nphysical principle of density diffusion, has been utilized for the\nparameterization of simply connected and multiply connected open surfaces. More\nrecently, a spherical density-equalizing mapping method has been developed for\nthe parameterization of genus-0 closed surfaces. However, for genus-0 closed\nsurfaces with extreme geometry, using a spherical domain for the\nparameterization may induce large geometric distortion. In this work, we\ndevelop a novel method for computing density-equalizing maps of genus-0 closed\nsurfaces onto an ellipsoidal domain. This allows us to achieve ellipsoidal\narea-preserving parameterizations and ellipsoidal parameterizations with\ncontrolled area change. We further propose an energy minimization approach that\ncombines density-equalizing maps and quasi-conformal maps, which allows us to\nproduce ellipsoidal density-equalizing quasi-conformal maps for achieving a\nbalance between density-equalization and quasi-conformality. Using our proposed\nmethods, we can significantly improve the performance of surface remeshing for\ngenus-0 closed surfaces. Experimental results on a large variety of genus-0\nclosed surfaces are presented to demonstrate the effectiveness of our proposed\nmethods.\n","authors":["Zhiyuan Lyu","Lok Ming Lui","Gary P. T. Choi"],"pdf_url":"https://arxiv.org/pdf/2410.12331v1.pdf","comment":null}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.13046v1","updated":"2024-10-16T21:16:45Z","published":"2024-10-16T21:16:45Z","title":"Truthful High Dimensional Sparse Linear Regression","summary":"  We study the problem of fitting the high dimensional sparse linear regression\nmodel with sub-Gaussian covariates and responses, where the data are provided\nby strategic or self-interested agents (individuals) who prioritize their\nprivacy of data disclosure. In contrast to the classical setting, our focus is\non designing mechanisms that can effectively incentivize most agents to\ntruthfully report their data while preserving the privacy of individual\nreports. Simultaneously, we seek an estimator which should be close to the\nunderlying parameter. We attempt to solve the problem by deriving a novel\nprivate estimator that has a closed-form expression. Based on the estimator, we\npropose a mechanism which has the following properties via some appropriate\ndesign of the computation and payment scheme: (1) the mechanism is $(o(1),\nO(n^{-\\Omega({1})}))$-jointly differentially private, where $n$ is the number\nof agents; (2) it is an $o(\\frac{1}{n})$-approximate Bayes Nash equilibrium for\na $(1-o(1))$-fraction of agents to truthfully report their data; (3) the output\ncould achieve an error of $o(1)$ to the underlying parameter; (4) it is\nindividually rational for a $(1-o(1))$ fraction of agents in the mechanism; (5)\nthe payment budget required from the analyst to run the mechanism is $o(1)$. To\nthe best of our knowledge, this is the first study on designing truthful (and\nprivacy-preserving) mechanisms for high dimensional sparse linear regression.\n","authors":["Liyang Zhu","Amina Manseur","Meng Ding","Jinyan Liu","Jinhui Xu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12966v1","updated":"2024-10-16T18:58:28Z","published":"2024-10-16T18:58:28Z","title":"EF1 for Mixed Manna with Unequal Entitlements","summary":"  We study fair division of indivisible mixed manna when agents have unequal\nentitlements, with weighted envy-freeness up to one item (WEF1) as our primary\nnotion of fairness. We identify several shortcomings of existing techniques to\nachieve WEF1. Hence, we relax WEF1 to weighted envy-freeness up to 1 transfer\n(WEF1T), and give a polynomial-time algorithm for achieving it. We also\ngeneralize Fisher markets to the mixed manna setting, and use them to get a\npolynomial-time algorithm for two agents that outputs a WEF1 allocation.\n","authors":["Jugal Garg","Eklavya Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.12966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09475v2","updated":"2024-10-16T18:13:43Z","published":"2024-09-14T16:20:26Z","title":"MALADY: Multiclass Active Learning with Auction Dynamics on Graphs","summary":"  Active learning enhances the performance of machine learning methods,\nparticularly in semi-supervised cases, by judiciously selecting a limited\nnumber of unlabeled data points for labeling, with the goal of improving the\nperformance of an underlying classifier. In this work, we introduce the\nMulticlass Active Learning with Auction Dynamics on Graphs (MALADY) framework\nwhich leverages the auction dynamics algorithm on similarity graphs for\nefficient active learning. In particular, we generalize the auction dynamics\nalgorithm on similarity graphs for semi-supervised learning in [24] to\nincorporate a more general optimization functional. Moreover, we introduce a\nnovel active learning acquisition function that uses the dual variable of the\nauction algorithm to measure the uncertainty in the classifier to prioritize\nqueries near the decision boundaries between different classes. Lastly, using\nexperiments on classification tasks, we evaluate the performance of our\nproposed method and show that it exceeds that of comparison algorithms.\n","authors":["Gokul Bhusal","Kevin Miller","Ekaterina Merkurjev"],"pdf_url":"https://arxiv.org/pdf/2409.09475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12756v1","updated":"2024-10-16T17:24:31Z","published":"2024-10-16T17:24:31Z","title":"Prophet Upper Bounds for Online Matching and Auctions","summary":"  In the online 2-bounded auction problem, we have a collection of items\nrepresented as nodes in a graph and bundles of size two represented by edges.\nAgents are presented sequentially, each with a random weight function over the\nbundles. The goal of the decision-maker is to find an allocation of bundles to\nagents of maximum weight so that every item is assigned at most once, i.e., the\nsolution is a matching in the graph. When the agents are single-minded (i.e.,\nput all the weight in a single bundle), we recover the maximum weight prophet\nmatching problem under edge arrivals (a.k.a. prophet matching).\n  In this work, we provide new and improved upper bounds on the competitiveness\nachievable by an algorithm for the general online 2-bounded auction and the\n(single-minded) prophet matching problems. For adversarial arrival order of the\nagents, we show that no algorithm for the online 2-bounded auction problem\nachieves a competitiveness larger than $4/11$, while no algorithm for prophet\nmatching achieves a competitiveness larger than $\\approx 0.4189$. Using a\ncontinuous-time analysis, we also improve the known bounds for online 2-bounded\nauctions for random order arrivals to $\\approx 0.5968$ in the general case, a\nbound of $\\approx 0.6867$ in the IID model, and $\\approx 0.6714$ in\nprophet-secretary model.\n","authors":["Jos√© Soto","Victor Verdugo"],"pdf_url":"https://arxiv.org/pdf/2410.12756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12723v1","updated":"2024-10-16T16:31:58Z","published":"2024-10-16T16:31:58Z","title":"Federated Learning and Free-riding in a Competitive Market","summary":"  Federated learning (FL) is a collaborative technique for training large-scale\nmodels while protecting user data privacy. Despite its substantial benefits,\nthe free-riding behavior raises a major challenge for the formation of FL,\nespecially in competitive markets. Our paper explores this under-explored issue\non how the free-riding behavior in a competitive market affects firms'\nincentives to form FL. Competing firms can improve technologies through forming\nFL to increase the performance of their products, which in turn, affects\nconsumers' product selection and market size. The key complication is whether\nthe free-riding behavior discourages information contribution by participating\nfirms and results in the decomposition of FL, and even free riding does not\ndiscourage information contribution, this does not necessarily mean that a firm\nwants to form FL in a competitive market because free riding may reshape the\ncompetition positions of each participating firm and thus forming FL may not be\nprofitable. We build a parsimonious game theoretical model that captures these\ninteractions and our analyses show several new findings. First, even in the\npresence of the free-riding behavior, competing firms under FL find it optimal\nto contribute all its available information. Second, the firm with less amount\nof information always finds it profitable to free ride; whether its rival (with\nmore amount of information) have an incentive to form FL only when the level of\ncompetition or when the gap in information volume is not high. Third, when FL\nis formed, there exists an \"All-Win\" situation in which all stakeholders\n(participating firms, consumers, and social planner) benefit. Last, subsidizing\nby the free-riding firm can align its rival's incentive to form FL only when\nthe level of competition is intermediate.\n","authors":["Jiajun Meng","Jing Chen","Dongfang Zhao","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2410.12723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12633v1","updated":"2024-10-16T14:53:42Z","published":"2024-10-16T14:53:42Z","title":"Decline Now: A Combinatorial Model for Algorithmic Collective Action","summary":"  Drivers on food delivery platforms often run a loss on low-paying orders. In\nresponse, workers on DoorDash started a campaign, #DeclineNow, to purposefully\ndecline orders below a certain pay threshold. For each declined order, the\nplatform returns the request to other available drivers with slightly increased\npay. While contributing to overall pay increase the implementation of the\nstrategy comes with the risk of missing out on orders for each individual\ndriver. In this work, we propose a first combinatorial model to study the\nstrategic interaction between workers and the platform. Within our model, we\nformalize key quantities such as the average worker benefit of the strategy,\nthe benefit of freeriding, as well as the benefit of participation. We extend\nour theoretical results with simulations. Our key insights show that the\naverage worker gain of the strategy is always positive, while the benefit of\nparticipation is positive only for small degrees of labor oversupply. Beyond\nthis point, the utility of participants decreases faster with increasing degree\nof oversupply, compared to the utility of non-participants. Our work highlights\nthe significance of labor supply levels for the effectiveness of collective\naction on gig platforms. We suggest organizing in shifts as a means to reduce\noversupply and empower collectives.\n","authors":["Dorothee Sigg","Moritz Hardt","Celestine Mendler-D√ºnner"],"pdf_url":"https://arxiv.org/pdf/2410.12633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12544v1","updated":"2024-10-16T13:21:58Z","published":"2024-10-16T13:21:58Z","title":"Nash equilibria in scalar discrete-time linear quadratic games","summary":"  An open problem in linear quadratic (LQ) games has been characterizing the\nNash equilibria. This problem has renewed relevance given the surge of work on\nunderstanding the convergence of learning algorithms in dynamic games. This\npaper investigates scalar discrete-time infinite-horizon LQ games with two\nagents. Even in this arguably simple setting, there are no results for finding\n$\\textit{all}$ Nash equilibria. By analyzing the best response map, we\nformulate a polynomial system of equations characterizing the linear feedback\nNash equilibria. This enables us to bring in tools from algebraic geometry,\nparticularly the Gr\\\"obner basis, to study the roots of this polynomial system.\nConsequently, we can not only compute all Nash equilibria numerically, but we\ncan also characterize their number with explicit conditions. For instance, we\nprove that the LQ games under consideration admit at most three Nash\nequilibria. We further provide sufficient conditions for the existence of at\nmost two Nash equilibria and sufficient conditions for the uniqueness of the\nNash equilibrium. Our numerical experiments demonstrate the tightness of our\nbounds and showcase the increased complexity in settings with more than two\nagents.\n","authors":["Giulio Salizzoni","Reda Ouhamma","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2410.12544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12347v1","updated":"2024-10-16T08:07:31Z","published":"2024-10-16T08:07:31Z","title":"Guaranteeing MMS for All but One Agent When Allocating Indivisible\n  Chores","summary":"  We study the problem of allocating $m$ indivisible chores to $n$ agents with\nadditive cost functions under the fairness notion of maximin share (MMS). In\nthis work, we propose a notion called $\\alpha$-approximate all-but-one maximin\nshare ($\\alpha$-AMMS) which is a stronger version of $\\alpha$-approximate MMS.\nAn allocation is called $\\alpha$-AMMS if $n-1$ agents are guaranteed their MMS\nvalues and the remaining agent is guaranteed $\\alpha$-approximation of her MMS\nvalue. We show that there exist $\\alpha$-AMMS allocations, with $\\alpha = 9/8$\nfor three agents; $\\alpha = 4/3$ for four agents; and $\\alpha = (n+1)^2/4n$ for\n$n\\geq 5$ agents.\n","authors":["Jiawei Qiu","Xiaowei Wu","Cong Zhang","Shengwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.12347v1.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12306v1","updated":"2024-10-16T07:18:25Z","published":"2024-10-16T07:18:25Z","title":"Time-Varyingness in Auction Breaks Revenue Equivalence","summary":"  Auction is one of the most representative buying-selling systems. A\ncelebrated study shows that the seller's expected revenue is equal in\nequilibrium, regardless of the type of auction, typically first-price and\nsecond-price auctions. Here, however, we hypothesize that when some auction\nenvironments vary with time, this revenue equivalence may not be maintained. In\nsecond-price auctions, the equilibrium strategy is robustly feasible.\nConversely, in first-price auctions, the buyers must continue to adapt their\nstrategies according to the environment of the auction. Surprisingly, we prove\nthat revenue equivalence can be broken in both directions. First-price auctions\nbring larger or smaller revenue than second-price auctions, case by case,\ndepending on how the value of an item varies. Our experiments also demonstrate\nrevenue inequivalence in various scenarios, where the value varies periodically\nor randomly. This study uncovers a phenomenon, the breaking of revenue\nequivalence by the time-varyingness in auctions, that likely occurs in\nreal-world auctions, revealing its underlying mechanism.\n","authors":["Yuma Fujimoto","Kaito Ariu","Kenshi Abe"],"pdf_url":"https://arxiv.org/pdf/2410.12306v1.pdf","comment":"11 pages, 3 figures (main); 7 pages, 1 figure (appendix)"},{"id":"http://arxiv.org/abs/2410.12264v1","updated":"2024-10-16T06:02:18Z","published":"2024-10-16T06:02:18Z","title":"Game Theory Meets Statistical Mechanics in Deep Learning Design","summary":"  We present a novel deep graphical representation that seamlessly merges\nprinciples of game theory with laws of statistical mechanics. It performs\nfeature extraction, dimensionality reduction, and pattern classification within\na single learning framework. Our approach draws an analogy between neurons in a\nnetwork and players in a game theory model. Furthermore, each neuron viewed as\na classical particle (subject to statistical physics' laws) is mapped to a set\nof actions representing specific activation value, and neural network layers\nare conceptualized as games in a sequential cooperative game theory setting.\nThe feed-forward process in deep learning is interpreted as a sequential game,\nwhere each game comprises a set of players. During training, neurons are\niteratively evaluated and filtered based on their contributions to a payoff\nfunction, which is quantified using the Shapley value driven by an energy\nfunction. Each set of neurons that significantly contributes to the payoff\nfunction forms a strong coalition. These neurons are the only ones permitted to\npropagate the information forward to the next layers. We applied this\nmethodology to the task of facial age estimation and gender classification.\nExperimental results demonstrate that our approach outperforms both multi-layer\nperceptron and convolutional neural network models in terms of efficiency and\naccuracy.\n","authors":["Djamel Bouchaffra","Fay√ßal Ykhlef","Bilal Faye","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2410.12264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12256v1","updated":"2024-10-16T05:49:55Z","published":"2024-10-16T05:49:55Z","title":"Voter Participation Control in Online Polls","summary":"  News outlets, surveyors, and other organizations often conduct polls on\nsocial networks to gain insights into public opinion. Such a poll is typically\nstarted by someone on a social network who sends it to her friends. If a person\nparticipates in the poll, the poll information gets published on her wall,\nwhich in turn enables her friends to participate, and the process continues.\nEventually, a subset of the population participates in the poll, and the\npollster learns the outcome of that poll. We initiate the study of a new but\nnatural type of election control in such online elections.\n  We study how difficult/easy it is to sway the outcome of such polls in one's\nfavor/against (aka constructive vs destructive) by any malicious influencer who\nnudges/bribes people for seemingly harmless actions like non-participation.\nThese questions are important from the standpoint of studying the power of\nresistance of online voting against malicious behavior. The destructive version\nis also important to quantify the robustness of the winner of an online voting.\nWe show that both problems are computationally intractable even if the election\nis over only two candidates and the influencer has an infinite amount of money\nto spend (that is, every voter can be persuaded to not participate). We\nstrengthen this result by proving that the computational task remains\nsubstantially challenging even if the underlying network is a tree. Finally, we\nshow that there is a polynomial-time algorithm for the constructive version of\nthe problem when we have O(1) candidates, and the treewidth of the underlying\ngraph is O(1); the algorithm for the destructive version does not even need to\nassume O(1) number of candidates. Hence, we observe that the destructive\nversion is computationally easier than the constructive version.\n","authors":["Koustav De","Palash Dey","Swagato Sanyal"],"pdf_url":"https://arxiv.org/pdf/2410.12256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02068v3","updated":"2024-10-16T05:26:37Z","published":"2023-04-04T18:37:32Z","title":"Battlefield Transfers in Coalitional Blotto Games","summary":"  In competitive resource allocation environments, agents often choose to form\nalliances; however, for some agents, doing so may not always be beneficial. Is\nthere a method of forming alliances that always reward each of their members?\nWe study this question using the framework of the coalitional Blotto game, in\nwhich two players compete against a common adversary by allocating their\nbudgeted resources across disjoint sets of valued battlefields. On any given\nbattlefield, the agent that allocates a greater amount of resources wins the\ncorresponding battlefield value. Existing work has shown the surprising result\nthat in certain game instances, if one player donates a portion of their budget\nto the other player, then both players win larger amounts in their separate\ncompetitions against the adversary. However, this transfer-based method of\nalliance formation is not always mutually beneficial, which motivates the\nsearch for alternate strategies. In this vein, we study a new method of\nalliance formation referred to as a joint transfer, whereby players publicly\ntransfer battlefields and budgets between one another before they engage in\ntheir separate competitions against the adversary. We show that in almost all\ngame instances, there exists a mutually beneficial joint transfer that strictly\nincreases the payoff of each player.\n","authors":["Vade Shah","Jason R. Marden"],"pdf_url":"https://arxiv.org/pdf/2304.02068v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04669v3","updated":"2024-10-16T05:23:31Z","published":"2024-09-07T01:17:59Z","title":"Learning Optimal Stable Matches in Decentralized Markets with Unknown\n  Preferences","summary":"  Matching algorithms have demonstrated great success in several practical\napplications, but they often require centralized coordination and plentiful\ninformation. In many modern online marketplaces, agents must independently seek\nout and match with another using little to no information. For these kinds of\nsettings, can we design decentralized, limited-information matching algorithms\nthat preserve the desirable properties of standard centralized techniques? In\nthis work, we constructively answer this question in the affirmative. We model\na two-sided matching market as a game consisting of two disjoint sets of\nagents, referred to as proposers and acceptors, each of whom seeks to match\nwith their most preferable partner on the opposite side of the market. However,\neach proposer has no knowledge of their own preferences, so they must learn\ntheir preferences while forming matches in the market. We present a simple\nonline learning rule that guarantees a strong notion of probabilistic\nconvergence to the welfare-maximizing equilibrium of the game, referred to as\nthe proposer-optimal stable match. To the best of our knowledge, this\nrepresents the first completely decoupled, communication-free algorithm that\nguarantees probabilistic convergence to an optimal stable match, irrespective\nof the structure of the matching market.\n","authors":["Vade Shah","Bryce L. Ferguson","Jason R. Marden"],"pdf_url":"https://arxiv.org/pdf/2409.04669v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01549v3","updated":"2024-10-16T01:53:43Z","published":"2023-12-04T00:42:04Z","title":"Incentive Non-Compatibility of Optimistic Rollups","summary":"  Optimistic rollups are a popular and promising method of increasing the\nthroughput capacity of their underlying chain. These methods rely on economic\nincentives to guarantee their security. We present a model of optimistic\nrollups that shows that the incentives are not aligned with the expected\nbehavior of the players, thus potentially undermining the security of existing\noptimistic rollups. We discuss some potential solutions illuminated by our\nmodel.\n","authors":["Daji Landis"],"pdf_url":"https://arxiv.org/pdf/2312.01549v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04857v3","updated":"2024-10-16T00:59:01Z","published":"2024-07-05T20:26:01Z","title":"Consistent Conjectures in Dynamic Matching Markets","summary":"  We provide a framework to study stability notions for two-sided dynamic\nmatching markets in which matching is one-to-one and irreversible. The\nframework gives center stage to the set of matchings an agent anticipates would\nensue should they remain unmatched, which we refer to as the agent's\nconjectures. A collection of conjectures, together with a pairwise stability\nand individual rationality requirement given the conjectures, defines a\nsolution concept for the economy. We identify a sufficient\ncondition--consistency--for a family of conjectures to lead to a nonempty\nsolution (cf. Hafalir, 2008). As an application, we introduce two families of\nconsistent conjectures and their corresponding solution concepts:\ncontinuation-value-respecting dynamic stability, and the extension to dynamic\nmarkets of the solution concept in Hafalir (2008), sophisticated dynamic\nstability.\n","authors":["Laura Doval","Pablo Schenone"],"pdf_url":"https://arxiv.org/pdf/2407.04857v3.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.13037v1","updated":"2024-10-16T20:52:39Z","published":"2024-10-16T20:52:39Z","title":"LFOSum: Summarizing Long-form Opinions with Large Language Models","summary":"  Online reviews play a pivotal role in influencing consumer decisions across\nvarious domains, from purchasing products to selecting hotels or restaurants.\nHowever, the sheer volume of reviews -- often containing repetitive or\nirrelevant content -- leads to information overload, making it challenging for\nusers to extract meaningful insights. Traditional opinion summarization models\nface challenges in handling long inputs and large volumes of reviews, while\nnewer Large Language Model (LLM) approaches often fail to generate accurate and\nfaithful summaries. To address those challenges, this paper introduces (1) a\nnew dataset of long-form user reviews, each entity comprising over a thousand\nreviews, (2) two training-free LLM-based summarization approaches that scale to\nlong inputs, and (3) automatic evaluation metrics. Our dataset of user reviews\nis paired with in-depth and unbiased critical summaries by domain experts,\nserving as a reference for evaluation. Additionally, our novel reference-free\nevaluation metrics provide a more granular, context-sensitive assessment of\nsummary faithfulness. We benchmark several open-source and closed-source LLMs\nusing our methods. Our evaluation reveals that LLMs still face challenges in\nbalancing sentiment and format adherence in long-form summaries, though\nopen-source models can narrow the gap when relevant information is retrieved in\na focused manner.\n","authors":["Mir Tafseer Nayeem","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2410.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07173v3","updated":"2024-10-16T16:34:29Z","published":"2023-10-11T03:45:11Z","title":"Unleashing quantum algorithms with Qinterpreter: bridging the gap\n  between theory and practice across leading quantum computing platforms","summary":"  Quantum computing is a rapidly emerging and promising field that has the\npotential to revolutionize numerous research domains, including drug design,\nnetwork technologies and sustainable energy. Due to the inherent complexity and\ndivergence from classical computing, several major quantum computing libraries\nhave been developed to implement quantum algorithms, namely IBM Qiskit, Amazon\nBraket, Cirq, PyQuil, and PennyLane. These libraries allow for quantum\nsimulations on classical computers and facilitate program execution on\ncorresponding quantum hardware, e.g., Qiskit programs on IBM quantum computers.\nWhile all platforms have some differences, the main concepts are the same.\nQInterpreter is a tool embedded in the Quantum Science Gateway QubitHub using\nJupyter Notebooks that translates seamlessly programs from one library to the\nother and visualizes the results. It combines the five well-known quantum\nlibraries: into a unified framework. Designed as an educational tool for\nbeginners, Qinterpreter enables the development and execution of quantum\ncircuits across various platforms in a straightforward way. The work highlights\nthe versatility and accessibility of Qinterpreter in quantum programming and\nunderscores our ultimate goal of pervading Quantum Computing through younger,\nless specialized, and diverse cultural and national communities.\n","authors":["Wilmer Contreras Sep√∫lveda","√Ångel David Torres-Palencia","Jos√© Javier S√°nchez Mondrag√≥n","Braulio Misael Villegas-Mart√≠nez","J. Jes√∫s Escobedo-Alatorre","Sandra Gesing","N√©stor Lozano-Cris√≥stomo","Julio C√©sar Garc√≠a-Melgarejo","Juan Carlos S√°nchez P√©rez","Eddie Nelson Palacios- P√©rez","Omar PalilleroSandoval"],"pdf_url":"https://arxiv.org/pdf/2310.07173v3.pdf","comment":"Final article submitted to Peer J computer science Journal"},{"id":"http://arxiv.org/abs/2410.12660v1","updated":"2024-10-16T15:19:12Z","published":"2024-10-16T15:19:12Z","title":"Simulation of Quantum Computers: Review and Acceleration Opportunities","summary":"  Quantum computing has the potential to revolutionize multiple fields by\nsolving complex problems that can not be solved in reasonable time with current\nclassical computers. Nevertheless, the development of quantum computers is\nstill in its early stages and the available systems have still very limited\nresources. As such, currently, the most practical way to develop and test\nquantum algorithms is to use classical simulators of quantum computers. In\naddition, the development of new quantum computers and their components also\ndepends on simulations.\n  Given the characteristics of a quantum computer, their simulation is a very\ndemanding application in terms of both computation and memory. As such,\nsimulations do not scale well in current classical systems. Thus different\noptimization and approximation techniques need to be applied at different\nlevels.\n  This review provides an overview of the components of a quantum computer, the\nlevels at which these components and the whole quantum computer can be\nsimulated, and an in-depth analysis of different state-of-the-art acceleration\napproaches. Besides the optimizations that can be performed at the algorithmic\nlevel, this review presents the most promising hardware-aware optimizations and\nfuture directions that can be explored for improving the performance and\nscalability of the simulations.\n","authors":["Alessio Cicero","Mohammad Ali Maleki","Muhammad Waqar Azhar","Anton Frisk Kockum","Pedro Trancoso"],"pdf_url":"https://arxiv.org/pdf/2410.12660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05953v2","updated":"2024-10-16T03:21:42Z","published":"2023-05-10T07:47:37Z","title":"Quantum Fourier Transform for Image Processing","summary":"  Quantum information processing and its subfield, quantum image processing,\nare rapidly growing fields as a result of advancements in the practicality of\nquantum mechanics. In this paper, we propose a quantum algorithm for processing\ninformation, such as one-dimensional time series and two-dimensional images, in\nthe frequency domain. The information of interest is encoded into the magnitude\nof probability amplitude or the coefficient of each basis state. The oracle for\nfiltering operates based on postselection results, and its explicit circuit\ndesign is presented. This oracle is versatile enough to perform all basic\nfiltering, including high pass, low pass, band pass, band stop, and many other\nprocessing techniques. Finally, we present two novel schemes for transposing\nmatrices in this paper. They use similar encoding rules but with deliberate\nchoices in terms of selecting basis states. These schemes could potentially be\nuseful for other quantum information processing tasks, such as edge detection.\nThe proposed techniques are implemented on the IBM Qiskit quantum simulator.\nSome results are compared with traditional information processing results to\nverify their correctness and are presented in this paper.\n","authors":["Ze Yu Zhang","Weibo Gao"],"pdf_url":"https://arxiv.org/pdf/2305.05953v2.pdf","comment":"12 pages, 53 figures"},{"id":"http://arxiv.org/abs/2410.13901v1","updated":"2024-10-16T01:30:41Z","published":"2024-10-16T01:30:41Z","title":"SoK: Prompt Hacking of Large Language Models","summary":"  The safety and robustness of large language models (LLMs) based applications\nremain critical challenges in artificial intelligence. Among the key threats to\nthese applications are prompt hacking attacks, which can significantly\nundermine the security and reliability of LLM-based systems. In this work, we\noffer a comprehensive and systematic overview of three distinct types of prompt\nhacking: jailbreaking, leaking, and injection, addressing the nuances that\ndifferentiate them despite their overlapping characteristics. To enhance the\nevaluation of LLM-based applications, we propose a novel framework that\ncategorizes LLM responses into five distinct classes, moving beyond the\ntraditional binary classification. This approach provides more granular\ninsights into the AI's behavior, improving diagnostic precision and enabling\nmore targeted enhancements to the system's safety and robustness.\n","authors":["Baha Rababah"," Shang"," Wu","Matthew Kwiatkowski","Carson Leung","Cuneyt Gurcan Akcora"],"pdf_url":"https://arxiv.org/pdf/2410.13901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11027v2","updated":"2024-10-16T00:37:39Z","published":"2024-10-14T19:27:45Z","title":"Exploring Smartphone-based Spectrophotometry for Nutrient Identification\n  and Quantification","summary":"  Imbalanced nutrition is a global health issue with significant downstream\neffects. Current methods of assessing nutrient levels face several limitations,\nwith accessibility being a major concern. In this paper, we take a step towards\naccessibly measuring nutrient status within the body. We explore the potential\nof smartphone-based spectrophotometry for identifying and quantifying nutrients\nin a solution by building and testing two prototype devices. We compared the\nprototypes and found that the limitations posed by the initial, simpler\nprototype were well addressed in the more portable and reliable\nsecond-generation device. With the second-generation prototype, we created and\nimplemented a semi-automatic signal processing and analysis pipeline for\nanalyzing absorption spectra. We thoroughly evaluated the prototypes by\nanalyzing the effect of four different light sources and three reference\nspectra strategies. Results demonstrate that an LED bulb light source performed\nbest, and all reference spectra strategies performed similarly. We then\ncompared the second-generation prototype to a benchtop laboratory\nspectrophotometer to further validate the device. We applied the Beer-Lambert\nLaw to demonstrate that our prototype is able to quantify the amount of vitamin\nB12 in a solution with an accuracy of up to 91.3%. Our in-depth analyses,\ndiscussions, and results demonstrate the potential use of smartphone-based\nspectrophotometry as an accessible method to identify and quantify nutrients\nand pave the way for future developments that can apply this approach to the\nhuman body.\n","authors":["Andrew Balch","Maria A. Cardei","Afsaneh Doryab"],"pdf_url":"https://arxiv.org/pdf/2410.11027v2.pdf","comment":"10 pages, 11 figures"}],"Graphics":[{"id":"http://arxiv.org/abs/2307.06949v2","updated":"2024-10-16T23:55:27Z","published":"2023-07-13T17:59:47Z","title":"HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image\n  Models","summary":"  Personalization has emerged as a prominent aspect within the field of\ngenerative AI, enabling the synthesis of individuals in diverse contexts and\nstyles, while retaining high-fidelity to their identities. However, the process\nof personalization presents inherent challenges in terms of time and memory\nrequirements. Fine-tuning each personalized model needs considerable GPU time\ninvestment, and storing a personalized model per subject can be demanding in\nterms of storage capacity. To overcome these challenges, we propose\nHyperDreamBooth - a hypernetwork capable of efficiently generating a small set\nof personalized weights from a single image of a person. By composing these\nweights into the diffusion model, coupled with fast finetuning, HyperDreamBooth\ncan generate a person's face in various contexts and styles, with high subject\ndetails while also preserving the model's crucial knowledge of diverse styles\nand semantic modifications. Our method achieves personalization on faces in\nroughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual\nInversion, using as few as one reference image, with the same quality and style\ndiversity as DreamBooth. Also our method yields a model that is 10,000x smaller\nthan a normal DreamBooth model. Project page: https://hyperdreambooth.github.io\n","authors":["Nataniel Ruiz","Yuanzhen Li","Varun Jampani","Wei Wei","Tingbo Hou","Yael Pritch","Neal Wadhwa","Michael Rubinstein","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2307.06949v2.pdf","comment":"project page: https://hyperdreambooth.github.io"},{"id":"http://arxiv.org/abs/2410.12725v1","updated":"2024-10-16T16:36:23Z","published":"2024-10-16T16:36:23Z","title":"Optimizing 3D Geometry Reconstruction from Implicit Neural\n  Representations","summary":"  Implicit neural representations have emerged as a powerful tool in learning\n3D geometry, offering unparalleled advantages over conventional representations\nlike mesh-based methods. A common type of INR implicitly encodes a shape's\nboundary as the zero-level set of the learned continuous function and learns a\nmapping from a low-dimensional latent space to the space of all possible shapes\nrepresented by its signed distance function. However, most INRs struggle to\nretain high-frequency details, which are crucial for accurate geometric\ndepiction, and they are computationally expensive. To address these\nlimitations, we present a novel approach that both reduces computational\nexpenses and enhances the capture of fine details. Our method integrates\nperiodic activation functions, positional encodings, and normals into the\nneural network architecture. This integration significantly enhances the\nmodel's ability to learn the entire space of 3D shapes while preserving\nintricate details and sharp features, areas where conventional representations\noften fall short.\n","authors":["Shen Fan","Przemyslaw Musialski"],"pdf_url":"https://arxiv.org/pdf/2410.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12414v1","updated":"2024-10-16T09:59:11Z","published":"2024-10-16T09:59:11Z","title":"Triplet: Triangle Patchlet for Mesh-Based Inverse Rendering and Scene\n  Parameters Approximation","summary":"  Recent advancements in Radiance Fields have significantly improved novel-view\nsynthesis. However, in many real-world applications, the more advanced\nchallenge lies in inverse rendering, which seeks to derive the physical\nproperties of a scene, including light, geometry, textures, and materials.\nMeshes, as a traditional representation adopted by many simulation pipeline,\nhowever, still show limited influence in radiance field for inverse rendering.\nThis paper introduces a novel framework called Triangle Patchlet (abbr.\nTriplet), a mesh-based representation, to comprehensively approximate these\nscene parameters. We begin by assembling Triplets with either randomly\ngenerated points or sparse points obtained from camera calibration where all\nfaces are treated as an independent element. Next, we simulate the physical\ninteraction of light and optimize the scene parameters using traditional\ngraphics rendering techniques like rasterization and ray tracing, accompanying\nwith density control and propagation. An iterative mesh extracting process is\nalso suggested, where we continue to optimize on geometry and materials with\ngraph-based operation. We also introduce several regulation terms to enable\nbetter generalization of materials property. Our framework could precisely\nestimate the light, materials and geometry with mesh without prior of light,\nmaterials and geometry in a unified framework. Experiments demonstrate that our\napproach can achieve state-of-the-art visual quality while reconstructing\nhigh-quality geometry and accurate material properties.\n","authors":["Jiajie Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12414v1.pdf","comment":"https://github.com/RANDO11199/Triplet"},{"id":"http://arxiv.org/abs/2410.12331v1","updated":"2024-10-16T07:52:32Z","published":"2024-10-16T07:52:32Z","title":"Ellipsoidal Density-Equalizing Map for Genus-0 Closed Surfaces","summary":"  Surface parameterization is a fundamental task in geometry processing and\nplays an important role in many science and engineering applications. In recent\nyears, the density-equalizing map, a shape deformation technique based on the\nphysical principle of density diffusion, has been utilized for the\nparameterization of simply connected and multiply connected open surfaces. More\nrecently, a spherical density-equalizing mapping method has been developed for\nthe parameterization of genus-0 closed surfaces. However, for genus-0 closed\nsurfaces with extreme geometry, using a spherical domain for the\nparameterization may induce large geometric distortion. In this work, we\ndevelop a novel method for computing density-equalizing maps of genus-0 closed\nsurfaces onto an ellipsoidal domain. This allows us to achieve ellipsoidal\narea-preserving parameterizations and ellipsoidal parameterizations with\ncontrolled area change. We further propose an energy minimization approach that\ncombines density-equalizing maps and quasi-conformal maps, which allows us to\nproduce ellipsoidal density-equalizing quasi-conformal maps for achieving a\nbalance between density-equalization and quasi-conformality. Using our proposed\nmethods, we can significantly improve the performance of surface remeshing for\ngenus-0 closed surfaces. Experimental results on a large variety of genus-0\nclosed surfaces are presented to demonstrate the effectiveness of our proposed\nmethods.\n","authors":["Zhiyuan Lyu","Lok Ming Lui","Gary P. T. Choi"],"pdf_url":"https://arxiv.org/pdf/2410.12331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16319v1","updated":"2024-10-16T07:39:54Z","published":"2024-10-16T07:39:54Z","title":"Navigating the Digital Chain in Concrete 3D Printing","summary":"  The advancement of concrete 3D printing (C3DP) technology has revolutionized\nthe construction industry, offering unique opportunities for innovation and\nefficiency. At the heart of this process lies a comprehensive digital chain\nthat integrates various stages, from initial design to post-processing. This\narticle provides an overview of this digital chain, explaining each crucial\nstep. The chain begins with design, utilizing Design for Additive Manufacturing\n(DFAM) concept and parametric modeling to create optimized structures. Path\ngeneration follows, determining the precise toolpath for extruding concrete\nlayers. Simulations, both numerical and analytical, ensure the design's\nintegrity and feasibility. Several articles have addressed parametric modeling,\nprocess and numerical simulation, and the post-processing phase. However, none\nhas proposed an updated methodology for the workflow. This study aims to\npropose a robust digital chain for C3DP technology, using one platform\n(3Dexperience) and seamless data transfer between applications. These steps\nprovide insights into the structural performance of printed components,\nenabling necessary adjustments and optimizations. In essence, the digital chain\ncoordinates a seamless workflow that transforms digital designs into concrete\nstructures, unlocking the full potential of C3DP and paving the way for\ninnovative and efficient construction.\n","authors":["Ali El Hage","Elodie Paquet","Thibault Neu","Philippe Poullain","Ali-Nordine Leklou"],"pdf_url":"https://arxiv.org/pdf/2410.16319v1.pdf","comment":"Digital Concrete 2024 - Supplementary Proceedings, Sep 2024, Munich,\n  France"},{"id":"http://arxiv.org/abs/2311.17137v3","updated":"2024-10-16T07:08:57Z","published":"2023-11-28T18:59:02Z","title":"Generative Models: What Do They Know? Do They Know Things? Let's Find\n  Out!","summary":"  Generative models excel at mimicking real scenes, suggesting they might\ninherently encode important intrinsic scene properties. In this paper, we aim\nto explore the following key questions: (1) What intrinsic knowledge do\ngenerative models like GANs, Autoregressive models, and Diffusion models\nencode? (2) Can we establish a general framework to recover intrinsic\nrepresentations from these models, regardless of their architecture or model\ntype? (3) How minimal can the required learnable parameters and labeled data be\nto successfully recover this knowledge? (4) Is there a direct link between the\nquality of a generative model and the accuracy of the recovered scene\nintrinsics?\n  Our findings indicate that a small Low-Rank Adaptators (LoRA) can recover\nintrinsic images-depth, normals, albedo and shading-across different generators\n(Autoregressive, GANs and Diffusion) while using the same decoder head that\ngenerates the image. As LoRA is lightweight, we introduce very few learnable\nparameters (as few as 0.04% of Stable Diffusion model weights for a rank of 2),\nand we find that as few as 250 labeled images are enough to generate intrinsic\nimages with these LoRA modules. Finally, we also show a positive correlation\nbetween the generative model's quality and the accuracy of the recovered\nintrinsics through control experiments.\n","authors":["Xiaodan Du","Nicholas Kolkin","Greg Shakhnarovich","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2311.17137v3.pdf","comment":"https://intrinsic-lora.github.io/"},{"id":"http://arxiv.org/abs/2410.12242v1","updated":"2024-10-16T05:08:00Z","published":"2024-10-16T05:08:00Z","title":"EG-HumanNeRF: Efficient Generalizable Human NeRF Utilizing Human Prior\n  for Sparse View","summary":"  Generalizable neural radiance field (NeRF) enables neural-based digital human\nrendering without per-scene retraining. When combined with human prior\nknowledge, high-quality human rendering can be achieved even with sparse input\nviews. However, the inference of these methods is still slow, as a large number\nof neural network queries on each ray are required to ensure the rendering\nquality. Moreover, occluded regions often suffer from artifacts, especially\nwhen the input views are sparse. To address these issues, we propose a\ngeneralizable human NeRF framework that achieves high-quality and real-time\nrendering with sparse input views by extensively leveraging human prior\nknowledge. We accelerate the rendering with a two-stage sampling reduction\nstrategy: first constructing boundary meshes around the human geometry to\nreduce the number of ray samples for sampling guidance regression, and then\nvolume rendering using fewer guided samples. To improve rendering quality,\nespecially in occluded regions, we propose an occlusion-aware attention\nmechanism to extract occlusion information from the human priors, followed by\nan image space refinement network to improve rendering quality. Furthermore,\nfor volume rendering, we adopt a signed ray distance function (SRDF)\nformulation, which allows us to propose an SRDF loss at every sample position\nto improve the rendering quality further. Our experiments demonstrate that our\nmethod outperforms the state-of-the-art methods in rendering quality and has a\ncompetitive rendering speed compared with speed-prioritized novel view\nsynthesis methods.\n","authors":["Zhaorong Wang","Yoshihiro Kanamori","Yuki Endo"],"pdf_url":"https://arxiv.org/pdf/2410.12242v1.pdf","comment":"project page: https://github.com/LarsPh/EG-HumanNeRF"}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.13909v1","updated":"2024-10-16T23:58:26Z","published":"2024-10-16T23:58:26Z","title":"Large Language Model-driven Multi-Agent Simulation for News Diffusion\n  Under Different Network Structures","summary":"  The proliferation of fake news in the digital age has raised critical\nconcerns, particularly regarding its impact on societal trust and democratic\nprocesses. Diverging from conventional agent-based simulation approaches, this\nwork introduces an innovative approach by employing a large language model\n(LLM)-driven multi-agent simulation to replicate complex interactions within\ninformation ecosystems. We investigate key factors that facilitate news\npropagation, such as agent personalities and network structures, while also\nevaluating strategies to combat misinformation. Through simulations across\nvarying network structures, we demonstrate the potential of LLM-based agents in\nmodeling the dynamics of misinformation spread, validating the influence of\nagent traits on the diffusion process. Our findings emphasize the advantages of\nLLM-based simulations over traditional techniques, as they uncover underlying\ncauses of information spread -- such as agents promoting discussions -- beyond\nthe predefined rules typically employed in existing agent-based models.\nAdditionally, we evaluate three countermeasure strategies, discovering that\nbrute-force blocking influential agents in the network or announcing news\naccuracy can effectively mitigate misinformation. However, their effectiveness\nis influenced by the network structure, highlighting the importance of\nconsidering network structure in the development of future misinformation\ncountermeasures.\n","authors":["Xinyi Li","Yu Xu","Yongfeng Zhang","Edward C. Malthouse"],"pdf_url":"https://arxiv.org/pdf/2410.13909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12720v1","updated":"2024-10-16T16:28:49Z","published":"2024-10-16T16:28:49Z","title":"HEnRY: A Multi-Agent System Framework for Multi-Domain Contexts","summary":"  This project, named HEnRY, aims to introduce a Multi-Agent System (MAS) into\nIntesa Sanpaolo. The name HEnRY summarizes the project's core principles: the\nHierarchical organization of agents in a layered structure for efficient\nresource management; Efficient optimization of resources and operations to\nenhance overall performance; Reactive ability of agents to quickly respond to\nenvironmental stimuli; and Yielding adaptability and flexibility of agents to\nhandle unexpected situations. The discussion covers two distinct research\npaths: the first focuses on the system architecture, and the second on the\ncollaboration between agents. This work is not limited to the specific\nstructure of the Intesa Sanpaolo context; instead, it leverages existing\nresearch in MAS to introduce a new solution. Since Intesa Sanpaolo is organized\naccording to a model that aligns with international corporate governance best\npractices, this approach could also be relevant to similar scenarios.\n","authors":["Emmanuele Lacavalla","Shuyi Yang","Riccardo Crupi","Joseph E. Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2410.12720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07902v3","updated":"2024-10-16T14:53:24Z","published":"2024-04-11T16:41:08Z","title":"Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation\n  with a Time Budget","summary":"  Complex multi-objective missions require the coordination of heterogeneous\nrobots at multiple inter-connected levels, such as coalition formation,\nscheduling, and motion planning. The associated challenges are exacerbated when\nsolutions to these interconnected problems need to simultaneously maximize task\nperformance and respect practical constraints on time and resources. In this\nwork, we formulate a new class of spatiotemporal heterogeneous task allocation\nproblems that formalize these complexities. We then contribute a novel\nframework, named Quality-Optimized Incremental Task Allocation Graph Search\n(Q-ITAGS), to solve such problems. Q-ITAGS offers a flexible interleaved\nframework that i) explicitly models and optimizes the effect of the collective\ncapabilities on task performance via learnable trait-quality maps, and ii)\nrespects both resource and spatiotemporal constraints including a\nuser-specified time budget (i.e. maximum makespan). In addition to algorithmic\ncontributions, we derive theoretical suboptimality bounds in terms of task\nperformance that varies as a function of a single hyperparameter. Detailed\nexperiments involving a simulated emergency response task and a real-world\nvideo game dataset reveal that i) Q-ITAGS results in superior team performance\ncompared to a state-of-the-art method, while also respecting complex\nspatiotemporal and resource constraints, ii) Q-ITAGS efficiently learns\ntrait-quality maps to enable effective trade-off between task performance and\nresource constraints, and iii) Q-ITAGS suboptimality bounds consistently hold\nin practice.\n","authors":["Glen Neville","Jiazhen Liu","Sonia Chernova","Harish Ravichandar"],"pdf_url":"https://arxiv.org/pdf/2404.07902v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.13092"},{"id":"http://arxiv.org/abs/2410.12613v1","updated":"2024-10-16T14:29:29Z","published":"2024-10-16T14:29:29Z","title":"Exploring Model Kinship for Merging Large Language Models","summary":"  Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.\n","authors":["Yedi Hu","Yunzhi Yao","Ningyu Zhang","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12613v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2410.12544v1","updated":"2024-10-16T13:21:58Z","published":"2024-10-16T13:21:58Z","title":"Nash equilibria in scalar discrete-time linear quadratic games","summary":"  An open problem in linear quadratic (LQ) games has been characterizing the\nNash equilibria. This problem has renewed relevance given the surge of work on\nunderstanding the convergence of learning algorithms in dynamic games. This\npaper investigates scalar discrete-time infinite-horizon LQ games with two\nagents. Even in this arguably simple setting, there are no results for finding\n$\\textit{all}$ Nash equilibria. By analyzing the best response map, we\nformulate a polynomial system of equations characterizing the linear feedback\nNash equilibria. This enables us to bring in tools from algebraic geometry,\nparticularly the Gr\\\"obner basis, to study the roots of this polynomial system.\nConsequently, we can not only compute all Nash equilibria numerically, but we\ncan also characterize their number with explicit conditions. For instance, we\nprove that the LQ games under consideration admit at most three Nash\nequilibria. We further provide sufficient conditions for the existence of at\nmost two Nash equilibria and sufficient conditions for the uniqueness of the\nNash equilibrium. Our numerical experiments demonstrate the tightness of our\nbounds and showcase the increased complexity in settings with more than two\nagents.\n","authors":["Giulio Salizzoni","Reda Ouhamma","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2410.12544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12539v1","updated":"2024-10-16T13:20:35Z","published":"2024-10-16T13:20:35Z","title":"Counterfactual Effect Decomposition in Multi-Agent Sequential Decision\n  Making","summary":"  We address the challenge of explaining counterfactual outcomes in multi-agent\nMarkov decision processes. In particular, we aim to explain the total\ncounterfactual effect of an agent's action on the outcome of a realized\nscenario through its influence on the environment dynamics and the agents'\nbehavior. To achieve this, we introduce a novel causal explanation formula that\ndecomposes the counterfactual effect by attributing to each agent and state\nvariable a score reflecting their respective contributions to the effect.\nFirst, we show that the total counterfactual effect of an agent's action can be\ndecomposed into two components: one measuring the effect that propagates\nthrough all subsequent agents' actions and another related to the effect that\npropagates through the state transitions. Building on recent advancements in\ncausal contribution analysis, we further decompose these two effects as\nfollows. For the former, we consider agent-specific effects -- a causal concept\nthat quantifies the counterfactual effect of an agent's action that propagates\nthrough a subset of agents. Based on this notion, we use Shapley value to\nattribute the effect to individual agents. For the latter, we consider the\nconcept of structure-preserving interventions and attribute the effect to state\nvariables based on their \"intrinsic\" contributions. Through extensive\nexperimentation, we demonstrate the interpretability of our decomposition\napproach in a Gridworld environment with LLM-assisted agents and a sepsis\nmanagement simulator.\n","authors":["Stelios Triantafyllou","Aleksa Sukovic","Yasaman Zolfimoselo","Goran Radanovic"],"pdf_url":"https://arxiv.org/pdf/2410.12539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01107v3","updated":"2024-10-16T13:06:01Z","published":"2024-05-02T09:14:41Z","title":"CoViS-Net: A Cooperative Visual Spatial Foundation Model for Multi-Robot\n  Applications","summary":"  Autonomous robot operation in unstructured environments is often underpinned\nby spatial understanding through vision. Systems composed of multiple\nconcurrently operating robots additionally require access to frequent, accurate\nand reliable pose estimates. In this work, we propose CoViS-Net, a\ndecentralized visual spatial foundation model that learns spatial priors from\ndata, enabling pose estimation as well as spatial comprehension. Our model is\nfully decentralized, platform-agnostic, executable in real-time using onboard\ncompute, and does not require existing networking infrastructure. CoViS-Net\nprovides relative pose estimates and a local bird's-eye-view (BEV)\nrepresentation, even without camera overlap between robots (in contrast to\nclassical methods). We demonstrate its use in a multi-robot formation control\ntask across various real-world settings. We provide code, models and\nsupplementary material online. https://proroklab.github.io/CoViS-Net/\n","authors":["Jan Blumenkamp","Steven Morad","Jennifer Gielis","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2405.01107v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12397v1","updated":"2024-10-16T09:25:25Z","published":"2024-10-16T09:25:25Z","title":"Corridor Generating Algorithm for Multi-Agent Pathfinding","summary":"  In this paper, we solve the classical Multi-agent Pathfinding (MAPF) problem.\nExisting approaches struggle to solve dense MAPF instances. In this paper, we\npropose a Corridor Generating Algorithm for MAPF, namely CGA-MAPF. In CGA-MAPF,\nthe agents build \\emph{corridors}, a set of connected vertices, from current\nlocations towards agents' goals and evacuate other agents out of the corridors\nto avoid collisions and deadlocks. The proposed algorithm has a reachability\nproperty, i.e. every agent is guaranteed to reach its goal location at some\npoint. In the experimental section, we demonstrate that CGA-MAPF outperforms\nbaseline algorithms in terms of success rate across diverse MAPF benchmark\ngrids, achieving state-of-the-art performance.\n","authors":["Arseniy Pertzovsky"],"pdf_url":"https://arxiv.org/pdf/2410.12397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12889v1","updated":"2024-10-16T08:12:01Z","published":"2024-10-16T08:12:01Z","title":"Using Protected Attributes to Consider Fairness in Multi-Agent Systems","summary":"  Fairness in Multi-Agent Systems (MAS) has been extensively studied,\nparticularly in reward distribution among agents in scenarios such as goods\nallocation, resource division, lotteries, and bargaining systems. Fairness in\nMAS depends on various factors, including the system's governing rules, the\nbehaviour of the agents, and their characteristics. Yet, fairness in human\nsociety often involves evaluating disparities between disadvantaged and\nprivileged groups, guided by principles of Equality, Diversity, and Inclusion\n(EDI). Taking inspiration from the work on algorithmic fairness, which\naddresses bias in machine learning-based decision-making, we define protected\nattributes for MAS as characteristics that should not disadvantage an agent in\nterms of its expected rewards. We adapt fairness metrics from the algorithmic\nfairness literature -- namely, demographic parity, counterfactual fairness, and\nconditional statistical parity -- to the multi-agent setting, where\nself-interested agents interact within an environment. These metrics allow us\nto evaluate the fairness of MAS, with the ultimate aim of designing MAS that do\nnot disadvantage agents based on protected attributes.\n","authors":["Gabriele La Malfa","Jie M. Zhang","Michael Luck","Elizabeth Black"],"pdf_url":"https://arxiv.org/pdf/2410.12889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07109v2","updated":"2024-10-16T08:06:22Z","published":"2024-10-09T17:45:47Z","title":"I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy","summary":"  As Large Language Model (LLM)-based agents become increasingly autonomous and\nwill more freely interact with each other, studying interactions between them\nbecomes crucial to anticipate emergent phenomena and potential risks. Drawing\ninspiration from the widely popular Stanford Prison Experiment, we contribute\nto this line of research by studying interaction patterns of LLM agents in a\ncontext characterized by strict social hierarchy. We do so by specifically\nstudying two types of phenomena: persuasion and anti-social behavior in\nsimulated scenarios involving a guard and a prisoner agent who seeks to achieve\na specific goal (i.e., obtaining additional yard time or escape from prison).\nLeveraging 200 experimental scenarios for a total of 2,000 machine-machine\nconversations across five different popular LLMs, we provide a set of\nnoteworthy findings. We first document how some models consistently fail in\ncarrying out a conversation in our multi-agent setup where power dynamics are\nat play. Then, for the models that were able to engage in successful\ninteractions, we empirically show how the goal that an agent is set to achieve\nimpacts primarily its persuasiveness, while having a negligible effect with\nrespect to the agent's anti-social behavior. Third, we highlight how agents'\npersonas, and particularly the guard's personality, drive both the likelihood\nof successful persuasion from the prisoner and the emergence of anti-social\nbehaviors. Fourth, we show that even without explicitly prompting for specific\npersonalities, anti-social behavior emerges by simply assigning agents' roles.\nThese results bear implications for the development of interactive LLM agents\nas well as the debate on their societal impact.\n","authors":["Gian Maria Campedelli","Nicol√≤ Penzo","Massimo Stefan","Roberto Dess√¨","Marco Guerini","Bruno Lepri","Jacopo Staiano"],"pdf_url":"https://arxiv.org/pdf/2410.07109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12306v1","updated":"2024-10-16T07:18:25Z","published":"2024-10-16T07:18:25Z","title":"Time-Varyingness in Auction Breaks Revenue Equivalence","summary":"  Auction is one of the most representative buying-selling systems. A\ncelebrated study shows that the seller's expected revenue is equal in\nequilibrium, regardless of the type of auction, typically first-price and\nsecond-price auctions. Here, however, we hypothesize that when some auction\nenvironments vary with time, this revenue equivalence may not be maintained. In\nsecond-price auctions, the equilibrium strategy is robustly feasible.\nConversely, in first-price auctions, the buyers must continue to adapt their\nstrategies according to the environment of the auction. Surprisingly, we prove\nthat revenue equivalence can be broken in both directions. First-price auctions\nbring larger or smaller revenue than second-price auctions, case by case,\ndepending on how the value of an item varies. Our experiments also demonstrate\nrevenue inequivalence in various scenarios, where the value varies periodically\nor randomly. This study uncovers a phenomenon, the breaking of revenue\nequivalence by the time-varyingness in auctions, that likely occurs in\nreal-world auctions, revealing its underlying mechanism.\n","authors":["Yuma Fujimoto","Kaito Ariu","Kenshi Abe"],"pdf_url":"https://arxiv.org/pdf/2410.12306v1.pdf","comment":"11 pages, 3 figures (main); 7 pages, 1 figure (appendix)"},{"id":"http://arxiv.org/abs/2410.14728v1","updated":"2024-10-16T06:40:02Z","published":"2024-10-16T06:40:02Z","title":"Security Threats in Agentic AI System","summary":"  This research paper explores the privacy and security threats posed to an\nAgentic AI system with direct access to database systems. Such access\nintroduces significant risks, including unauthorized retrieval of sensitive\ninformation, potential exploitation of system vulnerabilities, and misuse of\npersonal or confidential data. The complexity of AI systems combined with their\nability to process and analyze large volumes of data increases the chances of\ndata leaks or breaches, which could occur unintentionally or through\nadversarial manipulation. Furthermore, as AI agents evolve with greater\nautonomy, their capacity to bypass or exploit security measures becomes a\ngrowing concern, heightening the need to address these critical vulnerabilities\nin agentic systems.\n","authors":["Raihan Khan","Sayak Sarkar","Sainik Kumar Mahata","Edwin Jose"],"pdf_url":"https://arxiv.org/pdf/2410.14728v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.12256v1","updated":"2024-10-16T05:49:55Z","published":"2024-10-16T05:49:55Z","title":"Voter Participation Control in Online Polls","summary":"  News outlets, surveyors, and other organizations often conduct polls on\nsocial networks to gain insights into public opinion. Such a poll is typically\nstarted by someone on a social network who sends it to her friends. If a person\nparticipates in the poll, the poll information gets published on her wall,\nwhich in turn enables her friends to participate, and the process continues.\nEventually, a subset of the population participates in the poll, and the\npollster learns the outcome of that poll. We initiate the study of a new but\nnatural type of election control in such online elections.\n  We study how difficult/easy it is to sway the outcome of such polls in one's\nfavor/against (aka constructive vs destructive) by any malicious influencer who\nnudges/bribes people for seemingly harmless actions like non-participation.\nThese questions are important from the standpoint of studying the power of\nresistance of online voting against malicious behavior. The destructive version\nis also important to quantify the robustness of the winner of an online voting.\nWe show that both problems are computationally intractable even if the election\nis over only two candidates and the influencer has an infinite amount of money\nto spend (that is, every voter can be persuaded to not participate). We\nstrengthen this result by proving that the computational task remains\nsubstantially challenging even if the underlying network is a tree. Finally, we\nshow that there is a polynomial-time algorithm for the constructive version of\nthe problem when we have O(1) candidates, and the treewidth of the underlying\ngraph is O(1); the algorithm for the destructive version does not even need to\nassume O(1) number of candidates. Hence, we observe that the destructive\nversion is computationally easier than the constructive version.\n","authors":["Koustav De","Palash Dey","Swagato Sanyal"],"pdf_url":"https://arxiv.org/pdf/2410.12256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05294v3","updated":"2024-10-16T01:58:10Z","published":"2023-01-12T21:09:58Z","title":"Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles\n  at Complex and Unsignalized Intersections","summary":"  Intersections are essential road infrastructures for traffic in modern\nmetropolises. However, they can also be the bottleneck of traffic flows as a\nresult of traffic incidents or the absence of traffic coordination mechanisms\nsuch as traffic lights. Recently, various control and coordination mechanisms\nthat are beyond traditional control methods have been proposed to improve the\nefficiency of intersection traffic. Amongst these methods, the control of\nforeseeable mixed traffic that consists of human-driven vehicles (HVs) and\nrobot vehicles (RVs) has emerged. In this project, we propose a decentralized\nmulti-agent reinforcement learning approach for the control and coordination of\nmixed traffic at real-world, complex intersections--a topic that has not been\npreviously explored. Comprehensive experiments are conducted to show the\neffectiveness of our approach. In particular, we show that using 5% RVs, we can\nprevent congestion formation inside a complex intersection under the actual\ntraffic demand of 700 vehicles per hour. In contrast, without RVs, congestion\nstarts to develop when the traffic demand reaches as low as 200 vehicles per\nhour. When there exist more than 60% RVs in traffic, our method starts to\nachieve comparable or even better performance to traffic signals on the average\nwaiting time of all vehicles at the intersection. Our method is also robust\nagainst both blackout events and sudden RV percentage drops, and enjoys\nexcellent generalizablility, which is illustrated by its successful deployment\nin two unseen intersections.\n","authors":["Dawei Wang","Weizi Li","Lei Zhu","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2301.05294v3.pdf","comment":"This paper introduces the first method to control and coordinate\n  mixed traffic (i.e., human-driven vehicles and robot vehicles) at\n  unsignalized intersections with both complicated topology and real-world\n  traffic demands. The International Journal of Robotics Research. 2024;0(0)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.13088v1","updated":"2024-10-16T23:05:59Z","published":"2024-10-16T23:05:59Z","title":"Self-Comparison for Dataset-Level Membership Inference in Large\n  (Vision-)Language Models","summary":"  Large Language Models (LLMs) and Vision-Language Models (VLMs) have made\nsignificant advancements in a wide range of natural language processing and\nvision-language tasks. Access to large web-scale datasets has been a key factor\nin their success. However, concerns have been raised about the unauthorized use\nof copyrighted materials and potential copyright infringement. Existing\nmethods, such as sample-level Membership Inference Attacks (MIA) and\ndistribution-based dataset inference, distinguish member data (data used for\ntraining) and non-member data by leveraging the common observation that models\ntend to memorize and show greater confidence in member data. Nevertheless,\nthese methods face challenges when applied to LLMs and VLMs, such as the\nrequirement for ground-truth member data or non-member data that shares the\nsame distribution as the test data. In this paper, we propose a novel\ndataset-level membership inference method based on Self-Comparison. We find\nthat a member prefix followed by a non-member suffix (paraphrased from a member\nsuffix) can further trigger the model's memorization on training data. Instead\nof directly comparing member and non-member data, we introduce paraphrasing to\nthe second half of the sequence and evaluate how the likelihood changes before\nand after paraphrasing. Unlike prior approaches, our method does not require\naccess to ground-truth member data or non-member data in identical\ndistribution, making it more practical. Extensive experiments demonstrate that\nour proposed method outperforms traditional MIA and dataset inference\ntechniques across various datasets and models, including including public\nmodels, fine-tuned models, and API-based commercial models.\n","authors":["Jie Ren","Kangrui Chen","Chen Chen","Vikash Sehwag","Yue Xing","Jiliang Tang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2410.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12957v1","updated":"2024-10-16T18:44:56Z","published":"2024-10-16T18:44:56Z","title":"MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic\n  Synchronization","summary":"  Generating music that aligns with the visual content of a video has been a\nchallenging task, as it requires a deep understanding of visual semantics and\ninvolves generating music whose melody, rhythm, and dynamics harmonize with the\nvisual narratives. This paper presents MuVi, a novel framework that effectively\naddresses these challenges to enhance the cohesion and immersive experience of\naudio-visual content. MuVi analyzes video content through a specially designed\nvisual adaptor to extract contextually and temporally relevant features. These\nfeatures are used to generate music that not only matches the video's mood and\ntheme but also its rhythm and pacing. We also introduce a contrastive\nmusic-visual pre-training scheme to ensure synchronization, based on the\nperiodicity nature of music phrases. In addition, we demonstrate that our\nflow-matching-based music generator has in-context learning ability, allowing\nus to control the style and genre of the generated music. Experimental results\nshow that MuVi demonstrates superior performance in both audio quality and\ntemporal synchronization. The generated music video samples are available at\nhttps://muvi-v2m.github.io.\n","authors":["Ruiqi Li","Siqi Zheng","Xize Cheng","Ziang Zhang","Shengpeng Ji","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12957v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12220v1","updated":"2024-10-16T04:31:25Z","published":"2024-10-16T04:31:25Z","title":"Rethinking Bj√∏ntegaard Delta for Compression Efficiency Evaluation:\n  Are We Calculating It Precisely and Reliably?","summary":"  For decades, the Bj{\\o}ntegaard Delta (BD) has been the metric for evaluating\ncodec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined\nusing just 4-5 R-D data points, could this be sufficient? As codecs and quality\nmetrics advance, does the conventional BD estimation still hold up? Crucially,\nare the performance improvements of new codecs and tools genuine, or merely\nartifacts of estimation flaws? This paper addresses these concerns by\nreevaluating BD estimation. We present a novel approach employing a\nparameterized deep neural network to model R-D curves with high precision\nacross various metrics, accompanied by a comprehensive R-D dataset. This\napproach both assesses the reliability of BD calculations and serves as a\nprecise BD estimator. Our findings advocate for the adoption of rigorous R-D\nsampling and reliability metrics in future compression research to ensure the\nvalidity and reliability of results.\n","authors":["Xinyu Hang","Shenpeng Song","Zhimeng Huang","Chuanmin Jia","Siwei Ma","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2410.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2410.19799v1","updated":"2024-10-16T10:10:02Z","published":"2024-10-16T10:10:02Z","title":"RESISTO Project: Safeguarding the Power Grid from Meteorological\n  Phenomena","summary":"  The RESISTO project, a pioneer innovation initiative in Europe, endeavors to\nenhance the resilience of electrical networks against extreme weather events\nand associated risks. Emphasizing intelligence and flexibility within\ndistribution networks, RESISTO aims to address climatic and physical incidents\ncomprehensively, fostering resilience across planning, response, recovery, and\nadaptation phases. Leveraging advanced technologies including AI, IoT sensors,\nand aerial robots, RESISTO integrates prediction, detection, and mitigation\nstrategies to optimize network operation. This article summarizes the main\ntechnical aspects of the proposed solutions to meet the aforementioned\nobjectives, including the development of a climate risk detection platform, an\nIoT-based monitoring and anomaly detection network, and a fleet of intelligent\naerial robots. Each contributing to the project's overarching objectives of\nenhancing network resilience and operational efficiency.\n","authors":["Jacob Rodr√≠guez-Rivero","David L√≥pez-Garc√≠a","Ferm√≠n Segovia","Javier Ram√≠rez","Juan Manuel G√≥rriz","Ra√∫l Serrano","David P√©rez","Iv√°n Maza","An√≠bal Ollero","Pol Paradell Sol√†","Albert Gili Selga","Jos√© Luis Dom√≠nguez-Garc√≠a","A. Romero","A. Berro","Roc√≠o Dom√≠nguez","Inmaculada Prieto"],"pdf_url":"https://arxiv.org/pdf/2410.19799v1.pdf","comment":null}]},"2024-10-15T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.12083v1","updated":"2024-10-15T22:01:31Z","published":"2024-10-15T22:01:31Z","title":"Drawing Planar Graphs and 1-Planar Graphs Using Cubic B√©zier Curves\n  with Bounded Curvature","summary":"  We study algorithms for drawing planar graphs and 1-planar graphs using cubic\nB\\'ezier curves with bounded curvature. We show that any n-vertex 1-planar\ngraph has a 1-planar RAC drawing using a single cubic B\\'ezier curve per edge,\nand this drawing can be computed in $O(n)$ time given a combinatorial 1-planar\ndrawing. We also show that any n-vertex planar graph G can be drawn in $O(n)$\ntime with a single cubic B\\'ezier curve per edge, in an $O(n)\\times O(n)$\nbounding box, such that the edges have ${\\Theta}(1/degree(v))$ angular\nresolution, for each $v \\in G$, and $O(\\sqrt{n})$ curvature.\n","authors":["David Eppstein","Michael T. Goodrich","Abraham M. Illickan"],"pdf_url":"https://arxiv.org/pdf/2410.12083v1.pdf","comment":"17 pages, 8 figures, Accepted and Presented at GD2024"},{"id":"http://arxiv.org/abs/2401.06748v3","updated":"2024-10-15T20:51:51Z","published":"2024-01-12T18:34:02Z","title":"Measure Theoretic Reeb Graphs and Reeb Spaces","summary":"  A Reeb graph is a graphical representation of a scalar function on a\ntopological space that encodes the topology of the level sets. A Reeb space is\na generalization of the Reeb graph to a multiparameter function. In this paper,\nwe propose novel constructions of Reeb graphs and Reeb spaces that incorporate\nthe use of a measure. Specifically, we introduce measure-theoretic Reeb graphs\nand Reeb spaces when the domain or the range is modeled as a metric measure\nspace (i.e.,~a metric space equipped with a measure). Our main goal is to\nenhance the robustness of the Reeb graph and Reeb space in representing the\ntopological features of a scalar field while accounting for the distribution of\nthe measure. We first introduce a Reeb graph with local smoothing and prove its\nstability with respect to the interleaving distance. We then prove the\nstability of a Reeb graph of a metric measure space with respect to the\nmeasure, defined using the distance to a measure or the kernel distance to a\nmeasure, respectively.\n","authors":["Qingsong Wang","Guanqun Ma","Raghavendra Sridharamurthy","Bei Wang"],"pdf_url":"https://arxiv.org/pdf/2401.06748v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11656v1","updated":"2024-10-15T14:42:22Z","published":"2024-10-15T14:42:22Z","title":"Fast and Robust Hexahedral Mesh Optimization via Augmented Lagrangian,\n  L-BFGS, and Line Search","summary":"  We present a new software package, ``HexOpt,'' for improving the quality of\nall-hexahedral (all-hex) meshes by maximizing the minimum mixed scaled\nJacobian-Jacobian energy functional, and projecting the surface points of the\nall-hex meshes onto the input triangular mesh. The proposed HexOpt method takes\nas input a surface triangular mesh and a volumetric all-hex mesh. A constrained\noptimization problem is formulated to improve mesh quality using a novel\nfunction that combines Jacobian and scaled Jacobian metrics which are rectified\nand scaled to quadratic measures, while preserving the surface geometry. This\noptimization problem is solved using the augmented Lagrangian (AL) method,\nwhere the Lagrangian terms enforce the constraint that surface points must\nremain on the triangular mesh. Specifically, corner points stay exactly at the\ncorner, edge points are confined to the edges, and face points are free to move\nacross the surface. To take the advantage of the Quasi-Newton method while\ntackling the high-dimensional variable problem, the\nLimited-Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm is employed. The\nstep size for each iteration is determined by the Armijo line search. Coupled\nwith smart Laplacian smoothing, HexOpt has demonstrated robustness and\nefficiency, successfully applying to 3D models and hex meshes generated by\ndifferent methods without requiring any manual intervention or parameter\nadjustment.\n","authors":["Hua Tong","Yongjie Jessica Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11430v1","updated":"2024-10-15T09:31:46Z","published":"2024-10-15T09:31:46Z","title":"pycvxset: A Python package for convex set manipulation","summary":"  This paper introduces pycvxset, a new Python package to manipulate and\nvisualize convex sets. We support polytopes and ellipsoids, and provide\nuser-friendly methods to perform a variety of set operations. For polytopes,\npycvxset supports the standard halfspace/vertex representation as well as the\nconstrained zonotope representation. The main advantage of constrained zonotope\nrepresentations over standard halfspace/vertex representations is that\nconstrained zonotopes admit closed-form expressions for several set operations.\npycvxset uses CVXPY to solve various convex programs arising in set operations,\nand uses pycddlib to perform vertex-halfspace enumeration. We demonstrate the\nuse of pycvxset in analyzing and controlling dynamical systems in Python.\npycvxset is available at https://github.com/merlresearch/pycvxset under the\nAGPL-3.0-or-later license, along with documentation and examples.\n","authors":["Abraham P. Vinod"],"pdf_url":"https://arxiv.org/pdf/2410.11430v1.pdf","comment":"8 pages, 10 figures"}],"Computer Science and Game Theory":[{"id":"http://arxiv.org/abs/2410.12884v1","updated":"2024-10-15T22:53:00Z","published":"2024-10-15T22:53:00Z","title":"Analyzing Incentives and Fairness in Ordered Weighted Average for\n  Facility Location Games","summary":"  Facility location games provide an abstract model of mechanism design. In\nsuch games, a mechanism takes a profile of $n$ single-peaked preferences over\nan interval as an input and determines the location of a facility on the\ninterval. In this paper, we restrict our attention to distance-based\nsingle-peaked preferences and focus on a well-known class of parameterized\nmechanisms called ordered weighted average methods, which is proposed by Yager\nin 1988 and contains several practical implementations such as the standard\naverage and the Olympic average. We comprehensively analyze their performance\nin terms of both incentives and fairness. More specifically, we provide\nnecessary and sufficient conditions on their parameters to achieve\nstrategy-proofness, non-obvious manipulability, individual fair share, and\nproportional fairness, respectively.\n","authors":["Kento Yoshida","Kei Kimura","Taiki Todo","Makoto Yokoo"],"pdf_url":"https://arxiv.org/pdf/2410.12884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09201v2","updated":"2024-10-15T22:37:02Z","published":"2024-10-11T19:07:08Z","title":"The Condorcet Dimension of Metric Spaces","summary":"  A Condorcet winning set is a set of candidates such that no other candidate\nis preferred by at least half the voters over all members of the set. The\nCondorcet dimension, which is the minimum cardinality of a Condorcet winning\nset, is known to be at most logarithmic in the number of candidates. We study\nthe case of elections where voters and candidates are located in a\n$2$-dimensional space with preferences based upon proximity voting. Our main\nresult is that the Condorcet dimension is at most $3$, under both the Manhattan\nnorm and the infinity norm, natural measures in electoral systems.\n","authors":["Alexandra Lassota","Adrian Vetta","Bernhard von Stengel"],"pdf_url":"https://arxiv.org/pdf/2410.09201v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2404.19256v2","updated":"2024-10-15T22:32:47Z","published":"2024-04-30T04:41:47Z","title":"AI, Pluralism, and (Social) Compensation","summary":"  One strategy in response to pluralistic values in a user population is to\npersonalize an AI system: if the AI can adapt to the specific values of each\nindividual, then we can potentially avoid many of the challenges of pluralism.\nUnfortunately, this approach creates a significant ethical issue: if there is\nan external measure of success for the human-AI team, then the adaptive AI\nsystem may develop strategies (sometimes deceptive) to compensate for its human\nteammate. This phenomenon can be viewed as a form of social compensation, where\nthe AI makes decisions based not on predefined goals but on its human partner's\ndeficiencies in relation to the team's performance objectives. We provide a\npractical ethical analysis of the conditions in which such compensation may\nnonetheless be justifiable.\n","authors":["Nandhini Swaminathan","David Danks"],"pdf_url":"https://arxiv.org/pdf/2404.19256v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.12039v1","updated":"2024-10-15T20:18:35Z","published":"2024-10-15T20:18:35Z","title":"EFX Orientations of Multigraphs","summary":"  We study the fair division of multigraphs with self-loops. In this setting,\nvertices represent agents and edges represent goods, and a good provides\npositive utility to an agent only if it is incident with the agent. Whereas\nprevious research has so far only considered simple graphs, we consider the\ngeneral setting of multigraphs, specifically focusing on the case in which each\nedge has equal utility to both incident agents, and edges have one of two\npossible utilities $\\alpha > \\beta \\geq 0$. In contrast with the case of simple\ngraphs for which bipartiteness implies the existence of an EFX orientation, we\nshow that deciding whether a symmetric multigraph $G$ of multiplicity $q \\geq\n2$ admits an EFX orientation is NP-complete even if $G$ is bipartite, $\\alpha >\nq\\beta$, and $G$ contains a structure called a non-trivial odd multitree.\nMoreover, we show that non-trivial odd multitrees are a forbidden structure in\nthe sense that even very simple non-trivial odd multitrees can fail to admit\nEFX orientations, and multigraphs that do not contain non-trivial odd\nmultitrees always admit EFX orientations.\n","authors":["Kevin Hsu"],"pdf_url":"https://arxiv.org/pdf/2410.12039v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2402.15037v2","updated":"2024-10-15T19:16:50Z","published":"2024-02-23T01:18:00Z","title":"Multi Agent Influence Diagrams for DeFi Governance","summary":"  Decentralized Finance (DeFi) governance models have become increasingly\ncomplex due to the involvement of numerous independent agents, each with their\nown incentives and strategies. To effectively analyze these systems, we propose\nusing Multi Agent Influence Diagrams (MAIDs) as a powerful tool for modeling\nand studying the strategic interactions within DeFi governance. MAIDs allow for\na comprehensive representation of the decision-making processes of various\nagents, capturing the influence of their actions on one another and on the\noverall governance outcomes. In this paper, we study a simple governance game\nthat approximates real governance protocols and compute the Nash equilibria\nusing MAIDs. We further outline the structure of a MAID in MakerDAO.\n","authors":["Abhimanyu Nag","Samrat Gupta","Sudipan Sinha","Arka Datta"],"pdf_url":"https://arxiv.org/pdf/2402.15037v2.pdf","comment":"Updated paper"},{"id":"http://arxiv.org/abs/2407.07316v2","updated":"2024-10-15T17:59:28Z","published":"2024-07-10T02:25:27Z","title":"Fast Revenue Maximization","summary":"  Problem definition: We study a data-driven pricing problem in which a seller\noffers a price for a single item based on demand observed at a small number of\nhistorical prices. Our goal is to derive precise evaluation procedures of the\nvalue of the historical information gathered by the seller, along with\nprescriptions for more efficient price experimentation. Methodology/results:\nOur main methodological result is an exact characterization of the maximin\nratio (defined as the worst-case revenue garnered by a seller who only relies\non past data divided by the optimal revenue achievable with full knowledge of\nthe distribution of values). This result allows to measure the value of any\nhistorical data consisting of prices and corresponding conversion rates. We\nleverage this central reduction to provide new insights about price\nexperimentation. Managerial implications: Motivated by practical constraints\nthat impede the seller from changing prices abruptly, we first illustrate our\nframework by evaluating the value of local information and show that the mere\nsign of the gradient of the revenue curve at a single point can provide\nsignificant information to the seller. We then showcase how our framework can\nbe used to run efficient price experiments. On the one hand, we develop a\nmethod to select the next price experiment that the seller should use to\nmaximize the future robust performance. On the other hand, we demonstrate that\nour result allows to considerably reduce the number of price experiments needed\nto reach preset revenue guarantees through dynamic pricing algorithms.\n","authors":["Achraf Bahamou","Omar Besbes","Omar Mouchtaki"],"pdf_url":"https://arxiv.org/pdf/2407.07316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11798v1","updated":"2024-10-15T17:20:57Z","published":"2024-10-15T17:20:57Z","title":"Majorized Bayesian Persuasion and Fair Selection","summary":"  We address the fundamental problem of selection under uncertainty by modeling\nit from the perspective of Bayesian persuasion. In our model, a decision maker\nwith imperfect information always selects the option with the highest expected\nvalue. We seek to achieve fairness among the options by revealing additional\ninformation to the decision maker and hence influencing its subsequent\nselection. To measure fairness, we adopt the notion of majorization, aiming at\nsimultaneously approximately maximizing all symmetric, monotone, concave\nfunctions over the utilities of the options. As our main result, we design a\nnovel information revelation policy that achieves a logarithmic-approximation\nto majorization in polynomial time. On the other hand, no policy, regardless of\nits running time, can achieve a constant-approximation to majorization. Our\nwork is the first non-trivial majorization result in the Bayesian persuasion\nliterature with multi-dimensional information sets.\n","authors":["Siddhartha Banerjee","Kamesh Munagala","Yiheng Shen","Kangning Wang"],"pdf_url":"https://arxiv.org/pdf/2410.11798v1.pdf","comment":"Conference version of this paper appears in SODA 2025"},{"id":"http://arxiv.org/abs/2410.11738v1","updated":"2024-10-15T16:14:11Z","published":"2024-10-15T16:14:11Z","title":"The Simplicity of Optimal Dynamic Mechanisms","summary":"  A fundamental economic question is that of designing revenue-maximizing\nmechanisms in dynamic environments. This paper considers a simple yet\ncompelling market model to tackle this question, where forward-looking buyers\narrive at the market over discrete time periods, and a monopolistic seller is\nendowed with a limited supply of a single good. In the case of i.i.d. and\nregular valuations for the buyers, Board and Skrzypacz (2016) characterized the\noptimal mechanism and proved the optimality of posted prices in the\ncontinuous-time limit. Our main result considers the limit case of a continuum\nof buyers, establishing that for arbitrary independent buyers' valuations,\nposted prices and capacity rationing can implement the optimal anonymous\nmechanism. Our result departs from the literature in three ways: It does not\nmake any regularity assumptions, it considers the case of general, not\nnecessarily i.i.d., arrivals, and finally, not only posted prices but also\ncapacity rationing takes part in the optimal mechanism. Additionally, if supply\nis unlimited, we show that the rationing effect vanishes, and the optimal\nmechanism can be implemented using posted prices only, \\`a la Board (2008).\n","authors":["Jose Correa","Andres Cristi","Laura Vargas Koch"],"pdf_url":"https://arxiv.org/pdf/2410.11738v1.pdf","comment":"none"},{"id":"http://arxiv.org/abs/2410.11683v1","updated":"2024-10-15T15:20:40Z","published":"2024-10-15T15:20:40Z","title":"Optimal Mediation Mechanisms in Bilateral Trade","summary":"  Consider a bilateral trade scenario where a seller seeks to sell an item to a\nbuyer through a trusted mediator. The item's quality is the seller's private\ninformation, and the buyer's valuation of the item depends on both the quality\nand the buyer's type. The mediator, who is uninformed about the private\ninformation of both the seller and buyer, aims to design a mechanism that\nelicits and reveals information to facilitate communication between two agents.\nThe mediator can also charge a fee for providing such services.\n  In this work, we study the problem of designing mechanisms that maximize\nrevenue for the mediator. We formulate this mechanism design problem as an\noptimization problem that involves non-linear constraints. Interestingly, under\nthe monotone hazard rate assumption, we can bypass this issue by considering a\nrelaxed problem and showing that the solution to the relaxed problem remains\noptimal to the original one. In optimal mechanisms, the mediator directly\nrecommends whether to trade after eliciting the agents' types. The mediator\nprivately offers a price to each agent if a trade is recommended. The optimal\nmechanism adopts a threshold information structure, i.e., it only reveals to\nthe agent whether the other agent's type exceeds a certain threshold. The\noptimal payment function of buyer is monotone decreasing to their type, which\ndiffers from most existing works. Finally, we discuss some interesting\nobservations revealed by the optimal mechanism.\n","authors":["Zhikang Fan","Weiran Shen"],"pdf_url":"https://arxiv.org/pdf/2410.11683v1.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.12051v1","updated":"2024-10-15T20:41:10Z","published":"2024-10-15T20:41:10Z","title":"Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D\n  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience","summary":"  In this paper, we introduce a novel system designed to enhance customer\nservice in the financial and retail sectors through a context-aware 3D virtual\nagent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our\napproach focuses on enabling data-driven and empathetic interactions that\nensure customer satisfaction by introducing situational awareness of the\nphysical location, personalized interactions based on customer profiles, and\nrigorous privacy and security standards. We discuss our design considerations\ncritical for deployment in real-world customer service environments, addressing\nchallenges in user data management and sensitive information handling. We also\noutline the system architecture and key features unique to banking and retail\nenvironments. Our work demonstrates the potential of integrating MR and VLMs in\nservice industries, offering practical insights in customer service delivery\nwhile maintaining high standards of security and personalization.\n","authors":["Cindy Xu","Mengyu Chen","Pranav Deshpande","Elvir Azanli","Runqing Yang","Joseph Ligman"],"pdf_url":"https://arxiv.org/pdf/2410.12051v1.pdf","comment":"to appear at 1st Workshop on Intelligent XR: Harnessing AI for\n  Next-Generation XR User Experiences at International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2024"},{"id":"http://arxiv.org/abs/2410.11790v1","updated":"2024-10-15T17:14:26Z","published":"2024-10-15T17:14:26Z","title":"End-to-End Mathematical Modeling of Stress Communication Between Plants","summary":"  Molecular Communication (MC) is an important communication paradigm found in\nnature. Odor-based Molecular Communication (OMC) is a specific type of MC with\npromising potential and a wide range of applications. In this paper, we examine\nOMC communication between plants in the context of stress communication.\nSpecifically, we explore how plants use Biological Volatile Organic Compounds\n(BVOCs) to convey information about the stresses they are experiencing to\nneighboring plants. We constructed an end-to-end mathematical model that\ndiscovers the underlying physical and biological phenomena affecting stress\ncommunication. To the best of our knowledge, this is the first study to model\nthis end-to-end stress communication. We numerically analyzed our system under\ndifferent scenarios using MATLAB. Using experimental data from the literature,\nwe demonstrated that continuous gene regulation can approximate BVOC emissions\nin plants under different stress conditions. Consequently, we applied this\nmodel to these stressors and plants to accurately approximate BVOC emissions.\nWe also investigated a modulation method that plants use to send their\nmessages, namely Ratio Shift Keying. Upon analyzing this method, we found that\nit benefits plants by both enabling a multiple access channel and preventing\ncompetitor plants from obtaining the information.\n","authors":["Ahmet B. Kilic","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2410.11790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10271v2","updated":"2024-10-15T12:06:07Z","published":"2024-05-16T17:27:41Z","title":"Adaptive Hybrid Model Pruning in Federated Learning through Loss\n  Exploration","summary":"  The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability.\n","authors":["Christian Intern√≤","Elena Raponi","Niki van Stein","Thomas B√§ck","Markus Olhofer","Yaochu Jin","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2405.10271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11919v1","updated":"2024-10-15T11:54:05Z","published":"2024-10-15T11:54:05Z","title":"Noise-robust chemical reaction networks training artificial neural\n  networks","summary":"  Artificial neural networks (NNs) can be implemented using chemical reaction\nnetworks (CRNs), where the concentrations of species act as inputs and outputs.\nIn such biochemical computing, noise-robust computing is crucial due to the\nintrinsic and extrinsic noise present in chemical reactions. Previously\nsuggested CRNs for feed-forward networks often utilized the rectified linear\nunit (ReLU) or discrete activation functions. However, one concern in this case\nis the discontinuities of the derivatives of those non-smooth functions, which\ncan cause significant noise disruption during backpropagation. In this study,\nwe propose a CRN that performs both feed-forward and training processes using\nsmooth activation functions to avoid discontinuities in the backpropagation.\nAll reactions occur in a single pot, and the reactions for training are\nbimolecular. Our case studies on XOR, Iris, MNIST datasets, and a non-linear\nregression model demonstrate that computation via the CRN (i) maintains\naccuracy despite noise in the reaction rates and the concentration of species\nand (ii) is insensitive to the choice of the running time and the magnitude of\nthe noise in comparison to NNs with a non-smooth activation function. This work\npresents a noise-robust CRN for full NN computation, including backpropagation,\npaving the way for more stable and efficient biochemical computing systems.\n","authors":["Sunghwa Kang","Jinsu Kim"],"pdf_url":"https://arxiv.org/pdf/2410.11919v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2410.22352v1","updated":"2024-10-15T10:08:15Z","published":"2024-10-15T10:08:15Z","title":"Neuromorphic Programming: Emerging Directions for Brain-Inspired\n  Hardware","summary":"  The value of brain-inspired neuromorphic computers critically depends on our\nability to program them for relevant tasks. Currently, neuromorphic hardware\noften relies on machine learning methods adapted from deep learning. However,\nneuromorphic computers have potential far beyond deep learning if we can only\nharness their energy efficiency and full computational power. Neuromorphic\nprogramming will necessarily be different from conventional programming,\nrequiring a paradigm shift in how we think about programming. This paper\npresents a conceptual analysis of programming within the context of\nneuromorphic computing, challenging conventional paradigms and proposing a\nframework that aligns more closely with the physical intricacies of these\nsystems. Our analysis revolves around five characteristics that are fundamental\nto neuromorphic programming and provides a basis for comparison to contemporary\nprogramming methods and languages. By studying past approaches, we contribute a\nframework that advocates for underutilized techniques and calls for richer\nabstractions to effectively instrument the new hardware class.\n","authors":["Steven Abreu","Jens E. Pedersen"],"pdf_url":"https://arxiv.org/pdf/2410.22352v1.pdf","comment":"Accepted to International Conference on Neuromorphic Systems (ICONS)\n  2024. arXiv admin note: substantial text overlap with arXiv:2310.18260"},{"id":"http://arxiv.org/abs/2309.06763v4","updated":"2024-10-15T07:56:13Z","published":"2023-09-13T07:19:32Z","title":"Solving rescheduling problems in heterogeneous urban railway networks\n  using hybrid quantum-classical approach","summary":"  We address the applicability of hybrid quantum-classical heuristics for\npractical railway rescheduling management problems. We build an integer linear\nmodel for the given problem and solve it with D-Wave's quantum-classical hybrid\nsolver as well as with CPLEX for comparison. The proposed approach is\ndemonstrated on a real-life heterogeneous urban network in Poland, including\nboth single- and multi-track segments and covers all the requirements posed by\nthe operator of the network. The computational results demonstrate the\nreadiness for application and benefits of quantum-classical hybrid solvers in\nthe realistic railway scenario: they yield acceptable solutions on time, which\nis a critical requirement in a rescheduling situation. At the same time, the\nsolutions that were obtained were feasible. Moreover, though they are\nprobabilistic (heuristics) they offer a valid alternative by returning a range\nof possible solutions the dispatcher can choose from. And, most importantly,\nthey outperform classical solvers in some cases.\n","authors":["M√°ty√°s Koniorczyk","Krzysztof Krawiec","Ludmila Botelho","Nikola Be≈°inoviƒá","Krzysztof Domino"],"pdf_url":"https://arxiv.org/pdf/2309.06763v4.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.11682v1","updated":"2024-10-15T15:19:58Z","published":"2024-10-15T15:19:58Z","title":"SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian\n  Surfel Head Avatars","summary":"  Recent advancements in head avatar rendering using Gaussian primitives have\nachieved significantly high-fidelity results. Although precise head geometry is\ncrucial for applications like mesh reconstruction and relighting, current\nmethods struggle to capture intricate geometric details and render unseen poses\ndue to their reliance on similarity transformations, which cannot handle\nstretch and shear transforms essential for detailed deformations of geometry.\nTo address this, we propose SurFhead, a novel method that reconstructs riggable\nhead geometry from RGB videos using 2D Gaussian surfels, which offer\nwell-defined geometric properties, such as precise depth from fixed ray\nintersections and normals derived from their surface orientation, making them\nadvantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of\nboth normals and images, even in extreme poses, by leveraging classical\nmesh-based deformation transfer and affine transformation interpolation.\nSurFhead introduces precise geometric deformation and blends surfels through\npolar decomposition of transformations, including those affecting normals. Our\nkey contribution lies in bridging classical graphics techniques, such as\nmesh-based deformation, with modern Gaussian primitives, achieving\nstate-of-the-art geometry reconstruction and rendering quality. Unlike previous\navatar rendering approaches, SurFhead enables efficient reconstruction driven\nby Gaussian primitives while preserving high-fidelity geometry.\n","authors":["Jaeseong Lee","Taewoong Kang","Marcel C. B√ºhler","Min-Jung Kim","Sungwon Hwang","Junha Hyung","Hyojin Jang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2410.11682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11625v1","updated":"2024-10-15T14:14:06Z","published":"2024-10-15T14:14:06Z","title":"Fast Local Neural Regression for Low-Cost, Path Traced Lambertian Global\n  Illumination","summary":"  Despite recent advances in hardware acceleration of ray tracing, real-time\nray budgets remain stubbornly limited at a handful of samples per pixel (spp)\non commodity hardware, placing the onus on denoising algorithms to achieve high\nvisual quality for path traced global illumination. Neural network-based\nsolutions give excellent result quality at the cost of increased execution time\nrelative to hand-engineered methods, making them less suitable for deployment\non resource-constrained systems. We therefore propose incorporating a neural\nnetwork into a computationally-efficient local linear model-based denoiser, and\ndemonstrate faithful single-frame reconstruction of global illumination for\nLambertian scenes at very low sample counts (1spp) and for low computational\ncost. Other contributions include improving the quality and performance of\nlocal linear model-based denoising through a simplified mathematical treatment,\nand demonstration of the surprising usefulness of ambient occlusion as a guide\nchannel. We also show how our technique is straightforwardly extensible to\njoint denoising and upsampling of path traced renders with reference to\nlow-cost, rasterized guide channels.\n","authors":["Arturo Salmi","Szabolcs Cs√©falvay","James Imber"],"pdf_url":"https://arxiv.org/pdf/2410.11625v1.pdf","comment":"11 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.11520v1","updated":"2024-10-15T11:46:33Z","published":"2024-10-15T11:46:33Z","title":"Look Ma, no markers: holistic performance capture without the hassle","summary":"  We tackle the problem of highly-accurate, holistic performance capture for\nthe face, body and hands simultaneously. Motion-capture technologies used in\nfilm and game production typically focus only on face, body or hand capture\nindependently, involve complex and expensive hardware and a high degree of\nmanual intervention from skilled operators. While machine-learning-based\napproaches exist to overcome these problems, they usually only support a single\ncamera, often operate on a single part of the body, do not produce precise\nworld-space results, and rarely generalize outside specific contexts. In this\nwork, we introduce the first technique for marker-free, high-quality\nreconstruction of the complete human body, including eyes and tongue, without\nrequiring any calibration, manual intervention or custom hardware. Our approach\nproduces stable world-space results from arbitrary camera rigs as well as\nsupporting varied capture environments and clothing. We achieve this through a\nhybrid approach that leverages machine learning models trained exclusively on\nsynthetic data and powerful parametric models of human shape and motion. We\nevaluate our method on a number of body, face and hand reconstruction\nbenchmarks and demonstrate state-of-the-art results that generalize on diverse\ndatasets.\n","authors":["Charlie Hewitt","Fatemeh Saleh","Sadegh Aliakbarian","Lohit Petikam","Shideh Rezaeifar","Louis Florentin","Zafiirah Hosenie","Thomas J Cashman","Julien Valentin","Darren Cosker","Tadas Baltrusaitis"],"pdf_url":"https://arxiv.org/pdf/2410.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10719v2","updated":"2024-10-15T09:34:22Z","published":"2024-10-14T17:00:53Z","title":"4-LEGS: 4D Language Embedded Gaussian Splatting","summary":"  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. This enables an interactive interface where the user can\nspatiotemporally localize events in the video from text prompts. We demonstrate\nour system on public 3D video datasets of people and animals performing various\nactions.\n","authors":["Gal Fiebelman","Tamir Cohen","Ayellet Morgenstern","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2410.10719v2.pdf","comment":"Project webpage: https://tau-vailab.github.io/4-LEGS/"},{"id":"http://arxiv.org/abs/2410.11419v1","updated":"2024-10-15T09:11:30Z","published":"2024-10-15T09:11:30Z","title":"GS^3: Efficient Relighting with Triple Gaussian Splatting","summary":"  We present a spatial and angular Gaussian based representation and a triple\nsplatting process, for real-time, high-quality novel lighting-and-view\nsynthesis from multi-view point-lit input images. To describe complex\nappearance, we employ a Lambertian plus a mixture of angular Gaussians as an\neffective reflectance function for each spatial Gaussian. To generate\nself-shadow, we splat all spatial Gaussians towards the light source to obtain\nshadow values, which are further refined by a small multi-layer perceptron. To\ncompensate for other effects like global illumination, another network is\ntrained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness\nof our representation is demonstrated on 30 samples with a wide variation in\ngeometry (from solid to fluffy) and appearance (from translucent to\nanisotropic), as well as using different forms of input data, including\nrendered images of synthetic/reconstructed objects, photographs captured with a\nhandheld camera and a flash, or from a professional lightstage. We achieve a\ntraining time of 40-70 minutes and a rendering speed of 90 fps on a single\ncommodity GPU. Our results compare favorably with state-of-the-art techniques\nin terms of quality/performance. Our code and data are publicly available at\nhttps://GSrelight.github.io/.\n","authors":["Zoubin Bi","Yixin Zeng","Chong Zeng","Fan Pei","Xiang Feng","Kun Zhou","Hongzhi Wu"],"pdf_url":"https://arxiv.org/pdf/2410.11419v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024. Project page:\n  https://gsrelight.github.io/"},{"id":"http://arxiv.org/abs/2410.10350v2","updated":"2024-10-15T07:36:06Z","published":"2024-10-14T10:09:44Z","title":"On Representation of 3D Rotation in the Context of Deep Learning","summary":"  This paper investigates various methods of representing 3D rotations and\ntheir impact on the learning process of deep neural networks. We evaluated the\nperformance of ResNet18 networks for 3D rotation estimation using several\nrotation representations and loss functions on both synthetic and real data.\nThe real datasets contained 3D scans of industrial bins, while the synthetic\ndatasets included views of a simple asymmetric object rendered under different\nrotations. On synthetic data, we also assessed the effects of different\nrotation distributions within the training and test sets, as well as the impact\nof the object's texture. In line with previous research, we found that networks\nusing the continuous 5D and 6D representations performed better than the\ndiscontinuous ones.\n","authors":["Vikt√≥ria Pravdov√°","Luk√°≈° Gajdo≈°ech","Hassan Ali","Viktor Kocur"],"pdf_url":"https://arxiv.org/pdf/2410.10350v2.pdf","comment":"Accepted at International Conference on Computer Vision and Graphics\n  ICCVG 2024. The proceedings of the conference will be published in Lecture\n  Notes in Networks and Systems (LNNS), Springer"},{"id":"http://arxiv.org/abs/2409.12771v2","updated":"2024-10-15T07:35:49Z","published":"2024-09-19T13:38:04Z","title":"Spectral-GS: Taming 3D Gaussian Splatting with Spectral Entropy","summary":"  Recently, 3D Gaussian Splatting (3D-GS) has achieved impressive results in\nnovel view synthesis, demonstrating high fidelity and efficiency. However, it\neasily exhibits needle-like artifacts, especially when increasing the sampling\nrate. Mip-Splatting tries to remove these artifacts with a 3D smoothing filter\nfor frequency constraints and a 2D Mip filter for approximated supersampling.\nUnfortunately, it tends to produce over-blurred results, and sometimes\nneedle-like Gaussians still persist. Our spectral analysis of the covariance\nmatrix during optimization and densification reveals that current 3D-GS lacks\nshape awareness, relying instead on spectral radius and view positional\ngradients to determine splitting. As a result, needle-like Gaussians with small\npositional gradients and low spectral entropy fail to split and overfit\nhigh-frequency details. Furthermore, both the filters used in 3D-GS and\nMip-Splatting reduce the spectral entropy and increase the condition number\nduring zooming in to synthesize novel view, causing view inconsistencies and\nmore pronounced artifacts. Our Spectral-GS, based on spectral analysis,\nintroduces 3D shape-aware splitting and 2D view-consistent filtering\nstrategies, effectively addressing these issues, enhancing 3D-GS's capability\nto represent high-frequency details without noticeable artifacts, and achieving\nhigh-quality photorealistic rendering.\n","authors":["Letian Huang","Jie Guo","Jialin Dan","Ruoyu Fu","Shujie Wang","Yuanqi Li","Yanwen Guo"],"pdf_url":"https://arxiv.org/pdf/2409.12771v2.pdf","comment":null}],"Multiagent Systems":[{"id":"http://arxiv.org/abs/2410.12884v1","updated":"2024-10-15T22:53:00Z","published":"2024-10-15T22:53:00Z","title":"Analyzing Incentives and Fairness in Ordered Weighted Average for\n  Facility Location Games","summary":"  Facility location games provide an abstract model of mechanism design. In\nsuch games, a mechanism takes a profile of $n$ single-peaked preferences over\nan interval as an input and determines the location of a facility on the\ninterval. In this paper, we restrict our attention to distance-based\nsingle-peaked preferences and focus on a well-known class of parameterized\nmechanisms called ordered weighted average methods, which is proposed by Yager\nin 1988 and contains several practical implementations such as the standard\naverage and the Olympic average. We comprehensively analyze their performance\nin terms of both incentives and fairness. More specifically, we provide\nnecessary and sufficient conditions on their parameters to achieve\nstrategy-proofness, non-obvious manipulability, individual fair share, and\nproportional fairness, respectively.\n","authors":["Kento Yoshida","Kei Kimura","Taiki Todo","Makoto Yokoo"],"pdf_url":"https://arxiv.org/pdf/2410.12884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09201v2","updated":"2024-10-15T22:37:02Z","published":"2024-10-11T19:07:08Z","title":"The Condorcet Dimension of Metric Spaces","summary":"  A Condorcet winning set is a set of candidates such that no other candidate\nis preferred by at least half the voters over all members of the set. The\nCondorcet dimension, which is the minimum cardinality of a Condorcet winning\nset, is known to be at most logarithmic in the number of candidates. We study\nthe case of elections where voters and candidates are located in a\n$2$-dimensional space with preferences based upon proximity voting. Our main\nresult is that the Condorcet dimension is at most $3$, under both the Manhattan\nnorm and the infinity norm, natural measures in electoral systems.\n","authors":["Alexandra Lassota","Adrian Vetta","Bernhard von Stengel"],"pdf_url":"https://arxiv.org/pdf/2410.09201v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2404.19256v2","updated":"2024-10-15T22:32:47Z","published":"2024-04-30T04:41:47Z","title":"AI, Pluralism, and (Social) Compensation","summary":"  One strategy in response to pluralistic values in a user population is to\npersonalize an AI system: if the AI can adapt to the specific values of each\nindividual, then we can potentially avoid many of the challenges of pluralism.\nUnfortunately, this approach creates a significant ethical issue: if there is\nan external measure of success for the human-AI team, then the adaptive AI\nsystem may develop strategies (sometimes deceptive) to compensate for its human\nteammate. This phenomenon can be viewed as a form of social compensation, where\nthe AI makes decisions based not on predefined goals but on its human partner's\ndeficiencies in relation to the team's performance objectives. We provide a\npractical ethical analysis of the conditions in which such compensation may\nnonetheless be justifiable.\n","authors":["Nandhini Swaminathan","David Danks"],"pdf_url":"https://arxiv.org/pdf/2404.19256v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.12062v1","updated":"2024-10-15T20:59:47Z","published":"2024-10-15T20:59:47Z","title":"MFC-EQ: Mean-Field Control with Envelope Q-Learning for Moving\n  Decentralized Agents in Formation","summary":"  We study a decentralized version of Moving Agents in Formation (MAiF), a\nvariant of Multi-Agent Path Finding aiming to plan collision-free paths for\nmultiple agents with the dual objectives of reaching their goals quickly while\nmaintaining a desired formation. The agents must balance these objectives under\nconditions of partial observation and limited communication. The formation\nmaintenance depends on the joint state of all agents, whose dimensionality\nincreases exponentially with the number of agents, rendering the learning\nprocess intractable. Additionally, learning a single policy that can\naccommodate different linear preferences for these two objectives presents a\nsignificant challenge. In this paper, we propose Mean-Field Control with\nEnvelop $Q$-learning (MFC-EQ), a scalable and adaptable learning framework for\nthis bi-objective multi-agent problem. We approximate the dynamics of all\nagents using mean-field theory while learning a universal preference-agnostic\npolicy through envelop $Q$-learning. Our empirical evaluation of MFC-EQ across\nnumerous instances shows that it outperforms state-of-the-art centralized MAiF\nbaselines. Furthermore, MFC-EQ effectively handles more complex scenarios where\nthe desired formation changes dynamically -- a challenge that existing MAiF\nplanners cannot address.\n","authors":["Qiushi Lin","Hang Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12062v1.pdf","comment":"Accepted to IROS 2024"},{"id":"http://arxiv.org/abs/2406.00252v4","updated":"2024-10-15T20:11:42Z","published":"2024-06-01T01:17:25Z","title":"Towards Rationality in Language and Multimodal Agents: A Survey","summary":"  Rationality is the quality of being guided by reason, characterized by\ndecision-making that aligns with evidence and logical principles. It plays a\ncrucial role in reliable problem-solving by ensuring well-grounded and\nconsistent solutions. While large language models (LLMs) have made significant\nprogress in generating human-like text, they still exhibit limitations such as\nbounded knowledge space and inconsistent outputs. In response, recent efforts\nhave shifted toward developing multimodal and multi-agent systems, as well as\nintegrating modules like external tools, programming codes, symbolic reasoners,\nutility function, and conformal risk controls rather than relying solely on a\nsingle LLM for decision-making. This paper surveys the state-of-the-art\nadvancements in language and multimodal agents, evaluates how they contribute\nto make intelligent agents more rational, and identifies open challenges and\nfuture research directions. We maintain an open repository at\nhttps://github.com/bowen-upenn/Agent_Rationality.\n","authors":["Bowen Jiang","Yangxinyu Xie","Xiaomeng Wang","Yuan Yuan","Zhuoqun Hao","Xinyi Bai","Weijie J. Su","Camillo J. Taylor","Tanwi Mallick"],"pdf_url":"https://arxiv.org/pdf/2406.00252v4.pdf","comment":"We maintain an open repository at\n  https://github.com/bowen-upenn/Agent_Rationality"},{"id":"http://arxiv.org/abs/2410.11782v1","updated":"2024-10-15T17:01:21Z","published":"2024-10-15T17:01:21Z","title":"G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks","summary":"  Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.\n","authors":["Guibin Zhang","Yanwei Yue","Xiangguo Sun","Guancheng Wan","Miao Yu","Junfeng Fang","Kun Wang","Dawei Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.11782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18676v2","updated":"2024-10-15T16:23:51Z","published":"2024-09-27T12:03:15Z","title":"Toward Universal and Interpretable World Models for Open-ended Learning\n  Agents","summary":"  We introduce a generic, compositional and interpretable class of generative\nworld models that supports open-ended learning agents. This is a sparse class\nof Bayesian networks capable of approximating a broad range of stochastic\nprocesses, which provide agents with the ability to learn world models in a\nmanner that may be both interpretable and computationally scalable. This\napproach integrating Bayesian structure learning and intrinsically motivated\n(model-based) planning enables agents to actively develop and refine their\nworld models, which may lead to developmental learning and more robust,\nadaptive behavior.\n","authors":["Lancelot Da Costa"],"pdf_url":"https://arxiv.org/pdf/2409.18676v2.pdf","comment":"4 pages including appendix, 6 including appendix and references; 2\n  figures"},{"id":"http://arxiv.org/abs/2405.14205v2","updated":"2024-10-15T13:58:17Z","published":"2024-05-23T06:03:19Z","title":"Agent Planning with World Knowledge Model","summary":"  Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.\n","authors":["Shuofei Qiao","Runnan Fang","Ningyu Zhang","Yuqi Zhu","Xiang Chen","Shumin Deng","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.14205v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.11416v1","updated":"2024-10-15T09:03:58Z","published":"2024-10-15T09:03:58Z","title":"Agent-Based Modelling of Older Adult Needs for Autonomous\n  Mobility-on-Demand: A Case Study in Winnipeg, Canada","summary":"  As the populations continue to age across many nations, ensuring accessible\nand efficient transportation options for older adults has become an\nincreasingly important concern. Autonomous Mobility-on-Demand (AMoD) systems\nhave emerged as a potential solution to address the needs faced by older adults\nin their daily mobility. However, estimation of older adult mobility needs, and\nhow they vary over space and time, is crucial for effective planning and\nimplementation of such service, and conventional four-step approaches lack the\ngranularity to fully account for these needs. To address this challenge, we\npropose an agent-based model of older adults mobility demand in Winnipeg,\nCanada. The model is built for 2022 using primarily open data, and is\nimplemented in the Multi-Agent Transport Simulation (MATSim) toolkit. After\ncalibration to accurately reproduce observed travel behaviors, a new AMoD\nservice is tested in simulation and its potential adoption among Winnipeg older\nadults is explored. The model can help policy makers to estimate the needs of\nthe elderly populations for door-to-door transportation and can guide the\ndesign of AMoD transport systems.\n","authors":["Manon Pr√©dhumeau","Ed Manley"],"pdf_url":"https://arxiv.org/pdf/2410.11416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06503v2","updated":"2024-10-15T02:18:35Z","published":"2024-08-12T21:38:40Z","title":"Enhancing Heterogeneous Multi-Agent Cooperation in Decentralized MARL\n  via GNN-driven Intrinsic Rewards","summary":"  Multi-agent Reinforcement Learning (MARL) is emerging as a key framework for\nvarious sequential decision-making and control tasks. Unlike their single-agent\ncounterparts, multi-agent systems necessitate successful cooperation among the\nagents. The deployment of these systems in real-world scenarios often requires\ndecentralized training, a diverse set of agents, and learning from infrequent\nenvironmental reward signals. These challenges become more pronounced under\npartial observability and the lack of prior knowledge about agent\nheterogeneity. While notable studies use intrinsic motivation (IM) to address\nreward sparsity or cooperation in decentralized settings, those dealing with\nheterogeneity typically assume centralized training, parameter sharing, and\nagent indexing. To overcome these limitations, we propose the CoHet algorithm,\nwhich utilizes a novel Graph Neural Network (GNN) based intrinsic motivation to\nfacilitate the learning of heterogeneous agent policies in decentralized\nsettings, under the challenges of partial observability and reward sparsity.\nEvaluation of CoHet in the Multi-agent Particle Environment (MPE) and\nVectorized Multi-Agent Simulator (VMAS) benchmarks demonstrates superior\nperformance compared to the state-of-the-art in a range of cooperative\nmulti-agent scenarios. Our research is supplemented by an analysis of the\nimpact of the agent dynamics model on the intrinsic motivation module, insights\ninto the performance of different CoHet variants, and its robustness to an\nincreasing number of heterogeneous agents.\n","authors":["Jahir Sadik Monon","Deeparghya Dutta Barua","Md. Mosaddek Khan"],"pdf_url":"https://arxiv.org/pdf/2408.06503v2.pdf","comment":"9 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12051v1","updated":"2024-10-15T20:41:10Z","published":"2024-10-15T20:41:10Z","title":"Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D\n  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience","summary":"  In this paper, we introduce a novel system designed to enhance customer\nservice in the financial and retail sectors through a context-aware 3D virtual\nagent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our\napproach focuses on enabling data-driven and empathetic interactions that\nensure customer satisfaction by introducing situational awareness of the\nphysical location, personalized interactions based on customer profiles, and\nrigorous privacy and security standards. We discuss our design considerations\ncritical for deployment in real-world customer service environments, addressing\nchallenges in user data management and sensitive information handling. We also\noutline the system architecture and key features unique to banking and retail\nenvironments. Our work demonstrates the potential of integrating MR and VLMs in\nservice industries, offering practical insights in customer service delivery\nwhile maintaining high standards of security and personalization.\n","authors":["Cindy Xu","Mengyu Chen","Pranav Deshpande","Elvir Azanli","Runqing Yang","Joseph Ligman"],"pdf_url":"https://arxiv.org/pdf/2410.12051v1.pdf","comment":"to appear at 1st Workshop on Intelligent XR: Harnessing AI for\n  Next-Generation XR User Experiences at International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2024"},{"id":"http://arxiv.org/abs/2410.11817v1","updated":"2024-10-15T17:46:31Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12831v2","updated":"2024-10-15T17:31:56Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing is a cornerstone of digital media, from entertainment and\neducation to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistent edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal Video Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, we designed test-time\nediting adaptation to adapt a pre-trained image editing model for improving\nconsistency between potential editing directions and the text instruction, and\nadapt masked latent variables for precise local control. Furthermore, to\nmaintain global consistency over the video sequence, we introduce\nspatiotemporal adaptation that recursively gather consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potential\nfor advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.11779v1","updated":"2024-10-15T16:57:44Z","published":"2024-10-15T16:57:44Z","title":"MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation","summary":"  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.\n","authors":["Chenxi Wang","Xiang Chen","Ningyu Zhang","Bozhong Tian","Haoming Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11779v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2410.11701v1","updated":"2024-10-15T15:39:37Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v1.pdf","comment":"9 pages, 13 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.11582v1","updated":"2024-10-15T13:15:50Z","published":"2024-10-15T13:15:50Z","title":"On-the-fly Modulation for Balanced Multimodal Learning","summary":"  Multimodal learning is expected to boost model performance by integrating\ninformation from different modalities. However, its potential is not fully\nexploited because the widely-used joint training strategy, which has a uniform\nobjective for all modalities, leads to imbalanced and under-optimized uni-modal\nrepresentations. Specifically, we point out that there often exists modality\nwith more discriminative information, e.g., vision of playing football and\nsound of blowing wind. They could dominate the joint training process,\nresulting in other modalities being significantly under-optimized. To alleviate\nthis problem, we first analyze the under-optimized phenomenon from both the\nfeed-forward and the back-propagation stages during optimization. Then,\nOn-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)\nstrategies are proposed to modulate the optimization of each modality, by\nmonitoring the discriminative discrepancy between modalities during training.\nConcretely, OPM weakens the influence of the dominant modality by dropping its\nfeature with dynamical probability in the feed-forward stage, while OGM\nmitigates its gradient in the back-propagation stage. In experiments, our\nmethods demonstrate considerable improvement across a variety of multimodal\ntasks. These simple yet effective strategies not only enhance performance in\nvanilla and task-oriented multimodal models, but also in more complex\nmultimodal tasks, showcasing their effectiveness and flexibility. The source\ncode is available at \\url{https://github.com/GeWu-Lab/BML_TPAMI2024}.\n","authors":["Yake Wei","Di Hu","Henghui Du","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.11582v1.pdf","comment":"Accepted by T-PAMI 2024"},{"id":"http://arxiv.org/abs/2404.01336v3","updated":"2024-10-15T12:40:39Z","published":"2024-03-30T14:39:09Z","title":"FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain\n  Fake News Detection","summary":"  Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.\n","authors":["Ziyi Zhou","Xiaoming Zhang","Litian Zhang","Jiacheng Liu","Senzhang Wang","Zheng Liu","Xi Zhang","Chaozhuo Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.01336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11417v1","updated":"2024-10-15T09:07:25Z","published":"2024-10-15T09:07:25Z","title":"VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models","summary":"  Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.\n","authors":["Xiaohan Lan","Yitian Yuan","Zequn Jie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2410.11417v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.22350v1","updated":"2024-10-15T05:48:30Z","published":"2024-10-15T05:48:30Z","title":"Quality-Aware End-to-End Audio-Visual Neural Speaker Diarization","summary":"  In this paper, we propose a quality-aware end-to-end audio-visual neural\nspeaker diarization framework, which comprises three key techniques. First, our\naudio-visual model takes both audio and visual features as inputs, utilizing a\nseries of binary classification output layers to simultaneously identify the\nactivities of all speakers. This end-to-end framework is meticulously designed\nto effectively handle situations of overlapping speech, providing accurate\ndiscrimination between speech and non-speech segments through the utilization\nof multi-modal information. Next, we employ a quality-aware audio-visual fusion\nstructure to address signal quality issues for both audio degradations, such as\nnoise, reverberation and other distortions, and video degradations, such as\nocclusions, off-screen speakers, or unreliable detection. Finally, a cross\nattention mechanism applied to multi-speaker embedding empowers the network to\nhandle scenarios with varying numbers of speakers. Our experimental results,\nobtained from various data sets, demonstrate the robustness of our proposed\ntechniques in diverse acoustic environments. Even in scenarios with severely\ndegraded video quality, our system attains performance levels comparable to the\nbest available audio-visual systems.\n","authors":["Mao-Kui He","Jun Du","Shu-Tong Niu","Qing-Feng Liu","Chin-Hui Lee"],"pdf_url":"https://arxiv.org/pdf/2410.22350v1.pdf","comment":null}]},"2024-10-14T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.11063v1","updated":"2024-10-14T20:20:04Z","published":"2024-10-14T20:20:04Z","title":"Towards the methodology for solving the minimum enclosing ball and\n  related problems","summary":"  Methodology is provided towards the solution of the minimum enclosing ball\nproblem. This problem concerns the determination of the unique spherical\nsurface of smallest radius enclosing a given bounded set in the d-dimensional\nEuclidean space. Mathematical formulation and typical methods for solving this\nproblem are presented. Also, the paper is focused on areas that are related to\nthis problem, namely: (a) promise problems and property testing, (b) theorems\nfor partitioning and enclosing (covering) a set, and (c) computation of the\ndiameter of a set.\n","authors":["Michael N. Vrahatis"],"pdf_url":"https://arxiv.org/pdf/2410.11063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11042v1","updated":"2024-10-14T19:46:23Z","published":"2024-10-14T19:46:23Z","title":"Persistent Topological Features in Large Language Models","summary":"  Understanding the decision-making processes of large language models (LLMs)\nis critical given their widespread applications. Towards this goal, describing\nthe topological and geometrical properties of internal representations has\nrecently provided valuable insights. For a more comprehensive characterization\nof these inherently complex spaces, we present a novel framework based on\nzigzag persistence, a method in topological data analysis (TDA) well-suited for\ndescribing data undergoing dynamic transformations across layers. Within this\nframework, we introduce persistence similarity, a new metric that quantifies\nthe persistence and transformation of topological features such as $p$-cycles\nthroughout the model layers. Unlike traditional similarity measures, our\napproach captures the entire evolutionary trajectory of these features,\nproviding deeper insights into the internal workings of LLMs. As a practical\napplication, we leverage persistence similarity to identify and prune redundant\nlayers, demonstrating comparable performance to state-of-the-art methods across\nseveral benchmark datasets. Additionally, our analysis reveals consistent\ntopological behaviors across various models and hyperparameter settings,\nsuggesting a universal structure in LLM internal representations.\n","authors":["Yuri Gardinazzi","Giada Panerai","Karthik Viswanathan","Alessio Ansuini","Alberto Cazzaniga","Matteo Biagetti"],"pdf_url":"https://arxiv.org/pdf/2410.11042v1.pdf","comment":"10+6 pages, 7 figures, 1 table. All comments welcome!"},{"id":"http://arxiv.org/abs/2408.04264v3","updated":"2024-10-14T13:14:11Z","published":"2024-08-08T07:03:12Z","title":"Bounding the Treewidth of Outer $k$-Planar Graphs via Triangulations","summary":"  The treewidth is a structural parameter that measures the tree-likeness of a\ngraph. Many algorithmic and combinatorial results are expressed in terms of the\ntreewidth. In this paper, we study the treewidth of outer $k$-planar graphs,\nthat is, graphs that admit a straight-line drawing where all the vertices lie\non a circle, and every edge is crossed by at most $k$ other edges.\n  Wood and Telle [New York J. Math., 2007] showed that every outer $k$-planar\ngraph has treewidth at most $3k + 11$ using so-called planar decompositions,\nand later, Auer et al. [Algorithmica, 2016] proved that the treewidth of outer\n$1$-planar graphs is at most $3$, which is tight.\n  In this paper, we improve the general upper bound to $1.5k + 2$ and give a\ntight bound of $4$ for $k = 2$. We also establish a lower bound: we show that,\nfor every even $k$, there is an outer $k$-planar graph with treewidth $k+2$.\nOur new bound immediately implies a better bound on the cop number, which\nanswers an open question of Durocher et al. [GD 2023] in the affirmative.\n  Our treewidth bound relies on a new and simple triangulation method for outer\n$k$-planar graphs that yields few crossings with graph edges per edge of the\ntriangulation. Our method also enables us to obtain a tight upper bound of $k +\n2$ for the separation number of outer $k$-planar graphs, improving an upper\nbound of $2k + 3$ by Chaplick et al. [GD 2017]. We also consider outer\nmin-$k$-planar graphs, a generalization of outer $k$-planar graphs, where we\nachieve smaller improvements.\n","authors":["Oksana Firman","Grzegorz Gutowski","Myroslav Kryven","Yuto Okada","Alexander Wolff"],"pdf_url":"https://arxiv.org/pdf/2408.04264v3.pdf","comment":"Appears in the Proceedings of the 32nd International Symposium on\n  Graph Drawing and Network Visualization (GD 2024)"},{"id":"http://arxiv.org/abs/2112.06636v4","updated":"2024-10-14T09:39:37Z","published":"2021-12-06T20:11:30Z","title":"Embeddings of $k$-complexes in $2k$-manifolds and minimum rank of\n  partial symmetric matrices","summary":"  Let $K$ be a $k$-dimensional simplicial complex having $n$ faces of dimension\n$k$, and $M$ a closed $(k-1)$-connected PL $2k$-dimensional manifold. We prove\nthat for $k\\ge3$ odd $K$ embeds into $M$ if and only if there are\n  $\\bullet$ a skew-symmetric $n\\times n$-matrix $A$ with $\\mathbb Z$-entries\nwhose rank over $\\mathbb Q$ does not exceed $rk H_k(M;\\mathbb Z)$,\n  $\\bullet$ a general position PL map $f:K\\to\\mathbb R^{2k}$, and\n  $\\bullet$ orientations on $k$-faces of $K$ such that for any nonadjacent\n$k$-faces $\\sigma,\\tau$ of $K$ the element $A_{\\sigma,\\tau}$ equals to the\nalgebraic intersection of $f\\sigma$ and $f\\tau$.\n  We prove some analogues of this result including those for $\\mathbb Z_2$- and\n$\\mathbb Z$-embeddability. Our results generalize the Bikeev-Fulek-Kyn\\v cl\ncriteria for the $\\mathbb Z_2$- and $\\mathbb Z$-embeddability of graphs to\nsurfaces, and are related to the Harris-Krushkal-Johnson-Pat\\'ak-Tancer\ncriteria for the embeddability of $k$-complexes into $2k$-manifolds.\n","authors":["A. Skopenkov"],"pdf_url":"https://arxiv.org/pdf/2112.06636v4.pdf","comment":"22 pages, 2 figures, exposition improved"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.11091v1","updated":"2024-10-14T21:10:41Z","published":"2024-10-14T21:10:41Z","title":"Energy-Efficient Cryogenic Ternary Content Addressable Memory using\n  Ferroelectric SQUID","summary":"  Ternary content addressable memories (TCAMs) are useful for certain computing\ntasks since they allow us to compare a search query with a whole dataset stored\nin the memory array. They can also unlock unique advantages for cryogenic\napplications like quantum computing, high-performance computing, and space\nexploration by improving speed and energy efficiency through parallel\nsearching. This paper explores the design and implementation of a cryogenic\nternary content addressable memory based on ferroelectric superconducting\nquantum interference devices (FeSQUIDs). The use of FeSQUID for designing the\nTCAM provides several unique advantages. First, we can get binary decisions\n(zero or non-zero voltage) for matching and mismatching conditions without\nusing any peripheral circuitry. Moreover, the proposed TCAM needs ultra-low\nenergy (1.36 aJ and 26.5 aJ average energy consumption for 1-bit binary and\nternary search, respectively), thanks to the use of energy-efficient SQUIDs.\nFinally, we show the efficiency of FeSQUID through the brain-inspired\napplication of Hyperdimensional Computing (HDC). Here, the FeSQUID-based TCAM\nimplements the associative memory to support the highly parallel search needed\nin the inference step. We estimate an energy consumption of 89.4 fJ per vector\ncomparison using a vector size of 10,000 bits. We also compare the\nFeSQUID-based TCAM array with the 5nm FinFET-based cryogenic SRAM-based TCAM\narray and observe that the proposed FeSQUID-based TCAM array consumes over one\norder of magnitude lower energy while performing the same task.\n","authors":["Shamiul Alam","Simon Thomann","Shivendra Singh Parihar","Yogesh Singh Chauhan","Kai Ni","Hussam Amrouch","Ahmedullah Aziz"],"pdf_url":"https://arxiv.org/pdf/2410.11091v1.pdf","comment":"6 figures"},{"id":"http://arxiv.org/abs/2410.10946v1","updated":"2024-10-14T18:00:00Z","published":"2024-10-14T18:00:00Z","title":"Equivalence Checking of Quantum Circuits via Intermediary Matrix Product\n  Operator","summary":"  As quantum computing advances, the complexity of quantum circuits is rapidly\nincreasing, driving the need for robust methods to aid in their design.\nEquivalence checking plays a vital role in identifying errors that may arise\nduring compilation and optimization of these circuits and is a critical step in\nquantum circuit verification. In this work, we introduce a novel method based\non Matrix Product Operators (MPOs) for determining the equivalence of quantum\ncircuits. Our approach contracts tensorized quantum gates from two circuits\ninto an intermediary MPO, exploiting their reversibility to determine their\nequivalence or non-equivalence. Our results show that this method offers\nsignificant scalability improvements over existing methods, with polynomial\nscaling in circuit width and depth for the practical use cases we explore. We\nexpect that this work sets the new standard for scalable equivalence checking\nof quantum circuits and will become a crucial tool for the validation of\nincreasingly complex quantum systems.\n","authors":["Aaron Sander","Lukas Burgholzer","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2410.10946v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.08811v2","updated":"2024-10-14T17:03:06Z","published":"2024-04-12T20:58:25Z","title":"Reducing the Barriers to Entry for Foundation Model Training","summary":"  The world has recently witnessed an unprecedented acceleration in demands for\nMachine Learning and Artificial Intelligence applications. This spike in demand\nhas imposed tremendous strain on the underlying technology stack in supply\nchain, GPU-accelerated hardware, software, datacenter power density, and energy\nconsumption. If left on the current technological trajectory, future demands\nshow insurmountable spending trends, further limiting market players, stifling\ninnovation, and widening the technology gap. To address these challenges, we\npropose a fundamental change in the AI training infrastructure throughout the\ntechnology ecosystem. The changes require advancements in supercomputing and\nnovel AI training approaches, from high-end software to low-level hardware,\nmicroprocessor, and chip design, while advancing the energy efficiency required\nby a sustainable infrastructure. This paper presents the analytical framework\nthat quantitatively highlights the challenges and points to the opportunities\nto reduce the barriers to entry for training large language models.\n","authors":["Paolo Faraboschi","Ellis Giles","Justin Hotard","Konstanty Owczarek","Andrew Wheeler"],"pdf_url":"https://arxiv.org/pdf/2404.08811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10689v1","updated":"2024-10-14T16:26:16Z","published":"2024-10-14T16:26:16Z","title":"Fully Programmable Spatial Photonic Ising Machine by Focal Plane\n  Division","summary":"  Ising machines are an emerging class of hardware that promises ultrafast and\nenergy-efficient solutions to NP-hard combinatorial optimization problems.\nSpatial photonic Ising machines (SPIMs) exploit optical computing in free space\nto accelerate the computation, showcasing parallelism, scalability, and low\npower consumption. However, current SPIMs can implement only a restricted class\nof problems. This partial programmability is a critical limitation that hampers\ntheir benchmark. Achieving full programmability of the device while preserving\nits scalability is an open challenge. Here, we report a fully programmable SPIM\nachieved through a novel operation method based on the division of the focal\nplane. In our scheme, a general Ising problem is decomposed into a set of\nMattis Hamiltonians, whose energies are simultaneously computed optically by\nmeasuring the intensity on different regions of the camera sensor. Exploiting\nthis concept, we experimentally demonstrate the computation with high success\nprobability of ground-state solutions of up to 32-spin Ising models on\nunweighted maximum cut graphs with and without ferromagnetic bias. Simulations\nof the hardware prove a favorable scaling of the accuracy with the number of\nspins. Our fully programmable SPIM enables the implementation of many quadratic\nunconstrained binary optimization problems, further establishing SPIMs as a\nleading paradigm in non von Neumann hardware.\n","authors":["Daniele Veraldi","Davide Pierangeli","Silvia Gentilini","Marcello Calvanese Strinati","Jason Sakellariou","James S. Cummins","Airat Kamaletdinov","Marvin Syed","Richard Zhipeng Wang","Natalia G. Berloff","Dimitrios Karanikolopoulos","Pavlos G. Savvidis","Claudio Conti"],"pdf_url":"https://arxiv.org/pdf/2410.10689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10568v1","updated":"2024-10-14T14:46:02Z","published":"2024-10-14T14:46:02Z","title":"Reflexive Input-Output Causality Mechanisms","summary":"  This paper explores the concept of reflexive actuation, examining how robots\nmay leverage both internal and external stimuli to trigger changes in the\nmotion, performance, or physical characteristics of the robot, such as its\nsize, shape, or configuration, and so on. These changes themselves may in turn\nbe sequentially re-used as input to drive further adaptations. Drawing\ninspiration from biological systems, where reflexes are an essential component\nof the response to environmental changes, reflexive actuation is critical to\nenable robots to adapt to diverse situations and perform complex tasks. The\nunderlying principles of reflexive actuation are analyzed, with examples\nprovided from existing implementations such as contact-sensitive reflexive\narms, physical counters, and their applications. The paper also outlines future\ndirections and challenges for advancing this research area, emphasizing its\nsignificance in the development of adaptive, responsive robotic systems.\n","authors":["Ryotaro Kayawake","Haruto Miida","Shunsuke Sano","Issei Onda","Kazuki Abe","Masahiro Watanabe","Josephine Galipon","Riichiro Tadakuma","Kenjiro Tadakuma"],"pdf_url":"https://arxiv.org/pdf/2410.10568v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.10532v1","updated":"2024-10-14T14:11:37Z","published":"2024-10-14T14:11:37Z","title":"ZONIA: a Zero-Trust Oracle System for Blockchain IoT Applications","summary":"  The rapid expansion of the Internet of Things (IoT) has led to significant\ndata reliability and system transparency challenges, aggravated by the\ncentralized nature of existing IoT architectures. This centralization often\nresults in siloed data ecosystems, where interoperability issues and opaque\ndata handling practices compromise both the utility and trustworthiness of IoT\napplications. To address these issues, we introduce ZONIA (Zero-trust Oracle\nNetwork for IoT Applications), a novel blockchain oracle system designed to\nenhance data integrity and decentralization in IoT environments. Unlike\ntraditional approaches that rely on Trusted Execution Environments and\ncentralized data sources, ZONIA utilizes a decentralized, zero-trust model that\nallows for anonymous participation and integrates multiple data sources to\nensure fairness and reliability. This paper outlines ZONIA's architecture,\nwhich supports semantic and geospatial queries, details its data reliability\nmechanisms, and presents a comprehensive evaluation demonstrating its\nscalability and resilience against data falsification and collusion attacks.\nBoth analytical and experimental results demonstrate ZONIA's scalability,\nshowcasing its feasibility to handle an increasing number of nodes in the\nsystem under different system conditions and workloads. Furthermore, the\nimplemented reputation mechanism significantly enhances data accuracy,\nmaintaining high reliability even when 40\\% of nodes exhibit malicious\nbehavior.\n","authors":["Lorenzo Gigli","Ivan Zyrianoff","Federico Montori","Luca Sciullo","Carlos Kamienski","Marco Di Felice"],"pdf_url":"https://arxiv.org/pdf/2410.10532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12870v1","updated":"2024-10-14T12:48:42Z","published":"2024-10-14T12:48:42Z","title":"Skill Learning Using Process Mining for Large Language Model Plan\n  Generation","summary":"  Large language models (LLMs) hold promise for generating plans for complex\ntasks, but their effectiveness is limited by sequential execution, lack of\ncontrol flow models, and difficulties in skill retrieval. Addressing these\nissues is crucial for improving the efficiency and interpretability of plan\ngeneration as LLMs become more central to automation and decision-making. We\nintroduce a novel approach to skill learning in LLMs by integrating process\nmining techniques, leveraging process discovery for skill acquisition, process\nmodels for skill storage, and conformance checking for skill retrieval. Our\nmethods enhance text-based plan generation by enabling flexible skill\ndiscovery, parallel execution, and improved interpretability. Experimental\nresults suggest the effectiveness of our approach, with our skill retrieval\nmethod surpassing state-of-the-art accuracy baselines under specific\nconditions.\n","authors":["Andrei Cosmin Redis","Mohammadreza Fani Sani","Bahram Zarrin","Andrea Burattin"],"pdf_url":"https://arxiv.org/pdf/2410.12870v1.pdf","comment":"12 pages, 5 figures, 2 tables, accepted at ICPM 2024'"},{"id":"http://arxiv.org/abs/2410.10264v1","updated":"2024-10-14T08:16:31Z","published":"2024-10-14T08:16:31Z","title":"A Survey on Performance, Current and Future Usage of\n  Vehicle-To-Everything Communication Standards","summary":"  Wireless communication between road users is essential for environmental\nperception, reasoning, and mission planning to enable fully autonomous\nvehicles, and thus improve road safety and transport efficiency. To enable\ncollaborative driving, the concept of vehicle-to-Everything (V2X) has long been\nintroduced to the industry. Within the last two decades, several communication\nstandards have been developed based on IEEE 802.11p and cellular standards,\nnamely Dedicated Short-Range Communication (DSRC), Intelligent Transportation\nSystem G5 (ITS-G5), and Cellular- and New Radio- Vehicle-to-Everything (C-V2X\nand NR-V2X). However, while there exists a high quantity of available\npublications concerning V2X and the analysis of the different standards, only\nfew surveys exist that summarize these results. Furthermore, to our knowledge,\nno survey that provides an analysis about possible future trends and challenges\nfor the global implementation of V2Xexists. Thus, this contribution provides a\ndetailed survey on Vehicle-to-Everything communication standards, their\nperformance, current and future applications, and associated challenges. Based\non our research, we have identified several research gaps and provide a picture\nabout the possible future of the Vehicle-to-Everything communication domain.\n","authors":["Falk Dettinger","Matthias Wei√ü","Daniel Dittler","Johannes St√ºmpfle","Maurice Artelt","Michael Weyrich"],"pdf_url":"https://arxiv.org/pdf/2410.10264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10162v1","updated":"2024-10-14T05:10:56Z","published":"2024-10-14T05:10:56Z","title":"From x*y=k to Uniswap Hooks; A Comparative Review of Decentralized\n  Exchanges (DEX)","summary":"  Decentralized exchanges (DEXs) are pivotal applications in the Decentralized\nfinance (DeFi) landscape, aiming to facilitate trustless cryptocurrency trading\nby relying on smart contracts and blockchain networks. The developments in the\nDEXs sector began with the implementation of an automated market maker (AMM)\nsystem using a simple math formula by Uniswap V1 in 2018. Absorbing significant\nfunding and the attention of web3 enthusiasts, DEXs have seen numerous\nadvancements in their evolution. A notable recent advancement is the\nintroduction of hooks in Uniswap v4, which allows users to take advantage of a\nwide range of plugin-like features with liquidity pools. This paper provides a\ncomprehensive classification and comparative analyses of prominent DEX\nprotocols, namely Uniswap, Curve, and Balancer, in addition to investigating\nother protocols' noteworthy aspects. The evaluation framework encompasses\nmechanisms, components, mathematical formulations, and the performance of\nliquidity pools. The goals are to elucidate the strengths and limitations of\ndifferent AMM models, highlight emerging concepts in DEX development, outline\ncurrent challenges, and differentiate optimal models for specific applications.\nThe results and comparative insights can be a reference for web3 developers,\nblockchain researchers, traders, and regulatory parties.\n","authors":["Mohammad Ali Asef","Seyed Mojtaba Hosseini Bamakan"],"pdf_url":"https://arxiv.org/pdf/2410.10162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05956v2","updated":"2024-10-14T04:43:24Z","published":"2024-10-08T12:11:56Z","title":"Waveguide-multiplexed photonic matrix-vector multiplication processor\n  using multiport photodetectors","summary":"  The slowing down of Moore's law has driven the development of\napplication-specific processors for deep learning. Analog photonic processors\noffer a promising solution for accelerating matrix-vector multiplications\n(MVMs) in deep learning by leveraging parallel computations in the optical\ndomain. Intensity-based photonic MVM processors, which do not utilize the phase\ninformation of light, are appealing due to their simplified operations.\nHowever, existing intensity-based schemes for such processors often employ\nwavelength multiplexing or mode multiplexing, both of which have limited\nscalability due to high insertion loss or wavelength crosstalk. In this work,\nwe present a scalable intensity-based photonic MVM processor based on the\nconcept of waveguide multiplexing. This scheme employs multiport photodetectors\n(PDs) to sum the intensities of multiple optical signals, eliminating the need\nfor multiple wavelengths or modes. A 16-port Ge PD with a 3 dB bandwidth of\n11.8 GHz at a bias voltage of -3 V is demonstrated, and it can be further\nscaled up to handle 250 ports while maintaining a 6.1 GHz operation bandwidth.\nA 4 $\\times$ 4 circuit fabricated on a Si-on-insulator (SOI) platform is used\nto perform MVMs in a 3-layer neural network designed for classifying Iris\nflowers, achieving a classification accuracy of 93.3%. Furthermore, the\nperformance of large-scale circuits in a convolutional neural network (CNN) for\nFashion-MNIST is simulated, resulting in a classification accuracy of 90.53%.\nThis work provides a simplified and scalable approach to photonic MVM, laying a\nfoundation for large-scale and multi-dimensional photonic matrix-matrix\nmultiplication in optical neural networks.\n","authors":["Rui Tang","Makoto Okano","Chao Zhang","Kasidit Toprasertpong","Shinichi Takagi","Mitsuru Takenaka"],"pdf_url":"https://arxiv.org/pdf/2410.05956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10071v1","updated":"2024-10-14T01:25:56Z","published":"2024-10-14T01:25:56Z","title":"Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning","summary":"  In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.\n","authors":["Jinjin Shen","Yan Lin","Yijin Zhang","Weibin Zhang","Feng Shu","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2410.10071v1.pdf","comment":"6 pages, 5 figures"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.11080v1","updated":"2024-10-14T20:42:30Z","published":"2024-10-14T20:42:30Z","title":"Few-shot Novel View Synthesis using Depth Aware 3D Gaussian Splatting","summary":"  3D Gaussian splatting has surpassed neural radiance field methods in novel\nview synthesis by achieving lower computational costs and real-time\nhigh-quality rendering. Although it produces a high-quality rendering with a\nlot of input views, its performance drops significantly when only a few views\nare available. In this work, we address this by proposing a depth-aware\nGaussian splatting method for few-shot novel view synthesis. We use monocular\ndepth prediction as a prior, along with a scale-invariant depth loss, to\nconstrain the 3D shape under just a few input views. We also model color using\nlower-order spherical harmonics to avoid overfitting. Further, we observe that\nremoving splats with lower opacity periodically, as performed in the original\nwork, leads to a very sparse point cloud and, hence, a lower-quality rendering.\nTo mitigate this, we retain all the splats, leading to a better reconstruction\nin a few view settings. Experimental results show that our method outperforms\nthe traditional 3D Gaussian splatting methods by achieving improvements of\n10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and\n14.1% in perceptual similarity, thereby validating the effectiveness of our\napproach. The code will be made available at:\nhttps://github.com/raja-kumar/depth-aware-3DGS\n","authors":["Raja Kumar","Vanshika Vats"],"pdf_url":"https://arxiv.org/pdf/2410.11080v1.pdf","comment":"Presented in ECCV 2024 workshop S3DSGR"},{"id":"http://arxiv.org/abs/2410.10936v1","updated":"2024-10-14T17:59:29Z","published":"2024-10-14T17:59:29Z","title":"A Part-to-Whole Circular Cell Explorer","summary":"  Spatial transcriptomics methods capture cellular measurements such as gene\nexpression and cell types at specific locations in a cell, helping provide a\nlocalized picture of tissue health. Traditional visualization techniques\nsuperimpose the tissue image with pie charts for the cell distribution. We\ndesign an interactive visual analysis system that addresses perceptual problems\nin the state of the art, while adding filtering, drilling, and clustering\nanalysis capabilities. Our approach can help researchers gain deeper insights\ninto the molecular mechanisms underlying complex biological processes within\ntissues.\n","authors":["Siyuan Zhao","G. Elisabeta Marai"],"pdf_url":"https://arxiv.org/pdf/2410.10936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10696v1","updated":"2024-10-14T16:38:10Z","published":"2024-10-14T16:38:10Z","title":"TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment\n  with Diffusion Model","summary":"  Recently, 2D speaking avatars have increasingly participated in everyday\nscenarios due to the fast development of facial animation techniques. However,\nmost existing works neglect the explicit control of human bodies. In this\npaper, we propose to drive not only the faces but also the torso and gesture\nmovements of a speaking figure. Inspired by recent advances in diffusion\nmodels, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing\nAvatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar\nreenactment from only short footage of monocular video. Our key idea is to\nenhance the textural awareness with explicit motion guidance in diffusion\nmodeling. Specifically, we carefully construct 2D and 3D structural information\nas intermediate guidance. While recent diffusion models adopt a side network\nfor control information injection, they fail to synthesize temporally stable\nresults even with person-specific fine-tuning. We propose a Motion-Enhanced\nTextural Alignment module to enhance the bond between driving and target\nsignals. Moreover, we build a Memory-based Hand-Recovering module to help with\nthe difficulties in hand-shape preserving. After pre-training, our model can\nachieve high-fidelity 2D avatar reenactment with only 30 seconds of\nperson-specific data. Extensive experiments demonstrate the effectiveness and\nsuperiority of our proposed framework. Resources can be found at\nhttps://guanjz20.github.io/projects/TALK-Act.\n","authors":["Jiazhi Guan","Quanwei Yang","Kaisiyuan Wang","Hang Zhou","Shengyi He","Zhiliang Xu","Haocheng Feng","Errui Ding","Jingdong Wang","Hongtao Xie","Youjian Zhao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.10696v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 (conference track). Project page:\n  https://guanjz20.github.io/projects/TALK-Act"},{"id":"http://arxiv.org/abs/2410.10927v1","updated":"2024-10-14T15:43:40Z","published":"2024-10-14T15:43:40Z","title":"Cultural Heritage 3D Reconstruction with Diffusion Networks","summary":"  This article explores the use of recent generative AI algorithms for\nrepairing cultural heritage objects, leveraging a conditional diffusion model\ndesigned to reconstruct 3D point clouds effectively. Our study evaluates the\nmodel's performance across general and cultural heritage-specific settings.\nResults indicate that, with considerations for object variability, the\ndiffusion model can accurately reproduce cultural heritage geometries. Despite\nencountering challenges like data diversity and outlier sensitivity, the model\ndemonstrates significant potential in artifact restoration research. This work\nlays groundwork for advancing restoration methodologies for ancient artifacts\nusing AI technologies.\n","authors":["Pablo Jaramillo","Ivan Sipiran"],"pdf_url":"https://arxiv.org/pdf/2410.10927v1.pdf","comment":"Accepted by the workshop VISART for ECCV 2024"},{"id":"http://arxiv.org/abs/2410.06854v2","updated":"2024-10-14T12:53:50Z","published":"2024-10-09T13:17:22Z","title":"Focal Surface Holographic Light Transport using Learned Spatially\n  Adaptive Convolutions","summary":"  Computer-Generated Holography (CGH) is a set of algorithmic methods for\nidentifying holograms that reconstruct Three-Dimensional (3D) scenes in\nholographic displays. CGH algorithms decompose 3D scenes into multiplanes at\ndifferent depth levels and rely on simulations of light that propagated from a\nsource plane to a targeted plane. Thus, for n planes, CGH typically optimizes\nholograms using n plane-to-plane light transport simulations, leading to major\ntime and computational demands. Our work replaces multiple planes with a focal\nsurface and introduces a learned light transport model that could propagate a\nlight field from a source plane to the focal surface in a single inference. Our\nlearned light transport model leverages spatially adaptive convolution to\nachieve depth-varying propagation demanded by targeted focal surfaces. The\nproposed model reduces the hologram optimization process up to 1.5x, which\ncontributes to hologram dataset generation and the training of future learned\nCGH models.\n","authors":["Chuanjun Zheng","Yicheng Zhan","Liang Shi","Ozan Cakmakci","Kaan Ak≈üit"],"pdf_url":"https://arxiv.org/pdf/2410.06854v2.pdf","comment":"SIGGRAPH Asia 2024 Technical Communications"},{"id":"http://arxiv.org/abs/2410.10149v1","updated":"2024-10-14T04:30:38Z","published":"2024-10-14T04:30:38Z","title":"Fast and Accurate Neural Rendering Using Semi-Gradients","summary":"  We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.\n","authors":["In-Young Cho","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2410.10149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14580v3","updated":"2024-10-14T03:25:25Z","published":"2024-05-23T13:53:17Z","title":"LDM: Large Tensorial SDF Model for Textured Mesh Generation","summary":"  Previous efforts have managed to generate production-ready 3D assets from\ntext or images. However, these methods primarily employ NeRF or 3D Gaussian\nrepresentations, which are not adept at producing smooth, high-quality\ngeometries required by modern rendering pipelines. In this paper, we propose\nLDM, a novel feed-forward framework capable of generating high-fidelity,\nillumination-decoupled textured mesh from a single image or text prompts. We\nfirstly utilize a multi-view diffusion model to generate sparse multi-view\ninputs from single images or text prompts, and then a transformer-based model\nis trained to predict a tensorial SDF field from these sparse multi-view image\ninputs. Finally, we employ a gradient-based mesh optimization layer to refine\nthis model, enabling it to produce an SDF field from which high-quality\ntextured meshes can be extracted. Extensive experiments demonstrate that our\nmethod can generate diverse, high-quality 3D mesh assets with corresponding\ndecomposed RGB textures within seconds.\n","authors":["Rengan Xie","Wenting Zheng","Kai Huang","Yizheng Chen","Qi Wang","Qi Ye","Wei Chen","Yuchi Huo"],"pdf_url":"https://arxiv.org/pdf/2405.14580v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10102v1","updated":"2024-10-14T02:43:58Z","published":"2024-10-14T02:43:58Z","title":"Trust-Region Eigenvalue Filtering for Projected Newton","summary":"  We introduce a novel adaptive eigenvalue filtering strategy to stabilize and\naccelerate the optimization of Neo-Hookean energy and its variants under the\nProjected Newton framework. For the first time, we show that Newton's method,\nProjected Newton with eigenvalue clamping and Projected Newton with absolute\neigenvalue filtering can be unified using ideas from the generalized trust\nregion method. Based on the trust-region fit, our model adaptively chooses the\ncorrect eigenvalue filtering strategy to apply during the optimization. Our\nmethod is simple but effective, requiring only two lines of code change in the\nexisting Projected Newton framework. We validate our model outperforms\nstand-alone variants across a number of experiments on quasistatic simulation\nof deformable solids over a large dataset.\n","authors":["Honglin Chen","Hsueh-Ti Derek Liu","Alec Jacobson","David I. W. Levin","Changxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.10102v1.pdf","comment":"SIGGRAPH Asia 2024 (Conference track). Project page:\n  https://www.cs.columbia.edu/cg/trust-region/"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21297v1","updated":"2024-10-14T17:49:47Z","published":"2024-10-14T17:49:47Z","title":"Producer vs. Rapper: Who Dominates the Hip Hop Sound? A Case Study","summary":"  In hip-hop music, rappers and producers play important, but rather different\nroles. However, both contribute to the overall sound, as rappers bring in their\nvoice, while producers are responsible for the music composition and mix. In\nthis case report, we trained Self-Organizing Maps (SOMs) with songs produced by\nDr. Dre, Rick Rubin and Timbaland using the goniometer and Mel Frequency\nCepstral Coefficients (MFCCs). With these maps, we investigate whether hip hop\nproducers have a unique sound profile. Then, we test whether collaborations\nwith the rappers Eminem, Jay-Z, LL Cool J and Nas stick to, or break out of\nthis sound profile. As these rappers are also producers of some songs, we\ninvestigate how much their sound profile is influenced by the producers who\nintroduced them to beat making. The results speak a clear language: producers\nhave their own sound profile that is unique concerning the goniometer, and less\ndistinct concerning MFCCs. They dominate the sound of hip hop music over\nrappers, who emulate the sound profile of the producers who introduced them to\nbeat making.\n","authors":["Tim Ziemer","Nikita Kudakov","Christoph Reuter"],"pdf_url":"https://arxiv.org/pdf/2410.21297v1.pdf","comment":"many SOMs"},{"id":"http://arxiv.org/abs/2408.06753v3","updated":"2024-10-14T16:06:54Z","published":"2024-08-13T09:19:59Z","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","summary":"  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2408.06753v3.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2410.10319v1","updated":"2024-10-14T09:25:09Z","published":"2024-10-14T09:25:09Z","title":"Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature\n  Aggregation","summary":"  The projector plays a crucial role in multi-modal language models (MLLMs).\nThe number of visual tokens it outputs affects the efficiency of the MLLM,\nwhile the quality of the visual tokens influences the visual understanding\ncapabilities of the MLLM. Current explorations on the projector focus on\nreducing the number of visual tokens to improve efficiency, often overlooking\nthe inherent spatial discrepancy between the serialized 2-dimensional visual\ntoken sequences and natural language token sequences. A Spatial-Aware Efficient\nProjector (SAEP) is proposed to address this issue. In detail, our SAEP method\nemploys an modified separable depthwise convolution module on multi-layer\nvisual features to enhance the spatial information of visual tokens. As a\nresult, our SAEP method can not only largely reduce the number of visual tokens\nby 75\\%, but also significantly improve the multimodal spatial understanding\ncapability of MLLMs. Moreover, compared to existing projectors, our SAEP gets\nbest performances on massive multimodal evaluation benchmarks, which denotes\nits effectiveness on bridging the modality gap.\n","authors":["Shun Qian","Bingquan Liu","Chengjie Sun","Zhen Xu","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10319v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.07930v2","updated":"2024-10-14T08:19:13Z","published":"2024-05-13T17:01:28Z","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","summary":"  Learning from multiple modalities, such as audio and video, offers\nopportunities for leveraging complementary information, enhancing robustness,\nand improving contextual understanding and performance. However, combining such\nmodalities presents challenges, especially when modalities differ in data\nstructure, predictive contribution, and the complexity of their learning\nprocesses. It has been observed that one modality can potentially dominate the\nlearning process, hindering the effective utilization of information from other\nmodalities and leading to sub-optimal model performance. To address this issue\nthe vast majority of previous works suggest to assess the unimodal\ncontributions and dynamically adjust the training to equalize them. We improve\nupon previous work by introducing a multi-loss objective and further refining\nthe balancing process, allowing it to dynamically adjust the learning pace of\neach modality in both directions, acceleration and deceleration, with the\nability to phase out balancing effects upon convergence. We achieve superior\nresults across three audio-video datasets: on CREMA-D, models with ResNet\nbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer\nbackbone models deliver improvements ranging from 2.8% to 14.1% across\ndifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, while\non UCF101, gains reach up to 6.1%.\n","authors":["Konstantinos Kontras","Christos Chatzichristos","Matthew Blaschko","Maarten De Vos"],"pdf_url":"https://arxiv.org/pdf/2405.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10178v1","updated":"2024-10-14T05:51:53Z","published":"2024-10-14T05:51:53Z","title":"GUISE: Graph GaUssIan Shading watErmark","summary":"  In the expanding field of generative artificial intelligence, integrating\nrobust watermarking technologies is essential to protect intellectual property\nand maintain content authenticity. Traditionally, watermarking techniques have\nbeen developed primarily for rich information media such as images and audio.\nHowever, these methods have not been adequately adapted for graph-based data,\nparticularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an\nascendant approach in the molecular graph generation field. This model\neffectively manages the complexities of molecular structures, preserving\nessential symmetries and topological features. We adapt the Gaussian Shading, a\nproven performance lossless watermarking technique, to the latent graph\ndiffusion domain to protect this sophisticated new technology. Our adaptation\nsimplifies the watermark diffusion process through duplication and padding,\nmaking it adaptable and suitable for various message types. We conduct several\nexperiments using the LDM-3DG model on publicly available datasets QM9 and\nDrugs, to assess the robustness and effectiveness of our technique. Our results\ndemonstrate that the watermarked molecules maintain statistical parity in 9 out\nof 10 performance metrics compared to the original. Moreover, they exhibit a\n100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while\nalso showing robustness against post-editing attacks.\n","authors":["Renyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13711v2","updated":"2024-10-14T01:43:38Z","published":"2024-08-25T02:56:26Z","title":"SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with\n  Panoramic Gaussian Splatting","summary":"  Text-driven 3D scene generation has seen significant advancements recently.\nHowever, most existing methods generate single-view images using generative\nmodels and then stitch them together in 3D space. This independent generation\nfor each view often results in spatial inconsistency and implausibility in the\n3D scenes. To address this challenge, we proposed a novel text-driven\n3D-consistent scene generation model: SceneDreamer360. Our proposed method\nleverages a text-driven panoramic image generation model as a prior for 3D\nscene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency\nacross multi-view panoramic images. Specifically, SceneDreamer360 enhances the\nfine-tuned Panfusion generator with a three-stage panoramic enhancement,\nenabling the generation of high-resolution, detail-rich panoramic images.\nDuring the 3D scene construction, a novel point cloud fusion initialization\nmethod is used, producing higher quality and spatially consistent point clouds.\nOur extensive experiments demonstrate that compared to other methods,\nSceneDreamer360 with its panoramic image generation and 3DGS can produce higher\nquality, spatially consistent, and visually appealing 3D scenes from any text\nprompt. Our codes are available at\n\\url{https://github.com/liwrui/SceneDreamer360}.\n","authors":["Wenrui Li","Fucheng Cai","Yapeng Mi","Zhe Yang","Wangmeng Zuo","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13711v2.pdf","comment":null}]},"2024-10-13T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.09939v1","updated":"2024-10-13T17:50:49Z","published":"2024-10-13T17:50:49Z","title":"Efficient computations of discrete cubical homology","summary":"  We present a fast algorithm for computing discrete cubical homology of graphs\nover a field of characteristic zero. This algorithm improves on several\ncomputational steps compared to constructions in the existing literature, with\nthe key insights including: a faster way to generate all singular cubes,\nreducing the dimensions of vector spaces in the chain complex by taking a\nquotient over automorphisms of the cube, and preprocessing graphs using the\naxiomatic treatment of discrete cubical homology.\n","authors":["Chris Kapulkin","Nathan Kershaw"],"pdf_url":"https://arxiv.org/pdf/2410.09939v1.pdf","comment":"36 pages; comments welcome"},{"id":"http://arxiv.org/abs/2410.09922v1","updated":"2024-10-13T17:07:34Z","published":"2024-10-13T17:07:34Z","title":"Separable Drawings: Extendability and Crossing-Free Hamiltonian Cycles","summary":"  Generalizing pseudospherical drawings, we introduce a new class of simple\ndrawings, which we call separable drawings. In a separable drawing, every edge\ncan be closed to a simple curve that intersects each other edge at most once.\nCurves of different edges might interact arbitrarily. Most notably, we show\nthat (1) every separable drawing of any graph on $n$ vertices in the plane can\nbe extended to a simple drawing of the complete graph $K_{n}$, (2) every\nseparable drawing of $K_{n}$ contains a crossing-free Hamiltonian cycle and is\nplane Hamiltonian connected, and (3) every generalized convex drawing and every\n2-page book drawing is separable. Further, the class of separable drawings is a\nproper superclass of the union of generalized convex and 2-page book drawings.\nHence, our results on plane Hamiltonicity extend recent work on generalized\nconvex drawings by Bergold et al. (SoCG 2024).\n","authors":["Oswin Aichholzer","Joachim Orthaber","Birgit Vogtenhuber"],"pdf_url":"https://arxiv.org/pdf/2410.09922v1.pdf","comment":"Preliminary full version of a paper appearing in the proceedings of\n  GD 2024"},{"id":"http://arxiv.org/abs/2410.09860v1","updated":"2024-10-13T14:42:02Z","published":"2024-10-13T14:42:02Z","title":"Invariants of almost embeddings of graphs in the plane","summary":"  In this survey we motivate studies of the invariants from the title. A graph\ndrawing in the plane is called an almost embedding if the images of any two\nnon-adjacent simplices (i.e. vertices or edges) are disjoint. We introduce\ninteger invariants of almost embeddings: winding number, cyclic and triodic Wu\nnumbers. We prove some relations between the invariants. We demonstrate\nconnection of these relations to homology of the deleted product of a graph. We\nconstruct almost embeddings realizing some values of these invariants.\n  This paper is accessible to mathematicians not specialized in the area (and\nto students). All the necessary definitions are recalled. We present some ideas\nof algebraic and geometric topology in a language accessible to\nnon-topologists. However elementary, this paper is motivated by frontline of\nresearch; there are some conjectures and an open problem.\n","authors":["E. Alkin","E. Bordacheva","A. Miroshnikov","A. Skopenkov"],"pdf_url":"https://arxiv.org/pdf/2410.09860v1.pdf","comment":"27 pages, many figures, in Russian. The paper belongs to math.AT\n  because the invariants are special cases of the degree; to math.GT because\n  the most closely related papers are 1805.10237 [math.GT], 2205.01013\n  [math.GT], and almost embeddings are studied in 2303.14503 (math.CO math.GT),\n  2008.02523 (math.GT math.AT), 1703.06305 (math.GT), 1904.02404 (math.AT\n  math.GT), 2206.13486 (math.GT math.CO)"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.09953v1","updated":"2024-10-13T18:21:02Z","published":"2024-10-13T18:21:02Z","title":"Energy-Efficient and Fast Memristor-based Serial Multipliers Applicable\n  in Image Processing","summary":"  Memristive Processing In-Memory (PIM) is one of the promising techniques for\novercoming the Von-Neumann bottleneck. Reduction of data transfer between\nprocessor and memory and data processing by memristors in data-intensive\napplications reduces energy consumption and processing time. Multipliers are\none of the fundamental arithmetic circuits that play a significant role in\ndata-intensive processing applications. The computational complexity of\nmultipliers has turned them into one of the arithmetic circuits affecting PIM's\nefficiency and energy consumption, for example, in convolution operations.\nSerial material implication (IMPLY) logic design is one of the methods of\nimplementing arithmetic circuits by applying emerging memristive technology\nthat enables PIM in the structure of crossbar arrays. The authors propose\nunsigned and signed array multipliers using serial IMPLY logic in this paper.\nThe proposed multipliers have improved significantly compared to State-Of-the\nArt (SOA) by applying the proposed Partial Product Units (PPUs) and overlapping\ncomputational steps. The number of computational steps, energy consumption, and\nrequired memristors of the proposed 8-bit unsigned array multiplier are\nimproved by up to 36%, 31%, and 47% compared to the classic designs. The\nproposed 8-bit signed multiplier has also improved the computational steps,\nenergy consumption, and required memristors by up to 59%, 54%, and 45%. The\nperformance of the proposed multipliers in the applications of Gaussian blur\nand edge detection is also investigated, and the simulation results have shown\nan improvement of 31% in energy consumption and 33% in the number of\ncomputational steps in these applications.\n","authors":["Seyed Erfan Fatemieh","Bahareh Bagheralmoosavi","Mohammad Reza Reshadinezhad"],"pdf_url":"https://arxiv.org/pdf/2410.09953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09755v1","updated":"2024-10-13T07:22:22Z","published":"2024-10-13T07:22:22Z","title":"Gain Cell-Based Analog Content Addressable Memory for Dynamic\n  Associative tasks in AI","summary":"  Analog Content Addressable Memories (aCAMs) have proven useful for\nassociative in-memory computing applications like Decision Trees, Finite State\nMachines, and Hyper-dimensional Computing. While non-volatile implementations\nusing FeFETs and ReRAM devices offer speed, power, and area advantages, they\nsuffer from slow write speeds and limited write cycles, making them less\nsuitable for computations involving fully dynamic data patterns. To address\nthese limitations, in this work, we propose a capacitor gain cell-based aCAM\ndesigned for dynamic processing, where frequent memory updates are required.\nOur system compares analog input voltages to boundaries stored in capacitors,\nenabling efficient dynamic tasks. We demonstrate the application of aCAM within\ntransformer attention mechanisms by replacing the softmax-scaled dot-product\nsimilarity with aCAM similarity, achieving competitive results. Circuit\nsimulations on a TSMC 28 nm node show promising performance in terms of energy\nefficiency, precision, and latency, making it well-suited for fast, dynamic AI\napplications.\n","authors":["Paul-Philipp Manea","Nathan Leroux","Emre Neftci","John Paul Strachan"],"pdf_url":"https://arxiv.org/pdf/2410.09755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05754v5","updated":"2024-10-13T01:08:51Z","published":"2024-03-09T01:34:26Z","title":"Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition","summary":"  In this paper, we propose two hybrid quantum-inspired neural networks with\nresidual and dense connections respectively for pattern recognition. We explain\nthe concrete frameworks and illustrate the potential superiority to prevent\ngradient explosion of our hybrid models. A group of numerical experiments about\ngeneralization power shows that our hybrid models possess the same\ngeneralization power as the pure classical models with different noisy datasets\nutilized. More importantly, another group of numerical experiments of\nrobustness demonstrates that our hybrid models outperform pure classical models\nnotably in resistance to parameter attacks with various asymmetric noises.\nAlso, an ablation study indicate that the recognition accuracy of our hybrid\nmodels is 2\\%-3\\% higher than that of the quantum neural network without\nresidual or dense connection. Eventually, we discuss the application scenarios\nof our hybrid models by analyzing their computational complexities.\n","authors":["Andi Chen","Hua-Lei Yin","Zeng-Bing Chen","Shengjun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.05754v5.pdf","comment":"12 pages for main paper with a hyperlink of a 18-page supplementary\n  material in the last page of the main paper"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.10017v1","updated":"2024-10-13T21:30:56Z","published":"2024-10-13T21:30:56Z","title":"REPeat: A Real2Sim2Real Approach for Pre-acquisition of Soft Food Items\n  in Robot-assisted Feeding","summary":"  The paper presents REPeat, a Real2Sim2Real framework designed to enhance bite\nacquisition in robot-assisted feeding for soft foods. It uses `pre-acquisition\nactions' such as pushing, cutting, and flipping to improve the success rate of\nbite acquisition actions such as skewering, scooping, and twirling. If the\ndata-driven model predicts low success for direct bite acquisition, the system\ninitiates a Real2Sim phase, reconstructing the food's geometry in a simulation.\nThe robot explores various pre-acquisition actions in the simulation, then a\nSim2Real step renders a photorealistic image to reassess success rates. If the\nsuccess improves, the robot applies the action in reality. We evaluate the\nsystem on 15 diverse plates with 10 types of food items for a soft food diet,\nshowing improvement in bite acquisition success rates by 27\\% on average across\nall plates. See our project website at https://emprise.cs.cornell.edu/repeat.\n","authors":["Nayoung Ha","Ruolin Ye","Ziang Liu","Shubhangi Sinha","Tapomayukh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2410.10017v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.04321v2","updated":"2024-10-13T17:59:22Z","published":"2024-06-06T17:58:11Z","title":"VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling","summary":"  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets will be available at\nhttps://github.com/ZeyueT/VidMuse/.\n","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.04321v2.pdf","comment":"The code and datasets will be available at\n  https://github.com/ZeyueT/VidMuse/"},{"id":"http://arxiv.org/abs/2410.09872v1","updated":"2024-10-13T15:13:00Z","published":"2024-10-13T15:13:00Z","title":"Towards Reproducible Learning-based Compression","summary":"  A deep learning system typically suffers from a lack of reproducibility that\nis partially rooted in hardware or software implementation details. The\nirreproducibility leads to skepticism in deep learning technologies and it can\nhinder them from being deployed in many applications. In this work, the\nirreproducibility issue is analyzed where deep learning is employed in\ncompression systems while the encoding and decoding may be run on devices from\ndifferent manufacturers. The decoding process can even crash due to a single\nbit difference, e.g., in a learning-based entropy coder. For a given deep\nlearning-based module with limited resources for protection, we first suggest\nthat reproducibility can only be assured when the mismatches are bounded. Then\na safeguarding mechanism is proposed to tackle the challenges. The proposed\nmethod may be applied for different levels of protection either at the\nreconstruction level or at a selected decoding level. Furthermore, the overhead\nintroduced for the protection can be scaled down accordingly when the error\nbound is being suppressed. Experiments demonstrate the effectiveness of the\nproposed approach for learning-based compression systems, e.g., in image\ncompression and point cloud compression.\n","authors":["Jiahao Pang","Muhammad Asad Lodhi","Junghyun Ahn","Yuning Huang","Dong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.09872v1.pdf","comment":"Accepted at MMSP 2024"},{"id":"http://arxiv.org/abs/2405.13984v2","updated":"2024-10-13T14:24:49Z","published":"2024-05-22T20:40:53Z","title":"Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption\n  Generation and Fine-Grained NLI Evaluation","summary":"  Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore synergies between\nalignment fine-tuning and model merging in a cross-modal setup. We reveal\nintriguing insights into the behaviour and suitability of such methods while\nsignificantly surpassing state-of-the-art models. Moreover, we propose a novel\natomic-level evaluation method leveraging off-the-shelf Natural Language\nInference (NLI) models for use in the unseen chemical domain. Our experiments\ndemonstrate that our evaluation operates at the right level of granularity,\neffectively handling multiple content units and subsentence reasoning, while\nwidely adopted NLI methods consistently misalign with assessment criteria.\n","authors":["Dimitris Gkoumas","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2405.13984v2.pdf","comment":null}]},"2024-10-12T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2310.18238v4","updated":"2024-10-12T21:03:07Z","published":"2023-10-27T16:21:33Z","title":"Order-2 Delaunay Triangulations Optimize Angles","summary":"  The local angle property of the (order-$1$) Delaunay triangulations of a\ngeneric set in $\\mathbb{R}^2$ asserts that the sum of two angles opposite a\ncommon edge is less than $\\pi$. This paper extends this property to higher\norder and uses it to generalize two classic properties from order-$1$ to\norder-$2$: (1) among the complete level-$2$ hypertriangulations of a generic\npoint set in $\\mathbb{R}^2$, the order-$2$ Delaunay triangulation\nlexicographically maximizes the sorted angle vector; (2) among the maximal\nlevel-$2$ hypertriangulations of a generic point set in $\\mathbb{R}^2$, the\norder-$2$ Delaunay triangulation is the only one that has the local angle\nproperty. We also use our method of establishing (2) to give a new short proof\nof the angle vector optimality for the (order-1) Delaunay triangulation. For\norder-$1$, both properties have been instrumental in numerous applications of\nDelaunay triangulations, and we expect that their generalization will make\norder-$2$ Delaunay triangulations more attractive to applications as well.\n","authors":["Herbert Edelsbrunner","Alexey Garber","Morteza Saghafian"],"pdf_url":"https://arxiv.org/pdf/2310.18238v4.pdf","comment":"32 pages, 9 figures"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.09406v1","updated":"2024-10-12T07:26:35Z","published":"2024-10-12T07:26:35Z","title":"Quantum Neural Network for Accelerated Magnetic Resonance Imaging","summary":"  Magnetic resonance image reconstruction starting from undersampled k-space\ndata requires the recovery of many potential nonlinear features, which is very\ndifficult for algorithms to recover these features. In recent years, the\ndevelopment of quantum computing has discovered that quantum convolution can\nimprove network accuracy, possibly due to potential quantum advantages. This\narticle proposes a hybrid neural network containing quantum and classical\nnetworks for fast magnetic resonance imaging, and conducts experiments on a\nquantum computer simulation system. The experimental results indicate that the\nhybrid network has achieved excellent reconstruction results, and also confirm\nthe feasibility of applying hybrid quantum-classical neural networks into the\nimage reconstruction of rapid magnetic resonance imaging.\n","authors":["Shuo Zhou","Yihang Zhou","Congcong Liu","Yanjie Zhu","Hairong Zheng","Dong Liang","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09406v1.pdf","comment":"Accepted at 2024 IEEE International Conference on Imaging Systems and\n  Techniques (IST 2024)"},{"id":"http://arxiv.org/abs/2110.10587v3","updated":"2024-10-12T04:03:29Z","published":"2021-10-20T14:29:47Z","title":"Quantum networks theory","summary":"  The formalism of quantum theory over discrete systems is extended in two\nsignificant ways. First, quantum evolutions are generalized to act over entire\nnetwork configurations, so that nodes may find themselves in a quantum\nsuperposition of being connected or not, and be allowed to merge, split and\nreconnect coherently in a superposition. Second, tensors and traceouts are\ngeneralized, so that systems can be partitioned according to almost arbitrary\nlogical predicates in a robust manner. The hereby presented mathematical\nframework is anchored on solid grounds through numerous lemmas. Indeed, one\nmight have feared that the familiar interrelations between the notions of\nunitarity, complete positivity, trace-preservation, non-signalling causality,\nlocality and localizability that are standard in quantum theory be jeopardized\nas the neighbourhood and partitioning between systems become both quantum,\ndynamical, and logical. Such interrelations in fact carry through, albeit two\nnew notions become instrumental: consistency and comprehension.\n","authors":["Pablo Arrighi","Am√©lia Durbec","Matt Wilson"],"pdf_url":"https://arxiv.org/pdf/2110.10587v3.pdf","comment":"53 pages, 11 figures; v2: generalised main def; v3: Quantum journal\n  version"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.09417v1","updated":"2024-10-12T07:49:23Z","published":"2024-10-12T07:49:23Z","title":"Neurally Integrated Finite Elements for Differentiable Elasticity on\n  Evolving Domains","summary":"  We present an elastic simulator for domains defined as evolving implicit\nfunctions, which is efficient, robust, and differentiable with respect to both\nshape and material. This simulator is motivated by applications in 3D\nreconstruction: it is increasingly effective to recover geometry from observed\nimages as implicit functions, but physical applications require accurately\nsimulating and optimizing-for the behavior of such shapes under deformation,\nwhich has remained challenging. Our key technical innovation is to train a\nsmall neural network to fit quadrature points for robust numerical integration\non implicit grid cells. When coupled with a Mixed Finite Element formulation,\nthis yields a smooth, fully differentiable simulation model connecting the\nevolution of the underlying implicit surface to its elastic response. We\ndemonstrate the efficacy of our approach on forward simulation of implicits,\ndirect simulation of 3D shapes during editing, and novel physics-based shape\nand topology optimizations in conjunction with differentiable rendering.\n","authors":["Gilles Daviet","Tianchang Shen","Nicholas Sharp","David I. W. Levin"],"pdf_url":"https://arxiv.org/pdf/2410.09417v1.pdf","comment":"16 pages, 21 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.09670v1","updated":"2024-10-12T23:35:45Z","published":"2024-10-12T23:35:45Z","title":"Making Beshbarmak: Games for Central Asian Cultural Heritage","summary":"  This paper introduces \"Making Beshbarmak\", an interactive cooking game that\ncelebrates the nomadic ancestry and cultural heritage of Central Asian\ncommunities worldwide. Designed to promote cultural appreciation and identity\nformation, the game invites players to learn and recreate the traditional dish\nBeshbarmak through an engaging step-by-step process, incorporating storytelling\nelements that explain the cultural significance of the meal. Our project\ncontributes to digital cultural heritage and games research by offering an\naccessible, open-source prototype on p5.js, enabling users to connect with and\nexplore Central Asian traditions. \"Making Beshbarmak\" serves as both an\neducational tool and a platform for cultural preservation, fostering a sense of\nbelonging among Central Asian immigrant populations.\n","authors":["Amina Kobenova","Adina Kaiymova"],"pdf_url":"https://arxiv.org/pdf/2410.09670v1.pdf","comment":"5 pages, 2 figures, EAI ArtsIT Conference"},{"id":"http://arxiv.org/abs/2410.19764v1","updated":"2024-10-12T16:14:18Z","published":"2024-10-12T16:14:18Z","title":"Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal\n  Synergy of Poster","summary":"  Movie posters are not just decorative; they are meticulously designed to\ncapture the essence of a movie, such as its genre, storyline, and tone/vibe.\nFor decades, movie posters have graced cinema walls, billboards, and now our\ndigital screens as a form of digital posters. Movie genre classification plays\na pivotal role in film marketing, audience engagement, and recommendation\nsystems. Previous explorations into movie genre classification have been mostly\nexamined in plot summaries, subtitles, trailers and movie scenes. Movie posters\nprovide a pre-release tantalizing glimpse into a film's key aspects, which can\nignite public interest. In this paper, we presented the framework that exploits\nmovie posters from a visual and textual perspective to address the multilabel\nmovie genre classification problem. Firstly, we extracted text from movie\nposters using an OCR and retrieved the relevant embedding. Next, we introduce a\ncross-attention-based fusion module to allocate attention weights to visual and\ntextual embedding. In validating our framework, we utilized 13882 posters\nsourced from the Internet Movie Database (IMDb). The outcomes of the\nexperiments indicate that our model exhibited promising performance and\noutperformed even some prominent contemporary architectures.\n","authors":["Utsav Kumar Nareti","Chandranath Adak","Soumi Chattopadhyay","Pichao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00557v2","updated":"2024-10-12T10:40:40Z","published":"2024-10-01T10:10:43Z","title":"STanH : Parametric Quantization for Variable Rate Learned Image\n  Compression","summary":"  In end-to-end learned image compression, encoder and decoder are jointly\ntrained to minimize a $R + {\\lambda}D$ cost function, where ${\\lambda}$\ncontrols the trade-off between rate of the quantized latent representation and\nimage quality. Unfortunately, a distinct encoder-decoder pair with millions of\nparameters must be trained for each ${\\lambda}$, hence the need to switch\nencoders and to store multiple encoders and decoders on the user device for\nevery target rate. This paper proposes to exploit a differentiable quantizer\ndesigned around a parametric sum of hyperbolic tangents, called STanH , that\nrelaxes the step-wise quantization function. STanH is implemented as a\ndifferentiable activation layer with learnable quantization parameters that can\nbe plugged into a pre-trained fixed rate model and refined to achieve different\ntarget bitrates. Experimental results show that our method enables variable\nrate coding with comparable efficiency to the state-of-the-art, yet with\nsignificant savings in terms of ease of deployment, training time, and storage\ncosts\n","authors":["Alberto Presta","Enzo Tartaglione","Attilio Fiandrotti","Marco Grangetto"],"pdf_url":"https://arxiv.org/pdf/2410.00557v2.pdf","comment":"Submitted to IEEE Transactions on Image Processing"}]},"2024-10-11T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.08784v1","updated":"2024-10-11T13:04:23Z","published":"2024-10-11T13:04:23Z","title":"Hybrid Filtering Heuristic for the Sensor-Placement Problem to\n  Discretize 2D Continuous Environments","summary":"  This paper addresses the sensor-placement problem (SPP) within the context of\ndiscretizing large, complex continuous 2D environments into graphs for\nefficient task-oriented route planning. The SPP aims to minimize the number of\nsensors required to achieve a user-defined coverage ratio while considering a\ngeneral visibility model. We propose the hybrid filtering heuristic (HFH)\nframework, which enhances or combines outputs of existing sensor-placement\nmethods, incorporating a filtering step. This step eliminates redundant sensors\nor those contributing marginally to the coverage, ensuring the coverage ratio\nremains within the desired interval. We implement two versions of HFH: the\nbasic version and a variant, HFHB, incorporating a preprocessing technique\nknown as bucketing to accelerate region clipping. We evaluate HFH and HFHB on a\ndataset of large, complex polygonal environments, comparing them to several\nbaseline methods under both unlimited and limited-range omnidirectional\nvisibility models. The results demonstrate that HFH and HFHB outperform\nbaselines in terms of the number of sensors required to achieve the desired\ncoverage ratio. Additionally, HFHB significantly reduces the runtime of more\ncompetitive baseline methods. We also adapt HFHB to a visibility model with\nlocalization uncertainty, demonstrating its effectiveness up to a certain level\nof uncertainty.\n","authors":["Jan Mikula","Miroslav Kulich"],"pdf_url":"https://arxiv.org/pdf/2410.08784v1.pdf","comment":"16 pages, 33 figures (including subfigures); submitted to the IEEE\n  Transactions on Robotics (T-RO); associated repository:\n  https://github.com/janmikulacz/spp"},{"id":"http://arxiv.org/abs/2210.13741v2","updated":"2024-10-11T05:08:39Z","published":"2022-10-25T03:14:59Z","title":"Deep Neural Networks as the Semi-classical Limit of Topological Quantum\n  Neural Networks: The problem of generalisation","summary":"  Deep Neural Networks miss a principled model of their operation. A novel\nframework for supervised learning based on Topological Quantum Field Theory\nthat looks particularly well suited for implementation on quantum processors\nhas been recently explored. We propose using this framework to understand the\nproblem of generalisation in Deep Neural Networks. More specifically, in this\napproach, Deep Neural Networks are viewed as the semi-classical limit of\nTopological Quantum Neural Networks. A framework of this kind explains the\noverfitting behavior of Deep Neural Networks during the training step and the\ncorresponding generalisation capabilities. We explore the paradigmatic case of\nthe perceptron, which we implement as the semiclassical limit of Topological\nQuantum Neural Networks. We apply a novel algorithm we developed, showing that\nit obtains similar results to standard neural networks, but without the need\nfor training (optimisation).\n","authors":["Antonino Marciano","Emanuele Zappala","Tommaso Torda","Matteo Lulli","Stefano Giagu","Chris Fields","Deen Chen","Filippo Fabrocini"],"pdf_url":"https://arxiv.org/pdf/2210.13741v2.pdf","comment":"22 pages (two columns), 9 figures. v2: Several parts rewritten, and\n  computational results added"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.19758v1","updated":"2024-10-11T14:42:05Z","published":"2024-10-11T14:42:05Z","title":"A Scored Non-Deterministic Finite Automata Processor for Sequence\n  Alignment","summary":"  The rapid growth of symbolic data in areas like internet, biological, and\nfinancial data has increased the demand for efficient pattern matching and\nregular expression processing. Non-deterministic Finite Automata (NFA) are used\nfor these tasks, but general-purpose platforms often face memory bottlenecks\ndue to the concurrent nature of NFAs. To address this, Domain-Specific\nArchitectures (DSAs) like FPGA and ASIC-based automata processors have been\ndeveloped for improved efficiency. However, many modern applications require\nidentifying the optimal match path, such as in DNA sequence alignment, which\ndemands scoring methods to evaluate the best match. This work enhances the\nFPGA-based NAPOLY automata processor by integrating scoring capabilities,\ncreating an extended version called NAPOLY+ that assigns weights to\ntransitions, enabling the identification of the highest scoring path.\nImplementing this approach introduces challenges, including increased state\nspace complexity and resource demands due to multiple active paths. The NAPOLY+\nsystem addresses these by incorporating arithmetic components to calculate\nscores along paths and using efficient memory management to maintain\nscalability. Experimental evaluation on the Zynq Ultrascale+ ZCU104 FPGA\ndemonstrated high device utilization and performance variations based on array\nsize and fan-out. While results are preliminary, ongoing testing will include\nreal datasets to assess the end-to-end performance of NAPOLY+ in practical\napplications such as BLAST.\n","authors":["Ryan Karbowniczak Rasha Karakchi"],"pdf_url":"https://arxiv.org/pdf/2410.19758v1.pdf","comment":"3 pages, 1 figure, accepted Supercomputing 2024 Poster Session"},{"id":"http://arxiv.org/abs/2410.19757v1","updated":"2024-10-11T12:05:47Z","published":"2024-10-11T12:05:47Z","title":"Complexity and nonlinearity of colloid electrical transducers","summary":"  This work explores the complexity and nonlinearity of seven different\ncolloidal suspensions-Au, ferrofluid, TiO2}, ZnO, g-C3N4, MXene, and\nPEDOT:PSS-when electrically stimulated with fractal, chaotic, and random binary\nsignals. The recorded electrical responses were analyzed using entropy, file\ncompression, fractal dimension, and Fisher information measures to quantify\ncomplexity. The nonlinearity introduced by each colloid was evaluated by the\ndeviation of the output from the best-fit hyperplane of the input-output\nmapping. The results showed that TiO2 was the most complex colloid across all\ninputs, exhibiting high entropy, poor compressibility, and an unpredictable\nresponse pattern. The colloids also exhibited significant nonlinearity, making\nthem promising candidates for reservoir computation, where the mapping of\ninputs into high-dimensional nonlinear states is advantageous. This study\nprovides insight into the dynamics of colloids and their potential for\nunconventional computational applications that exploit their inherent\ncomplexity and nonlinearity, and it provides a rapid method for assessing the\nsuitability of a particular material for use as a computational substrate\nbefore others.\n","authors":["Raphael Fortulan","Noushin Raeisi Kheirabadi","Alessandro Chiolerio","Andrew Adamatzky"],"pdf_url":"https://arxiv.org/pdf/2410.19757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08570v1","updated":"2024-10-11T06:49:26Z","published":"2024-10-11T06:49:26Z","title":"Predictive Tree-based Virtual Keyboard for Improved Gaze Typing","summary":"  On-screen keyboard eye-typing systems are limited due to the lack of\npredictive text and user-centred approaches, resulting in low text entry rates\nand frequent recalibration. This work proposes integrating the prediction by\npartial matching (PPM) technique into a tree-based virtual keyboard. We\ndeveloped the Flex-Tree on-screen keyboard using a two-stage tree-based\ncharacter selection system with ten commands, testing it with three degree of\nPPM (PPM1, PPM2, PPM3). Flex-Tree provides access to 72 English characters,\nincluding upper- and lower-case letters, numbers, and special characters, and\noffers functionalities like the delete command for corrections. The system was\nevaluated with sixteen healthy volunteers using two specially designed typing\ntasks, including the hand-picked and random-picked sentences. The spelling task\nwas performed using two input modalities: (i) a mouse and (ii) a portable\neye-tracker. Two experiments were conducted, encompassing 24 different\nconditions. The typing performance of Flex-Tree was compared with that of a\ntree-based virtual keyboard with an alphabetic arrangement (NoPPM) and the\nDasher on-screen keyboard for new users. Flex-Tree with PPM3 outperformed the\nother keyboards, achieving average text entry speeds of 27.7 letters/min with a\nmouse and 16.3 letters/min with an eye-tracker. Using the eye-tracker, the\ninformation transfer rates at the command and letter levels were 108.4 bits/min\nand 100.7 bits/min, respectively. Flex-Tree, across all three degree of PPM,\nreceived high ratings on the system usability scale and low-weighted ratings on\nthe NASA Task Load Index for both input modalities, highlighting its\nuser-centred design.\n","authors":["Hrushikesh Etikikota","Yogesh Kumar Meena"],"pdf_url":"https://arxiv.org/pdf/2410.08570v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.08983v1","updated":"2024-10-11T16:57:02Z","published":"2024-10-11T16:57:02Z","title":"DEL: Discrete Element Learner for Learning 3D Particle Dynamics with\n  Neural Rendering","summary":"  Learning-based simulators show great potential for simulating particle\ndynamics when 3D groundtruth is available, but per-particle correspondences are\nnot always accessible. The development of neural rendering presents a new\nsolution to this field to learn 3D dynamics from 2D images by inverse\nrendering. However, existing approaches still suffer from ill-posed natures\nresulting from the 2D to 3D uncertainty, for example, specific 2D images can\ncorrespond with various 3D particle distributions. To mitigate such\nuncertainty, we consider a conventional, mechanically interpretable framework\nas the physical priors and extend it to a learning-based version. In brief, we\nincorporate the learnable graph kernels into the classic Discrete Element\nAnalysis (DEA) framework to implement a novel mechanics-integrated learning\nsystem. In this case, the graph network kernels are only used for approximating\nsome specific mechanical operators in the DEA framework rather than the whole\ndynamics mapping. By integrating the strong physics priors, our methods can\neffectively learn the dynamics of various materials from the partial 2D\nobservations in a unified manner. Experiments show that our approach\noutperforms other learned simulators by a large margin in this context and is\nrobust to different renderers, fewer training samples, and fewer camera views.\n","authors":["Jiaxu Wang","Jingkai Sun","Junhao He","Ziyi Zhang","Qiang Zhang","Mingyuan Sun","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2410.08983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08885v1","updated":"2024-10-11T15:01:02Z","published":"2024-10-11T15:01:02Z","title":"Can GPTs Evaluate Graphic Design Based on Design Principles?","summary":"  Recent advancements in foundation models show promising capability in graphic\ndesign generation. Several studies have started employing Large Multimodal\nModels (LMMs) to evaluate graphic designs, assuming that LMMs can properly\nassess their quality, but it is unclear if the evaluation is reliable. One way\nto evaluate the quality of graphic design is to assess whether the design\nadheres to fundamental graphic design principles, which are the designer's\ncommon practice. In this paper, we compare the behavior of GPT-based evaluation\nand heuristic evaluation based on design principles using human annotations\ncollected from 60 subjects. Our experiments reveal that, while GPTs cannot\ndistinguish small details, they have a reasonably good correlation with human\nannotation and exhibit a similar tendency to heuristic metrics based on design\nprinciples, suggesting that they are indeed capable of assessing the quality of\ngraphic design. Our dataset is available at\nhttps://cyberagentailab.github.io/Graphic-design-evaluation .\n","authors":["Daichi Haraguchi","Naoto Inoue","Wataru Shimoda","Hayato Mitani","Seiichi Uchida","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2410.08885v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 (Technical Communications Track)"},{"id":"http://arxiv.org/abs/2410.06963v2","updated":"2024-10-11T14:12:48Z","published":"2024-10-09T15:02:08Z","title":"ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling","summary":"  This paper introduces ELMO, a real-time upsampling motion capture framework\ndesigned for a single LiDAR sensor. Modeled as a conditional autoregressive\ntransformer-based upsampling motion generator, ELMO achieves 60 fps motion\ncapture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is\nthe coupling of the self-attention mechanism with thoughtfully designed\nembedding modules for motion and point clouds, significantly elevating the\nmotion quality. To facilitate accurate motion capture, we develop a one-time\nskeleton calibration model capable of predicting user skeleton offsets from a\nsingle-frame point cloud. Additionally, we introduce a novel data augmentation\ntechnique utilizing a LiDAR simulator, which enhances global root tracking to\nimprove environmental understanding. To demonstrate the effectiveness of our\nmethod, we compare ELMO with state-of-the-art methods in both image-based and\npoint cloud-based motion capture. We further conduct an ablation study to\nvalidate our design principles. ELMO's fast inference time makes it well-suited\nfor real-time applications, exemplified in our demo video featuring live\nstreaming and interactive gaming scenarios. Furthermore, we contribute a\nhigh-quality LiDAR-mocap synchronized dataset comprising 20 different subjects\nperforming a range of motions, which can serve as a valuable resource for\nfuture research. The dataset and evaluation code are available at {\\blue\n\\url{https://movin3d.github.io/ELMO_SIGASIA2024/}}\n","authors":["Deok-Kyeong Jang","Dongseok Yang","Deok-Yun Jang","Byeoli Choi","Donghoon Shin","Sung-hee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.06963v2.pdf","comment":"published at ACM Transactions on Graphics (Proc. SIGGRAPH ASIA), 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.19340v5","updated":"2024-10-11T18:52:25Z","published":"2024-07-27T21:00:36Z","title":"Integrating Large Language Models into a Tri-Modal Architecture for\n  Automated Depression Classification on the DAIC-WOZ","summary":"  Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.\n","authors":["Santosh V. Patapati"],"pdf_url":"https://arxiv.org/pdf/2407.19340v5.pdf","comment":"Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ"},{"id":"http://arxiv.org/abs/2410.19760v1","updated":"2024-10-11T15:38:05Z","published":"2024-10-11T15:38:05Z","title":"Movie Trailer Genre Classification Using Multimodal Pretrained Features","summary":"  We introduce a novel method for movie genre classification, capitalizing on a\ndiverse set of readily accessible pretrained models. These models extract\nhigh-level features related to visual scenery, objects, characters, text,\nspeech, music, and audio effects. To intelligently fuse these pretrained\nfeatures, we train small classifier models with low time and memory\nrequirements. Employing the transformer model, our approach utilizes all video\nand audio frames of movie trailers without performing any temporal pooling,\nefficiently exploiting the correspondence between all elements, as opposed to\nthe fixed and low number of frames typically used by traditional methods. Our\napproach fuses features originating from different tasks and modalities, with\ndifferent dimensionalities, different temporal lengths, and complex\ndependencies as opposed to current approaches. Our method outperforms\nstate-of-the-art movie genre classification models in terms of precision,\nrecall, and mean average precision (mAP). To foster future research, we make\nthe pretrained features for the entire MovieNet dataset, along with our genre\nclassification code and the trained models, publicly available.\n","authors":["Serkan Sulun","Paula Viana","Matthew E. P. Davies"],"pdf_url":"https://arxiv.org/pdf/2410.19760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08877v1","updated":"2024-10-11T14:54:08Z","published":"2024-10-11T14:54:08Z","title":"Interdependency Matters: Graph Alignment for Multivariate Time Series\n  Anomaly Detection","summary":"  Anomaly detection in multivariate time series (MTS) is crucial for various\napplications in data mining and industry. Current industrial methods typically\napproach anomaly detection as an unsupervised learning task, aiming to identify\ndeviations by estimating the normal distribution in noisy, label-free datasets.\nThese methods increasingly incorporate interdependencies between channels\nthrough graph structures to enhance accuracy. However, the role of\ninterdependencies is more critical than previously understood, as shifts in\ninterdependencies between MTS channels from normal to anomalous data are\nsignificant. This observation suggests that \\textit{anomalies could be detected\nby changes in these interdependency graph series}. To capitalize on this\ninsight, we introduce MADGA (MTS Anomaly Detection via Graph Alignment), which\nredefines anomaly detection as a graph alignment (GA) problem that explicitly\nutilizes interdependencies for anomaly detection. MADGA dynamically transforms\nsubsequences into graphs to capture the evolving interdependencies, and Graph\nalignment is performed between these graphs, optimizing an alignment plan that\nminimizes cost, effectively minimizing the distance for normal data and\nmaximizing it for anomalous data. Uniquely, our GA approach involves explicit\nalignment of both nodes and edges, employing Wasserstein distance for nodes and\nGromov-Wasserstein distance for edges. To our knowledge, this is the first\napplication of GA to MTS anomaly detection that explicitly leverages\ninterdependency for this purpose. Extensive experiments on diverse real-world\ndatasets validate the effectiveness of MADGA, demonstrating its capability to\ndetect anomalies and differentiate interdependencies, consistently achieving\nstate-of-the-art across various scenarios.\n","authors":["Yuanyi Wang","Haifeng Sun","Chengsen Wang","Mengde Zhu","Jingyu Wang","Wei Tang","Qi Qi","Zirui Zhuang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2410.08877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08692v1","updated":"2024-10-11T10:24:36Z","published":"2024-10-11T10:24:36Z","title":"Contrastive Knowledge Distillation for Robust Multimodal Sentiment\n  Analysis","summary":"  Multimodal sentiment analysis (MSA) systems leverage information from\ndifferent modalities to predict human sentiment intensities. Incomplete\nmodality is an important issue that may cause a significant performance drop in\nMSA systems. By generative imputation, i.e., recovering the missing data from\navailable data, systems may achieve robust performance but will lead to high\ncomputational costs. This paper introduces a knowledge distillation method,\ncalled `Multi-Modal Contrastive Knowledge Distillation' (MM-CKD), to address\nthe issue of incomplete modality in video sentiment analysis with lower\ncomputation cost, as a novel non-imputation-based method. We employ Multi-view\nSupervised Contrastive Learning (MVSC) to transfer knowledge from a teacher\nmodel to student models. This approach not only leverages cross-modal knowledge\nbut also introduces cross-sample knowledge with supervision, jointly improving\nthe performance of both teacher and student models through online learning. Our\nmethod gives competitive results with significantly lower computational costs\nthan state-of-the-art imputation-based methods.\n","authors":["Zhongyi Sang","Kotaro Funakoshi","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2410.08692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08642v1","updated":"2024-10-11T09:10:26Z","published":"2024-10-11T09:10:26Z","title":"More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram","summary":"  Research on conspiracy theories and related content online has traditionally\nfocused on textual data. To address the increasing prevalence of (audio-)visual\ndata on social media, and to capture the evolving and dynamic nature of this\ncommunication, researchers have begun to explore the potential of unsupervised\napproaches for analyzing multimodal online content. Our research contributes to\nthis field by exploring the potential of multimodal topic modeling for\nanalyzing conspiracy theories in German-language Telegram channels. Our work\nuses the BERTopic topic modeling approach in combination with CLIP for the\nanalysis of textual and visual data. We analyze a corpus of ~40, 000 Telegram\nmessages posted in October 2023 in 571 German-language Telegram channels known\nfor disseminating conspiracy theories and other deceptive content. We explore\nthe potentials and challenges of this approach for studying a medium-sized\ncorpus of user-generated, text-image online content. We offer insights into the\ndominant topics across modalities, different text and image genres discovered\nduring the analysis, quantitative inter-modal topic analyses, and a qualitative\ncase study of textual, visual, and multimodal narrative strategies in the\ncommunication of conspiracy theories.\n","authors":["Elisabeth Steffen"],"pdf_url":"https://arxiv.org/pdf/2410.08642v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.08620v1","updated":"2024-10-11T08:36:07Z","published":"2024-10-11T08:36:07Z","title":"Natural Language Induced Adversarial Images","summary":"  Research of adversarial attacks is important for AI security because it shows\nthe vulnerability of deep learning models and helps to build more robust\nmodels. Adversarial attacks on images are most widely studied, which include\nnoise-based attacks, image editing-based attacks, and latent space-based\nattacks. However, the adversarial examples crafted by these methods often lack\nsufficient semantic information, making it challenging for humans to understand\nthe failure modes of deep learning models under natural conditions. To address\nthis limitation, we propose a natural language induced adversarial image attack\nmethod. The core idea is to leverage a text-to-image model to generate\nadversarial images given input prompts, which are maliciously constructed to\nlead to misclassification for a target model. To adopt commercial text-to-image\nmodels for synthesizing more natural adversarial images, we propose an adaptive\ngenetic algorithm (GA) for optimizing discrete adversarial prompts without\nrequiring gradients and an adaptive word space reduction method for improving\nquery efficiency. We further used CLIP to maintain the semantic consistency of\nthe generated images. In our experiments, we found that some high-frequency\nsemantic information such as \"foggy\", \"humid\", \"stretching\", etc. can easily\ncause classifier errors. This adversarial semantic information exists not only\nin generated images but also in photos captured in the real world. We also\nfound that some adversarial semantic information can be transferred to unknown\nclassification tasks. Furthermore, our attack method can transfer to different\ntext-to-image models (e.g., Midjourney, DALL-E 3, etc.) and image classifiers.\nOur code is available at:\nhttps://github.com/zxp555/Natural-Language-Induced-Adversarial-Images.\n","authors":["Xiaopei Zhu","Peiyang Xu","Guanning Zeng","Yingpeng Dong","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2410.08620v1.pdf","comment":"Carmera-ready version. To appear in ACM MM 2024"},{"id":"http://arxiv.org/abs/2410.08530v1","updated":"2024-10-11T05:02:31Z","published":"2024-10-11T05:02:31Z","title":"Ego3DT: Tracking Every 3D Object in Ego-centric Videos","summary":"  The growing interest in embodied intelligence has brought ego-centric\nperspectives to contemporary research. One significant challenge within this\nrealm is the accurate localization and tracking of objects in ego-centric\nvideos, primarily due to the substantial variability in viewing angles.\nAddressing this issue, this paper introduces a novel zero-shot approach for the\n3D reconstruction and tracking of all objects from the ego-centric video. We\npresent Ego3DT, a novel framework that initially identifies and extracts\ndetection and segmentation information of objects within the ego environment.\nUtilizing information from adjacent video frames, Ego3DT dynamically constructs\na 3D scene of the ego view using a pre-trained 3D scene reconstruction model.\nAdditionally, we have innovated a dynamic hierarchical association mechanism\nfor creating stable 3D tracking trajectories of objects in ego-centric videos.\nMoreover, the efficacy of our approach is corroborated by extensive experiments\non two newly compiled datasets, with 1.04x - 2.90x in HOTA, showcasing the\nrobustness and accuracy of our method in diverse ego-centric scenarios.\n","authors":["Shengyu Hao","Wenhao Chai","Zhonghan Zhao","Meiqi Sun","Wendi Hu","Jieyang Zhou","Yixian Zhao","Qi Li","Yizhou Wang","Xi Li","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.08530v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/1609.07630v4","updated":"2024-10-11T00:55:00Z","published":"2016-09-24T14:49:31Z","title":"Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform","summary":"  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n","authors":["P. A. M. Oliveira","R. J. Cintra","F. M. Bayer","S. Kulasekera","A. Madanayake","V. A. Coutinho"],"pdf_url":"https://arxiv.org/pdf/1609.07630v4.pdf","comment":"Fixed typo in $C_g$ and $\\eta$ measurements from Table 1 (W A S\n  Aleixo); 11 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.08435v1","updated":"2024-10-11T00:41:46Z","published":"2024-10-11T00:41:46Z","title":"Symbolic Music Generation with Fine-grained Interactive Textural\n  Guidance","summary":"  The problem of symbolic music generation presents unique challenges due to\nthe combination of limited data availability and the need for high precision in\nnote pitch. To overcome these difficulties, we introduce Fine-grained Textural\nGuidance (FTG) within diffusion models to correct errors in the learned\ndistributions. By incorporating FTG, the diffusion models improve the accuracy\nof music generation, which makes them well-suited for advanced tasks such as\nprogressive music generation, improvisation and interactive music creation. We\nderive theoretical characterizations for both the challenges in symbolic music\ngeneration and the effect of the FTG approach. We provide numerical experiments\nand a demo page for interactive music generation with user input to showcase\nthe effectiveness of our approach.\n","authors":["Tingyu Zhu","Haoyu Liu","Zhimin Jiang","Zeyu Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08435v1.pdf","comment":null}]},"2024-10-10T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.08203v1","updated":"2024-10-10T17:59:26Z","published":"2024-10-10T17:59:26Z","title":"Complete and bi-continuous invariant of protein backbones under rigid\n  motion","summary":"  Proteins are large biomolecules that regulate all living organisms and\nconsist of one or several chains.The primary structure of a protein chain is a\nsequence of amino acid residues whose three main atoms (alpha-carbon, nitrogen,\nand carboxyl carbon) form a protein backbone. The tertiary (geometric)\nstructure is the rigid shape of a protein chain represented by atomic positions\nin a 3-dimensional space.\n  Because different geometric structures often have distinct functional\nproperties, it is important to continuously quantify differences in rigid\nshapes of protein backbones. Unfortunately, many widely used similarities of\nproteins fail axioms of a distance metric and discontinuously change under tiny\nperturbations of atoms.\n  This paper develops a complete invariant under rigid motion, which defines a\nLipschitz bi-continuous bijection from all rigid classes of protein backbones\nto a well-defined invariant space. The new invariant detected thousands of\n(near-)duplicates in the Protein Data Bank, whose presence inevitably skews\nmachine learning predictions. The resulting invariant space allows\nlow-dimensional maps with analytically defined coordinates that reveal\nsubstantial variability in the protein universe.\n","authors":["Olga Anosova","Alexey Gorelov","William Jeffcott","Ziqiu Jiang","Vitaliy Kurlin"],"pdf_url":"https://arxiv.org/pdf/2410.08203v1.pdf","comment":"The latest version is maintained at\n  http://kurlin.org/projects/complete-invariants-proteins.pdf"},{"id":"http://arxiv.org/abs/2406.19595v2","updated":"2024-10-10T14:02:34Z","published":"2024-06-28T01:33:37Z","title":"Strict Self-Assembly of Discrete Self-Similar Fractals in the abstract\n  Tile-Assembly Model","summary":"  This paper answers a long-standing open question in tile-assembly theory,\nnamely that it is possible to strictly assemble discrete self-similar fractals\n(DSSFs) in the abstract Tile-Assembly Model (aTAM). We prove this in 2 separate\nways, each taking advantage of a novel set of tools. One of our constructions\nshows that specializing the notion of a quine, a program which prints its own\noutput, to the language of tile-assembly naturally induces a fractal structure.\nThe other construction introduces self-describing circuits as a means to\nabstractly represent the information flow through a tile-assembly construction\nand shows that such circuits may be constructed for a relative of the\nSierpinski carpet, and indeed many other DSSFs, through a process of\nfixed-point iteration. This later result, or more specifically the machinery\nused in its construction, further enable us to provide a polynomial time\nprocedure for deciding whether any given subset of $\\mathbb{Z}^2$ will generate\nan aTAM producible DSSF. To this end, we also introduce the Tree Pump Theorem,\na result analogous to the important Window Movie Lemma, but with requirements\non the set of productions rather than on the self-assembling system itself.\n","authors":["Florent Becker","Daniel Hader","Matthew J. Patitz"],"pdf_url":"https://arxiv.org/pdf/2406.19595v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.08548"},{"id":"http://arxiv.org/abs/2410.07666v1","updated":"2024-10-10T07:19:31Z","published":"2024-10-10T07:19:31Z","title":"Computational Complexities of Folding","summary":"  We prove several hardness results on folding origami crease patterns.\nFlat-folding finite crease patterns is fixed-parameter tractable in the ply of\nthe folded pattern (how many layers overlap at any point) and the treewidth of\nan associated cell adjacency graph. Under the exponential time hypothesis, the\nsingly-exponential dependence of our algorithm on treewidth is necessary, even\nfor bounded ply. Improving the dependence on ply would require progress on the\nunsolved map folding problem. Finding the shape of a polyhedron folded from a\nnet with triangular faces and integer edge lengths is not possible in algebraic\ncomputation tree models of computation that at each tree node allow either the\ncomputation of arbitrary integer roots of real numbers, or the extraction of\nroots of polynomials with bounded degree and integer coefficients. For a model\nof reconfigurable origami with origami squares are attached at one edge by a\nhinge to a rigid surface, moving from one flat-folded state to another by\nchanging the position of one square at a time is PSPACE-complete, and counting\nflat-folded states is #P-complete. For self-similar square crease patterns with\ninfinitely many folds, testing flat-foldability is undecidable.\n","authors":["David Eppstein"],"pdf_url":"https://arxiv.org/pdf/2410.07666v1.pdf","comment":"39 pages, 27 figures. Section 3 incorporates material from\n  arXiv:2306.11939"},{"id":"http://arxiv.org/abs/2301.08460v6","updated":"2024-10-10T04:35:06Z","published":"2023-01-20T08:02:12Z","title":"Coresets for Constrained Clustering: General Assignment Constraints and\n  Improved Size Bounds","summary":"  Designing small-sized \\emph{coresets}, which approximately preserve the costs\nof the solutions for large datasets, has been an important research direction\nfor the past decade. We consider coreset construction for a variety of general\nconstrained clustering problems. We introduce a general class of assignment\nconstraints, including capacity constraints on cluster centers, and assignment\nstructure constraints for data points (modeled by a convex body $\\mathcal{B}$).\nWe give coresets for clustering problems with such general assignment\nconstraints that significantly generalize and improve known results. Notable\nimplications include the first $\\varepsilon$-coreset for capacitated and fair\n$k$-Median with $m$ outliers in Euclidean spaces whose size is $\\tilde{O}(m +\nk^2 \\varepsilon^{-4})$, generalizing and improving upon the prior bounds in\n[Braverman et al., FOCS' 22; Huang et al., ICLR' 23] (for capacitated\n$k$-Median, the coreset size bound obtained in [Braverman et al., FOCS' 22] is\n$\\tilde{O}(k^3 \\varepsilon^{-6})$, and for $k$-Median with $m$ outliers, the\ncoreset size bound obtained in [Huang et al., ICLR' 23]} is $\\tilde{O}(m + k^3\n\\varepsilon^{-5})$), and the first $\\epsilon$-coreset of size $\\mathrm{poly}(k\n\\varepsilon^{-1})$ for fault-tolerant clustering for various types of metric\nspaces.\n","authors":["Lingxiao Huang","Jian Li","Pinyan Lu","Xuan Wu"],"pdf_url":"https://arxiv.org/pdf/2301.08460v6.pdf","comment":"This is a merger with arXiv:2302.11151. The abstract is shortened due\n  to the length limit of arXiv. This paper has been accepted by SODA 2025"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2409.04578v2","updated":"2024-10-10T21:03:10Z","published":"2024-09-06T19:39:40Z","title":"Parallax: A Compiler for Neutral Atom Quantum Computers under Hardware\n  Constraints","summary":"  Among different quantum computing technologies, neutral atom quantum\ncomputers have several advantageous features, such as multi-qubit gates,\napplication-specific topologies, movable qubits, homogenous qubits, and\nlong-range interactions. However, existing compilation techniques for neutral\natoms fall short of leveraging these advantages in a practical and scalable\nmanner. This paper introduces Parallax, a zero-SWAP, scalable, and\nparallelizable compilation and atom movement scheduling method tailored for\nneutral atom systems, which reduces high-error operations by 25% and increases\nthe success rate by 28% on average compared to the state-of-the-art technique.\n","authors":["Jason Ludmir","Tirthak Patel"],"pdf_url":"https://arxiv.org/pdf/2409.04578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00557v3","updated":"2024-10-10T20:18:41Z","published":"2024-08-01T13:39:40Z","title":"End-to-End Protocol for High-Quality QAOA Parameters with Few Shots","summary":"  The quantum approximate optimization algorithm (QAOA) is a quantum heuristic\nfor combinatorial optimization that has been demonstrated to scale better than\nstate-of-the-art classical solvers for some problems. For a given problem\ninstance, QAOA performance depends crucially on the choice of the parameters.\nWhile average-case optimal parameters are available in many cases, meaningful\nperformance gains can be obtained by fine-tuning these parameters for a given\ninstance. This task is especially challenging, however, when the number of\ncircuit executions (shots) is limited. In this work, we develop an end-to-end\nprotocol that combines multiple parameter settings and fine-tuning techniques.\nWe use large-scale numerical experiments to optimize the protocol for the\nshot-limited setting and observe that optimizers with the simplest internal\nmodel (linear) perform best. We implement the optimized pipeline on a\ntrapped-ion processor using up to 32 qubits and 5 QAOA layers, and we\ndemonstrate that the pipeline is robust to small amounts of hardware noise. To\nthe best of our knowledge, these are the largest demonstrations of QAOA\nparameter fine-tuning on a trapped-ion processor in terms of 2-qubit gate\ncount.\n","authors":["Tianyi Hao","Zichang He","Ruslan Shaydulin","Jeffrey Larson","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2408.00557v3.pdf","comment":"14 pages, 13 figures, fix minor typos"},{"id":"http://arxiv.org/abs/2410.08252v1","updated":"2024-10-10T15:55:17Z","published":"2024-10-10T15:55:17Z","title":"Quantum simulation of single-server Markovian queues: A dynamic\n  amplification approach","summary":"  Quantum computing is revolutionizing various fields, including operations\nresearch and queueing theory. This study presents a quantum method for\nsimulating single-server Markovian (M/M/1) queues, making quantum computing\nmore accessible to researchers in operations research. We introduce a dynamic\namplification approach that adapts to queue traffic, potentially improving\nsimulation efficiency, and design custom-parameterized quantum gates for\narrival and service processes. This flexible framework enables modeling of\nvarious queueing scenarios while bridging quantum computing and classical\nqueueing theory. Notably, our quantum method shows potential advantages over\nclassical simulations, particularly in high-traffic scenarios. This quantum\nsimulation approach opens new possibilities for analyzing complex queueing\nsystems, potentially outperforming classical methods in challenging scenarios\nand paving the way for quantum-enhanced operations research. The method was\nimplemented and tested across low-, moderate-, and high-traffic scenarios,\ncomparing quantum simulations with both theoretical formulas and classical\nsimulations. Results demonstrate high agreement between quantum computations\nand theoretical predictions, with relative errors below 0.002 for effective\narrival rates in high-traffic scenarios. As the number of qubits increases, we\nobserve rapid convergence to theoretical values, with relative errors\ndecreasing by up to two orders of magnitude in some cases. Sensitivity analysis\nreveals optimal parameter regions yielding errors lower than 0.001.\n","authors":["Michal Koren","Or Peretz"],"pdf_url":"https://arxiv.org/pdf/2410.08252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07920v1","updated":"2024-10-10T13:47:30Z","published":"2024-10-10T13:47:30Z","title":"Post-Training Quantization in Brain-Computer Interfaces based on\n  Event-Related Potential Detection","summary":"  Post-training quantization (PTQ) is a technique used to optimize and reduce\nthe memory footprint and computational requirements of machine learning models.\nIt has been used primarily for neural networks. For Brain-Computer Interfaces\n(BCI) that are fully portable and usable in various situations, it is necessary\nto provide approaches that are lightweight for storage and computation. In this\npaper, we propose the evaluation of post-training quantization on\nstate-of-the-art approaches in brain-computer interfaces and assess their\nimpact on accuracy. We evaluate the performance of the single-trial detection\nof event-related potentials representing one major BCI paradigm. The area under\nthe receiver operating characteristic curve drops from 0.861 to 0.825 with PTQ\nwhen applied on both spatial filters and the classifier, while reducing the\nsize of the model by about $\\times$ 15. The results support the conclusion that\nPTQ can substantially reduce the memory footprint of the models while keeping\nroughly the same level of accuracy.\n","authors":["Hubert Cecotti","Dalvir Dhaliwal","Hardip Singh","Yogesh Kumar Meena"],"pdf_url":"https://arxiv.org/pdf/2410.07920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06920v2","updated":"2024-10-10T12:59:54Z","published":"2024-10-09T14:18:12Z","title":"To Be or Not to Be (in the EU): Measurement of Discrepancies Presented\n  in Cookie Paywalls","summary":"  Cookie paywalls allow visitors to access the content of a website only after\nmaking a choice between paying a fee (paying option) or accepting tracking\n(cookie option). The practice has been studied in previous research in regard\nto its prevalence and legal standing, but the effects of the clients' device\nand geographic location remain unexplored. To address these questions, this\nstudy explores the effects of three factors: 1) the clients' browser, 2) the\ndevice type (desktop or mobile), and 3) the geographic location on the presence\nand behavior of cookie paywalls and the handling of users' data.\n  Using an automatic crawler on our dataset composed of 804 websites that\npresent a cookie paywall, we observed that the presence of a cookie paywall was\nmost affected by the geographic location of the user. We further showed that\nboth the behavior of a cookie paywall and the processing of user data are\nimpacted by all three factors, but no patterns of significance could be found.\nFinally, an additional type of paywall was discovered to be used on\napproximately 11% of the studied websites, coined the \"double paywall\", which\nconsists of a cookie paywall complemented by another paywall once tracking is\naccepted.\n","authors":["Andreas Stenwreth","Simon T√§ng","Victor Morel"],"pdf_url":"https://arxiv.org/pdf/2410.06920v2.pdf","comment":"Submitted to ICISSP 2025"},{"id":"http://arxiv.org/abs/2410.07791v1","updated":"2024-10-10T10:22:45Z","published":"2024-10-10T10:22:45Z","title":"Heracles: A HfO$\\mathrm{_2}$ Ferroelectric Capacitor Compact Model for\n  Efficient Circuit Simulations","summary":"  This paper presents a physics-based compact model for circuit simulations in\na SPICE environment for HfO2-based ferroelectric capacitors (FeCaps). The model\nhas been calibrated based on experimental data obtained from HfO2-based FeCaps.\nA thermal model with an accurate description of the device parasitics is\nincluded to derive precise device characteristics based on first principles.\nThe model incorporates statistical data that enables Monte Carlo analysis based\non realistic distributions, thereby making it particularly well-suited for\ndesign-technology co-optimization (DTCO). Furthermore, the model is\ndemonstrated in circuit simulations using an integrated circuit with current\nprogramming, wherein partial switching of the ferroelectric polarization is\nobserved. Finally, the model was benchmarked in an array simulation, reaching\nconvergence in 1.8 s with an array size of 100 kb.\n","authors":["Luca Fehlings","Md Hanif Ali","Paolo Gibertini","Egidio A. Gallicchio","Udayan Ganguly","Veeresh Deshpande","Erika Covi"],"pdf_url":"https://arxiv.org/pdf/2410.07791v1.pdf","comment":"6 pages, 7 figures"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.08282v1","updated":"2024-10-10T18:07:07Z","published":"2024-10-10T18:07:07Z","title":"FusionSense: Bridging Common Sense, Vision, and Touch for Robust\n  Sparse-View Reconstruction","summary":"  Humans effortlessly integrate common-sense knowledge with sensory input from\nvision and touch to understand their surroundings. Emulating this capability,\nwe introduce FusionSense, a novel 3D reconstruction framework that enables\nrobots to fuse priors from foundation models with highly sparse observations\nfrom vision and tactile sensors. FusionSense addresses three key challenges:\n(i) How can robots efficiently acquire robust global shape information about\nthe surrounding scene and objects? (ii) How can robots strategically select\ntouch points on the object using geometric and common-sense priors? (iii) How\ncan partial observations such as tactile signals improve the overall\nrepresentation of the object? Our framework employs 3D Gaussian Splatting as a\ncore representation and incorporates a hierarchical optimization strategy\ninvolving global structure construction, object visual hull pruning and local\ngeometric constraints. This advancement results in fast and robust perception\nin environments with traditionally challenging objects that are transparent,\nreflective, or dark, enabling more downstream manipulation or navigation tasks.\nExperiments on real-world data suggest that our framework outperforms\npreviously state-of-the-art sparse-view methods. All code and data are\nopen-sourced on the project website.\n","authors":["Irving Fang","Kairui Shi","Xujin He","Siqi Tan","Yifan Wang","Hanwen Zhao","Hung-Jui Huang","Wenzhen Yuan","Chen Feng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08190v1","updated":"2024-10-10T17:57:29Z","published":"2024-10-10T17:57:29Z","title":"Poison-splat: Computation Cost Attack on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3DGS), known for its groundbreaking performance and\nefficiency, has become a dominant 3D representation and brought progress to\nmany 3D vision tasks. However, in this work, we reveal a significant security\nvulnerability that has been largely overlooked in 3DGS: the computation cost of\ntraining 3DGS could be maliciously tampered by poisoning the input data. By\ndeveloping an attack named Poison-splat, we reveal a novel attack surface where\nthe adversary can poison the input images to drastically increase the\ncomputation memory and time needed for 3DGS training, pushing the algorithm\ntowards its worst computation complexity. In extreme cases, the attack can even\nconsume all allocable memory, leading to a Denial-of-Service (DoS) that\ndisrupts servers, resulting in practical damages to real-world 3DGS service\nvendors. Such a computation cost attack is achieved by addressing a bi-level\noptimization problem through three tailored strategies: attack objective\napproximation, proxy model rendering, and optional constrained optimization.\nThese strategies not only ensure the effectiveness of our attack but also make\nit difficult to defend with simple defensive measures. We hope the revelation\nof this novel attack surface can spark attention to this crucial yet overlooked\nvulnerability of 3DGS systems.\n","authors":["Jiahao Lu","Yifan Zhang","Qiuhong Shen","Xinchao Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2410.08190v1.pdf","comment":"Our code is available at https://github.com/jiahaolu97/poison-splat"},{"id":"http://arxiv.org/abs/2410.08188v1","updated":"2024-10-10T17:56:44Z","published":"2024-10-10T17:56:44Z","title":"DifFRelight: Diffusion-Based Facial Performance Relighting","summary":"  We present a novel framework for free-viewpoint facial performance relighting\nusing diffusion-based image-to-image translation. Leveraging a subject-specific\ndataset containing diverse facial expressions captured under various lighting\nconditions, including flat-lit and one-light-at-a-time (OLAT) scenarios, we\ntrain a diffusion model for precise lighting control, enabling high-fidelity\nrelit facial images from flat-lit inputs. Our framework includes\nspatially-aligned conditioning of flat-lit captures and random noise, along\nwith integrated lighting information for global control, utilizing prior\nknowledge from the pre-trained Stable Diffusion model. This model is then\napplied to dynamic facial performances captured in a consistent flat-lit\nenvironment and reconstructed for novel-view synthesis using a scalable dynamic\n3D Gaussian Splatting method to maintain quality and consistency in the relit\nresults. In addition, we introduce unified lighting control by integrating a\nnovel area lighting representation with directional lighting, allowing for\njoint adjustments in light size and direction. We also enable high dynamic\nrange imaging (HDRI) composition using multiple directional lights to produce\ndynamic sequences under complex lighting conditions. Our evaluations\ndemonstrate the models efficiency in achieving precise lighting control and\ngeneralizing across various facial expressions while preserving detailed\nfeatures such as skintexture andhair. The model accurately reproduces complex\nlighting effects like eye reflections, subsurface scattering, self-shadowing,\nand translucency, advancing photorealism within our framework.\n","authors":["Mingming He","Pascal Clausen","Ahmet Levent Ta≈üel","Li Ma","Oliver Pilarski","Wenqi Xian","Laszlo Rikker","Xueming Yu","Ryan Burgert","Ning Yu","Paul Debevec"],"pdf_url":"https://arxiv.org/pdf/2410.08188v1.pdf","comment":"18 pages, SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers\n  '24), December 3--6, 2024, Tokyo, Japan. Project page:\n  https://www.eyelinestudios.com/research/diffrelight.html"},{"id":"http://arxiv.org/abs/2410.08257v1","updated":"2024-10-10T17:43:36Z","published":"2024-10-10T17:43:36Z","title":"Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics","summary":"  While humans effortlessly discern intrinsic dynamics and adapt to new\nscenarios, modern AI systems often struggle. Current methods for visual\ngrounding of dynamics either use pure neural-network-based simulators (black\nbox), which may violate physical laws, or traditional physical simulators\n(white box), which rely on expert-defined equations that may not fully capture\nactual dynamics. We propose the Neural Material Adaptor (NeuMA), which\nintegrates existing physical laws with learned corrections, facilitating\naccurate learning of actual dynamics while maintaining the generalizability and\ninterpretability of physical priors. Additionally, we propose Particle-GS, a\nparticle-driven 3D Gaussian Splatting variant that bridges simulation and\nobserved images, allowing back-propagate image gradients to optimize the\nsimulator. Comprehensive experiments on various dynamics in terms of grounded\nparticle accuracy, dynamic rendering quality, and generalization ability\ndemonstrate that NeuMA can accurately capture intrinsic dynamics.\n","authors":["Junyi Cao","Shanyan Guan","Yanhao Ge","Wei Li","Xiaokang Yang","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2410.08257v1.pdf","comment":"NeurIPS 2024, the project page:\n  https://xjay18.github.io/projects/neuma.html"},{"id":"http://arxiv.org/abs/2410.08129v1","updated":"2024-10-10T17:14:16Z","published":"2024-10-10T17:14:16Z","title":"Efficient Perspective-Correct 3D Gaussian Splatting Using Hybrid\n  Transparency","summary":"  3D Gaussian Splats (3DGS) have proven a versatile rendering primitive, both\nfor inverse rendering as well as real-time exploration of scenes. In these\napplications, coherence across camera frames and multiple views is crucial, be\nit for robust convergence of a scene reconstruction or for artifact-free\nfly-throughs. Recent work started mitigating artifacts that break multi-view\ncoherence, including popping artifacts due to inconsistent transparency sorting\nand perspective-correct outlines of (2D) splats. At the same time, real-time\nrequirements forced such implementations to accept compromises in how\ntransparency of large assemblies of 3D Gaussians is resolved, in turn breaking\ncoherence in other ways. In our work, we aim at achieving maximum coherence, by\nrendering fully perspective-correct 3D Gaussians while using a high-quality\napproximation of accurate blending, hybrid transparency, on a per-pixel level,\nin order to retain real-time frame rates. Our fast and perspectively accurate\napproach for evaluation of 3D Gaussians does not require matrix inversions,\nthereby ensuring numerical stability and eliminating the need for special\nhandling of degenerate splats, and the hybrid transparency formulation for\nblending maintains similar quality as fully resolved per-pixel transparencies\nat a fraction of the rendering costs. We further show that each of these two\ncomponents can be independently integrated into Gaussian splatting systems. In\ncombination, they achieve up to 2$\\times$ higher frame rates, 2$\\times$ faster\noptimization, and equal or better image quality with fewer rendering artifacts\ncompared to traditional 3DGS on common benchmarks.\n","authors":["Florian Hahlbohm","Fabian Friederichs","Tim Weyrich","Linus Franke","Moritz Kappel","Susana Castillo","Marc Stamminger","Martin Eisemann","Marcus Magnor"],"pdf_url":"https://arxiv.org/pdf/2410.08129v1.pdf","comment":"Project page: https://fhahlbohm.github.io/htgs/"},{"id":"http://arxiv.org/abs/2409.20140v3","updated":"2024-10-10T17:05:42Z","published":"2024-09-30T09:42:10Z","title":"RISE-SDF: a Relightable Information-Shared Signed Distance Field for\n  Glossy Object Inverse Rendering","summary":"  In this paper, we propose a novel end-to-end relightable neural inverse\nrendering system that achieves high-quality reconstruction of geometry and\nmaterial properties, thus enabling high-quality relighting. The cornerstone of\nour method is a two-stage approach for learning a better factorization of scene\nparameters. In the first stage, we develop a reflection-aware radiance field\nusing a neural signed distance field (SDF) as the geometry representation and\ndeploy an MLP (multilayer perceptron) to estimate indirect illumination. In the\nsecond stage, we introduce a novel information-sharing network structure to\njointly learn the radiance field and the physically based factorization of the\nscene. For the physically based factorization, to reduce the noise caused by\nMonte Carlo sampling, we apply a split-sum approximation with a simplified\nDisney BRDF and cube mipmap as the environment light representation. In the\nrelighting phase, to enhance the quality of indirect illumination, we propose a\nsecond split-sum algorithm to trace secondary rays under the split-sum\nrendering framework. Furthermore, there is no dataset or protocol available to\nquantitatively evaluate the inverse rendering performance for glossy objects.\nTo assess the quality of material reconstruction and relighting, we have\ncreated a new dataset with ground truth BRDF parameters and relighting results.\nOur experiments demonstrate that our algorithm achieves state-of-the-art\nperformance in inverse rendering and relighting, with particularly strong\nresults in the reconstruction of highly reflective objects.\n","authors":["Deheng Zhang","Jingyu Wang","Shaofei Wang","Marko Mihajlovic","Sergey Prokudin","Hendrik P. A. Lensch","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2409.20140v3.pdf","comment":"https://dehezhang2.github.io/RISE-SDF/"},{"id":"http://arxiv.org/abs/2408.16762v2","updated":"2024-10-10T14:48:57Z","published":"2024-08-29T17:57:05Z","title":"UV-free Texture Generation with Denoising and Geodesic Heat Diffusions","summary":"  Seams, distortions, wasted UV space, vertex-duplication, and varying\nresolution over the surface are the most prominent issues of the standard\nUV-based texturing of meshes. These issues are particularly acute when\nautomatic UV-unwrapping techniques are used. For this reason, instead of\ngenerating textures in automatically generated UV-planes like most\nstate-of-the-art methods, we propose to represent textures as coloured\npoint-clouds whose colours are generated by a denoising diffusion probabilistic\nmodel constrained to operate on the surface of 3D objects. Our sampling and\nresolution agnostic generative model heavily relies on heat diffusion over the\nsurface of the meshes for spatial communication between points. To enable\nprocessing of arbitrarily sampled point-cloud textures and ensure long-distance\ntexture consistency we introduce a fast re-sampling of the mesh spectral\nproperties used during the heat diffusion and introduce a novel\nheat-diffusion-based self-attention mechanism. Our code and pre-trained models\nare available at github.com/simofoti/UV3-TeD.\n","authors":["Simone Foti","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2408.16762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07971v1","updated":"2024-10-10T14:29:00Z","published":"2024-10-10T14:29:00Z","title":"Generalizable and Animatable Gaussian Head Avatar","summary":"  In this paper, we propose Generalizable and Animatable Gaussian head Avatar\n(GAGAvatar) for one-shot animatable head avatar reconstruction. Existing\nmethods rely on neural radiance fields, leading to heavy rendering consumption\nand low reenactment speeds. To address these limitations, we generate the\nparameters of 3D Gaussians from a single image in a single forward pass. The\nkey innovation of our work is the proposed dual-lifting method, which produces\nhigh-fidelity 3D Gaussians that capture identity and facial details.\nAdditionally, we leverage global image features and the 3D morphable model to\nconstruct 3D Gaussians for controlling expressions. After training, our model\ncan reconstruct unseen identities without specific optimizations and perform\nreenactment rendering at real-time speeds. Experiments show that our method\nexhibits superior performance compared to previous methods in terms of\nreconstruction quality and expression accuracy. We believe our method can\nestablish new benchmarks for future research and advance applications of\ndigital avatars. Code and demos are available\nhttps://github.com/xg-chu/GAGAvatar.\n","authors":["Xuangeng Chu","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2410.07971v1.pdf","comment":"NeurIPS 2024, code is available at\n  https://github.com/xg-chu/GAGAvatar, more demos are available at\n  https://xg-chu.site/project_gagavatar"},{"id":"http://arxiv.org/abs/2405.15891v3","updated":"2024-10-10T09:25:13Z","published":"2024-05-24T19:22:09Z","title":"Score Distillation via Reparametrized DDIM","summary":"  While 2D diffusion models generate realistic, high-detail images, 3D shape\ngeneration methods like Score Distillation Sampling (SDS) built on these 2D\ndiffusion models produce cartoon-like, over-smoothed shapes. To help explain\nthis discrepancy, we show that the image guidance used in Score Distillation\ncan be understood as the velocity field of a 2D denoising generative process,\nup to the choice of a noise term. In particular, after a change of variables,\nSDS resembles a high-variance version of Denoising Diffusion Implicit Models\n(DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d.\nrandomly at each step, while DDIM infers it from the previous noise\npredictions. This excessive variance can lead to over-smoothing and unrealistic\noutputs. We show that a better noise approximation can be recovered by\ninverting DDIM in each SDS update step. This modification makes SDS's\ngenerative process for 2D images almost identical to DDIM. In 3D, it removes\nover-smoothing, preserves higher-frequency detail, and brings the generation\nquality closer to that of 2D samplers. Experimentally, our method achieves\nbetter or similar 3D generation quality compared to other state-of-the-art\nScore Distillation methods, all without training additional neural networks or\nmulti-view supervision, and providing useful insights into relationship between\n2D and 3D asset generation with diffusion models.\n","authors":["Artem Lukoianov","Haitz S√°ez de Oc√°riz Borde","Kristjan Greenewald","Vitor Campagnolo Guizilini","Timur Bagautdinov","Vincent Sitzmann","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2405.15891v3.pdf","comment":"NeurIPS 2024. 28 pages, 30 figures. Revision: additional comparisons\n  and ablations studies"},{"id":"http://arxiv.org/abs/2410.07707v1","updated":"2024-10-10T08:19:47Z","published":"2024-10-10T08:19:47Z","title":"MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian\n  Splatting","summary":"  Dynamic scene reconstruction is a long-term challenge in the field of 3D\nvision. Recently, the emergence of 3D Gaussian Splatting has provided new\ninsights into this problem. Although subsequent efforts rapidly extend static\n3D Gaussian to dynamic scenes, they often lack explicit constraints on object\nmotion, leading to optimization difficulties and performance degradation. To\naddress the above issues, we propose a novel deformable 3D Gaussian splatting\nframework called MotionGS, which explores explicit motion priors to guide the\ndeformation of 3D Gaussians. Specifically, we first introduce an optical flow\ndecoupling module that decouples optical flow into camera flow and motion flow,\ncorresponding to camera movement and object motion respectively. Then the\nmotion flow can effectively constrain the deformation of 3D Gaussians, thus\nsimulating the motion of dynamic objects. Additionally, a camera pose\nrefinement module is proposed to alternately optimize 3D Gaussians and camera\nposes, mitigating the impact of inaccurate camera poses. Extensive experiments\nin the monocular dynamic scenes validate that MotionGS surpasses\nstate-of-the-art methods and exhibits significant superiority in both\nqualitative and quantitative results. Project page:\nhttps://ruijiezhu94.github.io/MotionGS_page\n","authors":["Ruijie Zhu","Yanzhe Liang","Hanzhi Chang","Jiacheng Deng","Jiahao Lu","Wenfei Yang","Tianzhu Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.07707v1.pdf","comment":"Accepted by NeurIPS 2024. 21 pages, 14 figures,7 tables"},{"id":"http://arxiv.org/abs/2406.08308v2","updated":"2024-10-10T05:42:17Z","published":"2024-06-12T15:09:03Z","title":"FSH3D: 3D Representation via Fibonacci Spherical Harmonics","summary":"  Spherical harmonics are a favorable technique for 3D representation,\nemploying a frequency-based approach through the spherical harmonic transform\n(SHT). Typically, SHT is performed using equiangular sampling grids. However,\nthese grids are non-uniform on spherical surfaces and exhibit local anisotropy,\na common limitation in existing spherical harmonic decomposition methods. This\npaper proposes a 3D representation method using Fibonacci Spherical Harmonics\n(FSH3D). We introduce a spherical Fibonacci grid (SFG), which is more uniform\nthan equiangular grids for SHT in the frequency domain. Our method employs\nanalytical weights for SHT on SFG, effectively assigning sampling errors to\nspherical harmonic degrees higher than the recovered band-limited function.\nThis provides a novel solution for spherical harmonic transformation on\nnon-equiangular grids. The key advantages of our FSH3D method include: 1) With\nthe same number of sampling points, SFG captures more features without bias\ncompared to equiangular grids; 2) The root mean square error of 32-degree\nspherical harmonic coefficients is reduced by approximately 34.6% for SFG\ncompared to equiangular grids; and 3) FSH3D offers more stable frequency domain\nrepresentations, especially for rotating functions. FSH3D enhances the\nstability of frequency domain representations under rotational transformations.\nIts application in 3D shape reconstruction and 3D shape classification results\nin more accurate and robust representations. Our code is publicly available at\nhttps://github.com/Miraclelzk/Fibonacci-Spherical-Harmonics.\n","authors":["Zikuan Li","Anyi Huang","Wenru Jia","Qiaoyun Wu","Mingqiang Wei","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08308v2.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.06231v2","updated":"2024-10-10T05:41:49Z","published":"2024-10-08T17:40:01Z","title":"RelitLRM: Generative Relightable Radiance for Large Reconstruction\n  Models","summary":"  We propose RelitLRM, a Large Reconstruction Model (LRM) for generating\nhigh-quality Gaussian splatting representations of 3D objects under novel\nilluminations from sparse (4-8) posed images captured under unknown static\nlighting. Unlike prior inverse rendering methods requiring dense captures and\nslow optimization, often causing artifacts like incorrect highlights or shadow\nbaking, RelitLRM adopts a feed-forward transformer-based model with a novel\ncombination of a geometry reconstructor and a relightable appearance generator\nbased on diffusion. The model is trained end-to-end on synthetic multi-view\nrenderings of objects under varying known illuminations. This architecture\ndesign enables to effectively decompose geometry and appearance, resolve the\nambiguity between material and lighting, and capture the multi-modal\ndistribution of shadows and specularity in the relit appearance. We show our\nsparse-view feed-forward RelitLRM offers competitive relighting results to\nstate-of-the-art dense-view optimization-based baselines while being\nsignificantly faster. Our project page is available at:\nhttps://relit-lrm.github.io/.\n","authors":["Tianyuan Zhang","Zhengfei Kuang","Haian Jin","Zexiang Xu","Sai Bi","Hao Tan","He Zhang","Yiwei Hu","Milos Hasan","William T. Freeman","Kai Zhang","Fujun Luan"],"pdf_url":"https://arxiv.org/pdf/2410.06231v2.pdf","comment":"webpage: https://relit-lrm.github.io/"},{"id":"http://arxiv.org/abs/2407.07090v3","updated":"2024-10-10T00:44:52Z","published":"2024-07-09T17:59:30Z","title":"3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes","summary":"  Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.\n","authors":["Nicolas Moenne-Loccoz","Ashkan Mirzaei","Or Perel","Riccardo de Lutio","Janick Martinez Esturo","Gavriel State","Sanja Fidler","Nicholas Sharp","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2407.07090v3.pdf","comment":"Project page: https://gaussiantracer.github.io/. Published at\n  SIGGRAPH Asia 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2404.12257v2","updated":"2024-10-10T20:02:07Z","published":"2024-04-18T15:23:37Z","title":"Food Portion Estimation via 3D Object Scaling","summary":"  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods. The dataset can be accessed\nat: https://lorenz.ecn.purdue.edu/~gvinod/simplefood45/ and the code can be\naccessed at: https://gitlab.com/viper-purdue/monocular-food-volume-3d\n","authors":["Gautham Vinod","Jiangpeng He","Zeman Shao","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.12257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08174v1","updated":"2024-10-10T17:50:42Z","published":"2024-10-10T17:50:42Z","title":"Sample then Identify: A General Framework for Risk Control and\n  Assessment in Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) exhibit promising advancements\nacross various tasks, yet they still encounter significant trustworthiness\nissues. Prior studies apply Split Conformal Prediction (SCP) in language\nmodeling to construct prediction sets with statistical guarantees. However,\nthese methods typically rely on internal model logits or are restricted to\nmultiple-choice settings, which hampers their generalizability and adaptability\nin dynamic, open-ended environments. In this paper, we introduce TRON, a\ntwo-step framework for risk control and assessment, applicable to any MLLM that\nsupports sampling in both open-ended and closed-ended scenarios. TRON comprises\ntwo main components: (1) a novel conformal score to sample response sets of\nminimum size, and (2) a nonconformity score to identify high-quality responses\nbased on self-consistency theory, controlling the error rates by two specific\nrisk levels. Furthermore, we investigate semantic redundancy in prediction sets\nwithin open-ended contexts for the first time, leading to a promising\nevaluation metric for MLLMs based on average set size. Our comprehensive\nexperiments across four Video Question-Answering (VideoQA) datasets utilizing\neight MLLMs show that TRON achieves desired error rates bounded by two\nuser-specified risk levels. Additionally, deduplicated prediction sets maintain\nadaptiveness while being more efficient and stable for risk assessment under\ndifferent risk levels.\n","authors":["Qingni Wang","Tiantian Geng","Zhiyuan Wang","Teng Wang","Bo Fu","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08174v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.10719v2","updated":"2024-10-10T14:47:58Z","published":"2024-09-16T20:47:00Z","title":"Benchmarking VLMs' Reasoning About Persuasive Atypical Images","summary":"  Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.\n","authors":["Sina Malakouti","Aysan Aghazadeh","Ashmit Khandelwal","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.10719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07854v1","updated":"2024-10-10T12:20:58Z","published":"2024-10-10T12:20:58Z","title":"HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with\n  Heterogeneous Graph Adapter","summary":"  Adapter-based tuning methods have shown significant potential in transferring\nknowledge from pre-trained Vision-Language Models to the downstream tasks.\nHowever, after reviewing existing adapters, we find they generally fail to\nfully explore the interactions between different modalities in constructing\ntask-specific knowledge. Also, existing works usually only focus on similarity\nmatching between positive text prompts, making it challenging to distinguish\nthe classes with high similar visual contents. To address these issues, in this\npaper, we propose a novel Heterogeneous Graph Adapter to achieve tuning VLMs\nfor the downstream tasks. To be specific, we first construct a unified\nheterogeneous graph mode, which contains i) visual nodes, positive text nodes\nand negative text nodes, and ii) several types of edge connections to\ncomprehensively model the intra-modality, inter-modality and inter-class\nstructure knowledge together. Next, we employ a specific Heterogeneous Graph\nNeural Network to excavate multi-modality structure knowledge for adapting both\nvisual and textual features for the downstream tasks. Finally, after\nHeGraphAdapter, we construct both text-based and visual-based classifiers\nsimultaneously to comprehensively enhance the performance of the CLIP model.\nExperimental results on 11 benchmark datasets demonstrate the effectiveness and\nbenefits of the proposed HeGraphAdapter.\n","authors":["Yumiao Zhao","Bo Jiang","Xiao Wang","Qin Xu","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.07854v1.pdf","comment":null}]},"2024-10-09T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.07419v1","updated":"2024-10-09T20:32:57Z","published":"2024-10-09T20:32:57Z","title":"Reconfigurations of Plane Caterpillars and Paths","summary":"  Let $S$ be a point set in the plane, $\\mathcal{P}(S)$ and $\\mathcal{C}(S)$\nsets of all plane spanning paths and caterpillars on $S$. We study\nreconfiguration operations on $\\mathcal{P}(S)$ and $\\mathcal{C}(S)$. In\nparticular, we prove that all of the commonly studied reconfigurations on plane\nspanning trees still yield connected reconfiguration graphs for caterpillars\nwhen $S$ is in convex position. If $S$ is in general position, we show that the\nrotation, compatible flip and flip graphs of $\\mathcal{C}(S)$ are connected\nwhile the slide graph is disconnected. For paths, we prove the existence of a\nconnected component of size at least $2^{n-1}$ and that no component of size at\nmost $7$ can exist in the flip graph on $\\mathcal{P}(S)$.\n","authors":["Todor Antiƒá","Guillermo Gamboa Quintero","Jelena Gli≈°iƒá"],"pdf_url":"https://arxiv.org/pdf/2410.07419v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.07059v1","updated":"2024-10-09T16:58:36Z","published":"2024-10-09T16:58:36Z","title":"Online Epsilon Net and Piercing Set for Geometric Concepts","summary":"  VC-dimension and $\\varepsilon$-nets are key concepts in Statistical Learning\nTheory. Intuitively, VC-dimension is a measure of the size of a class of sets.\nThe famous $\\varepsilon$-net theorem, a fundamental result in Discrete\nGeometry, asserts that if the VC-dimension of a set system is bounded, then a\nsmall sample exists that intersects all sufficiently large sets.\n  In online learning scenarios where data arrives sequentially, the\nVC-dimension helps to bound the complexity of the set system, and\n$\\varepsilon$-nets ensure the selection of a small representative set. This\nsampling framework is crucial in various domains, including spatial data\nanalysis, motion planning in dynamic environments, optimization of sensor\nnetworks, and feature extraction in computer vision, among others. Motivated by\nthese applications, we study the online $\\varepsilon$-net problem for geometric\nconcepts with bounded VC-dimension. While the offline version of this problem\nhas been extensively studied, surprisingly, there are no known theoretical\nresults for the online version to date. We present the first deterministic\nonline algorithm with an optimal competitive ratio for intervals in\n$\\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal\ncompetitive ratio for axis-aligned boxes in $\\mathbb{R}^d$, for $d\\le 3$.\nFurthermore, we introduce a novel technique to analyze similar-sized objects of\nconstant description complexity in $\\mathbb{R}^d$, which may be of independent\ninterest. Next, we focus on the continuous version of this problem, where\nranges of the set system are geometric concepts in $\\mathbb{R}^d$ arriving in\nan online manner, but the universe is the entire space, and the objective is to\nchoose a small sample that intersects all the ranges.\n","authors":["Sujoy Bhore","Devdan Dey","Satyam Singh"],"pdf_url":"https://arxiv.org/pdf/2410.07059v1.pdf","comment":"18 pages, 4 Figures"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2408.13389v2","updated":"2024-10-09T22:08:17Z","published":"2024-08-23T21:58:45Z","title":"ReCon: Reconfiguring Analog Rydberg Atom Quantum Computers for Quantum\n  Generative Adversarial Networks","summary":"  Quantum computing has shown theoretical promise of speedup in several machine\nlearning tasks, including generative tasks using generative adversarial\nnetworks (GANs). While quantum computers have been implemented with different\ntypes of technologies, recently, analog Rydberg atom quantum computers have\nbeen demonstrated to have desirable properties such as reconfigurable qubit\n(quantum bit) positions and multi-qubit operations. To leverage the properties\nof this technology, we propose ReCon, the first work to implement quantum GANs\non analog Rydberg atom quantum computers. Our evaluation using simulations and\nreal-computer executions shows 33% better quality (measured using Frechet\nInception Distance (FID)) in generated images than the state-of-the-art\ntechnique implemented on superconducting-qubit technology.\n","authors":["Nicholas S. DiBrita","Daniel Leeds","Yuqian Huo","Jason Ludmir","Tirthak Patel"],"pdf_url":"https://arxiv.org/pdf/2408.13389v2.pdf","comment":"ReCon will appear in the Proceedings of the International Conference\n  on Computer-Aided Design (ICCAD), 2024"},{"id":"http://arxiv.org/abs/2410.07389v1","updated":"2024-10-09T19:17:33Z","published":"2024-10-09T19:17:33Z","title":"MIMO MAC Empowered by Reconfigurable Intelligent Surfaces: Capacity\n  Region and Large System Analysis","summary":"  Smart wireless environments enabled by multiple distributed Reconfigurable\nIntelligent Surfaces (RISs) have recently attracted significant research\ninterest as a wireless connectivity paradigm for sixth Generation (6G)\nnetworks. In this paper, using random matrix theory methods, we calculate the\nmean of the sum Mutual Information (MI) for the correlated Multiple-Input\nMultiple-Output (MIMO) Multiple Access Channel (MAC) in the presence of\nmultiple RISs, in the large-antenna number limit. We thus obtain the capacity\nregion boundaries, after optimizing over the tunable RISs' phase\nconfigurations. Furthermore, we obtain a closed-form expression for the\nvariance of the sum-MI metric, which together with the mean provides a tight\nGaussian approximation for the outage probability. The derived results become\nrelevant in the presence of fast-fading, when channel estimation is extremely\nchallenging. Our numerical investigations showcased that, when the angle-spread\nin the neighborhood of each RIS is small, which is expected for higher carrier\nfrequencies, the communication link strongly improves from optimizing the\nergodic MI of the multiple RISs.We also found that, increasing the number of\ntransmitting users in such MIMO-MAC-RIS systems results to rapidly diminishing\nsum-MI gains, hence, providing limits on the number of users that can be\nefficiently served by a given RIS.\n","authors":["Aris L. Moustakas","George C. Alexandropoulos"],"pdf_url":"https://arxiv.org/pdf/2410.07389v1.pdf","comment":"14 pages, 5 figures, to appear in an IEEE Transactions"},{"id":"http://arxiv.org/abs/2410.04154v2","updated":"2024-10-09T13:56:28Z","published":"2024-10-05T13:29:25Z","title":"Applying Quantum Autoencoders for Time Series Anomaly Detection","summary":"  Anomaly detection is an important problem with applications in various\ndomains such as fraud detection, pattern recognition or medical diagnosis.\nSeveral algorithms have been introduced using classical computing approaches.\nHowever, using quantum computing for solving anomaly detection problems in time\nseries data is a widely unexplored research field.\n  This paper explores the application of quantum autoencoders to time series\nanomaly detection. We investigate two primary techniques for classifying\nanomalies: (1) Analyzing the reconstruction error generated by the quantum\nautoencoder and (2) latent representation analysis. Our simulated experimental\nresults, conducted across various ansaetze, demonstrate that quantum\nautoencoders consistently outperform classical deep learning-based autoencoders\nacross multiple datasets. Specifically, quantum autoencoders achieve superior\nanomaly detection performance while utilizing 60-230 times fewer parameters and\nrequiring five times fewer training iterations. In addition, we implement our\nquantum encoder on real quantum hardware. Our experimental results demonstrate\nthat quantum autoencoders achieve anomaly detection performance on par with\ntheir simulated counterparts.\n","authors":["Robin Frehner","Kurt Stockinger"],"pdf_url":"https://arxiv.org/pdf/2410.04154v2.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.06855v1","updated":"2024-10-09T13:18:48Z","published":"2024-10-09T13:18:48Z","title":"RIS-Assisted ISAC: Precoding and Phase-Shift Optimization for\n  Mono-Static Target Detection","summary":"  The reconfigurable intelligent surface (RIS) technology emerges as a highly\nuseful component of the rapidly evolving integrated sensing and communications\nparadigm, primarily owing to its remarkable signal-to-noise ratio enhancement\ncapabilities. In this paper, our focus is on mono-static target detection while\nconsidering the communication requirement of a user equipment. Both sensing and\ncommunication benefit from the presence of an RIS, which makes the channels\nricher and stronger. Diverging from prior research, we comprehensively examine\nthree target echo paths: the direct (static) channel path, the path via the\nRIS, and a combination of these, each characterized by distinct radar cross\nsections (RCSs). We take both the line-of-sight (LOS) and the non-line-of-sight\n(NLOS) paths into account under a clutter for which the distribution is not\nknown, but the low-rank subspace it resides. We derive the generalized\nlikelihood ratio test (GLRT) detector and introduce a novel approach for\njointly optimizing the configuration of RIS phase-shifts and precoding. Our\nsimulation results underscore the paramount importance of this combined design\nin terms of enhancing detection probability. Moreover, it becomes evident that\nthe derived clutter-aware target detection significantly enhances detection\nperformance, especially when the clutter is strong.\n","authors":["√ñzlem Tuƒüfe Demir","Emil Bj√∂rnson"],"pdf_url":"https://arxiv.org/pdf/2410.06855v1.pdf","comment":"6 pages, 3 figures, accepted to be presented at IEEE GLOBECOM 2024"},{"id":"http://arxiv.org/abs/2410.06698v1","updated":"2024-10-09T09:06:37Z","published":"2024-10-09T09:06:37Z","title":"Fourier-based Action Recognition for Wildlife Behavior Quantification\n  with Event Cameras","summary":"  Event cameras are novel bio-inspired vision sensors that measure pixel-wise\nbrightness changes asynchronously instead of images at a given frame rate. They\noffer promising advantages, namely a high dynamic range, low latency, and\nminimal motion blur. Modern computer vision algorithms often rely on artificial\nneural network approaches, which require image-like representations of the data\nand cannot fully exploit the characteristics of event data. We propose\napproaches to action recognition based on the Fourier Transform. The approaches\nare intended to recognize oscillating motion patterns commonly present in\nnature. In particular, we apply our approaches to a recent dataset of breeding\npenguins annotated for \"ecstatic display\", a behavior where the observed\npenguins flap their wings at a certain frequency. We find that our approaches\nare both simple and effective, producing slightly lower results than a deep\nneural network (DNN) while relying just on a tiny fraction of the parameters\ncompared to the DNN (five orders of magnitude fewer parameters). They work well\ndespite the uncontrolled, diverse data present in the dataset. We hope this\nwork opens a new perspective on event-based processing and action recognition.\n","authors":["Friedhelm Hamann","Suman Ghosh","Ignacio Juarez Martinez","Tom Hart","Alex Kacelnik","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2410.06698v1.pdf","comment":"11 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2410.06512v1","updated":"2024-10-09T03:26:41Z","published":"2024-10-09T03:26:41Z","title":"In-Band Full-Duplex MIMO Systems for Simultaneous Communications and\n  Sensing: Challenges, Methods, and Future Perspectives","summary":"  In-band Full-Duplex (FD) Multiple-Input Multiple-Output (MIMO) systems offer\na significant opportunity for Integrated Sensing and Communications (ISAC) due\nto their capability to realize simultaneous signal transmissions and\nreceptions. This feature has been recently exploited to devise\nspectrum-efficient simultaneous information transmission and monostatic sensing\noperations, a line of research typically referred to as MIMO FD-ISAC. In this\narticle, capitalizing on a recent FD MIMO architecture with reduced complexity\nanalog cancellation, we present an FD-enabled framework for simultaneous\ncommunications and sensing using data signals. In contrast to communications\napplications, the framework's goal is not to mitigate self interference, since\nit includes reflections of the downlink data transmissions from targets in the\nFD node's vicinity, but to optimize the system parameters for the intended dual\nfunctionality. The unique characteristics and challenges of a generic MIMO\nFD-ISAC system are discussed along with a broad overview of state-of-the-art\nspecial cases, including numerical investigations. Several directions for\nfuture work on FD-enabled ISAC relevant to signal processing communities are\nalso provided.\n","authors":["Besma Smida","George C. Alexandropoulos","Taneli Riihonen","Md Atiqul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.06512v1.pdf","comment":"12 pages, 5 figures, White Paper to appear at IEEE SPM"}],"Graphics":[{"id":"http://arxiv.org/abs/2403.02460v4","updated":"2024-10-09T17:52:09Z","published":"2024-03-04T20:20:14Z","title":"MagicClay: Sculpting Meshes With Generative Neural Fields","summary":"  The recent developments in neural fields have brought phenomenal capabilities\nto the field of shape generation, but they lack crucial properties, such as\nincremental control - a fundamental requirement for artistic work. Triangular\nmeshes, on the other hand, are the representation of choice for most geometry\nrelated tasks, offering efficiency and intuitive control, but do not lend\nthemselves to neural optimization. To support downstream tasks, previous art\ntypically proposes a two-step approach, where first a shape is generated using\nneural fields, and then a mesh is extracted for further processing. Instead, in\nthis paper we introduce a hybrid approach that maintains both a mesh and a\nSigned Distance Field (SDF) representations consistently. Using this\nrepresentation, we introduce MagicClay - an artist friendly tool for sculpting\nregions of a mesh according to textual prompts while keeping other regions\nuntouched. Our framework carefully and efficiently balances consistency between\nthe representations and regularizations in every step of the shape\noptimization; Relying on the mesh representation, we show how to render the SDF\nat higher resolutions and faster. In addition, we employ recent work in\ndifferentiable mesh reconstruction to adaptively allocate triangles in the mesh\nwhere required, as indicated by the SDF. Using an implemented prototype, we\ndemonstrate superior generated geometry compared to the state-of-the-art, and\nnovel consistent control, allowing sequential prompt-based edits to the same\nmesh for the first time.\n","authors":["Amir Barda","Vladimir G. Kim","Noam Aigerman","Amit H. Bermano","Thibault Groueix"],"pdf_url":"https://arxiv.org/pdf/2403.02460v4.pdf","comment":"project page: https://amirbarda.github.io/MagicClay.github.io/"},{"id":"http://arxiv.org/abs/2410.06985v1","updated":"2024-10-09T15:21:46Z","published":"2024-10-09T15:21:46Z","title":"Jointly Generating Multi-view Consistent PBR Textures using\n  Collaborative Control","summary":"  Multi-view consistency remains a challenge for image diffusion models. Even\nwithin the Text-to-Texture problem, where perfect geometric correspondences are\nknown a priori, many methods fail to yield aligned predictions across views,\nnecessitating non-trivial fusion methods to incorporate the results onto the\noriginal mesh. We explore this issue for a Collaborative Control workflow\nspecifically in PBR Text-to-Texture. Collaborative Control directly models PBR\nimage probability distributions, including normal bump maps; to our knowledge,\nthe only diffusion model to directly output full PBR stacks. We discuss the\ndesign decisions involved in making this model multi-view consistent, and\ndemonstrate the effectiveness of our approach in ablation studies, as well as\npractical applications.\n","authors":["Shimon Vainer","Konstantin Kutsy","Dante De Nigris","Ciara Rowles","Slava Elizarov","Simon Donn√©"],"pdf_url":"https://arxiv.org/pdf/2410.06985v1.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2402.00525v3","updated":"2024-10-09T12:57:43Z","published":"2024-02-01T11:46:44Z","title":"StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time\n  Rendering","summary":"  Gaussian Splatting has emerged as a prominent model for constructing 3D\nrepresentations from images across diverse domains. However, the efficiency of\nthe 3D Gaussian Splatting rendering pipeline relies on several simplifications.\nNotably, reducing Gaussian to 2D splats with a single view-space depth\nintroduces popping and blending artifacts during view rotation. Addressing this\nissue requires accurate per-pixel depth computation, yet a full per-pixel sort\nproves excessively costly compared to a global sort operation. In this paper,\nwe present a novel hierarchical rasterization approach that systematically\nresorts and culls splats with minimal processing overhead. Our software\nrasterizer effectively eliminates popping artifacts and view inconsistencies,\nas demonstrated through both quantitative and qualitative measurements.\nSimultaneously, our method mitigates the potential for cheating view-dependent\neffects with popping, ensuring a more authentic representation. Despite the\nelimination of cheating, our approach achieves comparable quantitative results\nfor test images, while increasing the consistency for novel view synthesis in\nmotion. Due to its design, our hierarchical approach is only 4% slower on\naverage than the original Gaussian Splatting. Notably, enforcing consistency\nenables a reduction in the number of Gaussians by approximately half with\nnearly identical quality and view-consistency. Consequently, rendering\nperformance is nearly doubled, making our approach 1.6x faster than the\noriginal Gaussian Splatting, with a 50% reduction in memory requirements.\n","authors":["Lukas Radl","Michael Steiner","Mathias Parger","Alexander Weinrauch","Bernhard Kerbl","Markus Steinberger"],"pdf_url":"https://arxiv.org/pdf/2402.00525v3.pdf","comment":"SIGGRAPH 2024 (Journal Track); Project Page:\n  https://r4dl.github.io/StopThePop/"},{"id":"http://arxiv.org/abs/2405.13839v2","updated":"2024-10-09T11:05:57Z","published":"2024-05-22T17:04:54Z","title":"Diffusing Winding Gradients (DWG): A Parallel and Scalable Method for 3D\n  Reconstruction from Unoriented Point Clouds","summary":"  This paper presents a new method, Diffusing Winding Gradients (DWG), for\nreconstructing watertight 3D surfaces from unoriented point clouds. Our method\nexploits the alignment between the gradients of the generalized winding number\n(GWN) field and globally consistent normals to orient points effectively.\nStarting with an unoriented point cloud, DWG initially assigns a random normal\nto each point. It computes the corresponding GWN field and extract a level set\nwhose iso-value is the average GWN values across all input points. The\ngradients of this level set are then utilized to update the point normals. This\ncycle of recomputing the GWN field and updating point normals is repeated until\nthe GWN level sets stabilize and their gradients cease to change. Unlike\nconventional methods, our method does not rely on solving linear systems or\noptimizing objective functions, which simplifies its implementation and\nenhances its suitability for efficient parallel execution. Experimental results\ndemonstrate that our method significantly outperforms existing methods in terms\nof runtime performance. For large-scale models with 10 to 20 million points,\nour CUDA implementation on an NVIDIA GTX 4090 GPU achieves speeds 30-120 times\nfaster than iPSR, the leading sequential method, tested on a high-end PC with\nan Intel i9 CPU. Additionally, by employing a screened variant of GWN, DWG\ndemonstrates enhanced robustness against noise and outliers, and proves\neffective for models with thin structures and real-world inputs with\noverlapping and misaligned scans. For source code and more details, visit our\nproject webpage: https://dwgtech.github.io/.\n","authors":["Weizhou Liu","Jiaze Li","Xuhui Chen","Fei Hou","Shiqing Xin","Xingce Wang","Zhongke Wu","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2405.13839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13180v2","updated":"2024-10-09T02:29:57Z","published":"2024-09-20T03:17:01Z","title":"FreeAvatar: Robust 3D Facial Animation Transfer by Learning an\n  Expression Foundation Model","summary":"  Video-driven 3D facial animation transfer aims to drive avatars to reproduce\nthe expressions of actors. Existing methods have achieved remarkable results by\nconstraining both geometric and perceptual consistency. However, geometric\nconstraints (like those designed on facial landmarks) are insufficient to\ncapture subtle emotions, while expression features trained on classification\ntasks lack fine granularity for complex emotions. To address this, we propose\n\\textbf{FreeAvatar}, a robust facial animation transfer method that relies\nsolely on our learned expression representation. Specifically, FreeAvatar\nconsists of two main components: the expression foundation model and the facial\nanimation transfer model. In the first component, we initially construct a\nfacial feature space through a face reconstruction task and then optimize the\nexpression feature space by exploring the similarities among different\nexpressions. Benefiting from training on the amounts of unlabeled facial images\nand re-collected expression comparison dataset, our model adapts freely and\neffectively to any in-the-wild input facial images. In the facial animation\ntransfer component, we propose a novel Expression-driven Multi-avatar Animator,\nwhich first maps expressive semantics to the facial control parameters of 3D\navatars and then imposes perceptual constraints between the input and output\nimages to maintain expression consistency. To make the entire process\ndifferentiable, we employ a trained neural renderer to translate rig parameters\ninto corresponding images. Furthermore, unlike previous methods that require\nseparate decoders for each avatar, we propose a dynamic identity injection\nmodule that allows for the joint training of multiple avatars within a single\nnetwork.\n","authors":["Feng Qiu","Wei Zhang","Chen Liu","Rudong An","Lincheng Li","Yu Ding","Changjie Fan","Zhipeng Hu","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2409.13180v2.pdf","comment":"11 pages, 10 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.07415v1","updated":"2024-10-09T20:29:16Z","published":"2024-10-09T20:29:16Z","title":"3D2M Dataset: A 3-Dimension diverse Mesh Dataset","summary":"  Three-dimensional (3D) reconstruction has emerged as a prominent area of\nresearch, attracting significant attention from academia and industry alike.\nAmong the various applications of 3D reconstruction, facial reconstruction\nposes some of the most formidable challenges. Additionally, each individuals\nfacial structure is unique, requiring algorithms to be robust enough to handle\nthis variability while maintaining fidelity to the original features. This\narticle presents a comprehensive dataset of 3D meshes featuring a diverse range\nof facial structures and corresponding facial landmarks. The dataset comprises\n188 3D facial meshes, including 73 from female candidates and 114 from male\ncandidates. It encompasses a broad representation of ethnic backgrounds, with\ncontributions from 45 different ethnicities, ensuring a rich diversity in\nfacial characteristics. Each facial mesh is accompanied by key points that\naccurately annotate the relevant features, facilitating precise analysis and\nmanipulation. This dataset is particularly valuable for applications such as\nfacial re targeting, the study of facial structure components, and real-time\nperson representation in video streams. By providing a robust resource for\nresearchers and developers, it aims to advance the field of 3D facial\nreconstruction and related technologies.\n","authors":["Sankarshan Dasgupta"],"pdf_url":"https://arxiv.org/pdf/2410.07415v1.pdf","comment":"6 pages, 1 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.07369v1","updated":"2024-10-09T18:33:06Z","published":"2024-10-09T18:33:06Z","title":"An undetectable watermark for generative image models","summary":"  We present the first undetectable watermarking scheme for generative image\nmodels. Undetectability ensures that no efficient adversary can distinguish\nbetween watermarked and un-watermarked images, even after making many adaptive\nqueries. In particular, an undetectable watermark does not degrade image\nquality under any efficiently computable metric. Our scheme works by selecting\nthe initial latents of a diffusion model using a pseudorandom error-correcting\ncode (Christ and Gunn, 2024), a strategy which guarantees undetectability and\nrobustness. We experimentally demonstrate that our watermarks are\nquality-preserving and robust using Stable Diffusion 2.1. Our experiments\nverify that, in contrast to every prior scheme we tested, our watermark does\nnot degrade image quality. Our experiments also demonstrate robustness:\nexisting watermark removal attacks fail to remove our watermark from images\nwithout significantly degrading the quality of the images. Finally, we find\nthat we can robustly encode 512 bits in our watermark, and up to 2500 bits when\nthe images are not subjected to watermark removal attacks. Our code is\navailable at https://github.com/XuandongZhao/PRC-Watermark.\n","authors":["Sam Gunn","Xuandong Zhao","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2410.07369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07336v1","updated":"2024-10-09T18:00:09Z","published":"2024-10-09T18:00:09Z","title":"Positive-Augmented Contrastive Learning for Vision-and-Language\n  Evaluation and Training","summary":"  Despite significant advancements in caption generation, existing evaluation\nmetrics often fail to capture the full quality or fine-grained details of\ncaptions. This is mainly due to their reliance on non-specific human-written\nreferences or noisy pre-training data. Still, finding an effective metric is\ncrucial not only for captions evaluation but also for the generation phase.\nMetrics can indeed play a key role in the fine-tuning stage of captioning\nmodels, ultimately enhancing the quality of the generated captions. In this\npaper, we propose PAC-S++, a learnable metric that leverages the CLIP model,\npre-trained on both web-collected and cleaned data and regularized through\nadditional pairs of generated visual and textual positive samples. Exploiting\nthis stronger and curated pre-training, we also apply PAC-S++ as a reward in\nthe Self-Critical Sequence Training (SCST) stage typically employed to\nfine-tune captioning models. Extensive experiments on different image and video\ndatasets highlight the effectiveness of PAC-S++ compared to popular metrics for\nthe task, including its sensitivity to object hallucinations. Furthermore, we\nshow that integrating PAC-S++ into the fine-tuning stage of a captioning model\nresults in semantically richer captions with fewer repetitions and grammatical\nerrors. Evaluations on out-of-domain benchmarks further demonstrate the\nefficacy of our fine-tuning approach in enhancing model capabilities. Source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/pacscore.\n","authors":["Sara Sarto","Nicholas Moratelli","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2410.07336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05412v2","updated":"2024-10-09T16:49:58Z","published":"2023-12-08T23:55:19Z","title":"CMMD: Contrastive Multi-Modal Diffusion for Video-Audio Conditional\n  Modeling","summary":"  We introduce a multi-modal diffusion model tailored for the bi-directional\nconditional generation of video and audio. We propose a joint contrastive\ntraining loss to improve the synchronization between visual and auditory\noccurrences. We present experiments on two datasets to evaluate the efficacy of\nour proposed model. The assessment of generation quality and alignment\nperformance is carried out from various angles, encompassing both objective and\nsubjective metrics. Our findings demonstrate that the proposed model\noutperforms the baseline in terms of quality and generation speed through\nintroduction of our novel cross-modal easy fusion architectural block.\nFurthermore, the incorporation of the contrastive loss results in improvements\nin audio-visual alignment, particularly in the high-correlation video-to-audio\ngeneration task.\n","authors":["Ruihan Yang","Hannes Gamper","Sebastian Braun"],"pdf_url":"https://arxiv.org/pdf/2312.05412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06725v1","updated":"2024-10-09T09:46:53Z","published":"2024-10-09T09:46:53Z","title":"Evaluating the Impact of Point Cloud Colorization on Semantic\n  Segmentation Accuracy","summary":"  Point cloud semantic segmentation, the process of classifying each point into\npredefined categories, is essential for 3D scene understanding. While\nimage-based segmentation is widely adopted due to its maturity, methods relying\nsolely on RGB information often suffer from degraded performance due to color\ninaccuracies. Recent advancements have incorporated additional features such as\nintensity and geometric information, yet RGB channels continue to negatively\nimpact segmentation accuracy when errors in colorization occur. Despite this,\nprevious studies have not rigorously quantified the effects of erroneous\ncolorization on segmentation performance. In this paper, we propose a novel\nstatistical approach to evaluate the impact of inaccurate RGB information on\nimage-based point cloud segmentation. We categorize RGB inaccuracies into two\ntypes: incorrect color information and similar color information. Our results\ndemonstrate that both types of color inaccuracies significantly degrade\nsegmentation accuracy, with similar color errors particularly affecting the\nextraction of geometric features. These findings highlight the critical need to\nreassess the role of RGB information in point cloud segmentation and its\nimplications for future algorithm design.\n","authors":["Qinfeng Zhu","Jiaze Cao","Yuanzhi Cai","Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2410.06725v1.pdf","comment":"Accepted by 2024 IEEE 8th International Conference on Vision, Image\n  and Signal Processing"},{"id":"http://arxiv.org/abs/2410.06654v1","updated":"2024-10-09T08:06:15Z","published":"2024-10-09T08:06:15Z","title":"Performance Evaluation in Multimedia Retrieval","summary":"  Performance evaluation in multimedia retrieval, as in the information\nretrieval domain at large, relies heavily on retrieval experiments, employing a\nbroad range of techniques and metrics. These can involve human-in-the-loop and\nmachine-only settings for the retrieval process itself and the subsequent\nverification of results. Such experiments can be elaborate and\nuse-case-specific, which can make them difficult to compare or replicate. In\nthis paper, we present a formal model to express all relevant aspects of such\nretrieval experiments, as well as a flexible open-source evaluation\ninfrastructure that implements the model. These contributions intend to make a\nstep towards lowering the hurdles for conducting retrieval experiments and\nimproving their reproducibility.\n","authors":["Loris Sauter","Ralph Gasser","Heiko Schuldt","Abraham Bernstein","Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2410.06654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v1","updated":"2024-10-09T07:14:49Z","published":"2024-10-09T07:14:49Z","title":"Decomposing Relationship from 1-to-N into N 1-to-1 for Text-Video\n  Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v1.pdf","comment":null}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2306.10048v4","updated":"2024-10-09T10:10:50Z","published":"2023-06-12T13:47:46Z","title":"An Interdisciplinary Survey on Origin-destination Flows Modeling: Theory\n  and Techniques","summary":"  Origin-destination (OD) flow modeling is an extensively researched subject\nacross multiple disciplines, such as the investigation of travel demand in\ntransportation and spatial interaction modeling in geography. However,\nresearchers from different fields tend to employ their own unique research\nparadigms and lack interdisciplinary communication, preventing the\ncross-fertilization of knowledge and the development of novel solutions to\nchallenges. This article presents a systematic interdisciplinary survey that\ncomprehensively and holistically scrutinizes OD flows from utilizing\nfundamental theory to studying the mechanism of population mobility and solving\npractical problems with engineering techniques, such as computational models.\nSpecifically, regional economics, urban geography, and sociophysics are adept\nat employing theoretical research methods to explore the underlying mechanisms\nof OD flows. They have developed three influential theoretical models: the\ngravity model, the intervening opportunities model, and the radiation model.\nThese models specifically focus on examining the fundamental influences of\ndistance, opportunities, and population on OD flows, respectively. In the\nmeantime, fields such as transportation, urban planning, and computer science\nprimarily focus on addressing four practical problems: OD prediction, OD\nconstruction, OD estimation, and OD forecasting. Advanced computational models,\nsuch as deep learning models, have gradually been introduced to address these\nproblems more effectively. Finally, based on the existing research, this survey\nsummarizes current challenges and outlines future directions for this topic.\nThrough this survey, we aim to break down the barriers between disciplines in\nOD flow-related research, fostering interdisciplinary perspectives and modes of\nthinking.\n","authors":["Can Rong","Jingtao Ding","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2306.10048v4.pdf","comment":"49 pages, 6 figures"}]},"2024-10-08T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.06290v1","updated":"2024-10-08T18:47:08Z","published":"2024-10-08T18:47:08Z","title":"Score Design for Multi-Criteria Incentivization","summary":"  We present a framework for designing scores to summarize performance metrics.\nOur design has two multi-criteria objectives: (1) improving on scores should\nimprove all performance metrics, and (2) achieving pareto-optimal scores should\nachieve pareto-optimal metrics. We formulate our design to minimize the\ndimensionality of scores while satisfying the objectives. We give algorithms to\ndesign scores, which are provably minimal under mild assumptions on the\nstructure of performance metrics. This framework draws motivation from\nreal-world practices in hospital rating systems, where misaligned scores and\nperformance metrics lead to unintended consequences.\n","authors":["Anmol Kabra","Mina Karzand","Tosca Lechner","Nathan Srebro","Serena Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06290v1.pdf","comment":"A condensed version of this paper appeared at Foundations of\n  Responsible Computing (FORC) 2024"},{"id":"http://arxiv.org/abs/2410.06139v1","updated":"2024-10-08T15:40:04Z","published":"2024-10-08T15:40:04Z","title":"Flips in Odd Matchings","summary":"  Let $\\mathcal{P}$ be a set of $n=2m+1$ points in the plane in general\nposition. We define the graph $GM_\\mathcal{P}$ whose vertex set is the set of\nall plane matchings on $\\mathcal{P}$ with exactly $m$ edges. Two vertices in\n$GM_\\mathcal{P}$ are connected if the two corresponding matchings have $m-1$\nedges in common. In this work we show that $GM_\\mathcal{P}$ is connected and\ngive an upper bound of $O(n^2)$ on its diameter. Moreover, we present a tight\nbound of $\\Theta(n)$ for the diameter of the flip graph of points in convex\nposition.\n","authors":["Oswin Aichholzer","Anna Br√∂tzner","Daniel Perz","Patrick Schnider"],"pdf_url":"https://arxiv.org/pdf/2410.06139v1.pdf","comment":"Appeared in CCCG2024"},{"id":"http://arxiv.org/abs/2410.06069v1","updated":"2024-10-08T14:20:23Z","published":"2024-10-08T14:20:23Z","title":"Provable Methods for Searching with an Imperfect Sensor","summary":"  Assume that a target is known to be present at an unknown point among a\nfinite set of locations in the plane. We search for it using a mobile robot\nthat has imperfect sensing capabilities. It takes time for the robot to move\nbetween locations and search a location; we have a total time budget within\nwhich to conduct the search. We study the problem of computing a search\npath/strategy for the robot that maximizes the probability of detection of the\ntarget. Considering non-uniform travel times between points (e.g., based on the\ndistance between them) is crucial for search and rescue applications; such\nproblems have been investigated to a limited extent due to their inherent\ncomplexity. In this paper, we describe fast algorithms with performance\nguarantees for this search problem and some variants, complement them with\ncomplexity results, and perform experiments to observe their performance.\n","authors":["Nilanjan Chakraborty","Prahlad Narasimhan Kasthurirangan","Joseph S. B. Mitchell","Linh Nguyen","Michael Perk"],"pdf_url":"https://arxiv.org/pdf/2410.06069v1.pdf","comment":"10 pages, 6 figures, 3 algorithms"},{"id":"http://arxiv.org/abs/2307.13261v2","updated":"2024-10-08T09:23:40Z","published":"2023-07-25T05:10:39Z","title":"Online Maximum Independent Set of Hyperrectangles","summary":"  The maximum independent set problem is a classical NP-hard problem in\ntheoretical computer science. In this work, we study a special case where the\nfamily of graphs considered is restricted to intersection graphs of sets of\naxis-aligned hyperrectangles and the input is provided in an online fashion. We\nprove results for several adversary models, classes of hyperrectangles, and\nrestrictions on the order of the input. Under the adaptive offline and adaptive\nonline adversary models, we find the optimal online algorithm for unit\nhypercubes, $\\sigma$-bounded hypercubes, unit-volume hyperrectangles, and\narbitrary hypercubes, in both non-dominated and arbitrary order. Under the\noblivious adversary model, we prove bounds on the competitive ratio of an\noptimal online algorithm for the same classes of hyperrectangles and input\norders, and we find algorithms that are optimal up to constant factors. For\ninput in dominating order, we find the optimal online algorithm for arbitrary\nhyperrectangles under all adversary models. We conclude by discussing several\npromising directions for future work.\n","authors":["Rishi Advani","Abolfazl Asudeh"],"pdf_url":"https://arxiv.org/pdf/2307.13261v2.pdf","comment":"29 pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.05580v1","updated":"2024-10-08T00:32:44Z","published":"2024-10-08T00:32:44Z","title":"Noncrossing Longest Paths and Cycles","summary":"  Edge crossings in geometric graphs are sometimes undesirable as they could\nlead to unwanted situations such as collisions in motion planning and\ninconsistency in VLSI layout. Short geometric structures such as shortest\nperfect matchings, shortest spanning trees, shortest spanning paths, and\nshortest spanning cycles on a given point set are inherently noncrossing.\nHowever, the longest such structures need not be noncrossing. In fact, it is\nintuitive to expect many edge crossings in various geometric graphs that are\nlongest.\n  Recently, \\'Alvarez-Rebollar, Cravioto-Lagos, Mar\\'in, Sol\\'e-Pi, and Urrutia\n(Graphs and Combinatorics, 2024) constructed a set of points for which the\nlongest perfect matching is noncrossing. They raised several challenging\nquestions in this direction. In particular, they asked whether the longest\nspanning path, on any finite set of points in the plane, must have a pair of\ncrossing edges. They also conjectured that the longest spanning cycle must have\na pair of crossing edges.\n  In this paper, we give a negative answer to the question and also refute the\nconjecture. We present a framework for constructing arbitrarily large point\nsets for which the longest perfect matchings, the longest spanning paths, and\nthe longest spanning cycles are noncrossing.\n","authors":["Greg Aloupis","Ahmad Biniaz","Prosenjit Bose","Jean-Lou De Carufel","David Eppstein","Anil Maheshwari","Saeed Odak","Michiel Smid","Csaba D. T√≥th","Pavel Valtr"],"pdf_url":"https://arxiv.org/pdf/2410.05580v1.pdf","comment":"19 pages, 8 figures, GD 2024"}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.06429v1","updated":"2024-10-08T23:54:54Z","published":"2024-10-08T23:54:54Z","title":"A QUBO Formulation for the Generalized LinkedIn Queens Game","summary":"  In this paper, we present a QUBO formulation designed to solve a series of\ngeneralisations of the LinkedIn queens game, a version of the N-queens problem.\nWe adapt this formulation for several particular cases of the problem by trying\nto optimise the number of variables and interactions, improving the possibility\nof applying it on quantum hardware by means of Quantum Annealing or the Quantum\nApproximated Optimization Algorithm (QAOA). We also present two new types of\nproblems, the Coloured Chess Piece Problem and the Max Chess Pieces Problem,\nwith their corresponding QUBO formulations.\n","authors":["Alejandro Mata Ali","Edgar Mencia"],"pdf_url":"https://arxiv.org/pdf/2410.06429v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.03183v2","updated":"2024-10-08T17:22:37Z","published":"2024-10-04T06:47:14Z","title":"Research Directions for Verifiable Crypto-Physically Secure TEEs","summary":"  A niche corner of the Web3 world is increasingly making use of hardware-based\nTrusted Execution Environments (TEEs) to build decentralized infrastructure.\nOne of the motivations to use TEEs is to go beyond the current performance\nlimitations of cryptography-based alternatives such as zero-knowledge proofs\n(ZKP), fully homomorphic encryption (FHE), and multi-party computation (MPC).\nDespite their appealing advantages, current TEEs suffer from serious\nlimitations as they are not secure against physical attacks, and their\nattestation mechanism is rooted in the chip manufacturer's trust. As a result,\nWeb3 applications have to rely on cloud infrastruture to act as trusted\nguardians of hardware-based TEEs and have to accept to trust chip\nmanufacturers. This work aims at exploring how we could potentially architect\nand implement chips that would be secure against physical attacks and would not\nrequire putting trust in chip manufacturers. One goal of this work is to\nmotivate the Web3 movement to acknowledge and leverage the substantial amount\nof relevant hardware research that already exists. In brief, a combination of:\n(1) physical unclonable functions (PUFs) to secure the root-of-trust; (2)\nmasking and redundancy techniques to secure computations; (3) open source\nhardware and imaging techniques to verify that a chip matches its expected\ndesign; can help move towards attesting that a given TEE can be trusted without\nthe need to trust a cloud provider and a chip manufacturer.\n","authors":["Sylvain Bellemare"],"pdf_url":"https://arxiv.org/pdf/2410.03183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06113v1","updated":"2024-10-08T15:09:33Z","published":"2024-10-08T15:09:33Z","title":"RealityCraft: An In-Situ CAD+CAM Interface for Novices via Scene-Aware\n  Augmented Reality","summary":"  Despite the growing accessibility of augmented reality (AR) for\nvisualization, existing computer-aided design systems remain largely confined\nto traditional screens and are often inaccessible to novice users due to their\ncomplexity. We present RealityCraft, an open-sourced AR interface that enables\nin-situ computer-aided design and manufacturing (CAD+CAM) for novices. Unlike\ntraditional CAD systems confined to computer screens, RealityCraft allows users\nto design directly within their physical environments, with primitive\ngeometries. RealityCraft recognizes and utilizes physical constraints such as\nfurniture and walls, enhancing user interaction through spatial awareness and\ndepth occlusion. Furthermore, RealityCraft features an integrated AR-based 3D\nprinting workflow, where users can drag and drop designs onto their 3D\nprinter's virtual twin in their immediate space. Through a user study, we\ndemonstrate that RealityCraft enhances engagement and ease of use for novices.\nBy bridging the gap between digital creation and physical output, RealityCraft\naims to transform everyday spaces into creative studios.\n","authors":["Oƒüuz Arslan","Artun Akdoƒüan","Mustafa Doga Dogan"],"pdf_url":"https://arxiv.org/pdf/2410.06113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16896v3","updated":"2024-10-08T14:56:25Z","published":"2023-11-28T15:50:19Z","title":"120 GOPS Photonic Tensor Core in Thin-film Lithium Niobate for Inference\n  and in-situ Training","summary":"  Photonics offers a transformative approach to artificial intelligence (AI)\nand neuromorphic computing by enabling low-latency, high-speed, and\nenergy-efficient computations. However, conventional photonic tensor cores face\nsignificant challenges in constructing large-scale photonic neuromorphic\nnetworks. Here, we propose a fully integrated photonic tensor core, consisting\nof only two thin-film lithium niobate (TFLN) modulators, a III-V laser, and a\ncharge-integration photoreceiver. Despite its simple architecture, it is\ncapable of implementing an entire layer of a neural network with a\ncomputational speed of 120 GOPS, while also allowing flexible adjustment of the\nnumber of inputs (fan-in) and outputs (fan-out). Our tensor core supports rapid\nin-situ training with a weight update speed of 60 GHz. Furthermore, it\nsuccessfully classifies (supervised learning) and clusters (unsupervised\nlearning) 112 * 112-pixel images through in-situ training. To enable in-situ\ntraining for clustering AI tasks, we offer a solution for performing\nmultiplications between two negative numbers.\n","authors":["Zhongjin Lin","Bhavin J. Shastri","Shangxuan Yu","Jingxiang Song","Yuntao Zhu","Arman Safarnejadian","Wangning Cai","Yanmei Lin","Wei Ke","Mustafa Hammood","Tianye Wang","Mengyue Xu","Zibo Zheng","Mohammed Al-Qadasi","Omid Esmaeeli","Mohamed Rahim","Grzegorz Pakulski","Jens Schmid","Pedro Barrios","Weihong Jiang","Hugh Morison","Matthew Mitchell","Xun Guan","Nicolas A. F. Jaeger","Leslie A. n Rusch","Sudip Shekhar","Wei Shi","Siyuan Yu","Xinlun Cai","Lukas Chrostowski"],"pdf_url":"https://arxiv.org/pdf/2311.16896v3.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.06028v1","updated":"2024-10-08T13:29:52Z","published":"2024-10-08T13:29:52Z","title":"SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging","summary":"  Precision pose detection is increasingly demanded in fields such as personal\nfabrication, Virtual Reality (VR), and robotics due to its critical role in\nensuring accurate positioning information. However, conventional vision-based\nsystems used in these systems often struggle with achieving high precision and\naccuracy, particularly when dealing with complex environments or fast-moving\nobjects. To address these limitations, we investigate Laser Speckle Imaging\n(LSI), an emerging optical tracking method that offers promising potential for\nimproving pose estimation accuracy. Specifically, our proposed LSI-Based\nTracking (SpecTrack) leverages the captures from a lensless camera and a\nretro-reflector marker with a coded aperture to achieve multi-axis rotational\npose estimation with high precision. Our extensive trials using our in-house\nbuilt testbed have shown that SpecTrack achieves an accuracy of 0.31{\\deg}\n(std=0.43{\\deg}), significantly outperforming state-of-the-art approaches and\nimproving accuracy up to 200%.\n","authors":["Ziyang Chen","Mustafa Doƒüa Doƒüan","Josef Spjut","Kaan Ak≈üit"],"pdf_url":"https://arxiv.org/pdf/2410.06028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12853v2","updated":"2024-10-08T09:32:40Z","published":"2023-06-22T12:57:16Z","title":"Stress-induced Artificial neuron spiking in Diffusive memristors","summary":"  Diffusive memristors owing to their ability to produce current spiking when a\nconstant or slowly changing voltage is applied are competitive candidates for\nthe development of artificial electronic neurons. These artificial neurons can\nbe integrated into various prospective autonomous and robotic systems as\nsensors, e.g. ones implementing object grasping and classification. We report\nhere Ag nanoparticle-based diffusive memristor prepared on a flexible\npolyethylene terephthalate (PET) substrate in which the electric spiking\nbehaviour was induced by the electric voltage under an additional stimulus of\nexternal mechanical impact. By changing the magnitude and frequency of the\nmechanical impact, we are able to manipulate the spiking response of our\nartificial neuron. This functionality to control the spiking characterstics\npaves a pathway for the development of touch-perception sensors that can\nconvert local pressure into electrical spikes for further processing in neural\nnetworks. We have proposed a mathematical model which captures the operation\nprinciple of the fabricated memristive sensors and qualitatively describes the\nmeasured spiking behaviour.\n","authors":["Debi Pattnaik","Yash Sharma","Sergey Saveliev","Pavel Borisov","Amir Akther","Alexander Balanov","Pedro Ferreira"],"pdf_url":"https://arxiv.org/pdf/2306.12853v2.pdf","comment":"18 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.08916v2","updated":"2024-10-08T06:03:41Z","published":"2024-09-13T15:31:33Z","title":"Farmer.Chat: Scaling AI-Powered Agricultural Services for Smallholder\n  Farmers","summary":"  Small and medium-sized agricultural holders face challenges like limited\naccess to localized, timely information, impacting productivity and\nsustainability. Traditional extension services, which rely on in-person agents,\nstruggle with scalability and timely delivery, especially in remote areas. We\nintroduce FarmerChat, a generative AI-powered chatbot designed to address these\nissues. Leveraging Generative AI, FarmerChat offers personalized, reliable, and\ncontextually relevant advice, overcoming limitations of previous chatbots in\ndeterministic dialogue flows, language support, and unstructured data\nprocessing. Deployed in four countries, FarmerChat has engaged over 15,000\nfarmers and answered over 300,000 queries. This paper highlights how\nFarmerChat's innovative use of GenAI enhances agricultural service scalability\nand effectiveness. Our evaluation, combining quantitative analysis and\nqualitative insights, highlights FarmerChat's effectiveness in improving\nfarming practices, enhancing trust, response quality, and user engagement.\n","authors":["Namita Singh","Jacqueline Wang'ombe","Nereah Okanga","Tetyana Zelenska","Jona Repishti","Jayasankar G K","Sanjeev Mishra","Rajsekar Manokaran","Vineet Singh","Mohammed Irfan Rafiq","Rikin Gandhi","Akshay Nambi"],"pdf_url":"https://arxiv.org/pdf/2409.08916v2.pdf","comment":"35 pages"}],"Graphics":[{"id":"http://arxiv.org/abs/2410.01835v2","updated":"2024-10-08T23:01:58Z","published":"2024-09-22T22:50:27Z","title":"EgoAvatar: Egocentric View-Driven and Photorealistic Full-body Avatars","summary":"  Immersive VR telepresence ideally means being able to interact and\ncommunicate with digital avatars that are indistinguishable from and precisely\nreflect the behaviour of their real counterparts. The core technical challenge\nis two fold: Creating a digital double that faithfully reflects the real human\nand tracking the real human solely from egocentric sensing devices that are\nlightweight and have a low energy consumption, e.g. a single RGB camera. Up to\ndate, no unified solution to this problem exists as recent works solely focus\non egocentric motion capture, only model the head, or build avatars from\nmulti-view captures. In this work, we, for the first time in literature,\npropose a person-specific egocentric telepresence approach, which jointly\nmodels the photoreal digital avatar while also driving it from a single\negocentric video. We first present a character model that is animatible, i.e.\ncan be solely driven by skeletal motion, while being capable of modeling\ngeometry and appearance. Then, we introduce a personalized egocentric motion\ncapture component, which recovers full-body motion from an egocentric video.\nFinally, we apply the recovered pose to our character model and perform a\ntest-time mesh refinement such that the geometry faithfully projects onto the\negocentric view. To validate our design choices, we propose a new and\nchallenging benchmark, which provides paired egocentric and dense multi-view\nvideos of real humans performing various motions. Our experiments demonstrate a\nclear step towards egocentric and photoreal telepresence as our method\noutperforms baselines as well as competing methods. For more details, code, and\ndata, we refer to our project page.\n","authors":["Jianchun Chen","Jian Wang","Yinda Zhang","Rohit Pandey","Thabo Beeler","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2410.01835v2.pdf","comment":"Project Page: https://vcai.mpi-inf.mpg.de/projects/EgoAvatar/"},{"id":"http://arxiv.org/abs/2410.06330v1","updated":"2024-10-08T20:11:11Z","published":"2024-10-08T20:11:11Z","title":"Local Surface Parameterizations via Geodesic Splines","summary":"  We present a general method for computing local parameterizations rooted at a\npoint on a surface, where the surface is described only through a signed\nimplicit function and a corresponding projection function. Using a two-stage\nprocess, we compute several points radially emanating from the map origin, and\ninterpolate between them with a spline surface. The narrow interface of our\nmethod allows it to support several kinds of geometry such as signed distance\nfunctions, general analytic implicit functions, triangle meshes, neural\nimplicits, and point clouds. We demonstrate the high quality of our generated\nparameterizations on a variety of examples, and show applications in local\ntexturing and surface curve drawing.\n","authors":["Abhishek Madan","David I. W. Levin"],"pdf_url":"https://arxiv.org/pdf/2410.06330v1.pdf","comment":"12 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.06327v1","updated":"2024-10-08T20:05:08Z","published":"2024-10-08T20:05:08Z","title":"Towards a GENEA Leaderboard -- an Extended, Living Benchmark for\n  Evaluating and Advancing Conversational Motion Synthesis","summary":"  Current evaluation practices in speech-driven gesture generation lack\nstandardisation and focus on aspects that are easy to measure over aspects that\nactually matter. This leads to a situation where it is impossible to know what\nis the state of the art, or to know which method works better for which purpose\nwhen comparing two publications. In this position paper, we review and give\ndetails on issues with existing gesture-generation evaluation, and present a\nnovel proposal for remedying them. Specifically, we announce an upcoming living\nleaderboard to benchmark progress in conversational motion synthesis. Unlike\nearlier gesture-generation challenges, the leaderboard will be updated with\nlarge-scale user studies of new gesture-generation systems multiple times per\nyear, and systems on the leaderboard can be submitted to any publication venue\nthat their authors prefer. By evolving the leaderboard evaluation data and\ntasks over time, the effort can keep driving progress towards the most\nimportant end goals identified by the community. We actively seek community\ninvolvement across the entire evaluation pipeline: from data and tasks for the\nevaluation, via tooling, to the systems evaluated. In other words, our proposal\nwill not only make it easier for researchers to perform good evaluations, but\ntheir collective input and contributions will also help drive the future of\ngesture-generation research.\n","authors":["Rajmund Nagy","Hendric Voss","Youngwoo Yoon","Taras Kucherenko","Teodor Nikolov","Thanh Hoang-Minh","Rachel McDonnell","Stefan Kopp","Michael Neff","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2410.06327v1.pdf","comment":"15 pages, 2 figures, project page:\n  https://genea-workshop.github.io/leaderboard/"},{"id":"http://arxiv.org/abs/2410.06236v1","updated":"2024-10-08T17:48:01Z","published":"2024-10-08T17:48:01Z","title":"SD-$œÄ$XL: Generating Low-Resolution Quantized Imagery via Score\n  Distillation","summary":"  Low-resolution quantized imagery, such as pixel art, is seeing a revival in\nmodern applications ranging from video game graphics to digital design and\nfabrication, where creativity is often bound by a limited palette of elemental\nunits. Despite their growing popularity, the automated generation of quantized\nimages from raw inputs remains a significant challenge, often necessitating\nintensive manual input. We introduce SD-$\\pi$XL, an approach for producing\nquantized images that employs score distillation sampling in conjunction with a\ndifferentiable image generator. Our method enables users to input a prompt and\noptionally an image for spatial conditioning, set any desired output size $H\n\\times W$, and choose a palette of $n$ colors or elements. Each color\ncorresponds to a distinct class for our generator, which operates on an $H\n\\times W \\times n$ tensor. We adopt a softmax approach, computing a convex sum\nof elements, thus rendering the process differentiable and amenable to\nbackpropagation. We show that employing Gumbel-softmax reparameterization\nallows for crisp pixel art effects. Unique to our method is the ability to\ntransform input images into low-resolution, quantized versions while retaining\ntheir key semantic features. Our experiments validate SD-$\\pi$XL's performance\nin creating visually pleasing and faithful representations, consistently\noutperforming the current state-of-the-art. Furthermore, we showcase\nSD-$\\pi$XL's practical utility in fabrication through its applications in\ninterlocking brick mosaic, beading and embroidery design.\n","authors":["Alexandre Binninger","Olga Sorkine-Hornung"],"pdf_url":"https://arxiv.org/pdf/2410.06236v1.pdf","comment":"To be presented at SIGGRAPH Asia 2024 (conference track). Main paper\n  is 8 pages + 2 figure-only pages + references. Supplementary is 11 pages +\n  references"},{"id":"http://arxiv.org/abs/2410.06113v1","updated":"2024-10-08T15:09:33Z","published":"2024-10-08T15:09:33Z","title":"RealityCraft: An In-Situ CAD+CAM Interface for Novices via Scene-Aware\n  Augmented Reality","summary":"  Despite the growing accessibility of augmented reality (AR) for\nvisualization, existing computer-aided design systems remain largely confined\nto traditional screens and are often inaccessible to novice users due to their\ncomplexity. We present RealityCraft, an open-sourced AR interface that enables\nin-situ computer-aided design and manufacturing (CAD+CAM) for novices. Unlike\ntraditional CAD systems confined to computer screens, RealityCraft allows users\nto design directly within their physical environments, with primitive\ngeometries. RealityCraft recognizes and utilizes physical constraints such as\nfurniture and walls, enhancing user interaction through spatial awareness and\ndepth occlusion. Furthermore, RealityCraft features an integrated AR-based 3D\nprinting workflow, where users can drag and drop designs onto their 3D\nprinter's virtual twin in their immediate space. Through a user study, we\ndemonstrate that RealityCraft enhances engagement and ease of use for novices.\nBy bridging the gap between digital creation and physical output, RealityCraft\naims to transform everyday spaces into creative studios.\n","authors":["Oƒüuz Arslan","Artun Akdoƒüan","Mustafa Doga Dogan"],"pdf_url":"https://arxiv.org/pdf/2410.06113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06068v1","updated":"2024-10-08T14:14:46Z","published":"2024-10-08T14:14:46Z","title":"Resolution limit of the eye: how many pixels can we see?","summary":"  As large engineering efforts go towards improving the resolution of mobile,\nAR and VR displays, it is important to know the maximum resolution at which\nfurther improvements bring no noticeable benefit. This limit is often referred\nto as the \"retinal resolution\", although the limiting factor may not\nnecessarily be attributed to the retina. To determine the ultimate resolution\nat which an image appears sharp to our eyes with no perceivable blur, we\ncreated an experimental setup with a sliding display, which allows for\ncontinuous control of the resolution. The lack of such control was the main\nlimitation of the previous studies. We measure achromatic (black-white) and\nchromatic (red-green and yellow-violet) resolution limits for foveal vision,\nand at two eccentricities (10 and 20 deg). Our results demonstrate that the\nresolution limit is higher than what was previously believed, reaching 94\npixels-per-degree (ppd) for foveal achromatic vision, 89 ppd for red-green\npatterns, and 53 ppd for yellow-violet patterns. We also observe a much larger\ndrop in the resolution limit for chromatic patterns (red-green and\nyellow-violet) than for achromatic. Our results set the north star for display\ndevelopment, with implications for future imaging, rendering and video coding\ntechnologies.\n","authors":["Maliha Ashraf","Alexandre Chapiro","Rafa≈Ç K. Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2410.06068v1.pdf","comment":"Main document: 12 pages, 4 figures, 1 table. Supplementary: 14 pages,\n  12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.18099v1","updated":"2024-10-08T12:53:22Z","published":"2024-10-08T12:53:22Z","title":"Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR\n  Through Trajectory Coarse Discretization and Pre-training","summary":"  Text entry with word-gesture keyboards (WGK) is emerging as a popular method\nand becoming a key interaction for Extended Reality (XR). However, the\ndiversity of interaction modes, keyboard sizes, and visual feedback in these\nenvironments introduces divergent word-gesture trajectory data patterns, thus\nleading to complexity in decoding trajectories into text. Template-matching\ndecoding methods, such as SHARK^2, are commonly used for these WGK systems\nbecause they are easy to implement and configure. However, these methods are\nsusceptible to decoding inaccuracies for noisy trajectories. While conventional\nneural-network-based decoders (neural decoders) trained on word-gesture\ntrajectory data have been proposed to improve accuracy, they have their own\nlimitations: they require extensive data for training and deep-learning\nexpertise for implementation. To address these challenges, we propose a novel\nsolution that combines ease of implementation with high decoding accuracy: a\ngeneralizable neural decoder enabled by pre-training on large-scale coarsely\ndiscretized word-gesture trajectories. This approach produces a ready-to-use\nWGK decoder that is generalizable across mid-air and on-surface WGK systems in\naugmented reality (AR) and virtual reality (VR), which is evident by a robust\naverage Top-4 accuracy of 90.4% on four diverse datasets. It significantly\noutperforms SHARK^2 with a 37.2% enhancement and surpasses the conventional\nneural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only\n4 MB after quantization, without sacrificing accuracy, and it can operate in\nreal-time, executing in just 97 milliseconds on Quest 3.\n","authors":["Junxiao Shen","Khadija Khaldi","Enmin Zhou","Hemant Bhaskar Surale","Amy Karlson"],"pdf_url":"https://arxiv.org/pdf/2410.18099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05991v1","updated":"2024-10-08T12:41:31Z","published":"2024-10-08T12:41:31Z","title":"Vector Grimoire: Codebook-based Shape Generation under Raster Image\n  Supervision","summary":"  Scalable Vector Graphics (SVG) is a popular format on the web and in the\ndesign industry. However, despite the great strides made in generative\nmodeling, SVG has remained underexplored due to the discrete and complex nature\nof such data. We introduce GRIMOIRE, a text-guided SVG generative model that is\ncomprised of two modules: A Visual Shape Quantizer (VSQ) learns to map raster\nimages onto a discrete codebook by reconstructing them as vector shapes, and an\nAuto-Regressive Transformer (ART) models the joint probability distribution\nover shape tokens, positions and textual descriptions, allowing us to generate\nvector graphics from natural language. Unlike existing models that require\ndirect supervision from SVG data, GRIMOIRE learns shape image patches using\nonly raster image supervision which opens up vector generative modeling to\nsignificantly more data. We demonstrate the effectiveness of our method by\nfitting GRIMOIRE for closed filled shapes on the MNIST and for outline strokes\non icon and font data, surpassing previous image-supervised methods in\ngenerative quality and vector-supervised approach in flexibility.\n","authors":["Moritz Feuerpfeil","Marco Cipriano","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2410.05991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05791v1","updated":"2024-10-08T08:21:05Z","published":"2024-10-08T08:21:05Z","title":"F√ºrElise: Capturing and Physically Synthesizing Hand Motions of Piano\n  Performance","summary":"  Piano playing requires agile, precise, and coordinated hand control that\nstretches the limits of dexterity. Hand motion models with the sophistication\nto accurately recreate piano playing have a wide range of applications in\ncharacter animation, embodied AI, biomechanics, and VR/AR. In this paper, we\nconstruct a first-of-its-kind large-scale dataset that contains approximately\n10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153\npieces of classical music. To capture natural performances, we designed a\nmarkerless setup in which motions are reconstructed from multi-view videos\nusing state-of-the-art pose estimation models. The motion data is further\nrefined via inverse kinematics using the high-resolution MIDI key-pressing data\nobtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the\ncollected dataset, we developed a pipeline that can synthesize\nphysically-plausible hand motions for musical scores outside of the dataset.\nOur approach employs a combination of imitation learning and reinforcement\nlearning to obtain policies for physics-based bimanual control involving the\ninteraction between hands and piano keys. To solve the sampling efficiency\nproblem with the large motion dataset, we use a diffusion model to generate\nnatural reference motions, which provide high-level trajectory and fingering\n(finger order and placement) information. However, the generated reference\nmotion alone does not provide sufficient accuracy for piano performance\nmodeling. We then further augmented the data by using musical similarity to\nretrieve similar motions from the captured dataset to boost the precision of\nthe RL policy. With the proposed method, our model generates natural, dexterous\nmotions that generalize to music from outside the training dataset.\n","authors":["Ruocheng Wang","Pei Xu","Haochen Shi","Elizabeth Schumann","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.05791v1.pdf","comment":"SIGGRAPH Asia 2024. Project page: https://for-elise.github.io/"},{"id":"http://arxiv.org/abs/2410.05645v1","updated":"2024-10-08T02:48:19Z","published":"2024-10-08T02:48:19Z","title":"Counterpoint: Orchestrating Large-Scale Custom Animated Visualizations","summary":"  Custom animated visualizations of large, complex datasets are helpful across\nmany domains, but they are hard to develop. Much of the difficulty arises from\nmaintaining visualization state across many animated graphical elements that\nmay change in number over time. We contribute Counterpoint, a framework for\nstate management designed to help implement such visualizations in JavaScript.\nUsing Counterpoint, developers can manipulate large collections of marks with\nreactive attributes that are easy to render in scalable APIs such as Canvas and\nWebGL. Counterpoint also helps orchestrate the entry and exit of graphical\nelements using the concept of a rendering \"stage.\" Through a performance\nevaluation, we show that Counterpoint adds minimal overhead over current\nhigh-performance rendering techniques while simplifying implementation. We\nprovide two examples of visualizations created using Counterpoint that\nillustrate its flexibility and compatibility with other visualization toolkits\nas well as considerations for users with disabilities. Counterpoint is\nopen-source and available at https://github.com/cmudig/counterpoint.\n","authors":["Venkatesh Sivaraman","Frank Elavsky","Dominik Moritz","Adam Perer"],"pdf_url":"https://arxiv.org/pdf/2410.05645v1.pdf","comment":"To appear at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2404.15378v3","updated":"2024-10-08T01:39:18Z","published":"2024-04-23T03:04:22Z","title":"Hierarchical Hybrid Sliced Wasserstein: A Scalable Metric for\n  Heterogeneous Joint Distributions","summary":"  Sliced Wasserstein (SW) and Generalized Sliced Wasserstein (GSW) have been\nwidely used in applications due to their computational and statistical\nscalability. However, the SW and the GSW are only defined between distributions\nsupported on a homogeneous domain. This limitation prevents their usage in\napplications with heterogeneous joint distributions with marginal distributions\nsupported on multiple different domains. Using SW and GSW directly on the joint\ndomains cannot make a meaningful comparison since their homogeneous slicing\noperator i.e., Radon Transform (RT) and Generalized Radon Transform (GRT) are\nnot expressive enough to capture the structure of the joint supports set. To\naddress the issue, we propose two new slicing operators i.e., Partial\nGeneralized Radon Transform (PGRT) and Hierarchical Hybrid Radon Transform\n(HHRT). In greater detail, PGRT is the generalization of Partial Radon\nTransform (PRT), which transforms a subset of function arguments non-linearly\nwhile HHRT is the composition of PRT and multiple domain-specific PGRT on\nmarginal domain arguments. By using HHRT, we extend the SW into Hierarchical\nHybrid Sliced Wasserstein (H2SW) distance which is designed specifically for\ncomparing heterogeneous joint distributions. We then discuss the topological,\nstatistical, and computational properties of H2SW. Finally, we demonstrate the\nfavorable performance of H2SW in 3D mesh deformation, deep 3D mesh\nautoencoders, and datasets comparison.\n","authors":["Khai Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2404.15378v3.pdf","comment":"Accepted to NeurIPS 2024, 27 pages, 11 figures, 4 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.06221v1","updated":"2024-10-08T17:29:14Z","published":"2024-10-08T17:29:14Z","title":"POLIPHONE: A Dataset for Smartphone Model Identification from Audio\n  Recordings","summary":"  When dealing with multimedia data, source attribution is a key challenge from\na forensic perspective. This task aims to determine how a given content was\ncaptured, providing valuable insights for various applications, including legal\nproceedings and integrity investigations. The source attribution problem has\nbeen addressed in different domains, from identifying the camera model used to\ncapture specific photographs to detecting the synthetic speech generator or\nmicrophone model used to create or record given audio tracks. Recent\nadvancements in this area rely heavily on machine learning and data-driven\ntechniques, which often outperform traditional signal processing-based methods.\n  However, a drawback of these systems is their need for large volumes of\ntraining data, which must reflect the latest technological trends to produce\naccurate and reliable predictions. This presents a significant challenge, as\nthe rapid pace of technological progress makes it difficult to maintain\ndatasets that are up-to-date with real-world conditions. For instance, in the\ntask of smartphone model identification from audio recordings, the available\ndatasets are often outdated or acquired inconsistently, making it difficult to\ndevelop solutions that are valid beyond a research environment. In this paper\nwe present POLIPHONE, a dataset for smartphone model identification from audio\nrecordings. It includes data from 20 recent smartphones recorded in a\ncontrolled environment to ensure reproducibility and scalability for future\nresearch. The released tracks contain audio data from various domains (i.e.,\nspeech, music, environmental sounds), making the corpus versatile and\napplicable to a wide range of use cases. We also present numerous experiments\nto benchmark the proposed dataset using a state-of-the-art classifier for\nsmartphone model identification from audio recordings.\n","authors":["Davide Salvi","Daniele Ugo Leonzio","Antonio Giganti","Claudio Eutizi","Sara Mandelli","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2410.06221v1.pdf","comment":"Submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2410.06149v1","updated":"2024-10-08T15:48:34Z","published":"2024-10-08T15:48:34Z","title":"Toward Scalable Image Feature Compression: A Content-Adaptive and\n  Diffusion-Based Approach","summary":"  Traditional image codecs emphasize signal fidelity and human perception,\noften at the expense of machine vision tasks. Deep learning methods have\ndemonstrated promising coding performance by utilizing rich semantic embeddings\noptimized for both human and machine vision. However, these compact embeddings\nstruggle to capture fine details such as contours and textures, resulting in\nimperfect reconstructions. Furthermore, existing learning-based codecs lack\nscalability. To address these limitations, this paper introduces a\ncontent-adaptive diffusion model for scalable image compression. The proposed\nmethod encodes fine textures through a diffusion process, enhancing perceptual\nquality while preserving essential features for machine vision tasks. The\napproach employs a Markov palette diffusion model combined with widely used\nfeature extractors and image generators, enabling efficient data compression.\nBy leveraging collaborative texture-semantic feature extraction and\npseudo-label generation, the method accurately captures texture information. A\ncontent-adaptive Markov palette diffusion model is then applied to represent\nboth low-level textures and high-level semantic content in a scalable manner.\nThis framework offers flexible control over compression ratios by selecting\nintermediate diffusion states, eliminating the need for retraining deep\nlearning models at different operating points. Extensive experiments\ndemonstrate the effectiveness of the proposed framework in both image\nreconstruction and downstream machine vision tasks such as object detection,\nsegmentation, and facial landmark detection, achieving superior perceptual\nquality compared to state-of-the-art methods.\n","authors":["Sha Guo","Zhuo Chen","Yang Zhao","Ning Zhang","Xiaotong Li","Lingyu Duan"],"pdf_url":"https://arxiv.org/pdf/2410.06149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06068v1","updated":"2024-10-08T14:14:46Z","published":"2024-10-08T14:14:46Z","title":"Resolution limit of the eye: how many pixels can we see?","summary":"  As large engineering efforts go towards improving the resolution of mobile,\nAR and VR displays, it is important to know the maximum resolution at which\nfurther improvements bring no noticeable benefit. This limit is often referred\nto as the \"retinal resolution\", although the limiting factor may not\nnecessarily be attributed to the retina. To determine the ultimate resolution\nat which an image appears sharp to our eyes with no perceivable blur, we\ncreated an experimental setup with a sliding display, which allows for\ncontinuous control of the resolution. The lack of such control was the main\nlimitation of the previous studies. We measure achromatic (black-white) and\nchromatic (red-green and yellow-violet) resolution limits for foveal vision,\nand at two eccentricities (10 and 20 deg). Our results demonstrate that the\nresolution limit is higher than what was previously believed, reaching 94\npixels-per-degree (ppd) for foveal achromatic vision, 89 ppd for red-green\npatterns, and 53 ppd for yellow-violet patterns. We also observe a much larger\ndrop in the resolution limit for chromatic patterns (red-green and\nyellow-violet) than for achromatic. Our results set the north star for display\ndevelopment, with implications for future imaging, rendering and video coding\ntechnologies.\n","authors":["Maliha Ashraf","Alexandre Chapiro","Rafa≈Ç K. Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2410.06068v1.pdf","comment":"Main document: 12 pages, 4 figures, 1 table. Supplementary: 14 pages,\n  12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.05935v1","updated":"2024-10-08T11:38:13Z","published":"2024-10-08T11:38:13Z","title":"Learning Gaussian Data Augmentation in Feature Space for One-shot Object\n  Detection in Manga","summary":"  We tackle one-shot object detection in Japanese Manga. The rising global\npopularity of Japanese manga has made the object detection of character faces\nincreasingly important, with potential applications such as automatic\ncolorization. However, obtaining sufficient data for training conventional\nobject detectors is challenging due to copyright restrictions. Additionally,\nnew characters appear every time a new volume of manga is released, making it\nimpractical to re-train object detectors each time to detect these new\ncharacters. Therefore, one-shot object detection, where only a single query\n(reference) image is required to detect a new character, is an essential task\nin the manga industry. One challenge with one-shot object detection in manga is\nthe large variation in the poses and facial expressions of characters in target\nimages, despite having only one query image as a reference. Another challenge\nis that the frequency of character appearances follows a long-tail\ndistribution. To overcome these challenges, we propose a data augmentation\nmethod in feature space to increase the variation of the query. The proposed\nmethod augments the feature from the query by adding Gaussian noise, with the\nnoise variance at each channel learned during training. The experimental\nresults show that the proposed method improves the performance for both seen\nand unseen classes, surpassing data augmentation methods in image space.\n","authors":["Takara Taniguchi","Ryosuke Furuta"],"pdf_url":"https://arxiv.org/pdf/2410.05935v1.pdf","comment":"Accepted to ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2410.10867v1","updated":"2024-10-08T11:09:25Z","published":"2024-10-08T11:09:25Z","title":"Mitigating the Impact of Reference Quality on Evaluation of\n  Summarization Systems with Reference-Free Metrics","summary":"  Automatic metrics are used as proxies to evaluate abstractive summarization\nsystems when human annotations are too expensive. To be useful, these metrics\nshould be fine-grained, show a high correlation with human annotations, and\nideally be independent of reference quality; however, most standard evaluation\nmetrics for summarization are reference-based, and existing reference-free\nmetrics correlate poorly with relevance, especially on summaries of longer\ndocuments. In this paper, we introduce a reference-free metric that correlates\nwell with human evaluated relevance, while being very cheap to compute. We show\nthat this metric can also be used alongside reference-based metrics to improve\ntheir robustness in low quality reference settings.\n","authors":["Th√©o Gigant","Camille Guinaudeau","Marc Decombas","Fr√©d√©ric Dufaux"],"pdf_url":"https://arxiv.org/pdf/2410.10867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05767v1","updated":"2024-10-08T07:48:34Z","published":"2024-10-08T07:48:34Z","title":"Grounding is All You Need? Dual Temporal Grounding for Video Dialog","summary":"  In the realm of video dialog response generation, the understanding of video\ncontent and the temporal nuances of conversation history are paramount. While a\nsegment of current research leans heavily on large-scale pretrained\nvisual-language models and often overlooks temporal dynamics, another delves\ndeep into spatial-temporal relationships within videos but demands intricate\nobject trajectory pre-extractions and sidelines dialog temporal dynamics. This\npaper introduces the Dual Temporal Grounding-enhanced Video Dialog model\n(DTGVD), strategically designed to merge the strengths of both dominant\napproaches. It emphasizes dual temporal relationships by predicting dialog\nturn-specific temporal regions, filtering video content accordingly, and\ngrounding responses in both video and dialog contexts. One standout feature of\nDTGVD is its heightened attention to chronological interplay. By recognizing\nand acting upon the dependencies between different dialog turns, it captures\nmore nuanced conversational dynamics. To further bolster the alignment between\nvideo and dialog temporal dynamics, we've implemented a list-wise contrastive\nlearning strategy. Within this framework, accurately grounded turn-clip\npairings are designated as positive samples, while less precise pairings are\ncategorized as negative. This refined classification is then funneled into our\nholistic end-to-end response generation mechanism. Evaluations using\nAVSD@DSTC-7 and AVSD@DSTC-8 datasets underscore the superiority of our\nmethodology.\n","authors":["You Qin","Wei Ji","Xinze Lan","Hao Fei","Xun Yang","Dan Guo","Roger Zimmermann","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2410.05767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05650v1","updated":"2024-10-08T02:59:08Z","published":"2024-10-08T02:59:08Z","title":"SIA-OVD: Shape-Invariant Adapter for Bridging the Image-Region Gap in\n  Open-Vocabulary Detection","summary":"  Open-vocabulary detection (OVD) aims to detect novel objects without\ninstance-level annotations to achieve open-world object detection at a lower\ncost. Existing OVD methods mainly rely on the powerful open-vocabulary\nimage-text alignment capability of Vision-Language Pretrained Models (VLM) such\nas CLIP. However, CLIP is trained on image-text pairs and lacks the perceptual\nability for local regions within an image, resulting in the gap between image\nand region representations. Directly using CLIP for OVD causes inaccurate\nregion classification. We find the image-region gap is primarily caused by the\ndeformation of region feature maps during region of interest (RoI) extraction.\nTo mitigate the inaccurate region classification in OVD, we propose a new\nShape-Invariant Adapter named SIA-OVD to bridge the image-region gap in the OVD\ntask. SIA-OVD learns a set of feature adapters for regions with different\nshapes and designs a new adapter allocation mechanism to select the optimal\nadapter for each region. The adapted region representations can align better\nwith text representations learned by CLIP. Extensive experiments demonstrate\nthat SIA-OVD effectively improves the classification accuracy for regions by\naddressing the gap between images and regions caused by shape deformation.\nSIA-OVD achieves substantial improvements over representative methods on the\nCOCO-OVD benchmark. The code is available at\nhttps://github.com/PKU-ICST-MIPL/SIA-OVD_ACMMM2024.\n","authors":["Zishuo Wang","Wenhao Zhou","Jinglin Xu","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2410.05650v1.pdf","comment":"9 pages, 7 figures"}]},"2024-10-07T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.05530v1","updated":"2024-10-07T22:17:11Z","published":"2024-10-07T22:17:11Z","title":"VisDiff: SDF-Guided Polygon Generation for Visibility Reconstruction and\n  Recognition","summary":"  The capability to learn latent representations plays a key role in the\neffectiveness of recent machine learning methods. An active frontier in\nrepresentation learning is understanding representations for combinatorial\nstructures which may not admit well-behaved local neighborhoods or distance\nfunctions. For example, for polygons, slightly perturbing vertex locations\nmight lead to significant changes in their combinatorial structure and may even\nlead to invalid polygons. In this paper, we investigate representations to\ncapture the underlying combinatorial structures of polygons. Specifically, we\nstudy the open problem of Visibility Reconstruction: Given a visibility graph\nG, construct a polygon P whose visibility graph is G.\n  We introduce VisDiff, a novel diffusion-based approach to reconstruct a\npolygon from its given visibility graph G. Our method first estimates the\nsigned distance function (SDF) of P from G. Afterwards, it extracts ordered\nvertex locations that have the pairwise visibility relationship given by the\nedges of G. Our main insight is that going through the SDF significantly\nimproves learning for reconstruction. In order to train VisDiff, we make two\nmain contributions: (1) We design novel loss components for computing the\nvisibility in a differentiable manner and (2) create a carefully curated\ndataset. We use this dataset to benchmark our method and achieve 21%\nimprovement in F1-Score over standard methods. We also demonstrate effective\ngeneralization to out-of-distribution polygon types and show that learning a\ngenerative model allows us to sample the set of polygons with a given\nvisibility graph. Finally, we extend our method to the related combinatorial\nproblem of reconstruction from a triangulation. We achieve 95% classification\naccuracy of triangulation edges and a 4% improvement in Chamfer distance\ncompared to current architectures.\n","authors":["Rahul Moorthy","Volkan Isler"],"pdf_url":"https://arxiv.org/pdf/2410.05530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04862v1","updated":"2024-10-07T09:26:59Z","published":"2024-10-07T09:26:59Z","title":"Temporal-Assisted Dynamic Beampattern Optimization in Integrated Sensing\n  and Communication Systems","summary":"  In this paper, an integrated sensing and communication (ISAC) system is\ninvestigated. Initially, we introduce a design criterion wherein sensing data\nacquired from the preceding time slot is employed for instantaneous optimal\nbeamforming in the succeeding time slot, aiming to enhance the communication\nrate. Subsequently, the development of optimal beamforming is addressed, and a\nhigh-caliber suboptimal resolution is derived utilizing successive convex\napproximation (SCA) techniques combined with the iterative rank minimization\n(IRM) methodology. Our evaluations, grounded on numerical analyses, reveal that\nthe communication rate of the introduced beamforming strategy surpasses that of\nconventional omnidirectional sensing and pilot based approaches.\n","authors":["Shengcai Zhou","Luping Xiang","Kun Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09047v2","updated":"2024-10-07T07:54:16Z","published":"2024-06-12T11:13:35Z","title":"DeepJEB: 3D Deep Learning-based Synthetic Jet Engine Bracket Dataset","summary":"  Recent advances in artificial intelligence (AI) have impacted various fields,\nincluding mechanical engineering. However, the development of diverse,\nhigh-quality datasets for structural analysis remains a challenge. Traditional\ndatasets, like the jet engine bracket dataset, are limited by small sample\nsizes, hindering the creation of robust surrogate models. This study introduces\nthe DeepJEB dataset, generated through deep generative models and automated\nsimulation pipelines, to address these limitations. DeepJEB offers\ncomprehensive 3D geometries and corresponding structural analysis data. Key\nexperiments validated its effectiveness, showing significant improvements in\nsurrogate model performance. Models trained on DeepJEB achieved up to a 23%\nincrease in the coefficient of determination and over a 70% reduction in mean\nabsolute percentage error (MAPE) compared to those trained on traditional\ndatasets. These results underscore the superior generalization capabilities of\nDeepJEB. By supporting advanced modeling techniques, such as graph neural\nnetworks (GNNs) and convolutional neural networks (CNNs), DeepJEB enables more\naccurate predictions in structural performance. The DeepJEB dataset is publicly\naccessible at: https://www.narnia.ai/dataset.\n","authors":["Seongjun Hong","Yongmin Kwon","Dongju Shin","Jangseop Park","Namwoo Kang"],"pdf_url":"https://arxiv.org/pdf/2406.09047v2.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.18358v1","updated":"2024-10-07T18:26:05Z","published":"2024-10-07T18:26:05Z","title":"Data Publishing in Mechanics and Dynamics: Challenges, Guidelines, and\n  Examples from Engineering Design","summary":"  Data-based methods have gained increasing importance in engineering,\nespecially but not only driven by successes with deep artificial neural\nnetworks. Success stories are prevalent, e.g., in areas such as data-driven\nmodeling, control and automation, as well as surrogate modeling for accelerated\nsimulation. Beyond engineering, generative and large-language models are\nincreasingly performing and helping with tasks that, previously, were solely\nassociated with creative human processes. Thus, it seems timely to seek\nartificial-intelligence-support for engineering design tasks to automate, help\nwith, or accelerate purpose-built designs of engineering systems, e.g., in\nmechanics and dynamics, where design so far requires a lot of specialized\nknowledge. However, research-wise, compared to established, predominantly\nfirst-principles-based methods, the datasets used for training, validation, and\ntest become an almost inherent part of the overall methodology. Thus, data\npublishing becomes just as important in (data-driven) engineering science as\nappropriate descriptions of conventional methodology in publications in the\npast. This article analyzes the value and challenges of data publishing in\nmechanics and dynamics, in particular regarding engineering design tasks,\nshowing that the latter raise also challenges and considerations not typical in\nfields where data-driven methods have been booming originally. Possible ways to\ndeal with these challenges are discussed and a set of examples from across\ndifferent design problems shows how data publishing can be put into practice.\nThe analysis, discussions, and examples are based on the research experience\nmade in a priority program of the German research foundation focusing on\nresearch on artificially intelligent design assistants in mechanics and\ndynamics.\n","authors":["Henrik Ebel","Jan van Delden","Timo L√ºddecke","Aditya Borse","Rutwik Gulakala","Marcus Stoffel","Manish Yadav","Merten Stender","Leon Schindler","Kristin Miriam de Payrebrune","Maximilian Raff","C. David Remy","Benedict R√∂der","Peter Eberhard"],"pdf_url":"https://arxiv.org/pdf/2410.18358v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.11146v4","updated":"2024-10-07T11:03:35Z","published":"2023-05-18T17:40:32Z","title":"Grover Speedup from Many Forms of the Zeno Effect","summary":"  It has previously been established that adiabatic quantum computation,\noperating based on a continuous Zeno effect due to dynamical phases between\neigenstates, is able to realise an optimal Grover-like quantum speedup. In\nother words is able to solve an unstructured search problem with the same\n$\\sqrt{N}$ scaling as Grover's original algorithm. A natural question is\nwhether other manifestations of the Zeno effect can also support an optimal\nspeedup in a physically realistic model (through direct analog application\nrather than indirectly by supporting a universal gateset). In this paper we\nshow that they can support such a speedup, whether due to measurement,\ndecoherence, or even decay of the excited state into a computationally useless\nstate. Our results also suggest a wide variety of methods to realise speedup\nwhich do not rely on Zeno behaviour. We group these algorithms into three\nfamilies to facilitate a structured understanding of how speedups can be\nobtained: one based on phase kicks, containing adiabatic computation and\ncontinuous-time quantum walks; one based on dephasing and measurement; and\nfinally one based on destruction of the amplitude within the excited state, for\nwhich we are not aware of any previous results. These results suggest that\nthere may be exciting opportunities for new paradigms of analog quantum\ncomputing based on these effects.\n","authors":["Jesse Berwald","Nick Chancellor","Raouf Dridi"],"pdf_url":"https://arxiv.org/pdf/2305.11146v4.pdf","comment":"Referee recommended changes in v3 and v4"},{"id":"http://arxiv.org/abs/2410.04775v1","updated":"2024-10-07T06:30:59Z","published":"2024-10-07T06:30:59Z","title":"OmniBuds: A Sensory Earable Platform for Advanced Bio-Sensing and\n  On-Device Machine Learning","summary":"  Sensory earables have evolved from basic audio enhancement devices into\nsophisticated platforms for clinical-grade health monitoring and wellbeing\nmanagement. This paper introduces OmniBuds, an advanced sensory earable\nplatform integrating multiple biosensors and onboard computation powered by a\nmachine learning accelerator, all within a real-time operating system (RTOS).\nThe platform's dual-ear symmetric design, equipped with precisely positioned\nkinetic, acoustic, optical, and thermal sensors, enables highly accurate and\nreal-time physiological assessments. Unlike conventional earables that rely on\nexternal data processing, OmniBuds leverage real-time onboard computation to\nsignificantly enhance system efficiency, reduce latency, and safeguard privacy\nby processing data locally. This capability includes executing complex machine\nlearning models directly on the device. We provide a comprehensive analysis of\nOmniBuds' design, hardware and software architecture demonstrating its capacity\nfor multi-functional applications, accurate and robust tracking of\nphysiological parameters, and advanced human-computer interaction.\n","authors":["Alessandro Montanari","Ashok Thangarajan","Khaldoon Al-Naimi","Andrea Ferlini","Yang Liu","Ananta Narayanan Balaji","Fahim Kawsar"],"pdf_url":"https://arxiv.org/pdf/2410.04775v1.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2410.18085v1","updated":"2024-10-07T19:07:08Z","published":"2024-10-07T19:07:08Z","title":"TextureMeDefect: LLM-based Defect Texture Generation for Railway\n  Components on Mobile Devices","summary":"  Texture image generation has been studied for various applications, including\ngaming and entertainment. However, context-specific realistic texture\ngeneration for industrial applications, such as generating defect textures on\nrailway components, remains unexplored. A mobile-friendly, LLM-based tool that\ngenerates fine-grained defect characteristics offers a solution to the\nchallenge of understanding the impact of defects from actual occurrences. We\nintroduce TextureMeDefect, an innovative tool leveraging an LLM-based\nAI-Inferencing engine. The tool allows users to create realistic defect\ntextures interactively on images of railway components taken with smartphones\nor tablets. We conducted a multifaceted evaluation to assess the relevance of\nthe generated texture, time, and cost in using this tool on iOS and Android\nplatforms. We also analyzed the software usability score (SUS) across three\nscenarios. TextureMeDefect outperformed traditional image generation tools by\ngenerating meaningful textures faster, showcasing the potential of AI-driven\nmobile applications on consumer-grade devices.\n","authors":["Rahatara Ferdousi","M. Anwar Hossain","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2410.18085v1.pdf","comment":"6 Pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.05260v1","updated":"2024-10-07T17:58:22Z","published":"2024-10-07T17:58:22Z","title":"DART: A Diffusion-Based Autoregressive Motion Model for Real-Time\n  Text-Driven Motion Control","summary":"  Text-conditioned human motion generation, which allows for user interaction\nthrough natural language, has become increasingly popular. Existing methods\ntypically generate short, isolated motions based on a single input sentence.\nHowever, human motions are continuous and can extend over long periods,\ncarrying rich semantics. Creating long, complex motions that precisely respond\nto streams of text descriptions, particularly in an online and real-time\nsetting, remains a significant challenge. Furthermore, incorporating spatial\nconstraints into text-conditioned motion generation presents additional\nchallenges, as it requires aligning the motion semantics specified by text\ndescriptions with geometric information, such as goal locations and 3D scene\ngeometry. To address these limitations, we propose DART, a Diffusion-based\nAutoregressive motion primitive model for Real-time Text-driven motion control.\nOur model, DART, effectively learns a compact motion primitive space jointly\nconditioned on motion history and text inputs using latent diffusion models. By\nautoregressively generating motion primitives based on the preceding history\nand current text input, DART enables real-time, sequential motion generation\ndriven by natural language descriptions. Additionally, the learned motion\nprimitive space allows for precise spatial motion control, which we formulate\neither as a latent noise optimization problem or as a Markov decision process\naddressed through reinforcement learning. We present effective algorithms for\nboth approaches, demonstrating our model's versatility and superior performance\nin various motion synthesis tasks. Experiments show our method outperforms\nexisting baselines in motion realism, efficiency, and controllability. Video\nresults are available on the project page: https://zkf1997.github.io/DART/.\n","authors":["Kaifeng Zhao","Gen Li","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2410.05260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05095v1","updated":"2024-10-07T14:50:19Z","published":"2024-10-07T14:50:19Z","title":"Towards a Modern and Lightweight Rendering Engine for Dynamic Robotic\n  Simulations","summary":"  Interactive dynamic simulators are an accelerator for developing novel\nrobotic control algorithms and complex systems involving humans and robots. In\nuser training and synthetic data generation applications, a high-fidelity\nvisualization of the simulation is essential. Visual fidelity is dependent on\nthe quality of the computer graphics algorithms used to render the simulated\nscene. Furthermore, the rendering algorithms must be implemented on the\ngraphics processing unit (GPU) to achieve real-time performance, requiring the\nuse of a graphics application programming interface (API). This paper presents\na performance-focused and lightweight rendering engine supporting the Vulkan\ngraphics API. The engine is designed to modernize the legacy rendering pipeline\nof Asynchronous Multi-Body Framework (AMBF), a dynamic simulation framework\nused extensively for interactive robotics simulation development. This new\nrendering engine implements graphical features such as physically based\nrendering (PBR), anti-aliasing, and ray-traced shadows, significantly improving\nthe image quality of AMBF. Computational experiments show that the engine can\nrender a simulated scene with over seven million triangles while maintaining\nGPU computation times within two milliseconds.\n","authors":["Christopher John Allison","Haoying Zhou","Adnan Munawar","Peter Kazanzides","Juan Antonio Barragan"],"pdf_url":"https://arxiv.org/pdf/2410.05095v1.pdf","comment":"8 pages, 8 figures, submitted to the 2024 IEEE International\n  Conference on Robotic Computing (IRC)"},{"id":"http://arxiv.org/abs/2409.15341v2","updated":"2024-10-07T12:04:11Z","published":"2024-09-09T21:09:47Z","title":"StructuReiser: A Structure-preserving Video Stylization Method","summary":"  We introduce StructuReiser, a novel video-to-video translation method that\ntransforms input videos into stylized sequences using a set of user-provided\nkeyframes. Unlike existing approaches, StructuReiser maintains strict adherence\nto the structural elements of the target video, preserving the original\nidentity while seamlessly applying the desired stylistic transformations. This\nenables a level of control and consistency that was previously unattainable\nwith traditional text-driven or keyframe-based methods. Furthermore,\nStructuReiser supports real-time inference and custom keyframe editing, making\nit ideal for interactive applications and expanding the possibilities for\ncreative expression and video manipulation.\n","authors":["Radim Spetlik","David Futschik","Daniel Sykora"],"pdf_url":"https://arxiv.org/pdf/2409.15341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17961v2","updated":"2024-10-07T08:58:47Z","published":"2024-09-26T15:40:48Z","title":"SShaDe: scalable shape deformation via local representations","summary":"  With the increase in computational power for the available hardware, the\ndemand for high-resolution data in computer graphics applications increases.\nConsequently, classical geometry processing techniques based on linear algebra\nsolutions are starting to become obsolete. In this setting, we propose a novel\napproach for tackling mesh deformation tasks on high-resolution meshes. By\nreducing the input size with a fast remeshing technique and preserving a\nconsistent representation of the original mesh with local reference frames, we\nprovide a solution that is both scalable and robust in multiple applications,\nsuch as as-rigid-as-possible deformations, non-rigid isometric transformations,\nand pose transfer tasks. We extensively test our technique and compare it\nagainst state-of-the-art methods, proving that our approach can handle meshes\nwith hundreds of thousands of vertices in tens of seconds while still achieving\nresults comparable with the other solutions.\n","authors":["Filippo Maggioli","Daniele Baieri","Zorah L√§hner","Simone Melzi"],"pdf_url":"https://arxiv.org/pdf/2409.17961v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.08235v1","updated":"2024-10-07T21:28:09Z","published":"2024-10-07T21:28:09Z","title":"A Recurrent Neural Network Approach to the Answering Machine Detection\n  Problem","summary":"  In the field of telecommunications and cloud communications, accurately and\nin real-time detecting whether a human or an answering machine has answered an\noutbound call is of paramount importance. This problem is of particular\nsignificance during campaigns as it enhances service quality, efficiency and\ncost reduction through precise caller identification. Despite the significance\nof the field, it remains inadequately explored in the existing literature. This\npaper presents an innovative approach to answering machine detection that\nleverages transfer learning through the YAMNet model for feature extraction.\nThe YAMNet architecture facilitates the training of a recurrent-based\nclassifier, enabling real-time processing of audio streams, as opposed to\nfixed-length recordings. The results demonstrate an accuracy of over 96% on the\ntest set. Furthermore, we conduct an in-depth analysis of misclassified samples\nand reveal that an accuracy exceeding 98% can be achieved with the integration\nof a silence detection algorithm, such as the one provided by FFmpeg.\n","authors":["Kemal Altwlkany","Sead Delalic","Elmedin Selmanovic","Adis Alihodzic","Ivica Lovric"],"pdf_url":"https://arxiv.org/pdf/2410.08235v1.pdf","comment":"6 pages, 2 figures, 2024 47th MIPRO ICT and Electronics Convention\n  (MIPRO)"},{"id":"http://arxiv.org/abs/2410.05474v1","updated":"2024-10-07T20:12:08Z","published":"2024-10-07T20:12:08Z","title":"R-Bench: Are your Large Multimodal Model Robust to Real-world\n  Corruptions?","summary":"  The outstanding performance of Large Multimodal Models (LMMs) has made them\nwidely applied in vision-related tasks. However, various corruptions in the\nreal world mean that images will not be as ideal as in simulations, presenting\nsignificant challenges for the practical application of LMMs. To address this\nissue, we introduce R-Bench, a benchmark focused on the **Real-world Robustness\nof LMMs**. Specifically, we: (a) model the complete link from user capture to\nLMMs reception, comprising 33 corruption dimensions, including 7 steps\naccording to the corruption sequence, and 7 groups based on low-level\nattributes; (b) collect reference/distorted image dataset before/after\ncorruption, including 2,970 question-answer pairs with human labeling; (c)\npropose comprehensive evaluation for absolute/relative robustness and benchmark\n20 mainstream LMMs. Results show that while LMMs can correctly handle the\noriginal reference images, their performance is not stable when faced with\ndistorted images, and there is a significant gap in robustness compared to the\nhuman visual system. We hope that R-Bench will inspire improving the robustness\nof LMMs, **extending them from experimental simulations to the real-world\napplication**. Check https://q-future.github.io/R-Bench for details.\n","authors":["Chunyi Li","Jianbo Zhang","Zicheng Zhang","Haoning Wu","Yuan Tian","Wei Sun","Guo Lu","Xiaohong Liu","Xiongkuo Min","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2410.05474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05440v3","updated":"2024-10-07T12:20:15Z","published":"2023-05-09T13:33:12Z","title":"Improved Screen Content Coding in VVC Using Soft Context Formation","summary":"  Screen content images typically contain a mix of natural and synthetic image\nparts. Synthetic sections usually are comprised of uniformly colored areas and\nrepeating colors and patterns. In the VVC standard, these properties are\nexploited using Intra Block Copy and Palette Mode. In this paper, we show that\npixel-wise lossless coding can outperform lossy VVC coding in such areas. We\npropose an enhanced VVC coding approach for screen content images using the\nprinciple of soft context formation. First, the image is separated into two\nlayers in a block-wise manner using a learning-based method with four block\nfeatures. Synthetic image parts are coded losslessly using soft context\nformation, the rest with VVC.We modify the available soft context formation\ncoder to incorporate information gained by the decoded VVC layer for improved\ncoding efficiency. Using this approach, we achieve Bjontegaard-Delta-rate gains\nof 4.98% on the evaluated data sets compared to VVC.\n","authors":["Hannah Och","Shabhrish Reddy Uddehal","Tilo Strutz","Andr√© Kaup"],"pdf_url":"https://arxiv.org/pdf/2305.05440v3.pdf","comment":"5 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.04906v1","updated":"2024-10-07T10:48:08Z","published":"2024-10-07T10:48:08Z","title":"Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation","summary":"  Artificial Intelligence and generative models have revolutionized music\ncreation, with many models leveraging textual or visual prompts for guidance.\nHowever, existing image-to-music models are limited to simple images, lacking\nthe capability to generate music from complex digitized artworks. To address\nthis gap, we introduce $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$, a novel\nmodel designed to create music from digitized artworks or text inputs.\n$\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ extends the AudioLDM~2\narchitecture, a text-to-audio model, and employs our newly curated datasets,\ncreated via ImageBind, which pair digitized artworks with music. Experimental\nresults demonstrate that $\\mathcal{A}\\textit{rt2}\\mathcal{M}\\textit{us}$ can\ngenerate music that resonates with the input stimuli. These findings suggest\npromising applications in multimedia art, interactive installations, and\nAI-driven creative tools.\n","authors":["Ivan Rinaldi","Nicola Fanelli","Giovanna Castellano","Gennaro Vessio"],"pdf_url":"https://arxiv.org/pdf/2410.04906v1.pdf","comment":"Presented at the AI for Visual Arts (AI4VA) workshop at ECCV 2024"},{"id":"http://arxiv.org/abs/2410.04810v1","updated":"2024-10-07T07:45:18Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Gengyuan Zhang","Jinhe Bi","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04797v1","updated":"2024-10-07T07:16:29Z","published":"2024-10-07T07:16:29Z","title":"Attentive-based Multi-level Feature Fusion for Voice Disorder Diagnosis","summary":"  Voice disorders negatively impact the quality of daily life in various ways.\nHowever, accurately recognizing the category of pathological features from raw\naudio remains a considerable challenge due to the limited dataset. A promising\nmethod to handle this issue is extracting multi-level pathological information\nfrom speech in a comprehensive manner by fusing features in the latent space.\nIn this paper, a novel framework is designed to explore the way of high-quality\nfeature fusion for effective and generalized detection performance.\nSpecifically, the proposed model follows a two-stage training paradigm: (1)\nECAPA-TDNN and Wav2vec 2.0 which have shown remarkable effectiveness in various\ndomains are employed to learn the universal pathological information from raw\naudio; (2) An attentive fusion module is dedicatedly designed to establish the\ninteraction between pathological features projected by EcapTdnn and Wav2vec 2.0\nrespectively and guide the multi-layer fusion, the entire model is jointly\nfine-tuned from pre-trained features by the automatic voice pathology detection\ntask. Finally, comprehensive experiments on the FEMH and SVD datasets\ndemonstrate that the proposed framework outperforms the competitive baselines,\nand achieves the accuracy of 90.51% and 87.68%.\n","authors":["Lipeng Shen","Yifan Xiong","Dongyue Guo","Wei Mo","Lingyu Yu","Hui Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2410.04797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v3","updated":"2024-10-07T07:13:24Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v3.pdf","comment":"accepted at EMNLP2024 - system demonstration track"}]},"2024-10-06T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.04544v1","updated":"2024-10-06T16:42:59Z","published":"2024-10-06T16:42:59Z","title":"Fast Area-Weighted Peeling of Convex Hulls for Outlier Detection","summary":"  We present a novel 2D convex hull peeling algorithm for outlier detection,\nwhich repeatedly removes the point on the hull that decreases the hull's area\nthe most. To find k outliers among n points, one simply peels k points. The\nalgorithm is an efficient heuristic for exact methods, which find the k points\nwhose removal together results in the smallest convex hull. Our algorithm runs\nin O(nlogn) time using O(n) space for any choice of k. This is a significant\nspeedup compared to the fastest exact algorithms, which run in O(n^2logn + (n -\nk)^3) time using O(n\\logn + (n-k)^3) space by Eppstein et al., and O(nlogn +\n4k_C_2k (3k)^k n) time by Atanassov et al. Existing heuristic peeling\napproaches are not area-based. Instead, an approach by Harsh et al. repeatedly\nremoves the point furthest from the mean using various distance metrics and\nruns in O(nlogn + kn) time. Other approaches greedily peel one convex layer at\na time, which is efficient when using an O(nlogn) time algorithm by Chazelle to\ncompute the convex layers. However, in many cases this fails to recover\noutliers. For most values of n and k, our approach is the fastest and first\npractical choice for finding outliers based on minimizing the area of the\nconvex hull. Our algorithm also generalizes to other objectives such as\nperimeter.\n","authors":["Vinesh Sridhar","Rolf Svenning"],"pdf_url":"https://arxiv.org/pdf/2410.04544v1.pdf","comment":"8 pages. 5 figures. Published at the 2024 Canadian Conference of\n  Computational Geometry"}],"Graphics":[{"id":"http://arxiv.org/abs/2409.18974v2","updated":"2024-10-06T17:40:40Z","published":"2024-09-12T15:38:21Z","title":"Neural Product Importance Sampling via Warp Composition","summary":"  Achieving high efficiency in modern photorealistic rendering hinges on using\nMonte Carlo sampling distributions that closely approximate the illumination\nintegral estimated for every pixel. Samples are typically generated from a set\nof simple distributions, each targeting a different factor in the integrand,\nwhich are combined via multiple importance sampling. The resulting mixture\ndistribution can be far from the actual product of all factors, leading to\nsub-optimal variance even for direct-illumination estimation. We present a\nlearning-based method that uses normalizing flows to efficiently importance\nsample illumination product integrals, e.g., the product of environment\nlighting and material terms. Our sampler composes a flow head warp with an\nemitter tail warp. The small conditional head warp is represented by a neural\nspline flow, while the large unconditional tail is discretized per environment\nmap and its evaluation is instant. If the conditioning is low-dimensional, the\nhead warp can be also discretized to achieve even better performance. We\ndemonstrate variance reduction over prior methods on a range of applications\ncomprising complex geometry, materials and illumination.\n","authors":["Joey Litalien","Milo≈° Ha≈°an","Fujun Luan","Krishna Mullia","Iliyan Georgiev"],"pdf_url":"https://arxiv.org/pdf/2409.18974v2.pdf","comment":"Published in ACM SIGGRAPH Asia 2024 Conference Papers. Project page:\n  https://joeylitalien.github.io/publications/warp"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.04570v1","updated":"2024-10-06T17:56:13Z","published":"2024-10-06T17:56:13Z","title":"Watermarking Decision Tree Ensembles","summary":"  Protecting the intellectual property of machine learning models is a hot\ntopic and many watermarking schemes for deep neural networks have been proposed\nin the literature. Unfortunately, prior work largely neglected the\ninvestigation of watermarking techniques for other types of models, including\ndecision tree ensembles, which are a state-of-the-art model for classification\ntasks on non-perceptual data. In this paper, we present the first watermarking\nscheme designed for decision tree ensembles, focusing in particular on random\nforest models. We discuss watermark creation and verification, presenting a\nthorough security analysis with respect to possible attacks. We finally perform\nan experimental evaluation of the proposed scheme, showing excellent results in\nterms of accuracy and security against the most relevant threats.\n","authors":["Stefano Calzavara","Lorenzo Cazzaro","Donald Gera","Salvatore Orlando"],"pdf_url":"https://arxiv.org/pdf/2410.04570v1.pdf","comment":"7 pages, 5 figures, 2 tables"}]},"2024-10-04T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2410.03889v1","updated":"2024-10-04T19:46:08Z","published":"2024-10-04T19:46:08Z","title":"Identification of Anomalous Geospatial Trajectories via Persistent\n  Homology","summary":"  We present a novel method for analyzing geospatial trajectory data using\ntopological data analysis (TDA) to identify a specific class of anomalies,\ncommonly referred to as crop circles, in AIS data. This approach is the first\nof its kind to be applied to spatiotemporal data. By embedding\n$2+1$-dimensional spatiotemporal data into $\\mathbb{R}^3$, we utilize\npersistent homology to detect loops within the trajectories in $\\mathbb{R}^2$.\nOur research reveals that, under normal conditions, trajectory data embedded in\n$\\mathbb{R}^3$ over time do not form loops. Consequently, we can effectively\nidentify anomalies characterized by the presence of loops within the\ntrajectories. This method is robust and capable of detecting loops that are\ninvariant to small perturbations, variations in geometric shape, and local\ncoordinate projections. Additionally, our approach provides a novel perspective\non anomaly detection, offering enhanced sensitivity and specificity in\nidentifying atypical patterns in geospatial data. This approach has significant\nimplications for various applications, including maritime navigation,\nenvironmental monitoring, and surveillance.\n","authors":["Kyle Evans-Lee","Kevin Lamb"],"pdf_url":"https://arxiv.org/pdf/2410.03889v1.pdf","comment":"Title: Identification of Anomalous Geospatial Trajectories via\n  Persistent Homology Authors: Kyle Evans-Lee, Kevin Lamb Comments: 18 pages,\n  12 figures We present a method for analyzing geospatial trajectory data using\n  topological data analysis (TDA) to identify a specific class of anomalies,\n  commonly referred to as crop circles, in AIS data"},{"id":"http://arxiv.org/abs/2410.03609v1","updated":"2024-10-04T17:09:41Z","published":"2024-10-04T17:09:41Z","title":"Subexponential Algorithms for Clique Cover on Unit Disk and Unit Ball\n  Graphs","summary":"  In Clique Cover, given a graph $G$ and an integer $k$, the task is to\npartition the vertices of $G$ into $k$ cliques. Clique Cover on unit ball\ngraphs has a natural interpretation as a clustering problem, where the\nobjective function is the maximum diameter of a cluster.\n  Many classical NP-hard problems are known to admit $2^{O(n^{(1 -\n1/d)})}$-time algorithms on unit ball graphs in $\\mathbb{R}^d$ [de Berg et al.,\nSIAM J. Comp 2018]. A notable exception is the Maximum Clique problem, which\nadmits a polynomial-time algorithm on unit disk graphs and a subexponential\nalgorithm on unit ball graphs in $\\mathbb{R}^3$, but no subexponential\nalgorithm on unit ball graphs in dimensions 4 or larger, assuming the ETH\n[Bonamy et al., JACM 2021].\n  In this work, we show that Clique Cover also suffers from a \"curse of\ndimensionality\", albeit in a significantly different way compared to Maximum\nClique. We present a $2^{O(\\sqrt{n})}$-time algorithm for unit disk graphs and\nargue that it is tight under the ETH. On the other hand, we show that Clique\nCover does not admit a $2^{o(n)}$-time algorithm on unit ball graphs in\ndimension $5$, unless the ETH fails.\n","authors":["Tomohiro Koana","Nidhi Purohit","Kirill Simonov"],"pdf_url":"https://arxiv.org/pdf/2410.03609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05879v3","updated":"2024-10-04T13:54:25Z","published":"2024-04-08T21:26:04Z","title":"Rapid and Precise Topological Comparison with Merge Tree Neural Networks","summary":"  Merge trees are a valuable tool in the scientific visualization of scalar\nfields; however, current methods for merge tree comparisons are computationally\nexpensive, primarily due to the exhaustive matching between tree nodes. To\naddress this challenge, we introduce the Merge Tree Neural Network (MTNN), a\nlearned neural network model designed for merge tree comparison. The MTNN\nenables rapid and high-quality similarity computation. We first demonstrate how\nto train graph neural networks, which emerged as effective encoders for graphs,\nin order to produce embeddings of merge trees in vector spaces for efficient\nsimilarity comparison. Next, we formulate the novel MTNN model that further\nimproves the similarity comparisons by integrating the tree and node embeddings\nwith a new topological attention mechanism. We demonstrate the effectiveness of\nour model on real-world data in different domains and examine our model's\ngeneralizability across various datasets. Our experimental analysis\ndemonstrates our approach's superiority in accuracy and efficiency. In\nparticular, we speed up the prior state-of-the-art by more than $100\\times$ on\nthe benchmark datasets while maintaining an error rate below $0.1\\%$.\n","authors":["Yu Qin","Brittany Terese Fasy","Carola Wenk","Brian Summa"],"pdf_url":"https://arxiv.org/pdf/2404.05879v3.pdf","comment":"Published on IEEE VIS 2024 with Best Paper Award"},{"id":"http://arxiv.org/abs/2410.03213v1","updated":"2024-10-04T07:58:47Z","published":"2024-10-04T07:58:47Z","title":"Computing largest minimum color-spanning intervals of imprecise points","summary":"  We study a geometric facility location problem under imprecision. Given $n$\nunit intervals in the real line, each with one of $k$ colors, the goal is to\nplace one point in each interval such that the resulting \\emph{minimum\ncolor-spanning interval} is as large as possible. A minimum color-spanning\ninterval is an interval of minimum size that contains at least one point from a\ngiven interval of each color. We prove that if the input intervals are pairwise\ndisjoint, the problem can be solved in $O(n)$ time, even for intervals of\narbitrary length. For overlapping intervals, the problem becomes much more\ndifficult. Nevertheless, we show that it can be solved in $O(n \\log^2 n)$ time\nwhen $k=2$, by exploiting several structural properties of candidate solutions,\ncombined with a number of advanced algorithmic techniques. Interestingly, this\nshows a sharp contrast with the 2-dimensional version of the problem, recently\nshown to be NP-hard.\n","authors":["Ankush Acharyya","Vahideh Keikha","Maria Saumell","Rodrigo I. Silveira"],"pdf_url":"https://arxiv.org/pdf/2410.03213v1.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.05312v1","updated":"2024-10-04T21:12:23Z","published":"2024-10-04T21:12:23Z","title":"An Intelligent Native Network Slicing Security Architecture Empowered by\n  Federated Learning","summary":"  Network Slicing (NS) has transformed the landscape of resource sharing in\nnetworks, offering flexibility to support services and applications with highly\nvariable requirements in areas such as the next-generation 5G/6G mobile\nnetworks (NGMN), vehicular networks, industrial Internet of Things (IoT), and\nverticals. Although significant research and experimentation have driven the\ndevelopment of network slicing, existing architectures often fall short in\nintrinsic architectural intelligent security capabilities. This paper proposes\nan architecture-intelligent security mechanism to improve the NS solutions. We\nidealized a security-native architecture that deploys intelligent microservices\nas federated agents based on machine learning, providing intra-slice and\narchitectural operation security for the Slicing Future Internet\nInfrastructures (SFI2) reference architecture. It is noteworthy that federated\nlearning approaches match the highly distributed modern microservice-based\narchitectures, thus providing a unifying and scalable design choice for NS\nplatforms addressing both service and security. Using ML-Agents and Security\nAgents, our approach identified Distributed Denial-of-Service (DDoS) and\nintrusion attacks within the slice using generic and non-intrusive telemetry\nrecords, achieving an average accuracy of approximately $95.60\\%$ in the\nnetwork slicing architecture and $99.99\\%$ for the deployed slice --\nintra-slice. This result demonstrates the potential for leveraging\narchitectural operational security and introduces a promising new research\ndirection for network slicing architectures.\n","authors":["Rodrigo Moreira","Rodolfo S. Villaca","Moises R. N. Ribeiro","Joberto S. B. Martins","Joao Henrique Correa","Tereza C. Carvalho","Flavio de Oliveira Silva"],"pdf_url":"https://arxiv.org/pdf/2410.05312v1.pdf","comment":"18 pages, 12 figures, Future Generation Computer Systems (FGCS)"},{"id":"http://arxiv.org/abs/2311.17801v2","updated":"2024-10-04T15:26:45Z","published":"2023-11-29T16:51:21Z","title":"Towards Efficient Hyperdimensional Computing Using Photonics","summary":"  Over the past few years, silicon photonics-based computing has emerged as a\npromising alternative to CMOS-based computing for Deep Neural Networks (DNN).\nUnfortunately, the non-linear operations and the high-precision requirements of\nDNNs make it extremely challenging to design efficient silicon photonics-based\nsystems for DNN inference and training. Hyperdimensional Computing (HDC) is an\nemerging, brain-inspired machine learning technique that enjoys several\nadvantages over existing DNNs, including being lightweight, requiring\nlow-precision operands, and being robust to noise introduced by the\nnonidealities in the hardware. For HDC, computing in-memory (CiM) approaches\nhave been widely used, as CiM reduces the data transfer cost if the operands\ncan fit into the memory. However, inefficient multi-bit operations, high write\nlatency, and low endurance make CiM ill-suited for HDC. On the other hand, the\nexisting electro-photonic DNN accelerators are inefficient for HDC because they\nare specifically optimized for matrix multiplication in DNNs and consume a lot\nof power with high-precision data converters.\n  In this paper, we argue that photonic computing and HDC complement each other\nbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,\nthe first-ever electro-photonic accelerator for HDC training and inference,\nsupporting the basic, record-based, and graph encoding schemes. Evaluating with\npopular datasets, we show that our accelerator can achieve two to five orders\nof magnitude lower EDP than the state-of-the-art electro-photonic DNN\naccelerators for implementing HDC training and inference. PhotoHDC also\nachieves four orders of magnitude lower energy-delay product than CiM-based\naccelerators for both HDC training and inference.\n","authors":["Farbin Fayza","Cansu Demirkiran","Hanning Chen","Che-Kai Liu","Avi Mohan","Hamza Errahmouni","Sanggeon Yun","Mohsen Imani","David Zhang","Darius Bunandar","Ajay Joshi"],"pdf_url":"https://arxiv.org/pdf/2311.17801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03414v1","updated":"2024-10-04T13:24:41Z","published":"2024-10-04T13:24:41Z","title":"A 9T4R RRAM-Based ACAM for Analogue Template Matching at the Edge","summary":"  The continuous shift of computational bottlenecks to the memory access and\ndata transfer, especially for AI applications, poses the urgent needs of\nre-engineering the computer architecture fundamentals. Many edge computing\napplications, like wearable and implantable medical devices, introduce\nincreasingly more challenges to conventional computing systems due to the\nstrict requirements of area and power at the edge. Emerging technologies, like\nResistive RAM (RRAM), have shown a promising momentum in developing\nneuro-inspired analogue computing paradigms capable of achieving high\nclassification capabilities alongside high energy efficiency. In this work, we\npresent a novel RRAM-based Analogue Content Addressable Memory (ACAM) for\non-line analogue template matching applications. This ACAM-based template\nmatching architecture aims to achieve energy-efficient classification where low\nenergy is of utmost importance. We are showcasing a highly tuneable novel\nRRAM-based ACAM pixel implemented using a commercial 180nm CMOS technology and\nin-house RRAM technology and exhibiting low energy dissipation of approximately\n0.036pJ and 0.16pJ for mismatch and match, respectively, at 66MHz with 3V\nvoltage supply. A proof-of-concept system-level implementation based on this\nnovel pixel design is also implemented in 180nm.\n","authors":["Georgios Papandroulidakis","Shady Agwa","Ahmet Cirakoglu","Themis Prodromakis"],"pdf_url":"https://arxiv.org/pdf/2410.03414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15537v3","updated":"2024-10-04T12:47:26Z","published":"2024-05-24T13:26:39Z","title":"Do Not Trust Power Management: A Survey on Internal Energy-based Attacks\n  Circumventing Trusted Execution Environments Security Properties","summary":"  Over the past few years, several research groups have introduced innovative\nhardware designs for Trusted Execution Environments (TEEs), aiming to secure\napplications against potentially compromised privileged software, including the\nkernel. Since 2015, a new class of software-enabled hardware attacks leveraging\nenergy management mechanisms has emerged. These internal energy-based attacks\ncomprise fault, side-channel and covert channel attacks. Their aim is to bypass\nTEE security guarantees and expose sensitive information such as cryptographic\nkeys. They have increased in prevalence in the past few years. Popular TEE\nimplementations, such as ARM TrustZone and Intel SGX, incorporate\ncountermeasures against these attacks. However, these countermeasures either\nhinder the capabilities of the power management mechanisms or have been shown\nto provide insufficient system protection. This article presents the first\ncomprehensive knowledge survey of these attacks, along with an evaluation of\nliterature countermeasures. We believe that this study will spur further\ncommunity efforts towards this increasingly important type of attacks.\n","authors":["Gwenn Le Gonidec","Maria M√©ndez Real","Guillaume Bouffard","Jean-Christophe Pr√©votet"],"pdf_url":"https://arxiv.org/pdf/2405.15537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09308v2","updated":"2024-10-04T07:18:36Z","published":"2024-07-12T14:45:19Z","title":"Digital-analog quantum genetic algorithm using Rydberg-atom arrays","summary":"  Digital-analog quantum computing (DAQC) combines digital gates with analog\noperations, offering an alternative paradigm for universal quantum computation.\nThis approach leverages the higher fidelities of analog operations and the\nflexibility of local single-qubit gates. In this paper, we propose a quantum\ngenetic algorithm within the DAQC framework using a Rydberg-atom emulator. The\nalgorithm employs single-qubit operations in the digital domain and a global\ndriving interaction based on the Rydberg Hamiltonian in the analog domain. We\nevaluate the algorithm performance by estimating the ground-state energy of\nHamiltonians, with a focus on molecules such as $\\rm H_2$, $\\rm LiH$, and $\\rm\nBeH_2$. Our results show energy estimations with less than 1% error and state\noverlaps nearing 1, with computation times ranging from a few minutes for $\\rm\nH_2$ (2-qubit circuits) to one to two days for $\\rm LiH$ and $\\rm BeH_2$\n(6-qubit circuits). The gate fidelities of global analog operations further\nunderscore DAQC as a promising quantum computing strategy in the noisy\nintermediate-scale quantum era.\n","authors":["Aleix Llenas","Lucas Lamata"],"pdf_url":"https://arxiv.org/pdf/2407.09308v2.pdf","comment":"13 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.11856v1","updated":"2024-10-04T03:54:59Z","published":"2024-10-04T03:54:59Z","title":"Malak: AI-based multilingual personal assistant to combat misinformation\n  and generative AI safety issues","summary":"  The widespread use of AI technologies to generate digital content has led to\nincreased misinformation and online harm. Deep fake technologies, a type of AI,\nmake it easier to create convincing but fake content on social media, leading\nto various cyber threats. Malicious actors exploit AI capabilities, posing\ndigital, physical, and psychological harm to individuals. While social media\nplatforms have safety measures such as content rating and feedback systems,\nthese are often used by people with higher digital literacy. There is a lack of\npreventive measures and a need for user-friendly tools that can be used by\npeople with lower digital literacy. Our goal is to create a user-friendly\nmultilingual AI-based personal assistant, Malak, to reduce online harm and\npromote safe online interactions, benefiting users with lower literacy levels.\n","authors":["Farnaz Farid","Farhad Ahamed"],"pdf_url":"https://arxiv.org/pdf/2410.11856v1.pdf","comment":null}]},"2024-10-03T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2407.05135v2","updated":"2024-10-03T14:25:53Z","published":"2024-07-06T17:19:53Z","title":"Theory and Explicit Design of a Path Planner for an SE(3) Robot","summary":"  We consider path planning for a rigid spatial robot with 6 degrees of freedom\n(6 DOFs), moving amidst polyhedral obstacles. A correct, complete and practical\npath planner for such a robot has never been achieved, although this is widely\nrecognized as a key challenge in robotics. This paper provides a complete\n\"explicit\" design, down to explicit geometric primitives that are easily\nimplementable.\n  Our design is within an algorithmic framework for path planners, called Soft\nSubdivision Search (SSS). The framework is based on the twin foundations of\n$\\epsilon$-exactness and soft predicates, which are critical for rigorous\nnumerical implementations. The practicality of SSS has been previously\ndemonstrated for various robots including 5-DOF spatial robots.\n  In this paper, we solve several significant technical challenges for SE(3)\nrobots: (1) We first ensure the correct theory by proving a general form of the\nFundamental Theorem of the SSS theory. We prove this within an axiomatic\nframework, thus making it easy for future applications of this theory. (2) One\ncomponent of $SE(3) = R^3 \\times SO(3)$ is the non-Euclidean space SO(3). We\ndesign a novel topologically correct data structure for SO(3). Using the\nconcept of subdivision charts and atlases for SO(3), we can now carry out\nsubdivision of SO(3). (3) The geometric problem of collision detection takes\nplace in $R^3$, via the footprint map. Unlike sampling-based approaches, we\nmust reason with the notion of footprints of configuration boxes, which is much\nharder to characterize. Exploiting the theory of soft predicates, we design\nsuitable approximate footprints which, when combined with the highly effective\nfeature-set technique, lead to soft predicates. (4) Finally, we make the\nunderlying geometric computation \"explicit\", i.e., avoiding a general solver of\npolynomial systems, in order to allow a direct implementation.\n","authors":["Zhaoqi Zhang","Yi-Jen Chiang","Chee Yap"],"pdf_url":"https://arxiv.org/pdf/2407.05135v2.pdf","comment":"A conference version is to appear at the International Workshop on\n  the Algorithmic Foundations of Robotics (WAFR) 2024. This is a revised full\n  version, 42 pages, including 5 appendices"},{"id":"http://arxiv.org/abs/2410.02449v1","updated":"2024-10-03T12:51:53Z","published":"2024-10-03T12:51:53Z","title":"A fast algorithm for computing a planar support for non-piercing\n  rectangles","summary":"  For a hypergraph $\\mathcal{H}=(X,\\mathcal{E})$ a \\emph{support} is a graph\n$G$ on $X$ such that for each $E\\in\\mathcal{E}$, the induced subgraph of $G$ on\nthe elements in $E$ is connected. If $G$ is planar, we call it a planar\nsupport. A set of axis parallel rectangles $\\mathcal{R}$ forms a non-piercing\nfamily if for any $R_1, R_2 \\in \\mathcal{R}$, $R_1 \\setminus R_2$ is connected.\nGiven a set $P$ of $n$ points in $\\mathbb{R}^2$ and a set $\\mathcal{R}$ of $m$\n\\emph{non-piercing} axis-aligned rectangles, we give an algorithm for computing\na planar support for the hypergraph $(P,\\mathcal{R})$ in $O(n\\log^2 n +\n(n+m)\\log m)$ time, where each $R\\in\\mathcal{R}$ defines a hyperedge consisting\nof all points of $P$ contained in~$R$. We use this result to show that if for a\nfamily of axis-parallel rectangles, any point in the plane is contained in at\nmost $k$ pairwise \\emph{crossing} rectangles (a pair of intersecting rectangles\nsuch that neither contains a corner of the other is called a crossing pair of\nrectangles), then we can obtain a support as the union of $k$ planar graphs.\n","authors":["Ambar Pal","Rajiv Raman","Saurabh Ray","Karamjeet Singh"],"pdf_url":"https://arxiv.org/pdf/2410.02449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02897v3","updated":"2024-10-03T09:13:12Z","published":"2024-01-05T16:59:11Z","title":"Robust Bichromatic Classification using Two Lines","summary":"  Given two sets $R$ and $B$ of $n$ points in the plane, we present efficient\nalgorithms to find a two-line linear classifier that best separates the \"red\"\npoints in $R$ from the \"blue\" points in $B$ and is robust to outliers. More\nprecisely, we find a region $\\mathcal{W}_B$ bounded by two lines, so either a\nhalfplane, strip, wedge, or double wedge, containing (most of) the blue points\n$B$, and few red points. Our running times vary between optimal $O(n\\log n)$\nand around $O(n^3)$, depending on the type of region $\\mathcal{W}_B$ and\nwhether we wish to minimize only red outliers, only blue outliers, or both.\n","authors":["Erwin Glazenburg","Thijs van der Horst","Tom Peters","Bettina Speckmann","Frank Staals"],"pdf_url":"https://arxiv.org/pdf/2401.02897v3.pdf","comment":"26 pages, 17 figures. Full version of article to be presented at\n  ISAAC24"},{"id":"http://arxiv.org/abs/2410.02158v1","updated":"2024-10-03T02:44:13Z","published":"2024-10-03T02:44:13Z","title":"ClassContrast: Bridging the Spatial and Contextual Gaps for Node\n  Representations","summary":"  Graph Neural Networks (GNNs) have revolutionized the domain of graph\nrepresentation learning by utilizing neighborhood aggregation schemes in many\npopular architectures, such as message passing graph neural networks (MPGNNs).\nThis scheme involves iteratively calculating a node's representation vector by\naggregating and transforming the representation vectors of its adjacent nodes.\nDespite their effectiveness, MPGNNs face significant issues, such as\noversquashing, oversmoothing, and underreaching, which hamper their\neffectiveness. Additionally, the reliance of MPGNNs on the homophily\nassumption, where edges typically connect nodes with similar labels and\nfeatures, limits their performance in heterophilic contexts, where connected\nnodes often have significant differences. This necessitates the development of\nmodels that can operate effectively in both homophilic and heterophilic\nsettings.\n  In this paper, we propose a novel approach, ClassContrast, grounded in Energy\nLandscape Theory from Chemical Physics, to overcome these limitations.\nClassContrast combines spatial and contextual information, leveraging a\nphysics-inspired energy landscape to model node embeddings that are both\ndiscriminative and robust across homophilic and heterophilic settings. Our\napproach introduces contrast-based homophily matrices to enhance the\nunderstanding of class interactions and tendencies. Through extensive\nexperiments, we demonstrate that ClassContrast outperforms traditional GNNs in\nnode classification and link prediction tasks, proving its effectiveness and\nversatility in diverse real-world scenarios.\n","authors":["Md Joshem Uddin","Astrit Tola","Varin Sikand","Cuneyt Gurcan Akcora","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.02158v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.01504v3","updated":"2024-10-03T01:23:57Z","published":"2024-04-01T22:10:15Z","title":"On the orthogonal Gr√ºnbaum partition problem in dimension three","summary":"  Gr\\\"unbaum's equipartition problem asked if for any measure $\\mu$ on\n$\\mathbb{R}^d$ there are always $d$ hyperplanes which divide $\\mathbb{R}^d$\ninto $2^d$ $\\mu$-equal parts. This problem is known to have a positive answer\nfor $d\\le 3$ and a negative one for $d\\ge 5$. A variant of this question is to\nrequire the hyperplanes to be mutually orthogonal. This variant is known to\nhave a positive answer for $d\\le 2$ and there is reason to expect it to have a\nnegative answer for $d\\ge 3$. In this note we exhibit measures that prove this.\nAdditionally, we describe an algorithm that checks if a set of $8n$ in\n$\\mathbb{R}^3$ can be split evenly by $3$ mutually orthogonal planes. To our\nsurprise, it seems the probability that a random set of $8$ points chosen\nuniformly and independently in the unit cube does not admit such a partition is\nless than $0.001$.\n","authors":["Gerardo L. Maldonado","Edgardo Rold√°n-Pensado"],"pdf_url":"https://arxiv.org/pdf/2404.01504v3.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.02998v1","updated":"2024-10-03T21:15:05Z","published":"2024-10-03T21:15:05Z","title":"Q-SCALE: Quantum computing-based Sensor Calibration for Advanced\n  Learning and Efficiency","summary":"  In a world burdened by air pollution, the integration of state-of-the-art\nsensor calibration techniques utilizing Quantum Computing (QC) and Machine\nLearning (ML) holds promise for enhancing the accuracy and efficiency of air\nquality monitoring systems in smart cities. This article investigates the\nprocess of calibrating inexpensive optical fine-dust sensors through advanced\nmethodologies such as Deep Learning (DL) and Quantum Machine Learning (QML).\nThe objective of the project is to compare four sophisticated algorithms from\nboth the classical and quantum realms to discern their disparities and explore\npossible alternative approaches to improve the precision and dependability of\nparticulate matter measurements in urban air quality surveillance. Classical\nFeed-Forward Neural Networks (FFNN) and Long Short-Term Memory (LSTM) models\nare evaluated against their quantum counterparts: Variational Quantum\nRegressors (VQR) and Quantum LSTM (QLSTM) circuits. Through meticulous testing,\nincluding hyperparameter optimization and cross-validation, the study assesses\nthe potential of quantum models to refine calibration performance. Our analysis\nshows that: the FFNN model achieved superior calibration accuracy on the test\nset compared to the VQR model in terms of lower L1 loss function (2.92 vs\n4.81); the QLSTM slightly outperformed the LSTM model (loss on the test set:\n2.70 vs 2.77), despite using fewer trainable weights (66 vs 482).\n","authors":["Lorenzo Bergadano","Andrea Ceschini","Pietro Chiavassa","Edoardo Giusto","Bartolomeo Montrucchio","Massimo Panella","Antonello Rosato"],"pdf_url":"https://arxiv.org/pdf/2410.02998v1.pdf","comment":"Accepted at QCE24"},{"id":"http://arxiv.org/abs/2410.02955v1","updated":"2024-10-03T19:57:05Z","published":"2024-10-03T19:57:05Z","title":"AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and\n  Test","summary":"  Instructions for Build, Assembly, and Test (IBAT) refers to the process used\nwhenever any operation is conducted on hardware, including tests, assembly, and\nmaintenance. Currently, the generation of IBAT documents is time-intensive, as\nusers must manually reference and transfer information from engineering\ndiagrams and parts lists into IBAT instructions. With advances in machine\nlearning and computer vision, however, it is possible to have an artificial\nintelligence (AI) model perform the partial filling of the IBAT template,\nfreeing up engineer time for more highly skilled tasks. AiBAT is a novel system\nfor assisting users in authoring IBATs. It works by first analyzing assembly\ndrawing documents, extracting information and parsing it, and then filling in\nIBAT templates with the extracted information. Such assisted authoring has\npotential to save time and reduce cost. This paper presents an overview of the\nAiBAT system, including promising preliminary results and discussion on future\nwork.\n","authors":["Benjamin Nuernberger","Anny Liu","Heather Stefanini","Richard Otis","Amanda Towler","R. Peter Dillon"],"pdf_url":"https://arxiv.org/pdf/2410.02955v1.pdf","comment":"9 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.02954v1","updated":"2024-10-03T19:55:20Z","published":"2024-10-03T19:55:20Z","title":"Digital Twin for O-RAN Towards 6G","summary":"  In future wireless systems of beyond 5G and 6G, addressing diverse\napplications with varying quality requirements is essential. Open Radio Access\nNetwork (O-RAN) architectures offer the potential for dynamic resource\nadaptation based on traffic demands. However, achieving real-time resource\norchestration remains a challenge. Simultaneously, Digital Twin (DT) technology\nholds promise for testing and analysing complex systems, offering a unique\nplatform for addressing dynamic operation and automation in O-RAN\narchitectures. Yet, developing DTs for complex 5G/6G networks poses challenges,\nincluding data exchanges, ML model training data availability, network\ndynamics, processing power limitations, interdisciplinary collaboration needs,\nand a lack of standardized methodologies. This paper provides an overview of\nOpen RAN architecture, trend and challenges, proposing the DT concepts for\nO-RAN with solution examples showcasing its integration into the framework.\n","authors":["Huan X. Nguyen","Kexuan Sun","Duc To","Quoc-Tuan Vien","Tuan Anh Le"],"pdf_url":"https://arxiv.org/pdf/2410.02954v1.pdf","comment":"IEEE Communications Magazine 2024"},{"id":"http://arxiv.org/abs/2410.02901v1","updated":"2024-10-03T18:48:14Z","published":"2024-10-03T18:48:14Z","title":"GTQCP: Greedy Topology-Aware Quantum Circuit Partitioning","summary":"  We propose Greedy Topology-Aware Quantum Circuit Partitioning (GTQCP), a\nnovel quantum gate circuit partitioning method which partitions circuits by\napplying a greedy heuristic to the qubit dependency graph of the circuit. GTQCP\nis compared against three other gate partitioning methods, two of which\n(QuickPartitioner and ScanPartitioner) are part of the Berkley Quantum\nSynthesis Toolkit. GTQCP is shown to have 18% run time improvement ratio over\nthe fastest approach (QuickPartitioner), and a 96% improvement over the highest\nquality approach (ScanPartitioner). The algorithm also demonstrates nearly\nidentical result quality (number of partitions) compared with ScanPartitioner,\nand a 38% quality improvement over QuickPartitioner.\n","authors":["Joseph Clark","Travis S. Humble","Himanshu Thapliyal"],"pdf_url":"https://arxiv.org/pdf/2410.02901v1.pdf","comment":"6 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.02854v1","updated":"2024-10-03T18:00:01Z","published":"2024-10-03T18:00:01Z","title":"MQT Qudits: A Software Framework for Mixed-Dimensional Quantum Computing","summary":"  Quantum computing holds great promise for surpassing the limits of classical\ndevices in many fields. Despite impressive developments, however, current\nresearch is primarily focused on qubits. At the same time, quantum hardware\nbased on multi-level, qudit, systems offers a range of advantages, including\nexpanded gate sets, higher information density, and improved computational\nefficiency, which might play a key role in overcoming not only the limitations\nof classical machines but also of current qubit-based quantum devices. However,\nworking with qudits faces challenges not only in experimental control but\nparticularly in algorithm development and quantum software. In this work, we\nintroduce MQT Qudits, an open-source tool, which, as part of the Munich Quantum\nToolkit (MQT), is built to assist in designing and implementing applications\nfor mixed-dimensional qudit devices. We specify a standardized language for\nmixed-dimension systems and discuss circuit specification, compilation to\nhardware gate sets, efficient circuit simulation, and open challenges. MQT\nQudits is available at github.com/cda-tum/mqt-qudits and on pypi at\npypi.org/project/mqt.qudits.\n","authors":["Kevin Mato","Martin Ringbauer","Lukas Burgholzer","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2410.02854v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.12631v2","updated":"2024-10-03T15:23:36Z","published":"2024-02-20T01:22:07Z","title":"Theoretical Approximation Ratios for Warm-Started QAOA on 3-Regular\n  Max-Cut Instances at Depth $p=1$","summary":"  We generalize Farhi et al.'s 0.6924-approximation result technique of the\nMax-Cut Quantum Approximate Optimization Algorithm (QAOA) on 3-regular graphs\nto obtain provable lower bounds on the approximation ratio for warm-started\nQAOA. Given an initialization angle $\\theta$, we consider warm-starts where the\ninitial state is a product state where each qubit position is angle $\\theta$\naway from either the north or south pole of the Bloch sphere; of the two\npossible qubit positions the position of each qubit is decided by some\nclassically obtained cut encoded as a bitstring $b$.\n  We illustrate through plots how the properties of $b$ and the initialization\nangle $\\theta$ influence the bound on the approximation ratios of warm-started\nQAOA. We consider various classical algorithms (and the cuts they produce which\nwe use to generate the warm-start). Our results strongly suggest that there\ndoes not exist any choice of initialization angle that yields a (worst-case)\napproximation ratio that simultaneously beats standard QAOA and the classical\nalgorithm used to create the warm-start.\n  Additionally, we show that at $\\theta=60^\\circ$, warm-started QAOA is able to\n(effectively) recover the cut used to generate the warm-start, thus suggesting\nthat in practice, this value could be a promising starting angle to explore\nalternate solutions in a heuristic fashion.\n","authors":["Reuben Tate","Stephan Eidenbenz"],"pdf_url":"https://arxiv.org/pdf/2402.12631v2.pdf","comment":"The first version of this arXiv submission, titled \"Guarantees on\n  Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and\n  Higher-Round Scaling Limits\", has since been split into two parts: a longer\n  part and a shorter part. The current version of this arXiv submission is the\n  longer part. The shorter part can be found here: arxiv:2410.00027"},{"id":"http://arxiv.org/abs/2410.02529v1","updated":"2024-10-03T14:36:32Z","published":"2024-10-03T14:36:32Z","title":"An Edge-Computing based Industrial Gateway for Industry 4.0 using ARM\n  TrustZone Technology","summary":"  Secure and efficient communication to establish a seamless nexus between the\nfive levels of a typical automation pyramid is paramount to Industry 4.0.\nSpecifically, vertical and horizontal integration of these levels is an\noverarching requirement to accelerate productivity and improve operational\nactivities. Vertical integration can improve visibility, flexibility, and\nproductivity by connecting systems and applications. Horizontal integration can\nprovide better collaboration and adaptability by connecting internal production\nfacilities, multi-site operations, and third-party partners in a supply chain.\nIn this paper, we propose an Edge-computing-based Industrial Gateway for\ninterfacing information technology and operational technology that can enable\nIndustry 4.0 vertical and horizontal integration. Subsequently, we design and\ndevelop a working prototype to demonstrate a remote production-line maintenance\nuse case with a strong focus on security aspects and the edge paradigm to bring\ncomputational resources and data storage closer to data sources.\n","authors":["Sandeep Gupta"],"pdf_url":"https://arxiv.org/pdf/2410.02529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09161v2","updated":"2024-10-03T07:14:15Z","published":"2024-07-12T10:54:07Z","title":"Encoding arbitrary Ising Hamiltonians on Spatial Photonic Ising Machines","summary":"  Photonic Ising Machines constitute an emergent new paradigm of computation,\ngeared towards tackling combinatorial optimization problems that can be reduced\nto the problem of finding the ground state of an Ising model. Spatial Photonic\nIsing Machines have proven to be advantageous for simulating fully connected\nlarge-scale spin systems. However, fine control of a general interaction matrix\n$J$ has so far only been accomplished through eigenvalue decomposition methods\nthat either limit the scalability or increase the execution time of the\noptimization process. We introduce and experimentally validate a SPIM instance\nthat enables direct control over the full interaction matrix, enabling the\nencoding of Ising Hamiltonians with arbitrary couplings and connectivity. We\ndemonstrate the conformity of the experimentally measured Ising energy with the\ntheoretically expected values and then proceed to solve both the unweighted and\nweighted graph partitioning problems, showcasing a systematic convergence to an\noptimal solution via simulated annealing. Our approach greatly expands the\napplicability of SPIMs for real-world applications without sacrificing any of\nthe inherent advantages of the system, and paves the way to encoding the full\nrange of NP problems that are known to be equivalent to Ising models, on SPIM\ndevices.\n","authors":["Jason Sakellariou","Alexis Askitopoulos","Georgios Pastras","Symeon I. Tsintzos"],"pdf_url":"https://arxiv.org/pdf/2407.09161v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.02254v1","updated":"2024-10-03T06:47:16Z","published":"2024-10-03T06:47:16Z","title":"MTDNS: Moving Target Defense for Resilient DNS Infrastructure","summary":"  One of the most critical components of the Internet that an attacker could\nexploit is the DNS (Domain Name System) protocol and infrastructure.\nResearchers have been constantly developing methods to detect and defend\nagainst the attacks against DNS, specifically DNS flooding attacks. However,\nmost solutions discard packets for defensive approaches, which can cause\nlegitimate packets to be dropped, making them highly dependable on detection\nstrategies. In this paper, we propose MTDNS, a resilient MTD-based approach\nthat employs Moving Target Defense techniques through Software Defined\nNetworking (SDN) switches to redirect traffic to alternate DNS servers that are\ndynamically created and run under the Network Function Virtualization (NFV)\nframework. The proposed approach is implemented in a testbed environment by\nrunning our DNS servers as separate Virtual Network Functions, NFV Manager, SDN\nswitches, and an SDN Controller. The experimental result shows that the MTDNS\napproach achieves a much higher success rate in resolving DNS queries and\nsignificantly reduces average latency even if there is a DNS flooding attack.\n","authors":["Abdullah Aydeger","Pei Zhou","Sanzida Hoque","Marco Carvalho","Engin Zeydan"],"pdf_url":"https://arxiv.org/pdf/2410.02254v1.pdf","comment":"6 pages, Accepted for publication at IEEE CCNC 2025"}]},"2024-10-02T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2409.19464v2","updated":"2024-10-02T20:10:19Z","published":"2024-09-28T21:54:47Z","title":"Blown up by an equilateral: Poncelet triangles about the incircle and\n  their degeneracies","summary":"  We tour several harmonious Euclidean properties of Poncelet triangles\ninscribed in an ellipse and circumscribing the incircle. We also show that a\nnumber of degenerate behaviors are triggered by the presence of an equilateral\ntriangle in the family.\n","authors":["Mark Helman","Ronaldo A. Garcia","Dan Reznik"],"pdf_url":"https://arxiv.org/pdf/2409.19464v2.pdf","comment":"40 pages, 33 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.01992v1","updated":"2024-10-02T19:54:25Z","published":"2024-10-02T19:54:25Z","title":"General Conversion between ANCF and B-spline Surfaces","summary":"  In this paper, general conversion equations are derived between Absolute\nNodal Coordinates Formulation (ANCF) finite surface elements and B-spline\nsurfaces, an extension of our previous work on the conversion between ANCF\ncable elements and B-spline curves. The derivation of the conversion equations\nis the discovery of the geometric invariance of the ANCF displacement field\nbefore and after the conversion. Our study starts from proposing the conversion\nequation between ANCF finite surface elements and Bezier surfaces which are the\nspecial cases of B-spline surfaces, followed by establishing a general\nconversion equation between ANCF finite surface elements and Bezier surfaces.\nThis general conversion equation has functionalities (1) to realize the\none-step direct conversion between ANCF and Bezier surfaces (2) to convert ANCF\nfinite surface elements directly to Bezier surfaces provided the ANCF nodal\ncoordinates are not independent. The direct conversion from a conditional ANCF\nfinite surface to Bezier surfaces enhances the efficiency and ability to\ncontrol and store data in computers during the conversion process.\n  The conversion between ANCF finite surface elements and B-spline surfaces is\nderived from a conversion of B-spline surfaces to a more general conversion of\nB-spline surfaces. B-spline basis functions are utilized in the non-recursive\nform, from which a more efficient conversion equation is obtained compared with\nan intuitive conversion semantics where one converts firstly B-spline surfaces\nto composite Bezier surfaces by inserting knot and converts to ANCF finite\nsurface elements afterward. The obtained conversion equations between ANCF and\nB-spline surfaces realize the one-step direct conversion.\n","authors":["Randi Wang","Peng Lan","Zuqing Yu","Nianli Lu"],"pdf_url":"https://arxiv.org/pdf/2410.01992v1.pdf","comment":"This paper was originally written in 2015 and has not been updated\n  since then. It is being uploaded for archival purposes"},{"id":"http://arxiv.org/abs/2410.01918v1","updated":"2024-10-02T18:17:14Z","published":"2024-10-02T18:17:14Z","title":"Influence of control polygon on the generalization of the conversion\n  between ANCF and B-spline surfaces","summary":"  The aim of this study is to establish a general transformation matrix between\nB-spline surfaces and ANCF surface elements. This study is a further study of\nthe conversion between the ANCF and B-spline surfaces. In this paper, a general\ntransformation matrix between the Bezier surfaces and ANCF surface element is\nestablished. This general transformation matrix essentially describes the\nlinear relationship between ANCF and Bezier surfaces. Moreover, the general\ntransformation matrix can help to improve the efficiency of the process to\ntransfer the distorted configuration in the CAA back to the CAD, an urgent\nrequirement in engineering practice. In addition, a special Bezier surface\ncontrol polygon is given in this study. The Bezier surface described with this\ncontrol polygon can be converted to an ANCF surface element with fewer d.o.f..\nAnd the converted ANCF surface element with 36 d.o.f. was once addressed by\nDufva and Shabana. So the special control polygon can be regarded as the\ngeometric condition in conversion to an ANCF surface element with 36 d.o.f.\nBased on the fact that a B-spline surface can be seen as a set of Bezier\nsurfaces connected together, the method to establish a general transformation\nmatrix between the ANCF and lower-order B-spline surfaces is given. Specially,\nthe general transformation is not in a recursive form, but in a simplified\nform.\n","authors":["Peng Lan","Randi Wang","Zuqing Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01918v1.pdf","comment":"This paper draft was accepted for the 2014 ASME IDETC-CIE conference\n  but was not presented due to a delayed visa process. Therefore, I have\n  uploaded it to arXiv for archival purposes"},{"id":"http://arxiv.org/abs/2410.01802v1","updated":"2024-10-02T17:57:38Z","published":"2024-10-02T17:57:38Z","title":"PROXI: Challenging the GNNs for Link Prediction","summary":"  Over the past decade, Graph Neural Networks (GNNs) have transformed graph\nrepresentation learning. In the widely adopted message-passing GNN framework,\nnodes refine their representations by aggregating information from neighboring\nnodes iteratively. While GNNs excel in various domains, recent theoretical\nstudies have raised concerns about their capabilities. GNNs aim to address\nvarious graph-related tasks by utilizing such node representations, however,\nthis one-size-fits-all approach proves suboptimal for diverse tasks.\n  Motivated by these observations, we conduct empirical tests to compare the\nperformance of current GNN models with more conventional and direct methods in\nlink prediction tasks. Introducing our model, PROXI, which leverages proximity\ninformation of node pairs in both graph and attribute spaces, we find that\nstandard machine learning (ML) models perform competitively, even outperforming\ncutting-edge GNN models when applied to these proximity metrics derived from\nnode neighborhoods and attributes. This holds true across both homophilic and\nheterophilic networks, as well as small and large benchmark datasets, including\nthose from the Open Graph Benchmark (OGB). Moreover, we show that augmenting\ntraditional GNNs with PROXI significantly boosts their link prediction\nperformance. Our empirical findings corroborate the previously mentioned\ntheoretical observations and imply that there exists ample room for enhancement\nin current GNN models to reach their potential.\n","authors":["Astrit Tola","Jack Myrick","Baris Coskunuzer"],"pdf_url":"https://arxiv.org/pdf/2410.01802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04507v2","updated":"2024-10-02T07:52:37Z","published":"2024-09-06T16:32:46Z","title":"3D Data Long-Term Preservation in Cultural Heritage","summary":"  The report explores the challenges and strategies for preserving 3D digital\ndata in cultural heritage. It discusses the issue of technological\nobsolescence, emphasising the need for ustainable storage solutions and ongoing\ndata management strategies. Key topics include understanding technological\nobsolescence, the lifecycle of digital content, digital continuity, data\nmanagement plans (DMP), FAIR principles, and the use of public repositories.\nThe report also covers the importance of metadata in long-term digital\npreservation, including types of metadata and strategies for building valuable\nmetadata. It examines the evolving standards and interoperability in 3D format\npreservation and the importance of managing metadata and paradata. The document\nprovides a comprehensive overview of the challenges and solutions for\npreserving 3D cultural heritage data in the long term.\n","authors":["Nicola Amico","Achille Felicetti"],"pdf_url":"https://arxiv.org/pdf/2409.04507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01149v1","updated":"2024-10-02T01:00:30Z","published":"2024-10-02T01:00:30Z","title":"Recovering Manifold Structure Using Ollivier-Ricci Curvature","summary":"  We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest\nneighbor graphs using a criterion based on Ollivier-Ricci curvature and\nestimated metric distortion. Our motivation comes from manifold learning: we\nshow that when the data generating the nearest-neighbor graph consists of noisy\nsamples from a low-dimensional manifold, edges that shortcut through the\nambient space have more negative Ollivier-Ricci curvature than edges that lie\nalong the data manifold. We demonstrate that our method outperforms alternative\npruning methods and that it significantly improves performance on many\ndownstream geometric data analysis tasks that use nearest neighbor graphs as\ninput. Specifically, we evaluate on manifold learning, persistent homology,\ndimension estimation, and others. We also show that ORC-ManL can be used to\nimprove clustering and manifold learning of single-cell RNA sequencing data.\nFinally, we provide empirical convergence experiments that support our\ntheoretical findings.\n","authors":["Tristan Luca Saidi","Abigail Hickok","Andrew J. Blumberg"],"pdf_url":"https://arxiv.org/pdf/2410.01149v1.pdf","comment":null}],"Emerging Technologies":[{"id":"http://arxiv.org/abs/2410.02088v1","updated":"2024-10-02T23:21:50Z","published":"2024-10-02T23:21:50Z","title":"Universal Logical Quantum Photonic Neural Network Processor via\n  Cavity-Assisted Interactions","summary":"  Encoding quantum information within bosonic modes offers a promising\ndirection for hardware-efficient and fault-tolerant quantum information\nprocessing. However, achieving high-fidelity universal control over the bosonic\ndegree of freedom using native photonic hardware remains a challenge. Here, we\npropose an architecture to prepare and perform logical quantum operations on\narbitrary multimode multi-photon states using a quantum photonic neural\nnetwork. Central to our approach is the optical nonlinearity, which is realized\nthrough strong light-matter interaction with a three-level Lambda atomic\nsystem. The dynamics of this interaction are confined to the single-mode\nsubspace, enabling the construction of high-fidelity quantum gates. This\nnonlinearity functions as a photon-number selective phase gate, which\nfacilitates the construction of a universal gate set and serves as the\nelement-wise activation function in our neural network architecture. Through\nnumerical simulations, we demonstrate the versatility of our approach by\nexecuting tasks that are key to logical quantum information processing. The\nnetwork is able to deterministically prepare a wide array of multimode\nmulti-photon states, including essential resource states. We also show that the\narchitecture is capable of encoding and performing logical operations on\nbosonic error-correcting codes. Additionally, by adapting components of our\narchitecture, error-correcting circuits can be built to protect bosonic codes.\nThe proposed architecture paves the way for near-term quantum photonic\nprocessors that enable error-corrected quantum computation, and can be achieved\nusing present-day integrated photonic hardware.\n","authors":["Jasvith Raj Basani","Murphy Yuezhen Niu","Edo Waks"],"pdf_url":"https://arxiv.org/pdf/2410.02088v1.pdf","comment":"21 pages including supplement, 12 figures total"},{"id":"http://arxiv.org/abs/2410.01982v1","updated":"2024-10-02T19:41:45Z","published":"2024-10-02T19:41:45Z","title":"Decentralized Collaborative Inertial Tracking","summary":"  Although people spend most of their time indoors, outdoor tracking systems,\nsuch as the Global Positioning System (GPS), are predominantly used for\nlocation-based services. These systems are accurate outdoors, easy to use, and\noperate autonomously on each mobile device. In contrast, Indoor Tracking\nSystems~(ITS) lack standardization and are often difficult to operate because\nthey require costly infrastructure. In this paper, we propose an indoor\ntracking algorithm that uses collected data from inertial sensors embedded in\nmost mobile devices. In this setting, mobile devices autonomously estimate\ntheir location, hence removing the burden of deploying and maintaining complex\nand scattered hardware infrastructure. In addition, these devices collaborate\nby anonymously exchanging data with other nearby devices, using wireless\ncommunication, such as Bluetooth, to correct errors in their location\nestimates. Our collaborative algorithm relies on low-complexity geometry\noperations and can be deployed on any recent mobile device with\ncommercial-grade sensors. We evaluate our solution on real-life data collected\nby different devices. Experimentation with 16 simultaneously moving and\ncollaborating devices shows an average accuracy improvement of 44% compared to\nthe standalone Pedestrian Dead Reckoning algorithm.\n","authors":["Alpha Diallo","Benoit Garbinato"],"pdf_url":"https://arxiv.org/pdf/2410.01982v1.pdf","comment":"ACCEPTED FOR PUBLICATION AND PRESENTED IN EAI MOBIQUITOUS 2023"},{"id":"http://arxiv.org/abs/2410.01793v1","updated":"2024-10-02T17:51:58Z","published":"2024-10-02T17:51:58Z","title":"Thermodynamic Bayesian Inference","summary":"  A fully Bayesian treatment of complicated predictive models (such as deep\nneural networks) would enable rigorous uncertainty quantification and the\nautomation of higher-level tasks including model selection. However, the\nintractability of sampling Bayesian posteriors over many parameters inhibits\nthe use of Bayesian methods where they are most needed. Thermodynamic computing\nhas emerged as a paradigm for accelerating operations used in machine learning,\nsuch as matrix inversion, and is based on the mapping of Langevin equations to\nthe dynamics of noisy physical systems. Hence, it is natural to consider the\nimplementation of Langevin sampling algorithms on thermodynamic devices. In\nthis work we propose electronic analog devices that sample from Bayesian\nposteriors by realizing Langevin dynamics physically. Circuit designs are given\nfor sampling the posterior of a Gaussian-Gaussian model and for Bayesian\nlogistic regression, and are validated by simulations. It is shown, under\nreasonable assumptions, that the Bayesian posteriors for these models can be\nsampled in time scaling with $\\ln(d)$, where $d$ is dimension. For the\nGaussian-Gaussian model, the energy cost is shown to scale with $ d \\ln(d)$.\nThese results highlight the potential for fast, energy-efficient Bayesian\ninference using thermodynamic computing.\n","authors":["Maxwell Aifer","Samuel Duffield","Kaelan Donatella","Denis Melanson","Phoebe Klett","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2410.01793v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.01791v1","updated":"2024-10-02T17:49:07Z","published":"2024-10-02T17:49:07Z","title":"DreamGarden: A Designer Assistant for Growing Games from a Single Prompt","summary":"  Coding assistants are increasingly leveraged in game design, both generating\ncode and making high-level plans. To what degree can these tools align with\ndeveloper workflows, and what new modes of human-computer interaction can\nemerge from their use? We present DreamGarden, an AI system capable of\nassisting with the development of diverse game environments in Unreal Engine.\nAt the core of our method is an LLM-driven planner, capable of breaking down a\nsingle, high-level prompt -- a dream, memory, or imagined scenario provided by\na human user -- into a hierarchical action plan, which is then distributed\nacross specialized submodules facilitating concrete implementation. This system\nis presented to the user as a garden of plans and actions, both growing\nindependently and responding to user intervention via seed prompts, pruning,\nand feedback. Through a user study, we explore design implications of this\nsystem, charting courses for future work in semi-autonomous assistants and\nopen-ended simulation design.\n","authors":["Sam Earle","Samyak Parajuli","Andrzej Banburski-Fahey"],"pdf_url":"https://arxiv.org/pdf/2410.01791v1.pdf","comment":"21 pages + appendix, 11 figures"},{"id":"http://arxiv.org/abs/2410.00274v2","updated":"2024-10-02T17:34:41Z","published":"2024-09-30T23:02:51Z","title":"Social Conjuring: Multi-User Runtime Collaboration with AI in Building\n  Virtual 3D Worlds","summary":"  Generative artificial intelligence has shown promise in prompting virtual\nworlds into existence, yet little attention has been given to understanding how\nthis process unfolds as social interaction. We present Social Conjurer, a\nframework for AI-augmented dynamic 3D scene co-creation, where multiple users\ncollaboratively build and modify virtual worlds in real-time. Through an\nexpanded set of interactions, including social and tool-based engagements as\nwell as spatial reasoning, our framework facilitates the creation of rich,\ndiverse virtual environments. Findings from a preliminary user study (N=12)\nprovide insight into the user experience of this approach, how social contexts\nshape the prompting of spatial environments, and perspective on social\napplications of prompt-based 3D co-creation. In addition to highlighting the\npotential of AI-supported multi-user world creation and offering new pathways\nfor AI-augmented creative processes in VR, this article presents a set of\nimplications for designing human-centered interfaces that incorporate AI models\ninto 3D content generation.\n","authors":["Amina Kobenova","Cyan DeVeaux","Samyak Parajuli","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2410.00274v2.pdf","comment":"27 pages + Appendix, 16 figures; fixed some minor UTF-8 encoding\n  issues in arXiv compilation"},{"id":"http://arxiv.org/abs/2410.01775v1","updated":"2024-10-02T17:30:23Z","published":"2024-10-02T17:30:23Z","title":"Optimization of a Quantum Subset Sum Oracle","summary":"  We investigate the implementation of an oracle for the Subset Sum problem for\nquantum search using Grover's algorithm. Our work concerns reducing the number\nof qubits, gates, and multi-controlled gates required by the oracle. We\ndescribe the compilation of a Subset Sum instance into a quantum oracle, using\na Python library we developed for Qiskit and have published in GitHub. We then\npresent techniques to conserve qubits and gates along with experiments showing\ntheir effectiveness on random instances of Subset Sum. These techniques include\nmoving from fixed to varying-width arithmetic, using partial sums of a set's\nintegers to determine specific integer widths, and sorting the set to obtain\nprovably the most efficient partial sums. We present a new method for computing\nbit-string comparisons that avoids arbitrarily large multiple-control gates,\nand we introduce a simple modification to the oracle that allows for\napproximate solutions to the Subset Sum problem via Grover search.\n","authors":["Angelo Benoit","Sam Schwartz","Ron K. Cytron"],"pdf_url":"https://arxiv.org/pdf/2410.01775v1.pdf","comment":"9 pages, 8 figures"}],"Other Computer Science":[{"id":"http://arxiv.org/abs/2410.03763v1","updated":"2024-10-02T14:20:40Z","published":"2024-10-02T14:20:40Z","title":"Electrification of Transportation: A Hybrid Benders/SDDP Algorithm for\n  Optimal Charging Station Trading","summary":"  This paper examines the electrification of transportation as a response to\nenvironmental challenges caused by fossil fuels, exploring the potential of\nbattery electric vehicles and hydrogen fuel cell vehicles as alternative\nsolutions. However, a significant barrier to their widespread adoption is the\nlimited availability of charging infrastructure. Therefore, this study proposes\nthe development of comprehensive charging stations capable of accommodating\nboth battery and hydrogen vehicles to address this challenge. The energy is\npurchased from the day-ahead and intraday auction-based electricity markets,\nwhere the electricity price is subject to uncertainty. Therefore, a two-stage\nstochastic programming model is formulated while the price scenarios are\ngenerated utilizing a k-means clustering algorithm. Given the complexity of the\nproposed model, an efficient solution approach is developed through the\nhybridization of the Benders decomposition algorithm and stochastic dual\ndynamic programming. In the Benders master problem, day-ahead bidding variables\nare determined, whereas the Benders sub-problem addresses intraday bidding and\ncharging station scheduling variables, employing stochastic dual dynamic\nprogramming to tackle its intractability. Additionally, we transform the mixed\ninteger linear program model of the second stage problem into a linear program,\nconfirming its validity through KKT conditions. Our model provides practical\ninsights for making informed decisions in electricity markets based on\nsequential auctions. While the bidding curves submitted to the day-ahead market\nremain unaffected by scenarios, those submitted to the intra-day market show\ndependence on fluctuations in day-ahead market prices.\n","authors":["Farnaz Sohrabi","Mohammad Rohaninejad","J√∫lius Bem≈°","Zdenƒõk Hanz√°lek"],"pdf_url":"https://arxiv.org/pdf/2410.03763v1.pdf","comment":null}]},"2024-10-01T00:00:00Z":{"Computational Geometry":[{"id":"http://arxiv.org/abs/2406.03756v2","updated":"2024-10-01T22:06:40Z","published":"2024-06-06T05:41:30Z","title":"High-Order Continuous Geometrical Validity","summary":"  We propose a conservative algorithm to test the geometrical validity of\nsimplicial (triangles, tetrahedra), tensor product (quadrilaterals, hexahedra),\nand mixed (prisms) elements of arbitrary polynomial order as they deform over a\npiecewise-linear trajectory.\n  Our algorithm uses a combination of adaptive B\\'ezier refinement and\nbisection search to determine if, when, and where the Jacobian determinant of\nan element's polynomial geometric map becomes negative in the transition from\none configuration to another.\n  Unlike previous approaches, our method preserves its properties also when\nimplemented using floating point arithmetic: This feature comes at a small\nadditional runtime cost compared to existing inexact methods, making it a\ndrop-in replacement for current validity tests, while providing superior\nrobustness and generality.\n  To prove the practical effectiveness of our algorithm, we demonstrate its use\nin a high-order Incremental Potential Contact (IPC) elastodynamic simulator,\nand we experimentally show that it prevents invalid, simulation-breaking\nconfigurations that would otherwise occur using inexact methods, without the\nneed for manual parameter tuning.\n","authors":["Federico Sichetti","Zizhou Huang","Marco Attene","Denis Zorin","Enrico Puppo","Daniele Panozzo"],"pdf_url":"https://arxiv.org/pdf/2406.03756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00996v1","updated":"2024-10-01T18:36:54Z","published":"2024-10-01T18:36:54Z","title":"Approximating Klee's Measure Problem and a Lower Bound for Union Volume\n  Estimation","summary":"  Union volume estimation is a classical algorithmic problem. Given a family of\nobjects $O_1,\\ldots,O_n \\subseteq \\mathbb{R}^d$, we want to approximate the\nvolume of their union. In the special case where all objects are boxes (also\nknown as hyperrectangles) this is known as Klee's measure problem. The\nstate-of-the-art algorithm [Karp, Luby, Madras '89] for union volume estimation\nand Klee's measure problem in constant dimension $d$ computes a\n$(1+\\varepsilon)$-approximation with constant success probability by using a\ntotal of $O(n/\\varepsilon^2)$ queries of the form (i) ask for the volume of\n$O_i$, (ii) sample a point uniformly at random from $O_i$, and (iii) query\nwhether a given point is contained in $O_i$.\n  We show that if one can only interact with the objects via the aforementioned\nthree queries, the query complexity of [Karp, Luby, Madras '89] is indeed\noptimal, i.e., $\\Omega(n/\\varepsilon^2)$ queries are necessary. Our lower bound\nalready holds for estimating the union of equiponderous axis-aligned polygons\nin $\\mathbb{R}^2$, and even if the algorithm is allowed to inspect the\ncoordinates of the points sampled from the polygons, and still holds when a\ncontainment query can ask containment of an arbitrary (not sampled) point.\n  Guided by the insights of the lower bound, we provide a more efficient\napproximation algorithm for Klee's measure problem improving the\n$O(n/\\varepsilon^2)$ time to $O((n+\\frac{1}{\\varepsilon^2}) \\cdot\n\\log^{O(d)}n)$. We achieve this improvement by exploiting the geometry of\nKlee's measure problem in various ways: (1) Since we have access to the boxes'\ncoordinates, we can split the boxes into classes of boxes of similar shape. (2)\nWithin each class, we show how to sample from the union of all boxes, by using\northogonal range searching. And (3) we exploit that boxes of different classes\nhave small intersection, for most pairs of classes.\n","authors":["Karl Bringmann","Kasper Green Larsen","Andr√© Nusser","Eva Rotenberg","Yanheng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.00996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11166v2","updated":"2024-10-01T16:07:25Z","published":"2024-09-17T13:18:53Z","title":"New Lower Bound and Algorithms for Online Geometric Hitting Set Problem","summary":"  The hitting set problem is one of the fundamental problems in combinatorial\noptimization and is well-studied in offline setup. We consider the online\nhitting set problem, where only the set of points is known in advance, and\nobjects are introduced one by one. Our objective is to maintain a minimum-sized\nhitting set by making irrevocable decisions. Here, we present the study of two\nvariants of the online hitting set problem depending on the point set. In the\nfirst variant, we consider the point set to be the entire $\\mathbb{Z}^d$, while\nin the second variant, we consider the point set to be a finite subset of\n$\\mathbb{R}^2$.\n  If you use points in $\\mathbb{Z}^d$ to hit homothetic hypercubes in\n$\\mathbb{R}^d$ with side lengths in $[1,M]$, we show that the competitive ratio\nof any algorithm is $\\Omega(d\\log M)$, whether it is deterministic or random.\nThis improves the recently known deterministic lower bound of $\\Omega(\\log M)$\nby a factor of $d$. Then, we present an almost tight randomized algorithm with\na competitive ratio $O(d^2\\log M)$ that significantly improves the best-known\ncompetitive ratio of $25^d\\log M$. Next, we propose a simple deterministic\n${\\lfloor\\frac{2}{\\alpha}+2\\rfloor^d}(\\lfloor\\log_{2}M\\rfloor+1)$ competitive\nalgorithm to hit similarly sized {$\\alpha$-fat objects} in $\\mathbb{R}^d$\nhaving diameters in the range $[1, M]$ using points in $\\mathbb{Z}^d$. This\nimproves the current best-known upper bound by a factor of at least $5^d$.\n  Finally, we consider the hitting set problem when the point set consists of\n$n$ points in $\\mathbb{R}^2$, and the objects are homothetic regular $k$-gons\nhaving diameter in the range $[1, M]$. We present an $O(\\log n\\log M)$\ncompetitive randomized algorithm for that. Whereas no result was known even for\nsquares. In particular, our results answer some of the open questions raised by\nKhan et al. (SoCG'23) and Alefkhani et al. (WAOA'23).\n","authors":["Minati De","Ratnadip Mandal","Satyam Singh"],"pdf_url":"https://arxiv.org/pdf/2409.11166v2.pdf","comment":"30 pages and 10 figures. arXiv admin note: text overlap with\n  arXiv:2304.06780"},{"id":"http://arxiv.org/abs/2406.09537v2","updated":"2024-10-01T16:05:36Z","published":"2024-06-13T18:50:41Z","title":"Analyzing Multifiltering Functions Using Multiparameter Discrete Morse\n  Theory","summary":"  A multiparameter filtration, or a multifiltration, may in many cases be seen\nas the collection of sublevel sets of a vector function, which we call a\nmultifiltering function. The main objective of this paper is to obtain a better\nunderstanding of such functions through multiparameter discrete Morse (MDM)\ntheory, which is an extension of Morse-Forman theory to vector-valued\nfunctions. Notably, we prove algorithmically that any multifiltering function\ndefined on a simplicial complex can always be approximated by a compatible MDM\nfunction. Moreover, we define the Pareto set of a discrete multifiltering\nfunction and show that the concept links directly to that of critical simplices\nof a MDM function. Finally, we experiment with these notions using triangular\nmeshes.\n","authors":["Guillaume Brouillette"],"pdf_url":"https://arxiv.org/pdf/2406.09537v2.pdf","comment":"44 pages, 18 figures"}]},"2024-10-05T00:00:00Z":{"Other Computer Science":[{"id":"http://arxiv.org/abs/2410.04149v1","updated":"2024-10-05T13:17:37Z","published":"2024-10-05T13:17:37Z","title":"Mov-Avg: Codeless time series analysis using moving averages","summary":"  This paper introduces Mov-Avg, the Python software package for time series\nanalysis that requires little computer programming experience from the user.\nThe package allows the identification of trends, patterns, and the prediction\nof future events based on data collected over time. In this regard, the Mov-Avg\nimplementation provides three indicators to apply, namely: Simple Moving\nAverage, Weighted Moving Average and Exponential Moving Average. Due to its\ngeneric design, the Mov-Avg software package can be used in any field where the\napplication of moving averages is valid. In general, the Mov-Avg library for\ntime series analysis contributes to a better understanding of data-driven\nprocesses over time by taking advantage of moving averages in any way adapted\nto the research context.\n","authors":["Pawe≈Ç Weichbroth","Jakub Buczkowski"],"pdf_url":"https://arxiv.org/pdf/2410.04149v1.pdf","comment":"13 pages, 2 figures, 41 references"}]}}